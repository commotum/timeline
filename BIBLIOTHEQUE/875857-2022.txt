                                                  TheThirty-SixthAAAIConferenceonArtificialIntelligence(AAAI-22)
                                 JFB:Jacobian-Free Backpropagation for Implicit Networks
                                       *1                        *2               3                         4                     4                3
                 SamyWuFung, HowardHeaton, QiuweiLi, DanielMcKenzie, StanleyOsher, WotaoYin
                                        1 Department of Applied Mathematics and Statistics, Colorado School of Mines
                                                                  2 Typal Research, Typal LLC
                                                             3 Alibaba Group (US), Damo Academy
                                              4 Department of Mathematics, University of California, Los Angeles
                              swufung@mines.edu, research@typal.llc, li.qiuwei@alibaba-inc.com, mckenzie@math.ucla.edu
                                          Abstract                                          input data    latent variable   output inference
                  Apromising trend in deep learning replaces traditional feed-                          QŒò               SŒò        y
                  forward networks with implicit networks. Unlike traditional                    d                u
                  networks, implicit networks solve a Ô¨Åxed point equation to
                  compute inferences. Solving for the Ô¨Åxed point varies in                         QŒò                 S
                  complexity, depending on provided data and an error tol-                                              Œò
                  erance. Importantly, implicit networks may be trained with                           RŒò
                  Ô¨Åxedmemorycostsinstarkcontrasttofeedforwardnetworks,                             uk            u?      latent variable
                  whosememoryrequirementsscalelinearly with depth. How-                                            d
                  ever, there is no free lunch ‚Äî backpropagation through im-                                             feedforward network
                  plicit networks often requires solving a costly Jacobian-based                loop until               implicit network
                  equation arising from the implicit function theorem. We pro-                convergence
                  pose Jacobian-Free Backpropagation (JFB), a Ô¨Åxed-memory
                  approach that circumvents the need to solve Jacobian-based          Figure 1: Feedforward networks act by computing SŒò‚ó¶QŒò.
                  equations. JFB makes implicit networks faster to train and          Implicit networks add a Ô¨Åxed point condition using RŒò.
                  signiÔ¨Åcantly easier to implement, without sacriÔ¨Åcing test ac-       WhenRŒòiscontractive (more generally: averaged) repeat-
                  curacy. Our experiments show implicit networks trained with         edly applying R     to update a latent variable uk converges
                  JFBarecompetitivewithfeedforwardnetworksandpriorim-                                  Œò
                                                                                      to a Ô¨Åxed point u? = RŒò(u?;QŒò(d)).
                  plicit networks given the same number of parameters.
                                      Introduction                                    which is illustrated by the red arrows in Figure 1. One can
                                                                                      allow for computation in the latent space U by introducing a
               Anewdirection has emerged from explicit to implicit neu-               self-map RŒò(¬∑;QŒò(d)) and the iteration
               ral networks (Winston and Kolter 2020; Bai, Kolter, and                                  uk+1 = R (uk;Q (d)).                      (2)
               Koltun2019;Bai,Koltun,andKolter2020;Chenetal.2018;                                                  Œò        Œò
               Ghaoui et al. 2019; Dupont, Doucet, and Teh 2019; Jeon,                Iterating k times may be viewed as a weight-tied, input-
               Lee,andChoi2021;Zhangetal.2020;Lawrenceetal.2020;                      injected network, where each feedforward step applies RŒò
               RevayandManchester2020;Looketal.2020;Gould,Hart-                       (Bai, Kolter, and Koltun 2019). As k ‚Üí ‚àû, i.e. the la-
               ley, and Campbell 2019). In the standard feedforward set-              tent space portion becomes deeper, the limit of (2) yields a
               ting, a network prescribes a series of computations that map           Ô¨Åxed point equation. Implicit networks capture this ‚ÄúinÔ¨Ånite
               input data d to an inference y. Networks can also explic-              depth‚Äù behaviour by using RŒò(¬∑ ;QŒò(d)) to deÔ¨Åne a Ô¨Åxed
               itly leverage the assumption that high dimensional signals             point condition rather than an explicit computation:
               typically admit low dimensional representations in some la-
               tent space (Van der Maaten and Hinton 2008; Osher, Shi,                   N (d) , S (u?) where u? = R (u?,Q (d)),                  (3)
                                                                                           Œò          Œò d               d      Œò d Œò
                                    ¬¥
               and Zhu 2017; Peyre 2009; Elad, Figueiredo, and Ma 2010;               as shown by blue in Figure 1. Special cases of the network
               Udell and Townsend 2019). This may be done by designing                in (3) recover architectures introduced in prior works:
               the network to Ô¨Årst map data to a latent space via a mapping
               QŒòandthenapply a second mapping SŒò to map the latent                   B Taking SŒò to be the identity recovers the well-known
               variable to the inference. Thus, a traditional feedforward EŒò             DeepEquilibriumModel(DEQ)(Bai,Kolter,andKoltun
               maytakethecompositional form                                              2019; Bai, Koltun, and Kolter 2020).
                                                                                      B ChoosingS astheidentity, Q tobeanafÔ¨Ånemapand
                                   E (d) = S (Q (d)),                      (1)                       Œò                    Œò
                                    Œò          Œò   Œò                                     R (u,Q (d)) = œÉ(Wu+Q (d))yieldsMonotoneOp-
                                                                                           Œò       Œò                    Œò
                  *These authors contributed equally.                                    erator Networks(WinstonandKolter2020)aslongasW
               Copyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial            and œÉ satisfy additional conditions. Allowing S        to be
                                                                                                                                             Œò
               Intelligence (www.aaai.org). All rights reserved.                         linear yields the model proposed in (Ghaoui et al. 2019).
                                                                               6648
              Three immediate questions arise from (3):                         network. However, implicit networks do not need to store
              I Is the deÔ¨Ånition in (3) well-posed?                             intermediate quantities of the forward pass for backpropaga-
              I HowisN (d)evaluated?                                            tion. Consequently, implicit networks are trained using con-
                           Œò                                                    stant memory costs with respect to depth ‚Äì relieving a major
              I HowaretheweightsŒòofNŒò updatedduringtraining?                    bottleneck of training deep networks.
              Since the Ô¨Årst two points are well-established (Winston and       Noloss of expressiveness    Implicit networks as deÔ¨Åned in
              Kolter 2020; Bai, Kolter, and Koltun 2019), we brieÔ¨Çy re-         (3) are at least as expressive as feedforward networks. This
              view these in Section and focus on the third point. Us-           can easily be observed by setting R   to simply return Q ;
              ing gradient-based methods for training requires comput-                                              Œò                    Œò
              ing dNŒòdŒò, and in particular, du?dŒò. Hitherto, previ-           in this case, the implicit NŒò reduces to the feedforward EŒò
                                                 d                             in (1). More interestingly, the class of implicit networks
              ous works computed du? dŒò by solving a Jacobian-based
                                      d                                         in which SŒò and QŒò are constrained to be afÔ¨Åne maps
              equation (see Section ). Solving this linear system is com-       contains all feedforward networks, and is thus at least as
              putationally expensive and prone to instability, particularly     expressive (Ghaoui et al. 2019), (Bai, Kolter, and Koltun
              when the dimension of the latent space is large and/or in-        2019, Theorem 3). Universal approximation properties
              cludes certain structures (e.g. batch normalization and/or        of implicit networks then follow immediately from such
              dropout) (Bai, Kolter, and Koltun 2019; Bai, Koltun, and          properties of conventional deep neural models (e.g. see
              Kolter 2020).                                                        ¬¥
                Our primary contribution is a new and simple Jacobian-          (Csaji et al. 2001; Lu et al. 2017; Kidger and Lyons 2020)).
              Free Backpropagation (JFB) technique for training im-             Wealsomentionacouplelimitations of implicit networks.
              plicit networks that avoids any linear system solves. Instead,
              our scheme backpropagates by omitting the Jacobian term,          Architectural limitations    As discussed above, in theory
              resulting in a form of preconditioned gradient descent. JFB       given any feedforward network one may write down an im-
              yields much faster training of implicit networks and allows       plicit network yielding the same output (for all inputs). In
                                              1
              for a wider array of architectures .                              practice, evaluating the implicit network requires Ô¨Ånding a
                                                                                Ô¨Åxed point of RŒò. The Ô¨Åxed point Ô¨Ånding algorithm then
                            WhyImplicitNetworks?                                places constraints on RŒò (e.g. Assumption 0.1). Guaran-
              Below, we discuss several advantages of implicit networks         teeing the existence and computability of dNŒòdŒò places
              over explicit, feedforward networks.                              further constraints on RŒò. For example, if Jacobian-based
              Implicit networks for implicitly deÔ¨Åned outputs          In       backpropagation is used, RŒò cannot contain batch normal-
                                                                                ization (Bai, Kolter, and Koltun 2019).
              some applications, the desired network output is most aptly       Slowerinference     Oncetrained, inference with an implicit
              described implicitly as a Ô¨Åxed point, not via an explicit
              function. As a toy example, consider predicting the variable      network requires solving for a Ô¨Åxed point of RŒò. Finding
              y ‚àà Rgivend ‚àà [‚àí1/2,1/2]when(d,y)isknowntosatisfy                 this Ô¨Åxed point using an iterative algorithm requires evaluat-
                                                5                     (4)       ing RŒò repeatedly and, thus, is often slower than inference
                                      y = d+y .                                 with a feedforward network.
              Using y1 = 0 and the iteration                                              Implicit Network Formulation
                      y     =T(y ;d),d+y5, forallk ‚ààN,                (5)
                       k+1        k             k                               All terms presented in this section are provided in a general
              oneobtains yk ‚Üí y. In this setting, y is exactly (and implic-     context, which is later made concrete for each application.
              itly) characterized by y = T(y,d). On the other hand, an          Weincludeasubscript Œò on various terms to emphasize the
              explicit solution to (4) requires an inÔ¨Ånite series representa-   indicated mapping will ultimately be parameterized in terms
                                                               5                                  2
              tion, unlike the simple formula T(y,d) = d + y . See ap-          oftunableweights Œò.Atthehighestlevel,weareinterested
              pendix for further details. Thus, it can be simpler and more      in constructing a neural network NŒò : D ‚Üí Y that maps
                                                                                                 3
              appropriate to model a relationship implicitly. For example,      from a data space D to an inference space Y. The implicit
              in areas as diverse as game theory and inverse problems, the      portion of the network uses a latent space U, and data is
              output of interest may naturally be characterized as the Ô¨Åxed     mapped to this latent space by QŒò: D ‚Üí U. We deÔ¨Åne the
              point to an operator parameterized by the input data d. Since     network operator TŒò : U √ó D ‚Üí U by
              implicit networks Ô¨Ånd Ô¨Åxed points by design, they are well-                       T (u;d) , R (u,Q (d)).                  (6)
              suited to such problems as shown by recent works (Heaton                           Œò            Œò      Œò
              et al. 2021a,b; Gilton, Ongie, and Willett 2021).                 Provided input data d, our aim is to Ô¨Ånd the unique Ô¨Åxed
              ‚ÄúInÔ¨Ånite depth‚Äù with constant memory training           As        point u? of TŒò(¬∑ ;d) and then map u? to the inference space
              mentioned, solving for the Ô¨Åxed point of R (¬∑ ;Q (d)) is                 d                            d
                                                          Œò      Œò                 2
              analogous to a forward pass through an ‚ÄúinÔ¨Ånite depth‚Äù (in            Weusethe same subscript for all terms, noting each operator
              practice, very deep) weight-tied, input injected feedforward      typically depends on a portion of the weights.
                                                                                   3Each space is assumed to be a real-valued Ô¨Ånite dimensional
                 1All codes can be found on Github:                             Hilbert space (e.g. Rn) endowedwithaproducth¬∑,¬∑iandnormk¬∑k.
              github.com/howardheaton/jacobian free backprop                    It will be clear from context which space is being used.
                                                                          6649
              Y via a Ô¨Ånal mapping SŒò : U ‚Üí Y. This enables us to               schemewhencomputinguÀú(i.e.Algorithm1).Inthecontem-
              deÔ¨Åne an implicit network NŒò by                                   poraneous (Gilton, Ongie, and Willett 2021), it is reported
                     N (d) , S (u?) where u? = T (u?;d).              (7)       that using Ô¨Åxed point iteration in conjunction with Anderson
                       Œò         Œò d              d     Œò d                     acceleration Ô¨Ånds uÀú faster than both vanilla Ô¨Åxed point itera-
                                                                                tion and Broyden‚Äôs method. Combining JFB with Anderson
              Algorithm 1: Implicit Network with Fixed Point Iteration          accelerated Ô¨Åxed point iteration is a promising research di-
                                                                                rection we leave for future work.
               1: NŒò(d):                         CInputdatais d
               2:   u1 ‚ÜêuÀÜ                       CAssignlatent term             Other Implicit Formulations      Arelated implicit learning
               3:   while kuk ‚àíT (uk;d)k > ŒµCLooptilconverge                    formulation is the well-known neural ODE model (Chen
                                   Œò                                            et al. 2018; Dupont, Doucet, and Teh 2019; Ruthotto and
               4:    uk+1 ‚ÜêTŒò(uk;d)              CReÔ¨Ånelatentterm               Haber2021).NeuralODEsleverageknownconnectionsbe-
               5:    k ‚Üêk+1                      CIncrementcounter              tween deep residual models and discretizations of differ-
               6: return SŒò(uk)                  COutputestimate                ential equations (Haber and Ruthotto 2017; Weinan 2017;
                                                                                Ruthotto and Haber 2019; Chang et al. 2018; Finlay et al.
                Implementation considerations for TŒò are discussed be-          2020; Lu et al. 2018), and replace these discretizations by
              low. We also introduce assumptions on TŒò that yield sufÔ¨Å-         black-box ODE solvers in forward and backward passes.
              cient conditions to use the simple procedure in Algorithm         The implicit property of these models arise from their
              1 to approximate NŒò(d). In this algorithm, the latent vari-       method for computing gradients. Rather than backpropa-
              able initialization uÀÜ can be any Ô¨Åxed quantity (e.g. the zero    gate through each layer, backpropagation is instead done by
              vector). The inequality in Step 3 gives a Ô¨Åxed point residual     solving the adjoint equation (Jameson 1988) using a black-
              condition that measures convergence. Step 4 implements a          box ODE solver as well. This is analogous to solving the
              Ô¨Åxed point update. The estimate of the inference NŒò(d) is         Jacobian-based equation when performing backpropagation
              computedbyapplyingSŒò tothelatentvariableuk inStep6.               for implicit networks (see (13)) and allows the user to al-
              ThebluepathinFigure1visually summarizes Algorithm 1.              leviate the memory costs of backpropagation through deep
              Convergence     Finitely many loops in Steps 3 and 4 of Al-       neural models by solving the adjoint equation at additional
              gorithm 1 is guaranteed by a classic functional analysis re-      computationalcosts. A drawbackis that the adjoint equation
              sult (Banach1922).Thisapproachisusedbyseveralimplicit             mustbesolvedtohigh-accuracy;otherwise,adescentdirec-
              networks (Ghaoui et al. 2019; Winston and Kolter 2020;            tion is not necessarily guaranteed (Gholami, Keutzer, and
              Jeon, Lee, and Choi 2021). Below we present a variation           Biros 2019; Onken and Ruthotto 2020; Onken et al. 2021).
              of Banach‚Äôs result for our setting.                                                  Backpropagation
              Assumption 0.1. The mapping TŒò is L-Lipschitz with re-
              spect to its inputs (u,d), i.e. ,                                 We present a simple way to backpropagate with implicit
                                                                                networks, called Jacobian-free backprop (JFB). Traditional
                  kTŒò(u; d)‚àíTŒò(v; w)k ‚â§ Lk(u,d)‚àí(v,w)k,               (8)       backpropagation will not work effectively for implicit net-
              for all (u,d),(v,w) ‚àà U √óD. Holding d Ô¨Åxed, the operator          workssinceforwardpropagationduringtrainingcouldentail
              T (¬∑;d)isacontraction,i.e.thereexists Œ≥ ‚àà [0,1) such that         hundreds or thousands of iterations, requiring ever growing
               Œò                                                                memory to store computational graphs. On the other hand,
               kTŒò(u;d)‚àíTŒò(v;d)k ‚â§ Œ≥ku‚àívk, forallu,v ‚àà U. (9)                   implicit models maintain Ô¨Åxed memory costs by backprop-
              Remark0.1. TheL-LipschitzconditiononT isusedsince                 agating ‚Äúthrough the Ô¨Åxed point‚Äù and solving a Jacobian-
                                                           Œò                    based equation (at potentially substantial added computa-
              recentworksshowLipschitzcontinuitywithrespecttoinputs             tional costs). The key step to circumvent this Jacobian-based
              improves generalization (Sokolic et al. 2017; Gouk et al.
                                              ¬¥                                 equation with JFB is to tune weights by using a precondi-
              2021; Finlay et al. 2018) and adversarial robustness (Cisse       tioned gradient. Let ` : Y √ó Y ‚Üí R be a smooth loss func-
              et al. 2017; Anil, Lucas, and Grosse 2019).                       tion, denoted by `(x,y), and consider the training problem
              Theorem 0.1. (BANACH) For any u1 ‚àà U, if the sequence
              {uk}is generated via the update relation                                          minEd‚àºD`(yd,NŒò(d)),                  (11)
                          uk+1 = T (uk; d), for all k ‚àà N,           (10)                        Œò
                                    Œò                                           where we abusively write D to also mean a distribution. For
              and if Assumption 0.1 holds, then {uk} converges linearly         clarity of presentation, in the remainder of this section we
              to the unique Ô¨Åxed point u? of TŒò(¬∑;d).
                                        d                                       notationally suppress the dependencies on weights Œò by let-
              Alternative Approaches      In (Bai, Kolter, and Koltun           ting u? denote the Ô¨Åxed point in (7). Unless noted otherwise,
                                                                                      d
              2019; Bai, Koltun, and Kolter 2020) Broyden‚Äôs method is           mapping arguments are implicit in this section; in each im-
              used for Ô¨Ånding u?. Broyden‚Äôs method is a quasi-Newton            plicit case, this will correspond to entries in (7). We begin
                                 d                                              with standard assumptions enabling us to differentiate N .
              scheme and so at each iteration it updates a stored approxi-                                                              Œò
              mation to the Jacobian Jk and then solves a linear system in      Assumption 0.2. The mappings S        and T    are continu-
              J . Since in this work our goal is to explore truly Jacobian-                                        Œò        Œò
               k                                                                ously differentiable with respect to u and Œò.
              free approaches, we stick to the simpler Ô¨Åxed point iteration
                                                                          6650
                                               Q (d)
                                                 Q (d)
                                                  ‚á•
                                                   ‚á•
                                                                                 data output to latent space is independent of u
                   d                                  ¬∑¬∑¬∑                  
                                                                                                             ¬∑¬∑¬∑                           T‚á•(u;d)
                  u                                   ¬∑¬∑¬∑                  
                                                                              T‚á•(u;d)
                                                                          T‚á•(u;d)              `-Lipschitz a ne map        1-Lipschitz a ne map
               Figure 2: Diagram of a possible architecture for network operator TŒò (in large rectangle). Data d and latent u variables are
               processed in two streams by nonlinearities (denoted by œÉ) and afÔ¨Åne mappings (denoted by rectangles). These streams merge
               into a Ô¨Ånal stream that may also contain transformations. Light gray and blue afÔ¨Åne maps are `-Lipschitz and 1-Lipschitz,
               respectively. The mapping QŒò from data space to latent space is enclosed by the red rectangle.
               Assumption 0.3. The weights Œò may be written as a tuple                   The omission of J‚àí1 admits two straightforward inter-
                                                                                                              Œò              ?
               Œò=(Œ∏S,Œ∏T)suchthatweightparamaterization of SŒò and                      pretations. Note NŒò(d) = SŒò(TŒò(ud;d)), and so pŒò is pre-
               T dependonlyonŒ∏ andŒ∏ ,respectively.4                                   cisely the gradient of the expression `(y ,S (T (u?;d))),
                 Œò                   S       T                                                                                    d   Œò Œò d
                  Let J   be deÔ¨Åned as the identity operator, denoted by I,           treating u? as a constant independent of Œò. The distinction is
                        Œò                                                                       d           ?
                                   5                                                  that using SŒò(TŒò(u ;d)) assumes, perhaps by chance, the
               minus the Jacobian of TŒò at (u,d), i.e.                                                      d        1
                                                  dT                                  user chose the Ô¨Årst iterate u    in their Ô¨Åxed point iteration
                                J (u;d) , I‚àí         Œò(u;d).              (12)        (see Algorithm 1) to be precisely the Ô¨Åxed point u?. This
                                  Œò                du                                                                                         d
               Following (Winston and Kolter 2020; Bai, Kolter, and                   makestheiteration trivial, ‚Äúconverging‚Äù in one iteration. We
               Koltun 2019), we differentiate both sides of the Ô¨Åxed point            can simulate this behavior by using the Ô¨Åxed point iteration
                                                                                      to Ô¨Ånd u? and only backpropagating through the Ô¨Ånal step
               relation in (7) to obtain, by the implicit function theorem,                     d
                                                                                      of the Ô¨Åxed point iteration, as shown in Figure 4.
                 du?     ‚àÇTŒòdu? ‚àÇTŒò                 du?            ‚àÇTŒò                   Since the weights Œò typically lie in a space of much
                    d =           d +        =‚áí        d = J‚àí1¬∑         , (13)
                 dŒò       ‚àÇu dŒò        ‚àÇŒò            dŒò       Œò     ‚àÇŒò                higher dimension than the latent space U, the Jacobians
               whereJ‚àí1 exists whenever JŒò exists (see Lemma ??). Us-                 ‚àÇSŒò/‚àÇŒòand‚àÇTŒò/‚àÇŒòeffectivelyalways have full column
                        Œò                                                             rank. We leverage this fact via the following assumption.
               ing the chain rule gives the loss gradient
                    d [`(yd,NŒò(d))] = d h`(yd,SŒò(TŒò(u?,d))i                           Assumption0.4. UnderAssumption0.3,givenanyweights
                   dŒò                      dŒò                   d                     Œò=(Œ∏S,Œ∏T)anddatad,thematrix
                                        = ‚àÇ` dSŒòJ‚àí1‚àÇTŒò + ‚àÇSŒò.                                                  " ‚àÇSŒò       0   #
                                                       Œò                                                            ‚àÇŒ∏
                                           ‚àÇy    du        ‚àÇŒò       ‚àÇŒò                                    M,          S    ‚àÇT                     (17)
                                                                          (14)                                       0        Œò
                                                                                                                           ‚àÇŒ∏T
               Thematrix JŒò satisÔ¨Åes the inequality (see Lemma ??)                    has full column rank and is sufÔ¨Åciently well conditioned to
                    
      ‚àí1        1‚àíŒ≥         2                                                         6
                      u,J     u ‚â•             kuk , for all u ‚àà U.        (15)        satisfy the inequality
                          Œò         (1+Œ≥)2
               Intuitively, this coercivity property makes it seem possible                         Œ∫(M>M)= Œªmax(M>M) ‚â§ 1.                        (18)
               to remove J‚àí1 from (14) and backpropagate using                                                     Œªmin(M>M)          Œ≥
                            Œò          h                     i
                          p ,‚àí d `(y ,S (T (u,d))                                     Remark 0.2. The conditioning portion of the above as-
                           Œò       dŒò       d   Œò Œò                ?
                                                            u=ud        (16)        sumption is useful for bounding the worst-case behavior in
                                   ‚àÇ` dSŒò‚àÇTŒò           ‚àÇSŒò                            our analysis. However, we found it unnecessary to enforce
                              =‚àí‚àÇy du ‚àÇŒò + ‚àÇŒò .                                       this in our experiments for effective training (e.g. see Figure
                                                                                      5), which we hypothesize is justiÔ¨Åed because worst case be-
                  4This assumption is easy to ensure in practice. For notational      havior rarely occurs in practice and we train using averages
               brevity, we use the subscript Œò throughout.                            of pŒò for samples drawn from large data sets.
                  5Under Assumption 0.1, the Jacobian J     exists almost every-
                                                         Œò                                6
               where. However, presentation is cleaner by assuming smoothness.            ThetermŒ≥ here refers to the contraction factor in (9).
                                                                                6651
                  Assumption 0.4 gives rise to a second interpretation of             within a torch.no_grad()block.WiththisÔ¨Åxedpoint,
               JFB. Namely, the full column rank of M enables us to                   explicit_model evaluates and returns S (T (u?,d))
                                                                                                                                       Œò Œò d
               rewrite pŒò as a preconditioned gradient, i.e.                          to y in train mode (to create the computational graph).
                                     I        0  + d`                             Thus, our scheme coincides with standard backpropagation
                            pŒò = M                   M           ,        (19)        throughanexplicitmodelwithonelatentspacelayer.Onthe
                                           0   JŒò            dŒò                       other hand, standard implicit models backpropagate by solv-
                                   |          {z           }                          ing a linear system to apply J‚àí1 as in (14). That approach
                                    preconditioning term                                                              Œò
                                                                                      requires users to manually update the parameters, use more
               where M+ is the Moore-Penrose pseudo inverse (Moore                    computationalresources,andmakeconsiderations(e.g.con-
                                                                                      ditioning of J‚àí1) for each architecture used.
               1920; Penrose 1955). These insights lead to our main result.                          Œò
               Theorem0.2. If Assumptions 0.1, 0.2, 0.3, and 0.4 hold for              Implicit Forward + Proposed Backprop
               given weights Œò and data d, then
                          p ,‚àí d h`(y ,S (T (u,d))i                       (20)         u_fxd_pt = find_fixed_point(d)
                           Œò               d   Œò Œò                                     y = explicit_model(u_fxd_pt, d)
                                   dŒò                         u=u?
                                                                  d                    loss = criterion(y, labels)
               is a descent direction for `(yd,NŒò(d)) with respect to Œò.               loss.backward()
                  Theorem 0.2 shows we can avoid difÔ¨Åcult computations                 optimizer.step()
               associated with J‚àí1 in (14) (i.e. solving an associated lin-
                                  Œò
               ear system/adjoint equation) in implicit network literature                Figure 3: Sample PyTorch code for backpropagation
               (Chen et al. 2018; Dupont, Doucet, and Teh 2019; Bai,
               Kolter, and Koltun 2019; Winston and Kolter 2020). Thus,               NeumannBackpropagation The inverse of the Jacobian
               our scheme more naturally applies to general multilayered              in (12) can be expanded using a Neumann series, i.e.
               TŒòandissubstantiallysimplertocode.Ourschemeisjuxta-                                                                  
                                                                                                                     ‚àí1     ‚àû            k
               posed in Figure 4 with classic and Jacobian-based schemes.                      J‚àí1 = I‚àí dTŒò              =X dTŒò .                (22)
                  Twoadditional considerations must be made when deter-                          Œò             du                  du
               miningtheefÔ¨Åcacyoftrainingamodelusing(20)ratherthan                                                          k=0
               Jacobian-based gradients (14).                                         Thus, JFB is a zeroth-order approximation to the Neumann
                  I DoesuseofpŒòin(20)degradetraining/testingperfor-                   series. In particular, JFB resembles the Neumann-RBP ap-
                     mancerelative to (14)?                                           proach for recurrent networks (Liao et al. 2018). However,
                  I Is the term p     in (20) resilient to errors in estimates        Neumann-RBP does not guarantee a descent direction or
                                   Œò                                                  guidelines on how to truncate the Neumann series. This is
                     of the Ô¨Åxed point u??
                                         d                                            generally difÔ¨Åcult to achieve in theory and practice (Aicher,
                  The Ô¨Årst answer is our training scheme takes a different            Foti, and Fox 2020). Our work differs from (Liao et al.
               path to minimizers than using gradients with the implicit              2018) in that we focus purely on implicit networks, prove
               model. Thus, for nonconvex problems, one should not ex-                descent guarantees for JFB, and provide simple PyTorch
               pect the results to be the same. In our experiments in Section         implementations. Similar approaches exist in hyperparam-
               , using (20) is competitive (14) for all tests (when applied to        eter optimization, where truncated Neumann series are is
               nearly identical models). The second inquiry is partly an-             used to approximate second-order updates during train-
               swered by the corollary below, which states JFB yields de-             ing (Luketina et al. 2016; Lorraine, Vicol, and Duvenaud
               scent even for approximate Ô¨Åxed points.                                2020). Finally, similar zeroth-order truncations of the Neu-
               Corollary 0.1. Given weights Œò and data d, there exists                mann series have been employed, albeit without proof, in
               Œµ > 0 such that if uŒµ ‚àà U satisÔ¨Åes kuŒµ ‚àí u?k ‚â§ Œµ and                   Meta-learning (Finn, Abbeel, and Levine 2017; Rajeswaran
                                      d                    d     d                    et al. 2019) and in training transformers (Geng et al. 2021).
               the assumptions of Theorem 0.2 hold, then
                          pŒµ , ‚àí d h`(yd,SŒò(TŒò(u,d))i                     (21)                               Experiments
                           Œò       dŒò                         u=uŒµ                    This section shows the effectiveness of JFB using PyTorch
                                                                  d
               is a descent direction of `(y ,N (d)) with respect to Œò.               (Paszke et al. 2017). All networks are ResNet-based such
                                            d    Œò                                                                7
                                                                                      that Assumption 0.3 holds. One can ensure Assumption 0.1
                  Wearenotawareofanyanalogousresults for error toler-                 holds (e.g. via spectral normalization). Yet, in our experi-
               ances in the implicit depth literature.                                ments we found this unnecessary since tuning the weights
                                                                                                                                        8
               CodingBackpropagation AkeyfeatureofJFBisitssim-                        automatically encouraged contractive behavior. All exper-
               plicity of implementation. In particular, the backpropagation          iments are run on a single NVIDIA TITAN X GPU with
               of our scheme is similar to that of a standard backpropa-              12GBRAM.Furtherdetailsareintheappendix.
               gation. We illustrate this in the sample of PyTorch (Paszke               7AweakerversionofAssumption0.2alsoholdsinpractice,i.e.
               et al. 2017) code in Figure 3. Here explicit_modelrep-                 differentiability almost everywhere.
               resents S (T (u;d)). The Ô¨Åxed point u? = u_fxd_pt is                      8We found (9) held for batches of data during training, even
                         Œò Œò                              d
               computed by successively applying TŒò (see Algorithm 1)                 whenusingbatchnormalization. See appendix for more details.
                                                                               6652
                                                                      ¬∑ ¬∑ ¬∑                     Traditional Backprop
                                   1      TŒò(¬∑,d)       2   TŒò(¬∑,d)           TŒò(¬∑,d)    K‚àí1     TŒò(¬∑,d)       K
                                  u                    u              ¬∑ ¬∑ ¬∑            u                     u
                                               Forward Prop                     Proposed Backprop
                                   d                      Jacobian-based Backprop
              Figure 4: Diagram of backpropagation schemes for recurrent implicit depth models. Forward propagation is tracked via solid
              arrows point to the right (n.b. each forward step uses d). Backpropagation is shown via dashed arrows pointing to the left.
              Traditional backpropagation requires memory capacity proportional to depth (which is implausible for large K). Jacobian-
              based backpropagation solves an associated equation dependent upon the data d and operator TŒò. JFB uses a single backward
              step, which avoids both large memory capacity requirements and solving a Jacobian-type equation.
                                        MNIST                                 DEQs(Bai, Koltun, and Kolter 2020), and MONs (Winston
               Method                               Network size    Acc.      and Kolter 2020). We also compare with corresponding ex-
               Explicit                                     54K    99.4%      plicit versions of our ResNet-based networks given in (1) as
               Neural ODE‚Ä†                                  84K    96.4%      well as with state-of-the-art ResNet results (He et al. 2016)
               Aug. Neural ODE‚Ä†                             84K    98.2%      on the augmented CIFAR10 dataset. The explicit networks
               MON‚Ä°                                         84K    99.2%      aretrainedwiththesamesetupastheirimplicitcounterparts.
               JFB-trained Implicit ResNet (ours)           54K    99.4%      Table 1 shows JFBs are an effective way to train implicit
                                                                              networks, substantially outperform all the ODE-based net-
                                         SVHN                                 works as well as MONs using similar or fewer parameters.
               Method                               Network size    Acc.      Moreover, JFB is competitive with Multiscale DEQs (Bai,
                                                                              Koltun, and Kolter 2020) despite having less than a tenth as
               Explicit                                   164K     93.7%      manyparameters. See appendix for additional results.
               Neural ODE‚Ä†                                172K     81.0%
               Aug. Neural ODE‚Ä†                           172K     83.5%      ComparisontoJacobian-basedBackpropagation
                                 ‚Ä°
               MON(Multi-tier lg)                         170K     92.3%
               JFB-trained Implicit ResNet (ours)         164K    94.1%       Table 2 compares performance between using the standard
                                       CIFAR-10                               Jacobian-based backpropagation and JFB. The experiments
               Method                               Network size    Acc.      are performed on all the datasets described in Section . To
                                                                              apply the Jacobian-based backpropagation in (13), we use
               Explicit (ResNet-56)‚àó                      0.85M    93.0%      the conjugate gradient (CG) method on an associated set of
                                 ‚Ä°‚àó                                           normal equations similarly to (Liao et al. 2018). To main-
               MON(Multi-tier lg)                         1.01M    89.7%
               JFB-trained Implicit ResNet (ours)‚àó        0.84M   93.7%       tain similar costs, we set the maximum number of CG iter-
               Multiscale DEQ‚àó                             10M     93.8%      ations to be the same as the maximum depth of the forward
                                                                              propagation. The remaining experimental settings are kept
              Table 1: Test accuracy of JFB-trained Implicit ResNet com-      the same in our proposed approach. Note the network ar-
              paredtoNeuralODEs,AugmentedNODEs,andMONs;‚Ä†as                    chitectures trained with JFB contain batch normalization in
              reported in (Dupont, Doucet, and Teh 2019); ‚Ä°as reported in     the latent space whereas those trained with Jacobian-based
              (Winston and Kolter 2020); *with data augmentation              backpropagationdonot.Removalofbatchnormalizationfor
                                                                              the Jacobian-based method was necessary due to a lack of
                                                                              convergence when solving (13), thereby increasing training
              ClassiÔ¨Åcation                                                   loss (see appendix for further details). This phenomena is
                                                                              also observed in previous works (Bai, Koltun, and Kolter
              Wetrainimplicitnetworksonthreebenchmarkimageclassi-             2020; Bai, Kolter, and Koltun 2019). Thus, we Ô¨Ånd JFB to
              Ô¨Åcation datasets licensed under CC-BY-SA: SVHN (Netzer          be (empirically) effective on a wider class of network archi-
              et al. 2011), MNIST (LeCun, Cortes, and Burges 2010), and       tectures (e.g. including batch normalization). The purpose
              CIFAR-10(KrizhevskyandHinton2009).Table1compares                of the Jacobian-based results in Figure 5 and Table 2 is to
              ourresults with state-of-the-art results for implicit networks, show speedups in training time while maintaining a com-
              including Neural ODEs (Chen et al. 2018), Augmented             petitive accuracy with previous state-of-the-art implicit net-
              Neural ODEs (Dupont, Doucet, and Teh 2019), Multiscale          works. More plots are given in the appendix.
                                                                        6653
                                             Dataset    Avgtimeperepoch(s)      # of J mat-vec products   Accuracy %
                                                                                                6
                              Jacobian       MNIST               28.4                   6.0 √ó107              99.2
                               based         SVHN                92.8                   1.4 √ó10               90.1
                                                                                                8
                                            CIFAR10             530.9                   9.7 √ó10               87.9
                                             MNIST               17.6                       0                 99.4
                                JFB          SVHN                36.9                       0                 94.1
                                            CIFAR10             146.6                       0                 93.67
             Table 2: Comparison of Jacobian-based backpropagation (Ô¨Årst three rows) and our proposed JFB approach. ‚ÄúMat-vecs‚Äù denotes
             matrix-vector products.
                  % 95   Proposed Backprop (JFB)                                 %
                                                                                   99
                    90
                    85                      Jacobian-based                              Proposed Backprop (JFB)
                  Accuracy                                                       Accuracy98
                  est80                        Backprop                          est    Neumann: k = 5
                  T                                                              T      Neumann: k = 10
                    75                                                             97
                       0        250      500       750      1,000                     0        25        50        75       100
                                        Epoch                                                          Epoch
                    % 95    Proposed Backprop (JFB)                               %
                                                                                    99
                      90
                      85                     Jacobian-based                             Proposed Backprop (JFB)
                    Accuracy                                                      Accuracy98
                    est80                       Backprop                          est   Neumann: k = 5
                    T                                                             T     Neumann: k = 10
                      75                                                            97
                        0    20 40 60 80 100 120 140                                   0      10      20     30      40      50
                                       Time (hr)                                                    Time (min)
             Figure 5: CIFAR10 results using comparable networks/-          Figure 6: MNIST training using different truncations k of
             conÔ¨Ågurations, but with two backpropagation schemes: our       theNeumannseries(22)toapproximatetheinverseJacobian
             proposed JFB method (blue) and standard Jacobian-based         J‚àí1. Plots show faster training with fewer terms (fastest
                                                                             Œò
             backpropagation in (14) (green), with Ô¨Åxed point tolerance     with JFB, i.e. k = 0) and competitive test accuracy.
              = 10‚àí4. JFB is faster and gives better test accuracy.
                                                                                                 Conclusion
             HigherOrderNeumannApproximation                                This work presents a new and simple Jacobian-free back-
                                                                            propagation (JFB) scheme. JFB enables training of implicit
                                                                            networks with Ô¨Åxed memory costs (regardless of depth), is
             As explained in Section , JFB can be interpreted as an ap-     easy to code (see Figure 3), and yields efÔ¨Åcient backpropa-
             proximation to the Jacobian-based approach using a zeroth      gation. Use of JFB is theoretically justiÔ¨Åed (even when Ô¨Åxed
             order (i.e. k = 0) truncation to the Neumann series expan-     pointsareapproximatelycomputed).ExperimentsshowJFB
             sion (22) of the Jacobian inverse J‚àí1. Figure 6 compares       yields competitive results for implicit networks. Extensions
                                               Œò
             JFB with training using more Neumann series terms in the       will enable satisfaction of additional constraints for imag-
             approximation of the Jacobian inverse J‚àí1. Figure 6 shows      ing (Klibanov 1986; Fienup 1982; Heaton et al. 2020; Fung
                                                   Œò
             JFB is competitive at reduced time cost. SigniÔ¨Åcantly, JFB     and Wendy 2020; Kan, Fung, and Ruthotto 2020), geo-
             is also much easier to implement (see Figure 3). See ap-       physics (Haber 2014; Fung and Ruthotto 2019a,b), and
             pendix for more experiments with SVHN and discussion           games (Von Neumann 1959; Lin et al. 2021; Ruthotto et al.
             about code.                                                    2020).
                                                                      6654
                                   Acknowledgements                                       Finn, C.; Abbeel, P.; and Levine, S. 2017. Model-agnostic meta-
                HH, DM, SO, SWF and QL were supported by AFOSR                            learningforfastadaptationofdeepnetworks. InInternationalCon-
                MURI FA9550-18-1-0502 and ONR grants: N00014-18-                          ference on Machine Learning, 1126‚Äì1135. PMLR.
                1-2527, N00014-20-1-2093, and N00014-20-1-2787. HH‚Äôs                      Fung, S. W.; and Ruthotto, L. 2019a. A multiscale method for
                work was also supported by the National Science Founda-                   model order reduction in PDE parameter estimation. Journal of
                tion (NSF) Graduate Research Fellowship under Grant No.                   Computational and Applied Mathematics, 350: 19‚Äì34.
                DGE-1650604. Any opinion, Ô¨Åndings, and conclusions or                     Fung, S. W.; and Ruthotto, L. 2019b. An uncertainty-weighted
                recommendations expressed in this material are those of the               asynchronous ADMM method for parallel PDE parameter estima-
                authors and do not necessarily reÔ¨Çect the views of the NSF.               tion. SIAM Journal on ScientiÔ¨Åc Computing, 41(5): S129‚ÄìS148.
                We thank Zaccharie Ramzi for the fruitful discussions and                 Fung,S.W.;andWendy,Z.2020. Multigridoptimizationforlarge-
                the anonymous referees for helping us improve the quality                 scale ptychographic phase retrieval. SIAM Journal on Imaging Sci-
                of our paper.                                                             ences, 13(1): 214‚Äì233.
                                                                                          Geng,Z.;Guo,M.-H.;Chen,H.;Li,X.;Wei,K.;andLin,Z.2021.
                                         References                                       Is Attention Better Than Matrix Decomposition? In International
                                                                                          Conference on Learning Representations.
                Aicher, C.; Foti, N. J.; and Fox, E. B. 2020. Adaptively truncating       Ghaoui, L. E.; Gu, F.; Travacca, B.; Askari, A.; and Tsai, A. Y.
                backpropagation through time to control gradient bias. In Uncer-          2019. Implicit Deep Learning. arXiv preprint arXiv:1908.06315.
                tainty in ArtiÔ¨Åcial Intelligence, 799‚Äì808. PMLR.                          Gholami, A.; Keutzer, K.; and Biros, G. 2019. ANODE: Uncon-
                Anil, C.; Lucas, J.; and Grosse, R. 2019. Sorting out Lipschitz           ditionally accurate memory-efÔ¨Åcient gradients for neural ODEs.
                function approximation. In International Conference on Machine            arXiv preprint arXiv:1902.10298.
                Learning, 291‚Äì301. PMLR.                                                  Gilton, D.; Ongie, G.; and Willett, R. 2021.     Deep Equilibrium
                Bai, S.; Kolter, J. Z.; and Koltun, V. 2019. Deep equilibrium mod-        Architectures for Inverse Problems in Imaging.      arXiv preprint
                els. In Advances in Neural Information Processing Systems, 690‚Äì           arXiv:2102.07944.
                701.                                                                      Gouk, H.; Frank, E.; Pfahringer, B.; and Cree, M. J. 2021. Reg-
                Bai,S.;Koltun,V.;andKolter,J.Z.2020. MultiscaleDeepEquilib-               ularisation of neural networks by enforcing Lipschitz continuity.
                riumModels. AdvancesinNeuralInformationProcessingSystems,                 Machine Learning, 110(2): 393‚Äì416.
                33.                                                                       Gould, S.; Hartley, R.; and Campbell, D. 2019. Deep declarative
                                            ¬¥                                             networks: A new hope. arXiv preprint arXiv:1909.04866.
                Banach, S. 1922. Sur les operations dans les ensembles abstraits et       Haber, E. 2014. Computational methods in geophysical electro-
                                     ¬¥            ¬¥
                leur application aux equations integrales. Fund. math, 3(1): 133‚Äì         magnetics. SIAM.
                181.
                Chang, B.; Meng, L.; Haber, E.; Ruthotto, L.; Begert, D.; and             Haber, E.; and Ruthotto, L. 2017. Stable architectures for deep
                Holtham, E. 2018. Reversible architectures for arbitrarily deep           neural networks. Inverse Problems, 34(1): 014004.
                residual neural networks. In Proceedings of the AAAI Conference           He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-
                onArtiÔ¨Åcial Intelligence, volume 32.                                      ing for image recognition. In Proceedings of the IEEE conference
                Chen, R. T.; Rubanova, Y.; Bettencourt, J.; and Duvenaud, D. K.           oncomputervision and pattern recognition, 770‚Äì778.
                2018. Neuralordinarydifferentialequations. InAdvancesinneural             Heaton,H.;Fung,S.W.;Gibali,A.;andYin,W.2021a. Feasibility-
                information processing systems, 6571‚Äì6583.                                based Fixed Point Networks. arXiv preprint arXiv:2104.14090.
                Cisse, M.; Bojanowski, P.; Grave, E.; Dauphin, Y.; and Usunier,           Heaton, H.; Fung, S. W.; Lin, A. T.; Osher, S.; and Yin, W. 2020.
                N. 2017. Parseval networks: Improving robustness to adversarial           Projecting to Manifolds via Unsupervised Learning. arXiv preprint
                examples. InInternationalConferenceonMachineLearning,854‚Äì                 arXiv:2008.02200.
                863. PMLR.                                                                Heaton, H.; McKenzie, D.; Li, Q.; Fung, S. W.; Osher, S.; and Yin,
                  ¬¥                                                                       W. 2021b. Learn to Predict Equilibria via Fixed Point Networks.
                Csaji, B. C.; et al. 2001. Approximation with artiÔ¨Åcial neural net-       arXiv preprint arXiv:2106.00906.
                works. Faculty of Sciences, Eotvos Lorand University, Hungary,
                                               ¬®  ¬®      `                                Jameson,A.1988. Aerodynamicdesignviacontroltheory. Journal
                24(48): 7.                                                                of scientiÔ¨Åc computing, 3(3): 233‚Äì260.
                Dupont, E.; Doucet, A.; and Teh, Y. W. 2019. Augmented Neural             Jeon, Y.; Lee, M.; and Choi, J. Y. 2021. Differentiable Forward and
                                                                                 ¬¥
                ODEs. In Wallach, H.; Larochelle, H.; Beygelzimer, A.; d'Alche-           BackwardFixed-Point Iteration Layers. IEEE Access.
                Buc, F.; Fox, E.; and Garnett, R., eds., Advances in Neural Infor-        Kan, K.; Fung, S. W.; and Ruthotto, L. 2020. PNKH-B: A pro-
                mation Processing Systems, volume 32. Curran Associates, Inc.             jected Newton-Krylov method for large-scale bound-constrained
                Elad,M.;Figueiredo,M.A.;andMa,Y.2010. Ontheroleofsparse                   optimization. arXiv preprint arXiv:2005.13639.
                andredundantrepresentationsinimageprocessing. Proceedingsof               Kidger, P.; and Lyons, T. 2020. Universal approximation with deep
                the IEEE, 98(6): 972‚Äì982.                                                 narrow networks. In Conference on Learning Theory, 2306‚Äì2327.
                Fienup, J. R. 1982. Phase retrieval algorithms: A comparison. Ap-         PMLR.
                plied optics, 21(15): 2758‚Äì2769.                                          Klibanov, M. V. 1986. Determination of a compactly supported
                Finlay, C.; Calder, J.; Abbasi, B.; and Oberman, A. 2018. Lipschitz       function from the argument of its Fourier transform. In Doklady
                regularized deep neural networks generalize and are adversarially         Akademii Nauk, volume 289, 539‚Äì540. Russian Academy of Sci-
                robust. arXiv preprint arXiv:1808.09540.                                  ences.
                Finlay, C.; Jacobsen, J.-H.; Nurbekyan, L.; and Oberman, A. M.            Krizhevsky, A.; and Hinton, G. 2009. Learning Multiple Layers
                2020.     How to train your neural ODE.            arXiv preprint         of Features from Tiny Images.      Technical report, University of
                arXiv:2002.02798.                                                         Toronto.
                                                                                    6655
               Lawrence, N.; Loewen, P.; Forbes, M.; Backstrom, J.; and                 Advances in Neural Information Processing Systems, volume 32.
               Gopaluni, B. 2020. Almost Surely Stable Deep Dynamics. In                Curran Associates, Inc.
               Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M. F.; and Lin,        Revay, M.; and Manchester, I. 2020. Contracting implicit recur-
               H., eds., Advances in Neural Information Processing Systems, vol-        rent neural networks: Stable models with improved trainability. In
               ume33,18942‚Äì18953.CurranAssociates, Inc.                                 Learning for Dynamics and Control, 393‚Äì403. PMLR.
               LeCun, Y.; Cortes, C.; and Burges, C. 2010.               MNIST          Ruthotto, L.; and Haber, E. 2019. Deep neural networks motivated
               handwritten digit database.      ATT Labs [Online]. Available:           bypartial differential equations. Journal of Mathematical Imaging
               http://yann.lecun.com/exdb/mnist, 2.                                     and Vision, 1‚Äì13.
               Liao, R.; Xiong, Y.; Fetaya, E.; Zhang, L.; Yoon, K.; Pitkow, X.;        Ruthotto, L.; and Haber, E. 2021. An Introduction to Deep Gener-
               Urtasun, R.; and Zemel, R. 2018. Reviving and improving recur-           ative Modeling. arXiv preprint arXiv:2103.05180.
               rent back-propagation. In International Conference on Machine            Ruthotto, L.; Osher, S. J.; Li, W.; Nurbekyan, L.; and Fung, S. W.
               Learning, 3082‚Äì3091. PMLR.                                               2020. Amachinelearningframeworkforsolvinghigh-dimensional
               Lin, A. T.; Fung, S. W.; Li, W.; Nurbekyan, L.; and Osher, S. J.         meanÔ¨Åeld game and mean Ô¨Åeld control problems. Proceedings of
               2021. Alternating the population and control neural networks to          the National Academy of Sciences, 117(17): 9183‚Äì9193.
               solve high-dimensional stochastic mean-Ô¨Åeld games. Proceedings                 ¬¥
               of the National Academy of Sciences, 118(31).                            Sokolic, J.; Giryes, R.; Sapiro, G.; and Rodrigues, M. R. 2017. Ro-
               Look, A.; Doneva, S.; Kandemir, M.; Gemulla, R.; and Pe-                 bust large margin deep neural networks. IEEE Transactions on
               ters, J. 2020.   Differentiable Implicit Layers.   arXiv preprint        Signal Processing, 65(16): 4265‚Äì4280.
               arXiv:2010.07078.                                                        Udell, M.; and Townsend, A. 2019. Why are big data matrices
               Lorraine, J.; Vicol, P.; and Duvenaud, D. 2020. Optimizing mil-          approximately low rank? SIAM Journal on Mathematics of Data
               lions of hyperparameters by implicit differentiation. In Interna-        Science, 1(1): 144‚Äì160.
               tional Conference on ArtiÔ¨Åcial Intelligence and Statistics, 1540‚Äì        Van der Maaten, L.; and Hinton, G. 2008. Visualizing data using
               1552. PMLR.                                                              t-SNE. Journal of machine learning research, 9(11).
               Lu, Y.; Zhong, A.; Li, Q.; and Dong, B. 2018. Beyond Ô¨Ånite layer         Von Neumann, J. 1959. On the theory of games of strategy. Con-
               neural networks: Bridging deep architectures and numerical differ-       tributions to the Theory of Games, 4: 13‚Äì42.
               ential equations. In International Conference on Machine Learn-          Weinan, E. 2017. A proposal on machine learning via dynamical
               ing, 3276‚Äì3285. PMLR.                                                    systems. Communications in Mathematics and Statistics, 5(1): 1‚Äì
               Lu,Z.;Pu,H.;Wang,F.;Hu,Z.;andWang,L.2017.Theexpressive                   11.
               power of neural networks: A view from the width. arXiv preprint          Winston,E.;andKolter,J.Z.2020. Monotoneoperatorequilibrium
               arXiv:1709.02540.                                                        networks. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan,
               Luketina, J.; Berglund, M.; Greff, K.; and Raiko, T. 2016. Scalable      M.F.; and Lin, H., eds., Advances in Neural Information Process-
               gradient-based tuning of continuous regularization hyperparame-          ing Systems, volume 33, 10718‚Äì10728. Curran Associates, Inc.
               ters. In International conference on machine learning, 2952‚Äì2960.        Zhang, Q.; Gu, Y.; Mateusz, M.; Baktashmotlagh, M.; and Eriks-
               PMLR.                                                                    son, A. 2020. Implicitly deÔ¨Åned layers in neural networks. arXiv
               Moore, E. H. 1920. On the reciprocal of the general algebraic ma-        preprint arXiv:2003.01822.
               trix. Bulletin of the American Mathematical Society, 26: 394‚Äì395.
               Netzer, Y.; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; and Ng,
               A. Y. 2011. Reading digits in natural images with unsupervised
               feature learning. In NIPS Workshop on Deep Learning and Unsu-
               pervised Feature Learning.
               Onken, D.; and Ruthotto, L. 2020.        Discretize-Optimize vs.
               Optimize-Discretize for Time-Series Regression and Continuous
               Normalizing Flows. arXiv preprint arXiv:2005.13420.
               Onken, D.; Wu Fung, S.; Li, X.; and Ruthotto, L. 2021.        OT-
               Flow: Fast and Accurate Continuous Normalizing Flows via Op-
               timal Transport. Proceedings of the AAAI Conference on ArtiÔ¨Åcial
               Intelligence, 35(10): 9223‚Äì9232.
               Osher, S.; Shi, Z.; and Zhu, W. 2017. Low dimensional manifold
               model for image processing. SIAM Journal on Imaging Sciences,
               10(4): 1669‚Äì1690.
               Paszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.; DeVito,
               Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A. 2017. Auto-
               matic differentiation in PyTorch.
               Penrose, R. 1955. A generalized inverse for matrices. In Mathe-
               matical Proceedings of the Cambridge Philosophical Society, vol-
               ume51,406‚Äì413.CambridgeUniversity Press.
                    ¬¥
               Peyre, G. 2009. Manifold models for signals and images. Com-
               puter vision and image understanding, 113(2): 249‚Äì260.
               Rajeswaran, A.; Finn, C.; Kakade, S. M.; and Levine, S. 2019.
               Meta-LearningwithImplicitGradients. InWallach,H.;Larochelle,
                                          ¬¥
               H.;Beygelzimer,A.;d'Alche-Buc,F.;Fox,E.;andGarnett,R.,eds.,
                                                                                 6656
