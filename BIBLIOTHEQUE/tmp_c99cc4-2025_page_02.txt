                2                                                                    Transactions of the Institute of Measurement and Control 00(0)
                detection in BEV space. Those methods achieve remarkable             networks that adopt BEV for spatial representation, the
                progress but suffering from high computation cost. Another           Vision Transformer (ViT) (Dosovitskiy et al., 2021) emerges
                line of work (Liu et al., 2022, 2023b; Wang et al., 2021b)           as a pivotal module. However, the self-attention’s structure
                explores the sparse query-based paradigm by initializing a set       inherently lacks explicit spatial priors, rendering it unable to
                of sparse reference points in 3D space. Specifically, DETR3D         directly process the spatial relationships among image pixels
                (Wang et al., 2021b) links the queries to image features using       and features. Motivated by these challenges, we aim to devise
                3D-to-two-dimensional (2D) projection. It has simpler struc-         a BEV generation technique that not only demands lower
                ture and faster speed, but its performance still lags far behind     computational power but also either maintains or slightly
                the dense ones. PETR series (Liu et al., 2022, 2023b; Wang           enhances     the   benchmarks      for   3D object      detection
                et al., 2022b) uses dense global attention for the interaction       performance.
                between query and image feature, which is computationally                The self-attention mechanism, central to the visual trans-
                expensive and buries the advantage of the sparse paradigm.           former, incurs significant computational costs in multi-camera
                Therefore, there remains a vast potential for exploration and        BEVperceptiontasksdue toits quadratic computational time
                optimization in BEV-based methods. This leads us to the cen-         complexity. Previous optimization methods for attention
                tral theme of our work: accelerating the training and infer-         mechanisms (Fan et al., 2023; Guo et al., 2022; Liu et al.,
                ence speed of BEV-based methods through an improved                  2021; Wu et al., 2021; Yang et al., 2022) compromise the spa-
                retentive mechanism.                                                 tial priors of the retentive mechanism. To address this, we
                   In this paper, our approach introduces the integration of         have integrated a decay method based on the Manhattan dis-
                decay weights of the retentive mechanism into the network’s          tance between ROI and query area in input images within the
                surround-view imagery, which addresses these challenges by           BEV perception framework. This structure’s decomposed
                enhancing detection precision while also reducing inference          form enables the network model to model global information
                latency. The existing BEV methods either sparsely construct          with linear complexity while preserving the spatial informa-
                target information within the perception range (Chambon              tion matrix inherent to the decay mechanism.
                et al., 2024; Liu et al., 2023a; Vedder and Eaton, 2022; Xu
                et al., 2022) or densely process every BEV grid (Huang               Related works.
                et al., 2022a; Huang and Huang, 2022; Li et al., 2022; Liu               Transformer. The transformer architecture, initially pro-
                et al., 2022, 2023b; Wang et al., 2021b), our approach inte-         posed in Vaswani et al. (2023), was designed to overcome the
                grates the strengths of both. Specifically, we draw inspira-         challenges associated with model training, quickly becoming
                tion from the sparse BEV detection approach, which is                a pivotal technology in numerous Natural Language
                based on the observation that ‘‘most of the vehicle’s percep-        Processing (NLP) tasks. This architecture has found extensive
                tion area is actually empty space, so first determine which          application across the fields of NLP and computer vision,
                areas contain targets to be detected, and then perform BEV           areas that were once led by Recurrent Neural Networks
                attention mechanism queries.’’ Our baseline, BEVFormer,              (RNNs) and Convolutional Neural Networks (CNNs) (Chu
                which follows a dense construction approach, merges the              et al., 2021a; Dosovitskiy et al., 2021; Pan et al., 2022; Xia
                tasks of determining whether each BEV grid is empty and              et al., 2022; Yao et al., 2022). When handling image inputs,
                detecting target labels simply by one transformer. Although          ViT (Dosovitskiy et al., 2021; Vaswani et al., 2023) divides
                this approach yields excellent results, it is relatively slow        the image into a series of non-overlapping blocks (Vaswani
                due to the need to execute the complete attention query              et al., 2023), which are then processed through its Query-
                mechanism across all BEV grids. To address this, we intro-           Value-Key attention mechanism. To enhance training effi-
                duced a decay mechanism based on the Manhattan distance              ciency and reduce computational costs, various studies have
                from image pixels to the ROI into the BEV network. This              proposed spatial priors and sparse attention strategies (Chu
                mechanism is decomposed along the horizontal and vertical            et al., 2021b; Hassani et al., 2023; Touvron et al., 2021).
                directions of the image and applied in the attention query           These advancements facilitate more efficient training and
                process. We achieved a significant improvement in detection          inference, especially in scenarios involving large data sets or
                precision on the nuScenes validation set, along with a nota-         complex spatial relationships. Also, there is much research
                ble reduction in inference latency, surpassing the baseline          aimed at decreasing the computational costs associated with
                BEVFormer-pure under similar conditions, and approach-               training and inferring self-attention (Fan et al., 2023; Wang
                ing the nuScenes Detection Score (NDS) score of existing             et al., 2021a, 2022a; Wu et al., 2021).
                state-of-the-art methods (Li et al., 2023a).
                                                                                         Prior knowledge in transformer. In visual transformers,
                Related works and motivation                                         prior knowledge is crucial for understanding universal pat-
                                                                                     terns within image data, such as the structure of images and
                Motivation. To enable vehicles to comprehend driving scenes,         the spatial distribution of objects. Integrating explicit spatial
                employing a BEV paradigm based on inputs from multiple               priors enhances the efficiency of the self-attention mechanism
                cameras serves as an effective spatial representation. BEV dis-      by clarifying spatial connections within the data. The early
                tinctly displays the location and size of objects in space, offer-   version of ViT (Dosovitskiy et al., 2021; Vaswani et al., 2023)
                ing an understanding of the surrounding scene from multi-            used trigonometric functions for positional encoding to inte-
                camera perspectives, making it apt for tasks like autonomous         grate pixel information into relevant feature dimensions.
                driving planning and perception. In the realm of neural              Recent advancements (Fan et al., 2024) have introduced a 2D
