                   7  Related Work
                   Combining neural and symbolic approaches to relational learning and reasoning has a long tradition
                   andlet to various proposed architectures over the past decades (see [38] for a review). Early proposals
                   for neural-symbolic networks are limited to propositional rules (e.g., EBL-ANN [39], KBANN [40]
                   and C-IL2P [41]). Other neural-symbolic approaches focus on ﬁrst-order inference, but do not
                   learn subsymbolic vector representations from training facts in a KB (e.g., SHRUTI [42], Neural
                   Prolog [43], CLIP++ [44], Lifted Relational Neural Networks [45], and TensorLog [46]). Logic
                   Tensor Networks [47] are in spirit similar to NTPs, but need to fully ground ﬁrst-order logic rules.
                   However, they support function terms, whereas NTPs currently only support function-free terms.
                   Recentquestion-answeringarchitecturessuchas[15,17,18]translatequeryrepresentationsimplicitly
                   in a vector space without explicit rule representations and can thus not easily incorporate domain-
                   speciﬁc knowledge. In addition, NTPs are related to random walk [48, 49, 11, 12] and path encoding
                   models [14, 16]. However, instead of aggregating paths from random walks or encoding paths to
                   predict a target predicate, reasoning steps in NTPs are explicit and only uniﬁcation uses subsymbolic
                   representations. This allows us to induce interpretable rules, as well as to incorporate prior knowledge
                   either in the form of rules or in the form of rule templates which deﬁne the structure of logical
                   relationships that we expect to hold in a KB. Another line of work [50–54] regularizes distributed
                   representations via domain-speciﬁc rules, but these approaches do not learn such rules from data and
                   only support a restricted subset of ﬁrst-order logic. NTPs are constructed from Prolog’s backward
                   chaining and are thus related to Uniﬁcation Neural Networks [55, 56]. However, NTPs operate on
                   vector representations of symbols instead of scalar values, which are more expressive.
                   As NTPs can learn rules from data, they are related to ILP systems such as FOIL [32], Sherlock
                   [57] and meta-interpretive learning of higher-order dyadic Datalog (Metagol) [58]. While these ILP
                   systems operate on symbols and search over the discrete space of logical rules, NTPs work with
                   subsymbolic representations and induce rules using gradient descent. Recently, [37] introduced
                   a differentiable rule learning system based on TensorLog and a neural network controller similar
                   to LSTMs [59]. Their method is more scalable than the NTPs introduced here. However, on
                   UMLSandKinshipourbaselinealreadyachieved stronger generalization by learning subsymbolic
                   representations. Still, scaling NTPs to larger KBs for competing with more scalable relational learning
                   methods is an open problem that we seek to address in future work.
                   8  Conclusion and Future Work
                   Weproposed an end-to-end differentiable prover for automated KB completion that operates on
                   subsymbolic representations. To this end, we used Prolog’s backward chaining algorithm as a recipe
                   for recursively constructing neural networks that can be used to prove queries to a KB. Speciﬁcally,
                   weintroduced a differentiable uniﬁcation operation between vector representations of symbols. The
                   constructed neural network allowed us to compute the gradient of proof successes with respect to
                   vector representations of symbols, and thus enabled us to train subsymbolic representations end-to-
                   end from facts in a KB, and to induce function-free ﬁrst-order logic rules using gradient descent. On
                   benchmark KBs, our model outperformed ComplEx, a state-of-the-art neural link prediction model,
                   onthree out of four KBs while at the same time inducing interpretable rules.
                   To overcome the computational limitations of the end-to-end differentiable prover introduced in
                   this paper, we want to investigate the use of hierarchical attention [25] and reinforcement learning
                   methods such as Monte Carlo tree search [60, 61] that have been used for learning to play Go [62]
                   and chemical synthesis planning [63]. In addition, we plan to support function terms in the future.
                   Basedon[64],wearefurthermoreinterested in applying NTPs to automated proving of mathematical
                   theorems, either in logical or natural language form, similar to recent approaches by [65] and [66].
                   Acknowledgements
                   WethankPasquale Minervini, Tim Dettmers, Matko Bosnjak, Johannes Welbl, Naoya Inoue, Kai
                   Arulkumaran, and the anonymous reviewers for very helpful comments on drafts of this paper. This
                   work has been supported by a Google PhD Fellowship in Natural Language Processing, an Allen
                   Distinguished Investigator Award, and a Marie Curie Career Integration Award.
                                                    9
