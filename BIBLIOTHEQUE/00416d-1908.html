<title>Who Invented Backpropagation?</title>
<body bgcolor=#277b98 link=#277b98 vlink=#277b98 MARGINWIDTH=0 MARGINHEIGHT=0 LEFTMARGIN=0 TOPMARGIN=0>
<center>
<font face="arial">
<table width=754 cellspacing=0 cellpadding=0  border=0 rules=none>

<tr>
<td  bgcolor=#ffffff>
<IMG src="backprop754x466.gif" alt="Who Invented Backpropagation?" border=0></td>
</tr>

<tr>
<td height=288  bgcolor=#ffffff valign=top>
<table width=754 cellspacing=0 cellpadding=20 border=0><tr><td> 


<center>


<h3> Who Invented Backpropagation?</h3>




<p>
<A HREF="http://www.idsia.ch/~juergen">J&uuml;rgen Schmidhuber</A>, 2014 (updated  2015)
<br> Pronounce: <font color=#277b98>You_again Shmidhoobuh</font> 


</center>

<p>Efficient backpropagation (BP) is central to the ongoing    
<A HREF="http://www.idsia.ch/~juergen/deeplearning.html">Neural Network (NN) ReNNaissance and "Deep Learning."</A> 
Who invented it? 

<p> Its modern version (also called the reverse mode of automatic differentiation) was first published in 1970 
by Finnish master student Seppo Linnainmaa. 

<p>Important  
concepts of BP were known even earlier though.  It is easy to find misleading accounts of BP's history (as of July 2014). I had a look at the original papers from the 1960s and 70s, and talked to BP pioneers. Here is a summary derived from my  
<A HREF=http://www.idsia.ch/~juergen/deep-learning-overview.html>survey (2014)</A>,
which has additional references:

<p>The minimisation of errors through gradient descent (Cauchy 1847, Hadamard, 1908) in the parameter space of complex, nonlinear, differentiable, multi-stage, NN-related systems has been discussed at least since the early 1960s (e.g., Kelley, 1960; Bryson, 1961; Bryson and Denham, 1961; Pontryagin et al., 1961; Dreyfus, 1962; Wilkinson, 1965; Amari, 1967; Bryson and Ho, 1969; Director and Rohrer, 1969), initially within the framework of Euler-LaGrange equations in the Calculus of Variations (e.g., Euler, 1744). 

<p>Steepest descent in the weight space of such systems can be performed (Bryson, 1961; Kelley, 1960; Bryson and Ho, 1969) by iterating the chain rule (Leibniz, 1676; L'Hopital, 1696) &agrave la Dynamic Programming (DP, Bellman, 1957). A simplified derivation of this backpropagation method uses the chain rule only (Dreyfus, 1962).

<p>The systems of the 1960s were already efficient in the DP sense. However, they backpropagated derivative information through standard Jacobian matrix calculations from one "layer" to the previous one, without explicitly addressing either direct links across several layers or potential additional efficiency gains due to network sparsity (but perhaps such enhancements seemed obvious to the authors). 

<p>Explicit, efficient error backpropagation (BP) in arbitrary, discrete, possibly sparsely connected, NN-like networks apparently was first described in a 1970 master's thesis (Linnainmaa, 1970, 1976), albeit without reference to NNs. BP is also known as the reverse mode of automatic differentiation (e.g., Griewank, 2012), where the costs of forward activation spreading essentially equal the costs of backward derivative calculation. See early BP FORTRAN code (Linnainmaa, 1970) and closely related work (Ostrovskii et al., 1971). 

<p>BP was soon explicitly used to minimize cost functions by adapting control parameters (weights) (Dreyfus, 1973). This was followed by some preliminary, NN-specific discussion (Werbos, 1974, section 5.5.1), and a computer program for automatically deriving and implementing BP for any given differentiable system (Speelpenning, 1980).

<p>To my knowledge, the first NN-specific application of efficient BP as above was described by  Werbos (1982). Related work was published several years later (Parker, 1985; LeCun, 1985). When computers had become 10,000 times faster per Dollar and much more accessible than those of 1960-1970, a paper of 1986 significantly contributed to the popularisation of BP for NNs (Rumelhart et al., 1986), experimentally demonstrating the emergence of useful internal representations in hidden layers.

<p>Compare also the first adaptive, deep, multilayer perceptrons (the GMDH networks; Ivakhnenko et al., since 1965), whose layers are incrementally grown and trained by regression analysis, as well as  a more recent method for multilayer threshold NNs (Bobrowski, 1978).

<p>Precise references and more history in:

<p>
<A HREF=http://www.idsia.ch/~juergen/deep-learning-overview.html>J. Schmidhuber. Deep Learning in Neural Networks: An Overview.</A> Neural Networks, 61, p 85-117, 2015.
(Based on 2014 TR with 88 pages and 888 references, with PDF & LATEX source & complete public BIBTEX file). 

<p> 
J. Schmidhuber. 
<A HREF=http://www.scholarpedia.org/article/Deep_Learning>Deep Learning.</A> 
Scholarpedia, 10(11):32832, 2015.

<p>
See also this <A HREF=https://plus.google.com/100849856540000067209/posts/gT8HuUSo9Uh?pid=6039928269674547682&oid=100849856540000067209>Google+ post</A>
and  
<A HREF=http://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/cp7adxn>backprop history in a nutshell</A> at the 
AMA (Ask Me Anything) on
reddit.

<p>The contents of this site may be used for educational and non-commercial purposes, including articles for Wikipedia and similar sites.



<hr>

<p>

<h3>Overview web sites with lots of additional details and papers on Deep Learning</b></h3>


<p>
[A] 1991: <em>Fundamental Deep Learning Problem</em> discovered and analysed: in standard NNs, backpropagated error gradients tend to vanish or explode.    <A HREF="http://www.idsia.ch/~juergen/fundamentaldeeplearningproblem.html">More</A>

<p>
[B] Our first Deep Learner of 1991 (RNN stack pre-trained in unsupervised fashion):   <A HREF="http://www.idsia.ch/~juergen/firstdeeplearner.html">More</A>, also under <A HREF="http://www.deeplearning.me">www.deeplearning.me</A> 

<p>
[C] 2009: First recurrent Deep Learner to win international competitions with secret test sets: deep LSTM recurrent neural networks [H] won three connected handwriting contests at ICDAR 2009 (French, Arabic, Farsi), performing simultaneous segmentation and recognition.   <A HREF="http://www.idsia.ch/~juergen/handwriting.html">More</A>

<p>
[D] Deep Learning 1991-2013 - our deep NNs have, so far, won 9 important contests in pattern recognition, image segmentation, object detection.
<A HREF="http://www.idsia.ch/~juergen/deeplearning.html">More</A>, also under <A HREF="http://www.deeplearning.it">www.deeplearning.it</A>

<p>
[E] 2011: First superhuman visual pattern recognition in an official international competition (with secret test set known only to the organisers) - twice better than humans, three times better than the closest artificial NN competitor, six times better than the best non-neural method.  <A HREF="http://www.idsia.ch/~juergen/superhumanpatternrecognition.html">More</A>

<p>
[F] 2012: First Deep Learner to win a contest on object detection in large images: our deep NNs won both the <em>ICPR 2012 Contest</em> and the <em>MICCAI 2013 Grand Challenge</em> on Mitosis Detection (important for cancer prognosis etc, perhaps the most important application area of Deep Learning).      <A HREF="http://www.idsia.ch/~juergen/deeplearningwinsMICCAIgrandchallenge.html">More</A>

<p>
[G] 2012: First Deep Learner to win a pure image segmentation competition: our deep NNs won the <em>ISBI'12 Brain Image Segmentation Challenge</em> (relevant for the billion Euro brain projects in EU and US).   <A HREF="http://www.idsia.ch/~juergen/deeplearningwinsbraincontest.html">More</A>

<p>
[H] Deep LSTM recurrent NNs since 1995: <A HREF="http://www.idsia.ch/~juergen/rnn.html">More</A>

<p>
[I] Deep Evolving NNs: <A HREF="http://www.idsia.ch/~juergen/evolution.html">More</A>

<p>
[J] Deep Reinforcement Learning NNs: <A HREF="http://www.idsia.ch/~juergen/rl.html">More</A>


<p>
[K] Compressed NN Search for Huge RNNs: <A HREF="http://www.idsia.ch/~juergen/compressednetworksearch.html">More</A>





<br>
<br><font color=#ffffff>.</font>

</small></tr></td></table></td>




</table>
</body>
