                                                            D                     D                     D                     D
                                          Model               1                     2                     3                     4
                                                    76-100    102-126     76-100    102-126     76-100    102-126     76-100    102-126
                                              −
                                           SA        100.0      98.88      14.52      0.006      32.62      5.50       42.94      9.080
                                              +
                                           SA        100.0      100.0      93.34      58.82      93.18      66.88      93.78      72.38
                                         LSTM−       100.0      99.64      88.30      73.20      85.16      65.06      78.92      60.24
                                         LSTM+       100.0      100.0      87.00      70.90      82.44      63.56      76.66      55.90
                             Table 1: Performance of SA and LSTM variants on Dyck-n languages for different sequence lengths.
                                                                                           4forD ,6forD ,8forD ),yˆ ∈ {0,1}andy are
                                                                                                    2           3            4    i                   i
                         25                              40                                the target and prediction for label i, respectively.
                                                         35
                         20                              30
                        h15                             h25
                        t                               t                                  3.1     Evaluation
                        p                               p
                        e                               e20
                        D10                             D
                                                         15                                                                               +           −
                                                                                           Table 1 compares the accuracy of SA               andSA on
                          5                              10                                D , D , D , and D languages. For both models,
                                                         5                                    1    2     3          4
                          0                                                                the performance on D is almost perfect (> 98%)
                           0   10   20  30   40  50          80  90  100  110 120                                       1
                                     Length                         Length                 and does not show any degradation with increase
                                                                                                                                              −
                      Figure 2: Joint distribution of D language based on                  in sequence length. The accuracy of SA                on D2 is
                                                              2
                      the length and depth of sequences in training (blue) and             14.52%forsequences with length 76-100 and com-
                      evaluation (red). The top and right axes also show the               pletely fails beyond it. In comparison, the perfor-
                      marginal distribution for length and depth respectively.             manceofSA+onD issigniﬁcantlybetter,93.34%
                                                                                                                     2
                                                                                           and 58.82% for sequences of length 76-100 and
                                                                                                                                                          −
                         Figure 2 shows the distribution of length and                     102-126, respectively. The performance of SA
                                                                                           improves on D and D , compared to D , with
                      depth of D sequences in training and evaluation.                                          3          4                       2
                                    2
                      For higher Dyck languages (Dn>2), the training                       an accuracy of 32.62% and 42.94%, respectively
                      and evaluation datasets have similar depth and                       for sequences of length 76-100. The performance
                                                                                           of SA+ is nearly constant (∼93%) on D                        for
                      length distributions because the PCFG give equal                                                                            n≥2
                      probability to different pairs of parentheses and                    sequences of length 76-10 but there is signiﬁcant
                                                                                           improvement from D (58.82%) to D (66.88%)
                      the total probability for rules of the form S −→ (S),                                             2                    3
                                                                                           and D (72.38%) for sequences of length 102-126.
                      S −→ [S], ... is 0.5. We perform experiments on D ,                          4
                                                                                     1
                      D ,D ,andD languages. Notethatthenumberof                                Unlike SA, the performance of LSTM degrades
                        2     3         4
                      pairs of parentheses cannot be increased arbitrarily                 after the addition of the starting symbol, with the
                      withoutrequiringmodiﬁcationstotheexperimental                        biggest drop (4.3%) on D for sequence length of
                                                                                                                             4
                      setup: We varied the length of sequences during                      102-106. The starting symbol has enabled SA to
                      training from 2 to 50, which could contain at most                   attend to the correct preceding token, but it has
                      25 different pairs.                                                  been ineffective for LSTM. For D sequences of
                                                                                                                                         2
                         In our sequence prediction task, the input vo-                    length 102-126, LSTM− achieves an accuracy of
                                     i                                                                                                              +
                      cabulary (V ) for a D language consists of 2n+1                      73.20%, an improvement of ∼14% over SA . On
                                     n            n
                                                                                                                             +                           −
                      symbols: n pairs of brackets (or parentheses), and                   all other comparisons, SA            outperforms LSTM .
                      an additional starting symbol T whereas the out-                         We observe another interesting distinction be-
                      put vocabulary (V o) does not include the starting                   tween the two architectures.              The accuracy of
                                             n
                      symbol T. Since there might exist multiple possi-                    LSTMdeteriorates as the number of pairs of brack-
                      bilities for the next bracket in a sequence, we adopt                                                                  +            −
                                                                                           ets increases, while the accuracy of SA              and SA
                      a multi-label classiﬁcation approach wherein the                     improves. To understand this phenomenon, we
                      outputs are encoded as a k-hot vector and the net-                   looked at the training, validation, and test sets of
                      work is optimized using the binary cross-entropy                     each language, and found that while validation
                      loss function given by                                               and test sets of each D language almost always
                                                                                                                          n
                               |V o|                                                       (> 99%) includes sequences of n different brack-
                                 n
                              X
                                    n                                         o            ets, the training set could include sequences of
                       L=             yˆ log(y )+(1−yˆ) log(1−y ) , (2)
                                        i        i           i              i              1 ≤ m < ntypesofbrackets. This implies that SA
                               i=1                                                         beneﬁts from data augmentation with sequences
                      where |V o| is the output vocabulary size (2 for D1,                 from other languages, and LSTM does not. Put dif-
                                 n
                                                                                      4303
