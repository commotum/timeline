                         Preprint, Under Review.
                                                        Table 1: SFT Training Configs
                                                               Data & Model
                                                ModelSize                   8B
                                                MaxContextWindow            8192
                                                LoRA                        False
                                                Dataset                     1024trajectories
                                                                Optimization
                                                Learning Rate               5e-6
                                                Beta (KL Coefficient)       0.1
                                                Numberofrollouts in batch   384
                                                Rollout length              16
                                                History length              16
                         custom Proximal Policy Optimization (PPO) trainer (Schulman et al., 2017) was implemented, with
                         modelweights broadcasted back to vLLM after each training epoch. The actor model was based on
                         the 8B parameter Llama 3.1 architecture, while the critic model utilized the 1B parameter Llama
                         3.3 architecture (Grattafiori et al., 2024). Similar to the SFT phase, the DeepSpeed ZeRO Stage 3
                         optimizer was employed for memory-efficient and scalable training.
                         Experiments were conducted using a total of 8 GPUs, allocating 6 GPUs for training and 2 for
                         data collection. Most experiments were executed on a node with 8xH100 GPUs and typically
                         completed within 24–48 hours. Initially, we experimented with reward penalties targeting invalid
                         actions, excessively long responses, and overly frequent planning. However, such explicit reward
                         shaping often resulted in agents refraining from planning entirely. Recognizing optimal planning
                         frequencies (”Goldilocks zones”), such as planning every four steps rather than at every opportunity,
                         weremovedexplicit reward shaping, thus allowing agents to autonomously learn optimal planning
                         frequencies. As illustrated in Figure 9, agents trained under this strategy gradually improved their
                         efficiency. Specifically, agents learned to execute plans less frequently (center panel) but with
                         increased effectiveness, leading to shorter, more concise plans (right panel), and improved overall
                         performance (left panel).
                         Figure 9: Comparison of (left) Normalized Score, (center) Planning Frequency, and (right) Plan
                         Lengthacross environment steps for two RL configurations. Both agents become more efficient
                         with time; they learn to execute plans for longer, reflected in reduced planning frequency, and also
                         generate more concise plans (reduced plan length).
                         Detailed hyperparameters for the RL experiments are provided in Table 2.
                                                                     29
