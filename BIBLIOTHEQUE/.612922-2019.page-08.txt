                     Arithmetic       expression       Many questions in                   Method                    Dev               Test
                     DROP require the model to locate multiple                                                    EM        F1      EM        F1
                     numbers in the passage and add or subtract them                       Heuristic Baselines
                     to get the ﬁnal answer. To model this process,                        Majority              0.09     1.38     0.07     1.44
                     weﬁrst extract all the numbers from the passage                       Q-only                4.28     8.07     4.18     8.59
                     and then learn to assign a plus, minus or zero for                    P-only                0.13     2.27     0.14     2.26
                     each number. In this way, we get an arithmetic                        Semantic Parsing
                     expression composed of signed numbers, which                          SynDep                9.38    11.64     8.51    10.84
                                                                                           OpenIE                8.80    11.31     8.53    10.77
                     can be evaluated to give the ﬁnal answer.                             SRL                   9.28    11.72     8.98    11.45
                        Todothis,weﬁrstapplyanotherQANetencoder                            SQuAD-styleRC
                     to M and get a new passage representation M .                         BiDAF                26.06    28.85    24.75    27.49
                           2                                                     3         QANet                27.50    30.44    25.50    28.36
                     Thenweselect an index over the concatenation of                       QANet+ELMo           27.71    30.33    27.08    29.67
                     M andM ,togetarepresentationforeachnumber                             BERT                 30.10    33.36    29.45    32.70
                        0         3
                                               th
                     in this passage. The i       number can be represented                NAQANet
                          N                                                                +QSpan               25.94    29.17    24.98    28.18
                     as h    and the probabilities of this number being
                          i                                                                +Count               30.09    33.92    30.04    32.75
                     assigned a plus, minus or zero are:                                   +Add/Sub             43.07    45.71    40.40    42.96
                                    sign                                                   Complete Model       46.20    49.24    44.07    47.01
                                                                N
                                  p      =softmax(FFN(h ))                     (8)
                                    i                           i                          Human                     -        -   94.09    96.42
                     Answer type prediction            We use a categorical            Table4: Performanceofthedifferentmodelsonourde-
                     variable to decide between the above four answer                  velopment and test set, in terms of Exact Match (EM),
                     types, with probabilities computed as:                            and numerically-focused F (§5). Both metrics are cal-
                                                                                                                      1
                                 type                         P    Q                   culated as the maximum against a set of gold answers.
                               p      =softmax(FFN([h ,h ]))                   (9)
                               Q                                                               12
                     where h is computed over Q, in a similar way as                   levels.     For example, BERT, the current state-of-
                                    P
                     wedidforh . Attest time, we ﬁrst determine this                   the-art on SQuAD, drops by more than 50 abso-
                     answer type greedily and then get the best answer                 lute F1 points. This is a positive indication that
                     from the selected type.                                           DROPisindeedachallenging reading comprehen-
                     6.2    Weakly-Supervised Training                                 sion dataset, which opens the door for tackling new
                                                                                       and complex reasoning problems on a large scale.
                     For supervision, DROP contains only the answer                       The best performance is obtained by our
                     string, not which of the above answer types is                    NAQANetmodel. Table6showsthatourgainsare
                     used to arrive at the answer. To train our model,                 obtained on the challenging and frequent number
                     we adopt the weakly supervised training method                    answer type, which requires various complex types
                     widely used in the semantic parsing literature (Be-               of reasoning. Future work may also try combining
                     rant et al., 2013a). We ﬁnd all executions that eval-             ourmodelwithBERT.Furthermore,weﬁndthatall
                     uate to the correct answer, including matching pas-               heuristic baselines do poorly on our data, hopefully
                     sage spans and question spans, correct count num-                 attesting to relatively small biases in DROP.
                     bers, as well as sign assignments for numbers. Our                Difﬁculties of building semantic parsers                  We
                     training objective is then to maximize the marginal
                     likelihood of these executions.11                                 see that all the semantic parsing baselines perform
                                                                                       quite poorly on DROP. This is mainly because of
                     7    Results and Discussion                                       our pipeline of extracting tabular information from
                     The performance of all tested models on the                       paragraphs, followed by the denotation-driven log-
                     DROPdatasetispresentedinTable4. Mostnotably,                      ical form search, can yield logical forms only for
                     all models perform signiﬁcantly worse than on                     a subset of the training data. For SRL and syntac-
                     other prominent reading comprehension datasets,                   tic dependency sentence representation schemes,
                     while human performance remains at similar high                      12Humanperformance was estimated by the authors collec-
                                                                                       tively answering 560 questions from the test set, which were
                        11Due to the exponential search space and the possible         then evaluated using the same metric as learned systems. This
                     noise, we only search the addition/subtraction of two numbers.    is in contrast to holding out one gold annotation and evaluating
                     Giventhislimitedsearchspace,thesearchandmarginalization           it against the other annotations, as done in prior work, which
                     are exact.                                                        underestimates human performance relative to systems.
                                                                                  2375
