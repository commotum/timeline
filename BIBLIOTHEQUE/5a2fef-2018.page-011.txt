             [22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
               Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing
               Systems, pages 6000–6010, 2017.
             [23] Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.
             [24] Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm.
               1999.
             [25] Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615v3, 2014.
             [26] Théophane Weber, Sébastien Racanière, David P Reichert, Lars Buesing, Arthur Guez, Danilo Jimenez
               Rezende,AdriaPuigdomènechBadia,OriolVinyals,NicolasHeess,YujiaLi,etal. Imagination-augmented
               agents for deep reinforcement learning. arXiv preprint arXiv:1707.06203, 2017.
             [27] Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
               Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
               machine translation. arXiv preprint arXiv:1406.1078, 2014.
             [28] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio. End-to-end
               attention-based large vocabulary speech recognition. In Acoustics, Speech and Signal Processing (ICASSP),
               2016 IEEE International Conference on, pages 4945–4949. IEEE, 2016.
             [29] Djoerd Hiemstra. Using language models for information retrieval. 2001.
             [30] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck:
               a high-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.
             [31] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus
               of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.
             [32] Jack W Rae, Chris Dyer, Peter Dayan, and Timothy P Lillicrap. Fast parametric learning with activation
               memorization. arXiv preprint arXiv:1803.10049, 2018.
             [33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.
               arXiv preprint arXiv:1609.07843, 2016.
             [34] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of
               language modeling. arXiv preprint arXiv:1602.02410, 2016.
             [35] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
               Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv
               preprint arXiv:1312.3005, 2013.
             [36] Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. English gigaword ﬁfth edition
               ldc2011t07. dvd. Philadelphia: Linguistic Data Consortium, 2011.
             [37] Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. How to construct deep recurrent
               neural networks. arXiv preprint arXiv:1312.6026, 2013.
             [38] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state
               with recurrent entity networks. In Fifth International Conference on Learning Representations, 2017.
             [39] Navdeep Jaitly Noam Shazeer Samy Bengio, Oriol Vinyals. Scheduled sampling for sequence prediction
               with recurrent neural networks. In Advances in Neural Information Processing Systems 28, 2015.
             [40] EdouardGrave,ArmandJoulin,andNicolasUsunier. Improvingneurallanguagemodelswithacontinuous
               cache. arXiv preprint arXiv:1612.04426, 2016.
             [41] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Convolutional sequence modeling revisited. 2018.
             [42] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
               convolutional networks. arXiv preprint arXiv:1612.08083, 2016.
             [43] Stephen Merity, Nitish Shirish Keskar, James Bradbury, and Richard Socher. Scalable language modeling:
               Wikitext-103 on a single gpu in 12 hours. 2018.
             [44] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
               arXiv:1412.6980, 2014.
                                  11
