# Depth-Adaptive Transformer (2020)
Source: 704ee2-2020.pdf

## Core reasons
- Proposes Transformer models that adapt the number of layers per input (variable depth) to trade speed and accuracy at inference time.
- Enables predictions at different stages of the network and predicts how much computation is required, indicating an adaptive computation/halting mechanism.

## Evidence extracts
- "                        In this paper, we propose Transformers which adapt the number of layers to each input in order to
                        achieve a good speed-accuracy trade off at inference time. We extend Graves (2016; ACT) who" (Introduction)
- "                                hard to process. In this paper, we train Transformer models which can make out-
                                put predictions at different stages of the network and we investigate different ways
                                to predict how much computation is required for a particular sequence. Unlike" (Abstract)

## Classification
Class name: Computation & Reasoning Mechanism Proposal
Class code: 3

$$
\boxed{3}
$$
