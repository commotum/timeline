                                    Repeated examples help learn arithmetic
                                    FrancÂ¸ois Charton                             Julia Kempe
                               FAIR, Meta & Ecole des Ponts       FAIR, Meta & NYUCDSandCourantInstitute
                                   fcharton@meta.com                            kempe@meta.com
                                                                 Abstract
                                 Westudy small transformers trained on two problems of arithmetic: the greatest
                                 commondivisor(GCD)andmodularmultiplication,andshowthatmodelstrained
                                 on a limited set of repeated examples achieve better performance than models
                                 trained from unlimited data. In fact, modular multiplication is only learned on
                                 small training sets. We also demonstrate that two-set training - repeated use of
                                 a small random subset of examples, along normal sampling on the rest of the
                                 training set - provides for faster learning and better performance. These experi-
                                 mentshighlightthatthebenefitsofrepetitioncanoutweighthoseofdatadiversity;
                                 and shed light on the still poorly understood interplay between generalization and
                                 memorization in deep learning.
                         1   Introduction
                         Whentrainingneuralnetworks,ithasbecomecustomarytousethelargestandmostdiversedatasets
                         available, and to limit example reuse as much as possible. On problems of arithmetic, the training
                         data is easy to generate in very large quantities, sometimes even on the fly. For this reason, mod-
                         els are trained on very large sets of single-use examples, with very low repetition. In this paper,
                         weinvestigate the low data regime: transformers trained on smaller sets of repeated examples. We
                         consider two problems: the greatest common divisor (GCD, Charton (2024)) of two integers, and
                         multiplication modulo 67 of two positive integers between 1 and a million, and measure model per-
                         formance for different data budgets (DB, the number of distinct examples in the training set), and
                         training budgets (TB, the total number of training examples). On the GCD problem, we show that,
                         for a given training budget, models trained on small data budgets (but large enough to avoid over-
                         fitting) outperform models trained on large or unlimited data budgets. We also show that modular
                         multiplication is only learned for small data budgets. These experiments demonstrate the benefit of
                         training on repeated examples, challenging the common idea that one or two epochs is all we need.
                         Pushing this observation further, we demonstrate that for a given data budget, model performance
                         can be greatly improved by two-set training: selecting at random a small subset of training exam-
                         ples, and repeating them more often during training. This two-set effect is all the more surprising as
                         the repeated examples are not curated, and only differ from the rest of the training sample by their
                         frequency of reuse. In fact, ablation experiments indicate that the performance of two-set training
                         cannot be improved by curating the set of repeated examples, or refreshing it as training proceeds.
                         Wealsoshowthatmixingrepeatedandnon-repeated examples in the same mini-batches is a neces-
                         sary step for the two-set effect to appear.
                         The benefits of repetition are significant in both problems, but come in different flavors. For GCD,
                         repetition allows for better performance and faster learning. For modular multiplication, it unlocks
                         anemergentcapability: without repetition, the model does not learn. We believe these findings have
                         profound implications and should lead to a paradigm shift where the training set size becomes a
                         mere hyper-parameter, not solely governed by the availability of data and the belief that more is
                         always better.
                         38th Conference on Neural Information Processing Systems (NeurIPS 2024).
