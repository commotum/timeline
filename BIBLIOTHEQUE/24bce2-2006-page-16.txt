       1542               G.Hinton,S.Osindero,andY.-W.Teh
       Figure6: The125testcasesthatthenetworkgotwrong.Eachcaseislabeledby
       the network’s guess. The true classes are arranged in standard scan order.
       (John Platt, personal communication, 2005). An almost identical result of
       1.51%wasachievedinanetthathad500unitsintheﬁrsthiddenlayerand
       300 in the second hidden layer by using “softmax” output units and a reg-
       ularizer that penalizes the squared weights by an amount carefully chosen
       usingavalidationset.Forcomparison,nearestneighborhasareportederror
       rate (http://oldmill.uchicago.edu/wilder/Mnist/) of 3.1% if all 60,000
       training cases are used (which is extremely slow) and 4.4% if 20,000 are
       used. This can be reduced to 2.8% and 4.0% by using an L3 norm.
         The only standard machine learning technique that comes close to the
       1.25%errorrateofourgenerativemodelonthebasictaskisasupportvector
       machine that gives an error rate of 1.4% (Decoste & Schoelkopf, 2002). But
       it is hard to see how support vector machines can make use of the domain-
       speciﬁctricks, like weight sharing and subsampling,whichLeCun,Bottou,
       and Haffner (1998) use to improve the performance of discriminative
