           Preprint, Under Review.
           (POGSandCrafter) to more diverse domains would further validate the generality of our approach.
           Future research could also explore more sophisticated compute allocation mechanisms, scale up our
           experiments, or investigate methods to more explicitly integrate our conceptual framework’s insights
           into novel RL algorithms.
           In summary, this paper establishes that in zero-shot evaluation, per-timestep planning or reasoning
           strategies akin to ReAct are outperformed by “Goldilocks” planning frequencies, due to instability
           that results from excessive planning or “overthinking.” By using a two-stage training methodology
           combining SFT and RL, we successfully train agents to dynamically allocate planning resources at
           test-time. This approach yields more effective and efficient behaviour compared to fixed planning
           strategies, marking a step towards more autonomous and scalable agentic systems.
           ETHICS STATEMENT
           This work studies dynamic planning in language model agents within simulated environments,
           without human subjects or personal data. While improved planning could be misused in real-world
           autonomous systems, our evaluations are confined to sandboxed benchmarks, and we believe the
           scientific benefits outweigh these risks.
           REPRODUCIBILITY STATEMENT
           Westrive to make all experiments in this paper fully reproducible. We share the anonymous codebase
           at: https://anonymous.4open.science/r/LLM-Agents-Planning. We provide further descriptions of the
           training details in Appendix C.
           REFERENCES
           Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton
            Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. Deepspeed-
            inference: Enabling efficient inference of transformer models at unprecedented scale. In SC22:
            International Conference for High Performance Computing, Networking, Storage and Analysis, pp.
            1–15, 2022. doi: 10.1109/SC41404.2022.00051.
           MaciejBesta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi,
            Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts:
            Solving elaborate problems with large language models. In Proceedings of the AAAI Conference
            onArtificial Intelligence, volume 38, pp. 17682–17690, 2024.
           Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, and Chi Wang. Steering large
            language models between code execution and textual reasoning. CoRR, abs/2410.03524, 2024.
            URLhttps://doi.org/10.48550/arXiv.2410.03524.
            ´
           RemiCoulom. Efficient selectivity and backup operators in monte-carlo tree search. In International
            conference on computers and games, pp. 72–83. Springer, 2006.
           Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anu-
            manchipalli, Kurt Keutzer, and Amir Gholami. Plan-and-act: Improving planning of agents
            for long-horizon tasks. arXiv preprint arXiv:2503.09572, 2025.
           Meta Fundamental AI Research Diplomacy Team (FAIR)†, Anton Bakhtin, Noam Brown, Emily
            Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu,
            et al. Human-level play in the game of diplomacy by combining language models with strategic
            reasoning. Science, 378(6624):1067–1074, 2022.
           Chrisantha Fernando, Dylan Sunil Banarse, Henryk Michalewski, Simon Osindero, and Tim
               ¨
            Rocktaschel. Promptbreeder: Self-referential self-improvement via prompt evolution, 2024.
            URLhttps://openreview.net/forum?id=HKkiX32Zw1.
                               10
