E. T. JAYNES

more general applicability than conventional arguments
would lead one to suppose. In the problem of prediction,
the maximization of entropy is not an application of a
law of physics, but merely a method of reasoning which
ensures that no unconscious arbitrary assumptions
have been introduced.

APPENDIX A. ENTROPY OF A PROBABILITY
DISTRIBUTION

The variable x can assume the discrete values
(1,°°°%,). Our partial understanding of the processes
which determine the value of « can be represented by
assigning corresponding probabilities (f1,---,p,). We
ask, with Shannon,’ whether it is possible to find any
quantity H(pi-++p.) which measures in a unique way
the amount of uncertainty represented by this proba-
bility distribution. It might at first seem very difficult
to specify conditions for such a measure which would
ensure both uniqueness and consistency, to say nothing
of usefulness. Accordingly it is a very remarkable fact
that the most elementary conditions of consistency,
amounting really to only one composition law, already
determines the function H(pi---p,) to within a con-
stant factor. The three conditions are:

(i) # is a continuous function of the pi.

(2) Tf all p; are equal, the quantity A (w)
= H(i/n,-+-,1/#) is a monotonic increasing function
of #.

(3) The composition law. Instead of giving the
probabilities of the events (@;---x,) directly, we might
group the first & of them together as a single event, and
give its probability wi= (pit---+ pz); then the next
m possibilities are assigned the total probability
w= (papit-+++ Perm), etc. When this much has been
specified, the amount of uncertainty as to the composite
events is H(w,---w,). Then we give the conditional
probabilities (f1/w1,---+,p./wi) of the ultimate events
(a1---+x.), given that the first composite event had
occurred, the conditional probabilities for the second
composite event, and so on. We arrive ultimately at
the same state of knowledge as if the (f1---p,) had
been given directly, therefore if our information measure
is to be consistent, we must obtain the same ultimate
uncertainty no matter how the choices were broken

630

down in this way. Thus, we must have

A (pi ++ Pa) =A (wr: we) + wild (pi/wr,:-- ,Px/er)
well (pigr/ We, ++ Prim/Wi)bes+.  (A-D)

The weighting factor w; appears in the second term
because the additional uncertainty A (pi/wi,-+-,px/w1)
is encountered only with probability w:. For example,
H(i/2, 1/3, 1/6)=H (1/2, 1/2)+-44 (2/3, 1/3).

From condition (1), it is sufficient to determine H
for all rational values

p= nif d, ni,

with », integers. But then condition (3) imphes that H
is determined already from the symmetrical quantities
A(n). For we can regard a choice of one of the alter-
natives (41--+%,) as a first step in the choice of one of

equally likely alternatives, the second step of which is
also a choice between #,; equally likely alternatives.
As an example, with »=3, we might. choose (01,'2,/3)
= (3,4,2). For this case the composition law becomes

342 3 4 2
a(-- =) +-4G) +A) 4 AQ)=A0),
999 9 9 9

In general, it could be written

A (firs pa) +2: pA (= AQ).

In particular, we could choose all #; equal to #, where-
upon (A-2) reduces to

(A-2)

A(m)+A (2) = A (mn). (A-3)
Evidently this is solved by setting
A(n)=K Inn, (A-4)

where, by condition (2), A>0O. For a proof that (A-4)
is the only solution of (A-3), we refer the reader to
Shannon’s paper.* Substituting (A-4) into (A-2), we
have the desired result,
