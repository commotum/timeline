623

It is now evident how to solve our problem; in making
inferences on the basis of partial information we must
use that probability distribution which has maximum
entropy subject to whatever is known. This is the only
unbiased assignment we can make; to use any other
would amount to arbitrary assumption of information
which by hypothesis we do not have. To maximize
(2-3) subject to the constraints (2-1) and (2-2), one
introduces Lagrangian multipliers A, u, in the usual

way, and obtains the result
p= ees (23) | (2-4)

The constants A, » are determined by substituting into
(2-1) and (2-2). The result may be written in the form

0
(/@))=—— InZn), (25)

du
, A=InZ (nu), (2-6)
where ZWw=d; ees fei) (2-7)

will be called the partition function.
This may be generalized to any number of functions
f(x): given the averages

(P(2))= Dos pif (aa), (2-8)
form the partition function
=S exp{—[AsfiCr +++ Amfm (aa) }. (2-9)

Then the maximum-entropy probability distribution is
given by

in which the constants are determined from
re
(fr(%))=—— InZ, (2-11)
an,
ho=inZ. (2-12)

The entropy of the distribution (2-10) then reduces to
Sinax™ Roba (2))-+ nas +n fm (X))s (2-13)

where the constant K in (2-3) has been set equal to
unity. The variance of the distribution of f,(«) is found
to be

&
Wf=(fP)— f= (nz). (2-14)
oA

Tn addition to its dependence on x, the function /, may
contain other parameters ai, a2, ---, and it is easily
shown that the maximum-entropy estimates of the
derivatives are given by
Of,
—-——~InZ.
Oa, Ap OG,

(2-15)

INFORMATION THEORY AND STATISTICAL MECHANICS

The principle of maximum entropy may be regarded
as an extension of the principle of insufficient reason
(to which it reduces in case no information is given
except enumeration of the possibilities x,), with the
following essential difference. The maximum-entropy
distribution may be asserted for the positive reason
that it is uniquely determined as the one which is
maximally noncommittal with regard to missing infor-
mation, instead of the negative one that there was no
reason to think otherwise. Thus the concept of entropy
supplies the missing criterion of choice which Laplace
needed to remove the apparent arbitrariness of the
principle of insufficient reason, and in addition it shows
precisely how this principle is to be modified in case
there are reasons for “thinking otherwise.”

Mathematically, the maximum-entropy distribution
has the important property that no possibility is
ignored; it assigns positive weight to every situation
that is not absolutely excluded by the given information.
This is quite similar in effect to an ergodic property.
in this connection it is interesting to note that prior to
the work of Shannon other information measures had
been proposed”:® and used in statistical inference,
although in a different way than in the present paper.
In particular, the quantity —}0 p? has many of the
qualitative properties of Shannon’s information meas-
ure, and in many cases leads to substantially the same
results. However, it is much more difficult to apply in
practice. Conditional maxima of —)>° ~ cannot be
found by a stationary property involving Lagrangian
multipliers, because the distribution which makes this
quantity stationary subject to prescribed averages does
not in general satisfy the condition ;20. A much more
important reason for preferring the Shannon measure
is that it is the only one which satisfies the condition of
consistency represented by the composition law (Ap-
pendix A). Therefore one expects that deductions made
from any other information measure, if carried far
enough, will eventually lead to contradictions.

3. APPLICATION TO STATISTICAL MECHANICS

It will be apparent from the equations in the pre-
ceding section that the theory of maximum-entropy
inference is identical in mathematical form with the
rules of calculation provided by statistical mechanics.
Specifically, let the energy levels of a system be

E;(a1,02, i );

where the external parameters a; may include the
volume, strain tensor applied electric or magnetic
fields, gravitational potential, etc. Then if we know
only the average energy (~), the maximum-entropy
probabilities of the levels #, are given by a special case
of (2-16), which we recognize as the Boltzmann distri-
bution. This observation really completes our derivation

WR, A. Fisher, Proc. Cambridge Phil. Soc. 22, 700 (1925),
8 J, L. Doob, Trans. Am. Math. Soc. 39, 410 (1936),
