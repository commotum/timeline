627

entropy inference is one in which we choose the broadest
possible probability distribution over the microscopic
states, compatible with the initial data. Evidently, such
sharp distributions for macroscopic quantities can
emerge only if it is true that for each of the overwhelm-
ing majority of those states to which appreciable weight
is assigned, we would have the same macroscopic
behavior. We regard this, not merely as an interesting
side remark, but as the essential fact without which
statistical mechanics could have no experimental va-
lidity, and indeed without which matter would have no
definite macroscopic properties, and experimental
physics would be impossible. It is this principle of
“macroscopic uniformity” which provides the objective
content of the calculations, not the probabilities per se.
Because of it, the predictions of the theory are to a
large extent independent of the probability distributions
over microstates. For example, if we choose at random
one out of each 10° of the possible states and arbi-
trarily assign zero probability to all the others, this
would in most cases have no discernible effect on the
macroscopic predictions.

Consider now the case where the theory makes
definite predictions and they are not borne out by
experiment, This situation cannot be explained away
by concluding that the initial information was not
sufficient to lead to the correct prediction; if that were
the case the theory would not have given a sharp
distribution at all. The most reasonable conclusion in
this case is that the enumeration of the different
possible states (ie, the part of the theory which
involves our knowledge of the laws of physics) was not
correctly given. Thus, experimental proof that a defintie
prediction ts incorrect gives evidence of the existence of new
laws of physics. The failures of classical statistical
mechanics, and their resolution by quantum theory,
provide several examples of this phenomenon.

Although the principle of maximum-entropy inference
appears capable of handling most of the prediction
problems of statistical mechanics, it is to be noted that
prediction is only one of the functions of statistical
mechanics. Equally important is the problem of inter-
pretation; given certain observed behavior of a system,
what conclusions can we draw as to the microscopic
causes of that behavior? To treat this problem and
others like it, a different theory, which we may call
objective statistical mechanics, is needed. Considerable
semantic confusion has resulted from failure to distin-
guish between the prediction and interpretation prob-
lems, and attempting to make a single formalism do
for both.

In the problem of interpretation, one will, of course,
consider the probabilities of different states in the
objective sense; i.e., the probability of state is the
fraction of the time that the system spends in state x.
It is readily seen that one can never deduce the ob-
jective probabilities of individual states from macro-
scopic measurements. There will be a great number of

INFORMATION THEORY AND STATISTICAL

MECHANICS

different probability assignments that are indistin-
guishable experimentally; very severe unknown con-
straints on the possible states could exist. We see that,
although it is now a relevant question, metric transi-
tivity is far from necessary, either for justifying the
rules of calculation used in prediction, or for interpreting
observed behavior. Bohm and Schiitzer!” have come to
similar conclusions on the basis of entirely different
arguments.

5. GENERALIZED STATISTICAL MECHANICS

In conventional statistical mechanics the energy
plays a preferred role among all dynamical quantities
because it is conserved both in the time development
of isolated systems and in the interaction of different
systems. Since, however, the principles of maximum-
entropy inference are independent of any physical
properties, it appears that in subjective statistical
mechanics all measurable quantities may be treated on
the same basis, subject to certain precautions. To
exhibit this equivalence, we return to the general
problem of maximum-entropy inference of Sec, 2, and
consider the effect of a small change in the problem.
Suppose we vary the functions f,(«) whose expectation
values are given, in an arbitrary way; 6f,(x;) may be
specified independently for each value of k and 7. In
addition we change the expectation values of the f, in
a manner independent of the 6/,; Le., there is no
relation between &(f,) and (éf;,). We thus pass from
one maximum-entropy probability distribution to a
slightly different one, the variations in probabilities 69;
and in the Lagrangian multipliers 6A, being determined
from the 6(f;) and 6f;,(x,) by the relations of Sec. 2.
How does this affect the entropy? The change in the
partition function (2-9) is given by

Bro= 4 InZ= — PUL BAK fa) tro fey], % (5-4)
and therefore, using (2-13),
5S = Din AL 6Cfi— fed]
= Din A860: (5-2)
The quantity
5Qn= 5 fi) — (OSe) (5-3)

provides a generalization of the notion of infinitesimal
heat supplied to the system, and might be called the
“heat of the &th type.” If f: is the energy, 6O; is the
heat in the ordinary sense. We see that the Lagrangian
multiplier A; is the integrating factor for the &th type
of heat, and therefore it is possible to speak of the kth
type of temperature. However, we shall refer to \; as
the quantity “statistically conjugate” to fi, and use
the terms “heat” and “‘temperature” only in their
conventional sense. Up to this point, the theory is
completely symmetrical with respect to all quantities fy.

“1. Bohm and W. Schiitzer, Nuovo cimento, Suppl. IF, 1004
(1955).
