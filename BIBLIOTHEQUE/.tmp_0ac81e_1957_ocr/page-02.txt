621

trary assumptions, and (c) automatically include an
explanation of nonequilibrium conditions and _ irre-
versible processes as well as those of conventional
thermodynamics, since equilibrium thermodynamics is
merely an ideal limiting case of the behavior of matter.

It might appear that condition (b) is too severe,
since we expect that a physical theory will involve
certain unproved assumptions, whose consequences are
deduced and compared with experiment. For example,
in the statistical mechanics of Gibbs? there were several
difficulties which could not be understood in terms of
classical mechanics, and before the models which he
constructed could be made to correspond to the observed
facts, it was necessary to incorporate into them addi-
tional restrictions not contained in the laws of classical
mechanics, First was the “freezing up” of certain
degrees of freedom, which caused the specific heat of
diatomic gases to be only % of the expected value.
Secondly, the paradox regarding the entropy of com-
bined systems, which was resolved only by adoption of
the generic instead of the specific definition of phase,
an assumption which seems impossible to justify in
terms of classical notions.* Thirdly, in order to account
for the actual values of vapor pressures and equilibrium
constants, an additional assumption about a natural
unit of volume (4°) of phase space was needed.
However, with the development of quantum mechanics
the originally arbitrary assumptions are now seen as
necessary consequences of the laws of physics. This
suggests the possibility that we have now reached a
state where statistical mechanics is no longer dependent
on physical hypotheses, but may become merely an
example of statistical inference,

That the present may be an opportune time to
re-examine these questions is due to two recent de-
velopments. Statistical methods are being applied to a
variety of specific phenomena involving irreversible
processes, and the mathematical methods which have
proven successful have not yet been incorporated into
the basic apparatus of statistical mechanics. In addition,
the development of information theory has been felt
by many people to be of great significance for statistica]
mechanics, although the exact way in which it should
be applied has remained obscure. In this connection it

2J. W. Gibbs, Elementary Principles in Statistical Mechanics
(Longmans Green and Company, New York, 1928), Vol. IT of
collected works.

3 We may note here that although Gibbs (reference 2, Chap.
XV) started his discussion of this question by saying that the
generic definition ‘‘seems in accordance with the spirit of the
statistical method,” he concluded it with, “The perfect similarity
of several particles of a system will not in the least interfere with
the identification of a particular particle in one case with a
particular particle in another. The question is one to be decided
in accordance with the requirements of practical convenience in
the discussion of the problems with which we are engaged.”

‘C. E. Shannon, Bell System Tech. J. 27, 379, 623 (1948);
these papers are reprinted in C. E, Shannon and W. Weaver,
The Mathematical Theory of Communication (University of
Illinois Press, Urbana, 1949).

INFORMATION THEORY AND STATISTICAL MECHANICS

is essential to note the following. The mere fact that
the same mathematical expression —)}. p; logp; occurs
both in statistical mechanics and in information theory
does not in itself establish any connection between
these fields. This can be done only by finding new
viewpoints from which thermodynamic entropy and
information-theory entropy appear as the same concept.
In this paper we suggest a reinterpretation of statistical
mechanics which accomplishes this, so that information
theory can be applied to the problem of justification of
statistical mechanics. We shall be concerned with the
prediction of equilibrium thermodynamic properties,
by an elementary treatment which involves only the
probabilities assigned to stationary states. Refinements
obtainable by use of the density matrix and discussion
of irreversible processes will be taken up in later papers.

Section 2 defines and establishes some of the ele-
mentary properties of maximum-entropy inference, and
in Secs. 3 and 4 the application to statistical mechanics
is discussed. The mathematical facts concerning maxi-
mization of entropy, as given in Sec. 2, were pointed
out long ago by Gibbs. In the past, however, these
properties were given the status of side remarks not
essential to the theory and not providing in themselves
any justification for the methods of statistical me-
chanics. The feature which was missing has been
supplied only recently by Shannon‘ in the demon-
stration that the expression for entropy has a deeper
meaning, quite independent of thermodynamics. This
makes possible a reversal of the usual line of reasoning in
statistical mechanics. Previously, one constructed a
theory based on the equations of motion, supplemented
by additional hypotheses of ergodicity, metric transi-
tivity, or equal @ priori probabilities, and the identifi-
cation of entropy was made only at the end, by com-
parison of the resulting equations with the laws of
phenomenological thermodynamics. Now, however, we
can take entropy as our starting concept, and the fact
that a probability distribution maximizes the entropy
subject to certain constraints becomes the essential fact
which justifies use of that distribution for inference.

The most important consequence of this reversal of
viewpoint is not, however, the conceptual and mathe-
matical simplification which results. In freeing the
theory from its apparent dependence on physical
hypotheses of the above type, we make it possible to
see statistical mechanics in a much more general light.
Its principles and mathematical methods become
available for treatment of many new physical problems.
Two examples are provided by the derivation of Siegert’s
“pressure ensemble’’ and treatment of a nuclear polari-
zation effect, in Sec. 5.

2. MAXIMUM-ENTROPY ESTIMATES

The quantity x is capable of assuming the discrete
values x; (t=1,2 ---,z). We are not given the corre-
sponding probabilities »;; all we know is the expectation
