E. T.

of the conventional rules of statistical mechanics as an
example of statistical inference; the identification of
temperature, free energy, etc., proceeds in a familiar
manner,! with results summarized as

d= (1/kT), (3-1)
U—TS=F(T ,e1,02,---)=—kT inZ(Tyorje,---), (3-2)
oF
S=—-—=—-k>D p; \np,, (3-3)
oT i
fe)
B;=kT—— InZ. (3-4)
Oa,

The thermodynamic entropy is identical with the
information-theory entropy of the probability distri-
bution except for the presence of Boltzmann’s con-
stant.!® The “forces” 8; include pressure, stress tensor,
electric or magnetic moment, etc., and Eqs. (3-2),
(3-3), (3-4) then give a complete description of the
thermodynamic properties of the system, in which the
forces are given by special cases of (2-15); le., as
maximum-entropy estimates of the derivatives
(0E;/ dar).

In the above relations we have assumed the number of
molecules of each type to be fixed. Now let 2; be the
number of molecules of type 1, #2 the number of type
2, etc. Hf the ~, are not known, then a possible “state”
of the system requires a specification of all the x, as well
as a particular energy level E;(ayw2--+|12---). If we
are given the expectation values

(E), (m1), (a). 0s

then in order to make maximum-entropy inferences,
we need to form, according to (2-9), the partition
function

nmingess 7

+--+ BE,(on! 2) ]},

and the corresponding maximum-entropy distribution
(2-10) is that of the “quantum-mechanical grand
canonical ensemble;” the Eqs. (2-11) fixing the con-
stants, are recognized as giving the relation between
the chemical potentials

ME. Schrédinger, Statistical Thermodynamics (Cambridge
University Press, Cambridge, 1948).

15 Boltzmann’s constant may be regarded as a correction factor
necessitated by our custom of measuring temperature in arbitrary
units derived from the freezing and boiling points of water. Since
the product T'S must have the dimensions of energy, the units in
which entropy is measured depend on those chosen for tempera-
ture. It would be convenient in general arguments to define an
“absolute cgs unit” of temperature such that Boltzmann’s
constant is made equal to unity. Then entropy would become
dimensionless (as the considerations of Sec. 2 indicate it should be),
and the temperature would be equal to twice the average energy

per degree of freedom; it is, of course, just the “modulus” © of
Gibbs.

(3-5)

JAYNES

624

and the (#,):
(ni) = OF /Ou3, (3-7)

where the free-energy function F= — R&T Ao, and Ao= InZ
is called the “grand potential.’?® Writing out (2-13)
for this case and rearranging, we have the usual
expression

F(T onerg: + + pape - +)
= (E)— TS+ uikny)+2ln2)-+ ort (3-8)

It is interesting to note the ease with which these
rules of calculation are set up when we make entropy
the primitive concept. Conventional arguments, which
exploit all that is known about the laws of physics, in
particular the constants of the motion, lead to exactly
the same predictions that one obtains directly from
maximizing the entropy. In the light of information
theory, this can be recognized as telling us a simple
but important fact: there is nothing in the general laws
of motion that can provide us with any additional infor-
mation about the state of a system beyond what we have
obtained from measuremeni. This refers to interpretation
of the state of a system at time ¢ on the basis of meas-
urements carried out at time ¢. For predicting the course
of time-dependent phenomena, knowledge of the equa-
tions of motion is of course needed. By restricting our
attention to the prediction of equilibrium properties as
in the present paper, we are in effect deciding at the
outset that the only type of initial information allowed
will be values of quantities which are observed to be
constant in time. Any prior knowledge that these
quantities would be constant (within macroscopic
experimental error) in consequence of the laws of
physics, is then redundant and cannot help us in
assigning probabilities.

This principle has interesting consequences. Suppose
that a super-mathematician were to discover a new
class of uniform integrals of the motion, hitherto
unsuspected. In view of the importance ascribed to
uniform integrals of the motion in conventional sta-
tistical mechanics, and the assumed nonexistence of
new ones, one might expect that our equations would
be completely changed by this development. This would
not be the case, however, unless we also supplemented
our prediction problem with new experimental data
which provided us with some information as to the
likely values of these new constants. Even if we had a
clear proof that a system is nol metrically transitive, we
would still have no rational basis for excluding any region
of phase space that is allowed by the information available
te us. In its effect on our ultimate predictions, this fact
is equivalent to an ergodic hypothesis, quite independ-
ently of whether physical systems are in fact ergodic.

This shows the great practical convenience of the
subjective point of view. If we were attempting to
establish the probabilities of different states in the

16 TD). ter Haar, Elements of Statistical Mechanics (Rinehart and
Company, New York, 1954), Chap. 7.
