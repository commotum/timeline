# JFB: Jacobian-Free Backpropagation for Implicit Networks (2022)
Source: 875857-2022.pdf

## Core reasons
- Introduces Jacobian-free backpropagation (JFB) as a method for backpropagating through implicit networks, i.e., a training/optimization contribution.
- Defines the method by changing how gradients are computed (omitting the Jacobian term and using a preconditioned gradient descent form).

## Evidence extracts
- "We present a simple way to backpropagate with implicit networks, called Jacobian-free backprop (JFB)." (p. 3)
- "our scheme backpropagates by omitting the Jacobian term, resulting in a form of preconditioned gradient descent." (p. 2)

## Classification
Class name: ML Foundations & Principles
Class code: 5

$$
\boxed{5}
$$
