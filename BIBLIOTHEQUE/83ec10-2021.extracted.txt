                SwinTransformer: Hierarchical Vision Transformer using Shifted Windows
                                          *
                                       1,2†                1,3†*              1*             1*‡                 1,4†
                               ZeLiu          Yutong Lin            YueCao         HanHu            Yixuan Wei
                                                               1                  1                   1
                                               ZhengZhang           Stephen Lin        Baining Guo
                            1Microsoft Research Asia          2University of Science and Technology of China
                                            3Xian Jiaotong University          4Tsinghua University
                        {v-zeliu1,v-yutlin,yuecao,hanhu,v-yixwe,zhez,stevelin,bainguo}@microsoft.com
                                     Abstract
                This paper presents a new vision Transformer, called
             SwinTransformer,thatcapablyservesasageneral-purpose
             backbone for computer vision.     Challenges in adapting
             Transformer from language to vision arise from differences
             between the two domains, such as large variations in the
             scale of visual entities and the high resolution of pixels
             in images compared to words in text.     To address these
             differences, we propose a hierarchical Transformer whose
             representation is computed with Shifted windows.      The
             shifted windowing scheme brings greater efﬁciency by lim-
             iting self-attention computation to non-overlapping local       Figure 1. (a) The proposed Swin Transformer builds hierarchical
             windows while also allowing for cross-window connection.        feature maps by merging image patches (shown in gray) in deeper
             This hierarchical architecture has the ﬂexibility to model      layers and has linear computation complexity to input image size
             at various scales and has linear computational complexity       due to computation of self-attention only within each local win-
             with respect to image size. These qualities of Swin Trans-      dow (shown in red). It can thus serve as a general-purpose back-
             former make it compatible with a broad range of vision          bone for both image classiﬁcation and dense recognition tasks.
             tasks, including image classiﬁcation (87.3 top-1 accuracy       (b) In contrast, previous vision Transformers [19] produce fea-
             on ImageNet-1K) and dense prediction tasks such as object       ture maps of a single low resolution and have quadratic compu-
                                                                             tation complexity to input image size due to computation of self-
             detection (58.7 box AP and 51.1 mask AP on COCO test-           attention globally.
             dev) and semantic segmentation (53.5 mIoU on ADE20K
             val). Its performance surpasses the previous state-of-the-      tureshaveevolvedtobecomeincreasinglypowerfulthrough
             art by a large margin of +2.7 box AP and +2.6 mask AP on        greater scale [27, 69], more extensive connections [31], and
             COCO,and+3.2mIoUonADE20K,demonstratingthepo-                    more sophisticated forms of convolution [64, 17, 75]. With
             tential of Transformer-based models as vision backbones.        CNNsservingasbackbonenetworks for a variety of vision
             The hierarchical design and the shifted window approach         tasks, these architectural advances have led to performance
             also prove beneﬁcial for all-MLP architectures. The code        improvements that have broadly lifted the entire ﬁeld.
             andmodelsarepubliclyavailableathttps://github.                     Ontheotherhand,theevolutionofnetworkarchitectures
             com/microsoft/Swin-Transformer.                                 in natural language processing (NLP) has taken a different
                                                                             path, where the prevalent architecture today is instead the
             1. Introduction                                                 Transformer [58]. Designed for sequence modeling and
                                                                             transduction tasks, the Transformer is notable for its use
                Modeling in computer vision has long been dominated          of attention to model long-range dependencies in the data.
             by convolutional neural networks (CNNs). Beginning with         Its tremendous success in the language domain has led re-
             AlexNet [35] and its revolutionary performance on the           searchers to investigate its adaptation to computer vision,
             ImageNet image classiﬁcation challenge, CNN architec-           whereithasrecentlydemonstratedpromisingresultsoncer-
                                                                             tain tasks, speciﬁcally image classiﬁcation [19] and joint
                *Equal contribution. †Interns at MSRA. ‡Contact person.      vision-language modeling [43].
                                                                         10012
                    In this paper, we seek to expand the applicability of
                Transformer such that it can serve as a general-purpose
                backbone for computer vision, as it does for NLP and
                as CNNs do in vision. We observe that signiﬁcant chal-
                lenges in transferring its high performance in the language
                domain to the visual domain can be explained by differ-
                ences between the two modalities.              One of these differ-
                ences involves scale. Unlike the word tokens that serve
                as the basic elements of processing in language Trans-                          Figure 2. An illustration of the shifted window approach for com-
                formers, visual elements can vary substantially in scale, a                     puting self-attention in the proposed Swin Transformer architec-
                problem that receives attention in tasks such as object de-                     ture.  In layer l (left), a regular window partitioning scheme is
                tection [38, 49, 50]. In existing Transformer-based mod-                        adopted, and self-attention is computed within each window. In
                els [58, 19], tokens are all of a ﬁxed scale, a property un-                    the next layer l + 1 (right), the window partitioning is shifted, re-
                suitable for these vision applications. Another difference                      sulting in new windows. Theself-attentioncomputationinthenew
                                                                                                windows crosses the boundaries of the previous windows in layer
                is the much higher resolution of pixels in images com-                          l, providing connections among them.
                pared to words in passages of text. There exist many vi-
                sion tasks such as semantic segmentation that require dense                                     2
                                                                                                query pixels .       Our experiments show that the proposed
                prediction at the pixel level, and this would be intractable                    shifted window approach has much lower latency than the
                for Transformer on high-resolution images, as the compu-                        sliding window method, yet is similar in modeling power
                tational complexity of its self-attention is quadratic to im-                   (see Tables 5 and 6). The shifted window approach also
                age size. To overcome these issues, we propose a general-                       proves beneﬁcial for all-MLP architectures [56].
                purpose Transformer backbone, called Swin Transformer,                              The proposed Swin Transformer achieves strong perfor-
                which constructs hierarchical feature maps and has linear                       mance on the recognition tasks of image classiﬁcation, ob-
                computational complexity to image size. As illustrated in                       ject detection and semantic segmentation. It outperforms
                Figure1(a),SwinTransformerconstructsahierarchicalrep-                           the ViT / DeiT [19, 57] and ResNe(X)t models [27, 64] sig-
                resentation by starting from small-sized patches (outlined in                   niﬁcantly with similar latency on the three tasks. Its 58.7
                gray) and gradually merging neighboring patches in deeper                       box AP and 51.1 mask AP on the COCO test-dev set sur-
                Transformer layers. With these hierarchical feature maps,                       pass the previous state-of-the-art results by +2.7 box AP
                the Swin Transformer model can conveniently leverage ad-                        (Copy-paste [23] without external data) and +2.6 mask AP
                vancedtechniquesfordensepredictionsuchasfeaturepyra-                            (DetectoRS [42]). On ADE20K semantic segmentation, it
                midnetworks(FPN)[38]orU-Net[47]. Thelinearcompu-                                obtains 53.5 mIoU on the val set, an improvement of +3.2
                tational complexity is achieved by computing self-attention                     mIoUoverthepreviousstate-of-the-art(SETR[73]). Italso
                locally within non-overlapping windows that partition an                        achieves a top-1 accuracy of 87.3% on ImageNet-1K image
                image (outlined in red). The number of patches in each                          classiﬁcation.
                window is ﬁxed, and thus the complexity becomes linear                              It is our belief that a uniﬁed architecture across com-
                to image size. These merits make Swin Transformer suit-                         puter vision and natural language processing could beneﬁt
                able as a general-purpose backbone for various vision tasks,                    both ﬁelds, since it would facilitate joint modeling of vi-
                in contrast to previous Transformer based architectures [19]                    sual and textual signals and the modeling knowledge from
                whichproducefeature maps of a single resolution and have                        both domains can be more deeply shared. We hope that
                quadratic complexity.                                                           Swin Transformer’s strong performance on various vision
                    A key design element of Swin Transformer is its shift                       problems can drive this belief deeper in the community and
                of the window partition between consecutive self-attention                      encourage uniﬁed modeling of vision and language signals.
                layers, as illustrated in Figure 2.           The shifted windows               2. Related Work
                bridge the windows of the preceding layer, providing con-
                nections among them that signiﬁcantly enhance modeling                          CNNandvariants CNNs serve as the standard network
                power (see Table 4). This strategy is also efﬁcient in re-                      modelthroughout computer vision. While the CNN has ex-
                gards to real-world latency: all query patches within a win-                    isted for several decades [36], it was not until the introduc-
                dow share the same key set1, which facilitates memory ac-                       tion of AlexNet [35] that the CNN took off and became
                cess in hardware. In contrast, earlier sliding window based                     mainstream. Since then, deeper and more effective con-
                self-attention approaches [30, 46] suffer from low latency
                on general hardware due to different key sets for different                         2While there are efﬁcient methods to implement a sliding-window
                                                                                                based convolution layer on general hardware, thanks to its shared kernel
                                                                                                weights across a feature map, it is difﬁcult for a sliding-window based
                   1The query and key are projection vectors in a self-attention layer.         self-attention layer to have efﬁcient memory access in practice.
                                                                                            2
                                                                                            10013
              volutional neural architectures have been proposed to fur-            on image classiﬁcation are encouraging, but its architec-
              ther propel the deep learning wave in computer vision, e.g.,          ture is unsuitable for use as a general-purpose backbone
              VGG [48], GoogleNet [53], ResNet [27], DenseNet [31],                 network on dense vision tasks or when the input image
              HRNet [59], and EfﬁcientNet [54]. In addition to these                resolution is high, due to its low-resolution feature maps
              architectural advances, there has also been much work on              and the quadratic increase in complexity with image size.
              improving individual convolution layers, such as depth-               There are a few works applying ViT models to the dense
              wise convolution [64] and deformable convolution [17, 75].            vision tasks of object detection and semantic segmenta-
              While the CNN and its variants are still the primary back-            tion by direct upsampling or deconvolution but with rela-
              bone architectures for computer vision applications, we               tively lower performance [2, 73]. Concurrent to our work
              highlight the strong potential of Transformer-like architec-          are some that modify the ViT architecture [66, 15, 25]
              tures for uniﬁed modeling between vision and language.                for better image classiﬁcation.      Empirically, we ﬁnd our
              Our work achieves strong performance on several basic vi-             Swin Transformer architecture to achieve the best speed-
              sual recognition tasks, and we hope it will contribute to a           accuracy trade-off among these methods on image classi-
              modeling shift.                                                       ﬁcation, even though our work focuses on general-purpose
                                                                                    performance rather than speciﬁcally on classiﬁcation. An-
              Self-attention based backbone architectures            Also in-       other concurrent work [60] explores a similar line of think-
              spired by the success of self-attention layers and Trans-             ing to build multi-resolution feature maps on Transform-
              former architectures in the NLP ﬁeld, some works employ               ers. Its complexity is still quadratic to image size, while
              self-attention layers to replace some or all of the spatial con-      ours is linear and also operates locally which has proven
              volution layers in the popular ResNet [30, 46, 72]. In these          beneﬁcial in modeling the high correlation in visual sig-
              works, the self-attention is computed within a local window           nals [32, 22, 37]. Our approach is both efﬁcient and ef-
              ofeachpixeltoexpediteoptimization[30],andtheyachieve                  fective, achieving state-of-the-art accuracy on both COCO
              slightly better accuracy/FLOPs trade-offs than the counter-           object detection and ADE20K semantic segmentation.
              part ResNet architecture. However, their costly memory
              access causes their actual latency to be signiﬁcantly larger          3. Method
              than that of the convolutional networks [30]. Instead of us-          3.1. Overall Architecture
              ing sliding windows, we propose to shift windows between
              consecutive layers, which allows for a more efﬁcient imple-               AnoverviewoftheSwinTransformerarchitectureispre-
              mentation in general hardware.                                        sented in Figure 3, which illustrates the tiny version (Swin-
                                                                                    T). It ﬁrst splits an input RGB image into non-overlapping
              Self-attention/TransformerstocomplementCNNs                 An-       patches by a patch splitting module, like ViT. Each patch is
              other line of work is to augment a standard CNN architec-             treated as a “token” and its feature is set as a concatenation
              ture with self-attention layers or Transformers. The self-            oftherawpixelRGBvalues. Inourimplementation,weuse
              attention layers can complement backbones [61, 7, 3, 65,              a patch size of 4×4 and thus the feature dimension of each
              21, 68, 51] or head networks [29, 24] by providing the ca-            patch is 4 × 4 × 3 = 48. A linear embedding layer is ap-
              pability to encode distant dependencies or heterogeneous              plied on this raw-valued feature to project it to an arbitrary
              interactions. More recently, the encoder-decoder design in            dimension (denoted as C).
              Transformer has been applied for the object detection and                 Several Transformer blocks with modiﬁed self-attention
              instance segmentation tasks [8, 13, 76, 52]. Our work ex-             computation(SwinTransformerblocks)areappliedonthese
              plores the adaptation of Transformers for basic visual fea-           patch tokens. The Transformer blocks maintain the number
              ture extraction and is complementary to these works.                  of tokens (H × W ), and together with the linear embedding
                                                                                                4     4
                                                                                    are referred to as “Stage 1”.
              Transformer based vision backbones             Most related to            To produce a hierarchical representation, the number of
              our work is the Vision Transformer (ViT) [19] and its                 tokens is reduced by patch merging layers as the network
              follow-ups [57, 66, 15, 25, 60]. The pioneering work of               gets deeper. The ﬁrst patch merging layer concatenates the
              ViT directly applies a Transformer architecture on non-               features of each group of 2 × 2 neighboring patches, and
              overlapping medium-sized image patches for image clas-                applies a linear layer on the 4C-dimensional concatenated
              siﬁcation. It achieves an impressive speed-accuracy trade-            features. This reduces the number of tokens by a multiple
              off on image classiﬁcation compared to convolutional net-             of 2×2 = 4(2×downsamplingofresolution),andtheout-
              works.    While ViT requires large-scale training datasets            put dimension is set to 2C. Swin Transformer blocks are
              (i.e., JFT-300M) to perform well, DeiT [57] introduces sev-           applied afterwards for feature transformation, with the res-
              eral training strategies that allow ViT to also be effective          olution kept at H × W . This ﬁrst block of patch merging
                                                                                                      8     8
              using the smaller ImageNet-1K dataset. The results of ViT             andfeaturetransformationisdenotedas“Stage2”. Thepro-
                                                                                 3
                                                                                10014
                                                                                                            MLP              MLP
                                 Stage 1           Stage 2            Stage 3           Stage 4
                                                                                                             LN               LN
                                     Swin               Swin              Swin               Swin
              Images               Transformer       Transformer       Transformer        Transformer
                                                                                                           W-MSA            SW-MSA
                                     Block             Block              Block             Block
                                                  Patch Merging
                                                                    Patch Merging
                                                                                       Patch Merging
                        Patch Partition
                                                                                                             LN               LN
                               Linear Embedding
                                       2
                                                          2                 6                  2
                                                   (a) Architecture                                 (b) Two Successive Swin Transformer Blocks
             Figure 3. (a) The architecture of a Swin Transformer (Swin-T); (b) two successive Swin Transformer Blocks (notation presented with
             Eq. (3)). W-MSA and SW-MSA are multi-head self attention modules with regular and shifted windowing conﬁgurations, respectively.
                                                                                                              3
             cedure is repeated twice, as “Stage 3” and “Stage 4”, with      onanimageofh×wpatchesare :
             output resolutions of H × W and H × W, respectively.
                                   16    16      32    32                               Ω(MSA)=4hwC2+2(hw)2C,                       (1)
             These stages jointly produce a hierarchical representation,
             with the same feature map resolutions as those of typical                  Ω(W-MSA)=4hwC2+2M2hwC,                      (2)
             convolutional networks, e.g., VGG [48] and ResNet [27].
             As a result, the proposed architecture can conveniently re-     where the former is quadratic to patch number hw, and the
             place the backbone networks in existing methods for vari-       latter is linear when M is ﬁxed (set to 7 by default). Global
             ous vision tasks.                                               self-attention computation is generally unaffordable for a
                                                                             large hw, while the window based self-attention is scalable.
             Swin Transformer block       Swin Transformer is built by       Shifted window partitioning in successive blocks      The
             replacing the standard multi-head self attention (MSA)          window-based self-attention module lacks connections
             module in a Transformer block by a module based on              across windows, which limits its modeling power. To intro-
             shifted windows (described in Section 3.2), with other lay-     duce cross-window connections while maintaining the efﬁ-
             ers kept the same. As illustrated in Figure 3(b), a Swin        cient computationofnon-overlappingwindows,wepropose
             TransformerblockconsistsofashiftedwindowbasedMSA                ashiftedwindowpartitioningapproachwhichalternatesbe-
             module, followed by a 2-layer MLP with GELU non-                tween two partitioning conﬁgurations in consecutive Swin
             linearity in between. A LayerNorm (LN) layer is applied         Transformer blocks.
             before each MSA module and each MLP, and a residual                Asillustrated in Figure 2, the ﬁrst module uses a regular
             connection is applied after each module.                        window partitioning strategy which starts from the top-left
             3.2. Shifted Window based Self-Attention                        pixel, and the 8 × 8 feature map is evenly partitioned into
                ThestandardTransformerarchitecture [58] and its adap-        2×2windowsofsize4×4(M =4). Then,thenextmod-
                                                                             ule adopts a windowing conﬁguration that is shifted from
             tation for image classiﬁcation [19] both conduct global self-   that of the preceding layer, by displacing the windows by
             attention, where the relationships between a token and all      (bMc,bMc)pixelsfromtheregularlypartitionedwindows.
             other tokens are computed. Theglobalcomputationleadsto             2     2
                                                                                With the shifted window partitioning approach, consec-
             quadratic complexity with respect to the number of tokens,      utive Swin Transformer blocks are computed as
             makingitunsuitable for many vision problems requiring an                                            
             immensesetoftokensfordensepredictionortorepresenta                         ˆl                    l−1      l−1
                                                                                        z =W-MSA LN z               +z ,
             high-resolution image.                                                       l              ˆl   ˆl
                                                                                        z =MLP LN z           +z,
                                                                                        ˆl+1                     l     l
                                                                                        z     =SW-MSA LN z           +z,
             Self-attention in non-overlapped windows      For efﬁcient                   l+1              ˆl+1    ˆl+1
             modeling, we propose to compute self-attention within lo-                  z     =MLP LN z            +z ,             (3)
             cal windows. The windows are arranged to evenly partition              ˆl       l
             the image in a non-overlapping manner. Supposing each           where z and z denote the output features of the (S)W-
             windowcontains M ×M patches, the computational com-             MSAmoduleandtheMLPmoduleforblockl,respectively;
             plexity of a global MSA module and a window based one             3WeomitSoftMaxcomputationindetermining complexity.
                                                                         4
                                                                         10015
                                     C
                                A
                                     C                             A
                                                                        C                   Weobserve signiﬁcant improvements over counterparts
                                                       masked
                                                         MSA                            without this bias term or that use absolute position embed-
                                B         B                        B          B
                                                   ...
                                                                                        ding, as shown in Table 4. Further adding absolute posi-
                                                       masked
                                                                              AAA
                                     C    A
                                                                        C     A         tion embedding to the input as in [19] drops performance
               window partition
                                                         MSA
                                  cyclic shift
                                                               reverse cyclic shift     slightly, thus it is not adopted in our implementation.
               Figure 4. Illustration of an efﬁcient batch computation approach             The learnt relative position bias in pre-training can be
               for self-attention in shifted window partitioning.                       also used to initialize a model for ﬁne-tuning with a differ-
                                                                                        ent window size through bi-cubic interpolation [19, 57].
               W-MSA and SW-MSA denote window based multi-head                          3.3. Architecture Variants
               self-attention using regular and shifted window partitioning                 We build our base model, called Swin-B, to have of
               conﬁgurations, respectively.                                             model size and computation complexity similar to ViT-
                  The shifted window partitioning approach introduces                   B/DeiT-B. We also introduce Swin-T, Swin-S and Swin-L,
               connections between neighboring non-overlapping win-                     whichareversions of about 0.25×, 0.5× and 2× the model
               dowsinthepreviouslayerandisfoundtobeeffectiveinim-                       size and computational complexity, respectively. Note that
               age classiﬁcation, object detection, and semantic segmenta-              the complexity of Swin-T and Swin-S are similar to those
               tion, as shown in Table 4.                                               of ResNet-50 (DeiT-S) and ResNet-101, respectively. The
                                                                                        windowsize is set to M = 7 by default. The query dimen-
               Efﬁcient batch computation for shifted conﬁguration                      sion of each head is d = 32, and the expansion layer of
               Anissue with shifted window partitioning is that it will re-             each MLP is α = 4, for all experiments. The architecture
               sult in more windows, from d h e × d w e to (d h e + 1) ×                hyper-parameters of these model variants are:
                  w                              M        M          M
               (dMe+1)intheshiftedconﬁguration,andsomeofthewin-
               dows will be smaller than M × M4. A naive solution is to                     • Swin-T: C = 96, layer numbers = {2,2,6,2}
               pad the smaller windows to a size of M × M and mask                          • Swin-S: C = 96, layer numbers ={2,2,18,2}
               out the padded values when computing attention. When
               the number of windows in regular partitioning is small, e.g.                 • Swin-B: C = 128, layer numbers ={2,2,18,2}
               2×2,theincreased computation with this naive solution is                     • Swin-L: C = 192, layer numbers ={2,2,18,2}
               considerable (2 × 2 → 3 × 3, which is 2.25 times greater).
               Here, we propose a more efﬁcient batch computation ap-                   where C is the channel number of the hidden layers in the
               proachbycyclic-shifting toward the top-left direction, as il-            ﬁrst stage. The model size, theoretical computational com-
               lustrated in Figure 4. After this shift, a batched window may            plexity (FLOPs), and throughput of the model variants for
               be composed of several sub-windows that are not adjacent                 ImageNet image classiﬁcation are listed in Table 1.
               in the feature map, so a masking mechanism is employed to
               limit self-attention computation to within each sub-window.              4. Experiments
               With the cyclic-shift, the number of batched windows re-
               mains the same as that of regular window partitioning, and                   Weconduct experiments on ImageNet-1K image classi-
               thus is also efﬁcient. The low latency of this approach is               ﬁcation [18], COCO object detection [39], and ADE20K
               showninTable5.                                                           semantic segmentation [74]. In the following, we ﬁrst com-
                                                                                        pare the proposed Swin Transformer architecture with the
               Relative position bias       In computing self-attention, we             previous state-of-the-arts on the three tasks. Then, we ab-
               follow [45, 1, 29, 30] by including a relative position bias             late the important design elements of Swin Transformer.
               B∈RM2×M2toeachheadincomputingsimilarity:                                 4.1. Image Classiﬁcation on ImageNet-1K
                                                           T √                          Settings     For image classiﬁcation, we benchmark the pro-
                 Attention(Q,K,V) = SoftMax(QK / d+B)V, (4)                             posed Swin Transformer on ImageNet-1K [18], which con-
               where Q,K,V ∈ RM2×d are the query, key and value ma-                     tains 1.28M training images and 50K validation images
               trices; d is the query/key dimension, and M2 is the number               from 1,000 classes. The top-1 accuracy on a single crop
               of patches in a window. Since the relative position along                is reported. We consider two training settings:
               each axis lies in the range [−M +1,M −1], we parameter-                      • Regular ImageNet-1K training. This setting mostly
                                                   ˆ      (2M−1)×(2M−1)
               ize a smaller-sized bias matrix B ∈ R                        , and             follows [57]. We employ an AdamW [33] optimizer
                                              ˆ
               values in B are taken from B.                                                  for 300 epochs using a cosine decay learning rate
                  4Tomakethewindowsize(M,M)divisiblebythefeaturemapsizeof                     scheduler and 20 epochs of linear warm-up. A batch
               (h,w), bottom-right padding is employed on the feature map if needed.          size of 1024, an initial learning rate of 0.001, and a
                                                                                    5
                                                                                    10016
                  weight decay of 0.05 are used. We include most of                    (a) Regular ImageNet-1K trained models
                  the augmentation and regularization strategies of [57]           method      image#param. FLOPs throughput ImageNet
                  in training, except for repeated augmentation [28] and                        size                (image / s) top-1 acc.
                                                                                                   2
                  EMA[41], which do not enhance performance. Note             RegNetY-4G[44] 224       21M    4.0G    1156.7     80.0
                                                                                                   2
                  that this is contrary to [57] where repeated augmenta-      RegNetY-8G[44] 224       39M    8.0G     591.6     81.7
                                                                                                   2
                  tion is crucial to stabilize the training of ViT.           RegNetY-16G[44] 224      84M 16.0G       334.7     82.9
                                                                                                   2
                                                                                ViT-B/16 [19]   384    86M 55.4G       85.9      77.9
                                                                                                   2
                • Pre-training on ImageNet-22K and ﬁne-tuning on                ViT-L/16 [19]   384   307M 190.7G      27.3      76.5
                                                                                                   2
                  ImageNet-1K. We also pre-train on the ImageNet-22K             DeiT-S [57]    224    22M    4.6G     940.4     79.8
                                                                                                   2
                  dataset, which contains 14.2 million images and 22K            DeiT-B [57]    224    86M 17.5G       292.3     81.8
                                                                                                   2
                  classes.  We employ an AdamW optimizer for 90                  DeiT-B [57]    384    86M 55.4G       85.9      83.1
                                                                                                   2
                                                                                   Swin-T       224    29M    4.5G     755.2     81.3
                  epochs using a cosine learning rate scheduler with a                             2
                                                                                   Swin-S       224    50M    8.7G     436.9     83.0
                  5-epoch linear warm-up. A batch size of 4096, an ini-                            2
                                                                                   Swin-B       224    88M 15.4G       278.1     83.5
                  tial learning rate of 0.001, and a weight decay of 0.01                          2
                                                                                   Swin-B       384    88M 47.0G       84.7      84.5
                  are used. In ImageNet-1K ﬁne-tuning, we train for 30                   (b) ImageNet-22K pre-trained models
                  epochs with a batch size of 1024, a constant learning            method      image#param. FLOPs throughput ImageNet
                           −5                           −8                                      size                (image / s) top-1 acc.
                  rate of 10  , and a weight decay of 10   .
                                                                                                   2
                                                                                R-101x3 [34]    384   388M 204.6G        -       84.4
                                                                                                   2
             Results with regular ImageNet-1K training       Table 1(a)         R-152x4 [34]    480   937M 840.5G        -       85.4
                                                                                                   2
             presents comparisons to other backbones, including both            ViT-B/16 [19]   384    86M 55.4G       85.9      84.0
                                                                                                   2
             Transformer-based and ConvNet-based, using regular                 ViT-L/16 [19]   384   307M 190.7G      27.3      85.2
                                                                                                   2
                                                                                   Swin-B       224    88M 15.4G       278.1     85.2
             ImageNet-1K training.                                                                 2
                                                                                   Swin-B       384    88M 47.0G       84.7      86.4
                Compared to the previous state-of-the-art Transformer-                             2
                                                                                   Swin-L       384   197M 103.9G      42.1      87.3
             based architecture, i.e. DeiT [57], Swin Transformers no-       Table1.ComparisonofdifferentbackbonesonImageNet-1Kclas-
             ticeably surpass the counterpart DeiT architectures with        siﬁcation. Throughput is measured using the GitHub repository
             similar complexities:   +1.5% for Swin-T (81.3%) over           of [62] and a V100 GPU, following [57].
             DeiT-S (79.8%) using 2242 input, and +1.5%/1.4% for
             Swin-B (83.3%/84.5%) over DeiT-B (81.8%/83.1%) using            118Ktraining, 5K validation and 20K test-dev images. An
                 2    2
             224 /384 input, respectively.                                   ablation study is performed using the validation set, and a
                Compared with the state-of-the-art ConvNets, i.e. Reg-       system-level comparison is reported on test-dev. For the
             Net [44], the Swin Transformer achieves a slightly better       ablation study, we consider four typical object detection
             speed-accuracy trade-off. Noting that while RegNet [44]         frameworks: Cascade Mask R-CNN [26, 6], ATSS [71],
             are obtained via a thorough architecture search, the Swin       RepPoints v2 [12], and Sparse RCNN [52] in mmdetec-
             Transformer is manually adapted from a standard Trans-          tion [10]. For these four frameworks, we utilize the same
             former and has potential for further improvement.               settings: multi-scale training [8, 52] (resizing the input such
                                                                             that the shorter side is between 480 and 800 while the longer
             Results with ImageNet-22K pre-training        We also pre-      side is at most 1333), AdamW [40] optimizer (initial learn-
             train the larger-capacity Swin-B and Swin-L on ImageNet-        ing rate of 0.0001, weight decay of 0.05, and batch size of
             22K. Results ﬁne-tuned on ImageNet-1K image classiﬁca-          16), and 3x schedule (36 epochs). For system-level compar-
             tion are shown in Table 1(b). For Swin-B, the ImageNet-         ison, we adopt an improved HTC [9] (denoted as HTC++)
             22K pre-training brings 1.8%∼1.9% gains over training           with instaboost [20], stronger multi-scale training [7], 6x
             on ImageNet-1K from scratch. Compared with the previ-           schedule (72 epochs), soft-NMS [5], and ImageNet-22K
             ous best results for ImageNet-22K pre-training, our mod-        pre-trained model as initialization.
             els achieve signiﬁcantly better speed-accuracy trade-offs:         We compare our Swin Transformer to standard Con-
             Swin-Bobtains86.4%top-1accuracy,whichis2.4%higher               vNets, i.e. ResNe(X)t, and previous Transformer networks,
             than that of ViT with similar inference throughput (84.7        e.g. DeiT.Thecomparisonsareconductedbychangingonly
             vs. 85.9 images/sec) and slightly lower FLOPs (47.0G vs.        the backbones with other settings unchanged. Note that
             55.4G). The larger Swin-L model achieves 87.3% top-1 ac-        while Swin Transformer and ResNe(X)t are directly appli-
             curacy, +0.9% better than that of the Swin-B model.             cable to all the above frameworks because of their hierar-
             4.2. Object Detection on COCO                                   chical feature maps, DeiT only produces a single resolu-
                                                                             tion of feature maps and cannot be directly applied. For fair
             Settings   Object detection and instance segmentation ex-       comparison,wefollow[73]toconstructhierarchicalfeature
             periments are conducted on COCO 2017, which contains            mapsforDeiTusingdeconvolution layers.
                                                                          6
                                                                          10017
                                 (a) Various frameworks                                           ADE20K                val   test  #param. FLOPs FPS
                                            box   box    box                               Method         Backbone     mIoU score
                  Method      Backbone AP      AP    AP     #param. FLOPs FPS
                                                  50     75
                  Cascade       R-50     46.3 64.3 50.5       82M 739G 18.0             DLab.v3+[11] ResNet-101        44.1     -     63M 1021G 16.0
               MaskR-CNN Swin-T 50.5 69.3 54.9                86M 745G 15.3               DNL[65]        ResNet-101    46.0   56.2    69M 1249G 14.8
                   ATSS         R-50     43.5 61.9 47.0       32M 205G 28.3              OCRNet[67] ResNet-101 45.3 56.0              56M     923G 19.3
                               Swin-T    47.2 66.5 51.3       36M 215G 22.3              UperNet [63]    ResNet-101    44.9     -     86M 1029G 20.1
               RepPointsV2      R-50     46.5 64.6 50.3       42M 274G 13.6              OCRNet[67] HRNet-w48 45.7              -     71M     664G 12.5
                               Swin-T    50.0 68.5 54.2       45M 283G 12.0             DLab.v3+[11] ResNeSt-101 46.9 55.1            66M 1051G 11.9
                   Sparse       R-50     44.5 63.4 48.2      106M 166G 21.0             DLab.v3+[11] ResNeSt-200 48.4           -     88M 1381G 8.1
                  R-CNN        Swin-T    47.9 67.3 52.3      110M 172G 18.4               SETR[73]        T-Large‡     50.3   61.7   308M       -      -
                                                                                                                  †
                    (b) Various backbones w. Cascade Mask R-CNN                            UperNet         DeiT-S      44.0     -     52M 1099G 16.2
                           box   box   box   mask   mask   mask                            UperNet         Swin-T      46.1     -     60M     945G 18.5
                        AP AP AP AP AP AP paramFLOPsFPS
                                 50    75           50     75
               DeiT-S† 48.0 67.2 51.7 41.4        64.2   44.3   80M 889G 10.4              UperNet         Swin-S      49.3     -     81M 1038G 15.2
                                                                                                                  ‡
                 R50     46.3 64.3 50.5 40.1      61.7   43.4   82M 739G 18.0              UperNet        Swin-B       51.6     -    121M 1841G 8.7
                                                                                                                  ‡
               Swin-T 50.5 69.3 54.9 43.7         66.6   47.1   86M 745G 15.3              UperNet        Swin-L       53.5   62.8   234M 3230G 6.2
               X101-32 48.1 66.5 52.4 41.6        63.9   45.2 101M 819G 12.8           Table 3. Results of semantic segmentation on the ADE20K val
               Swin-S 51.8 70.4 56.3 44.7         67.9   48.5 107M 838G 12.0           and test set. † indicates additional deconvolution layers are used
               X101-64 48.3 66.4 52.3 41.7        64.0   45.1 140M 972G 10.4           to produce hierarchical feature maps. ‡ indicates that the model is
               Swin-B 51.9 70.9 56.5 45.0         68.4   48.7 145M 982G 11.6           pre-trained on ImageNet-22K.
                                 (c) System-level Comparison
                      Method           mini-val       test-dev   #param. FLOPs         Comparison to DeiT The performance of DeiT-S us-
                                        box   mask    box    mask
                                     AP    AP      AP    AP                            ing the Cascade Mask R-CNN framework is shown in Ta-
                RepPointsV2* [12]      -      -     52.1    -        -       -         ble 2(b). The results of Swin-T are +2.5 box AP and +2.3
                   GCNet*[7]         51.8   44.7    52.3   45.4      -    1041G        maskAPhigherthanDeiT-S with similar model size (86M
               RelationNet++* [13]     -      -     52.7    -        -       -         vs. 80M)andsigniﬁcantlyhigherinferencespeed(15.3FPS
                 DetectoRS* [42]       -      -     55.7   48.5      -       -         vs. 10.4 FPS). The lower inference speed of DeiT is mainly
                 YOLOv4P7*[4]          -      -     55.8    -        -       -         due to its quadratic complexity to input image size.
                 Copy-paste [23]     55.9   47.2    56.0   47.4   185M 1440G
                X101-64(HTC++) 52.3 46.0             -      -     155M 1033G
                 Swin-B(HTC++)       56.4   49.1     -      -     160M 1043G           Comparison to previous state-of-the-art               Table 2(c)
                 Swin-L(HTC++)       57.1   49.5    57.7   50.2   284M 1470G           compares our best results with those of previous state-of-
                Swin-L(HTC++)* 58.0 50.4 58.7 51.1                284M       -
               Table 2. Results on COCO object detection and instance segmen-          the-art models. Our best model achieves 58.7 box AP and
               tation. †denotes that additional decovolution layers are used to        51.1 mask AP on COCO test-dev, surpassing the previous
               produce hierarchical feature maps. * indicates multi-scale testing.     best results by +2.7 box AP (Copy-paste [23] without exter-
                                                                                       nal data) and +2.6 mask AP (DetectoRS [42]).
               Comparison to ResNe(X)t           Table 2(a) lists the results of       4.3. Semantic Segmentation on ADE20K
               Swin-T and ResNet-50 on the four object detection frame-
               works. OurSwin-Tarchitecturebringsconsistent+3.4∼4.2                    Settings     ADE20K [74] is a widely-used semantic seg-
               box AP gains over ResNet-50, with slightly larger model                 mentation dataset, covering a broad range of 150 semantic
               size, FLOPs and latency.                                                categories. It has 25K imagesintotal, with20Kfortraining,
                  Table 2(b) compares Swin Transformer and ResNe(X)t                   2K for validation, and another 3K for testing. We utilize
               under different model capacity using Cascade Mask R-                    UperNet [63] in mmseg [16] as our base framework for its
               CNN.SwinTransformerachievesahighdetectionaccuracy                       highefﬁciency. MoredetailsarepresentedintheAppendix.
               of 51.9 box AP and 45.0 mask AP, which are signiﬁcant
               gains of +3.6 box AP and +3.3 mask AP over ResNeXt101-
               64x4d, which has similar model size, FLOPs and latency.                 Results     Table 3 lists the mIoU, model size (#param),
               Onahigherbaseline of 52.3 box AP and 46.0 mask AP us-                   FLOPsandFPSfordifferentmethod/backbonepairs. From
               ing an improved HTC framework, the gains by Swin Trans-                 these results, it can be seen that Swin-S is +5.3 mIoU higher
               formerarealsohigh,at+4.1boxAPand+3.1maskAP(see                          (49.3 vs. 44.0) than DeiT-S with similar computation cost.
               Table 2(c)). Regarding inference speed, while ResNe(X)t is              It is also +4.4 mIoU higher than ResNet-101, and +2.4
               built by highly optimized Cudnn functions, our architecture             mIoU higher than ResNeSt-101 [70]. Our Swin-L model
               is implemented with built-in PyTorch functions that are not             withImageNet-22Kpre-trainingachieves53.5mIoUonthe
               all well-optimized. A thorough kernel optimization is be-               val set, surpassing the previous best model by +3.2 mIoU
               yondthe scope of this paper.                                            (50.3 mIoU by SETR [73] which has a larger model size).
                                                                                   7
                                                                                   10018
                                     ImageNet        COCO         ADE20k                      method           MSAinastage(ms) Arch. (FPS)
                                                     box    mask                                                S1    S2   S3 S4 T        S   B
                                    top-1 top-5 AP       AP        mIoU
                    w/o shifting    80.2   95.1   47.7    41.5      43.3              sliding window (naive)   122.5 38.3 12.1 7.6 183 109 77
                 shifted windows    81.3   95.6   50.5    43.7      46.1              sliding window (kernel)   7.6   4.7  2.7 1.8 488 283 187
                      nopos.        80.1   94.9   49.2    42.6      43.8                  Performer [14]        4.8   2.8  1.8 1.5 638 370 241
                     abs. pos.      80.5   95.2   49.0    42.4      43.2               window(w/oshifting)      2.8   1.7  1.2 0.9 770 444 280
                   abs.+rel. pos.   81.3   95.6   50.2    43.4      44.0             shifted window (padding)   3.3   2.3  1.9 2.2 670 371 236
                 rel. pos. w/o app. 79.3   94.7   48.2    41.9      44.1              shifted window (cyclic)   3.0   1.9  1.3 1.0 755 437 278
                     rel. pos.      81.3   95.6   50.5    43.7      46.1           Table 5. Real speed of different self-attention computation meth-
              Table 4. Ablation study on the shifted windows approach and dif-     ods and implementations on a V100 GPU.
              ferent position embedding methods on three benchmarks, using
              the Swin-T architecture. w/o shifting: all self-attention modules                                 ImageNet      COCO       ADE20k
              adopt regular window partitioning, without shifting; abs. pos.: ab-                                             box   mask
              solute position embedding term of ViT; rel. pos.: the default set-                    Backbone top-1 top-5 AP      AP       mIoU
              tings with an additional relative position bias term (see Eq. (4));    sliding window Swin-T     81.4 95.6 50.2     43.5     45.8
              app.: the ﬁrst scaled dot-product term in Eq. (4).                     Performer [14]   Swin-T   79.0 94.2     -      -        -
                                                                                     shifted window Swin-T     81.3 95.6 50.5     43.7     46.1
                                                                                   Table 6. Accuracy of Swin Transformer using different methods
              4.4. Ablation Study                                                  for self-attention computation on three benchmarks.
                 In this section, we ablate important design elements in              The self-attention modules built on the proposed
              the proposed Swin Transformer, using ImageNet-1K image               shifted window approach are 40.8×/2.5×, 20.2×/2.5×,
              classiﬁcation, Cascade Mask R-CNN on COCO object de-                 9.3×/2.1×,and7.6×/1.8×moreefﬁcientthanthoseofslid-
              tection, and UperNet on ADE20K semantic segmentation.                ing windows in naive/kernel implementations on four net-
              Shifted windows       Ablations of the shifted window ap-            work stages, respectively. Overall, the Swin Transformer
              proach on the three tasks are reported in Table 4. Swin-T            architectures built on shifted windows are 4.1/1.5, 4.0/1.5,
              with the shifted window partitioning outperforms the coun-           3.6/1.5 times faster than variants built on sliding windows
              terpart built on a single window partitioning at each stage by       for Swin-T,Swin-S,andSwin-B,respectively. Table6com-
              +1.1% top-1 accuracy on ImageNet-1K, +2.8 box AP/+2.2                parestheiraccuracyonthethreetasks,showingthattheyare
              mask AP on COCO, and +2.8 mIoU on ADE20K. The re-                    similarly accurate in visual modeling.
              sults indicate the effectiveness of using shifted windows to            Compared to Performer [14], which is one of the fastest
              build connections among windows in the preceding layers.             Transformer architectures (see [55]), the proposed shifted
              The latency overhead by shifted window is also small, as             window based self-attention computation and the overall
              showninTable5.                                                       Swin Transformer architectures are slightly faster (see Ta-
                                                                                   ble 5), while achieving +2.3% top-1 accuracy compared to
                                                                                   Performer on ImageNet-1K using Swin-T (see Table 6).
              Relative position bias     Table 4 shows comparisons of dif-
              ferent position embedding approaches. Swin-T with rela-              5. Conclusion
              tive position bias yields +1.2%/+0.8% top-1 accuracy on
              ImageNet-1K, +1.3/+1.5 box AP and +1.1/+1.3 mask AP                     This paper presents Swin Transformer, a new vision
              on COCO, and +2.3/+2.9 mIoU on ADE20K in relation to                 Transformer which produces a hierarchical feature repre-
              those without position encoding and with absolute position           sentation and has linear computational complexity with re-
              embedding, respectively, indicating the effectiveness of the         spect to input image size. Swin Transformer achieves the
              relative position bias. Also note that while the inclusion of        state-of-the-art performance on COCO object detection and
              absolute position embedding improves image classiﬁcation             ADE20K semantic segmentation, signiﬁcantly surpassing
              accuracy (+0.4%), it harms object detection and semantic             previous best methods. We hope that Swin Transformer’s
              segmentation (-0.2 box/mask AP on COCO and -0.6 mIoU                 strong performanceonvariousvisionproblemswillencour-
              onADE20K).                                                           age uniﬁed modeling of vision and language signals.
              Different self-attention methods       The real speed of dif-        Acknowledgement
              ferent self-attention computation methods and implementa-
              tions are compared in Table 5. Our cyclic implementation                Wethank Li Dong and Furu Wei for useful discussions;
              is more hardware efﬁcient than naive padding, particularly           Bin Xiao, Lu Yuan and Lei Zhang for help on datasets;
              for deeper stages. Overall, it brings a 13%, 18% and 18%             Jiarui Xu for help on the mmdetection and mmsegmenta-
              speed-up on Swin-T, Swin-S and Swin-B, respectively.                 tion codebases.
                                                                                8
                                                                               10019
              References                                                           [14] Krzysztof Marcin Choromanski, Valerii Likhosherstov,
               [1] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang,                 David Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
                   XiaodongLiu,YuWang,JianfengGao,SonghaoPiao,Ming                       los, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,
                   Zhou, et al. Unilmv2: Pseudo-masked language models for               Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell,
                   uniﬁed language model pre-training. In International Con-             and Adrian Weller. Rethinking attention with performers.
                   ference on Machine Learning, pages 642–652. PMLR, 2020.               In International Conference on Learning Representations,
                   5                                                                     2021. 8
               [2] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew          [15] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and
                   Zhai, and Dmitry Kislyuk. Toward transformer-based object             Huaxia Xia. Do we really need explicit position encodings
                   detection. arXiv preprint arXiv:2012.09958, 2020. 3                   for vision transformers? arXiv preprint arXiv:2102.10882,
                                                                                         2021. 3
               [3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,      [16] MMSegmentation       Contributors.      MMSegmentation:
                   and Quoc V. Le. Attention augmented convolutional net-                Openmmlab semantic segmentation toolbox and bench-
                   works, 2020. 3                                                        mark.         https://github.com/open-mmlab/
               [4] Alexey   Bochkovskiy,    Chien-Yao Wang,      and Hong-               mmsegmentation,2020. 7
                   Yuan Mark Liao. Yolov4: Optimal speed and accuracy of           [17] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong
                   object detection. arXiv preprint arXiv:2004.10934, 2020. 7            Zhang, Han Hu, and Yichen Wei. Deformable convolutional
               [5] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and                    networks. In Proceedings of the IEEE International Confer-
                   Larry S. Davis. Soft-nms – improving object detection with            ence on Computer Vision, pages 764–773, 2017. 1, 3
                   one line of code. In Proceedings of the IEEE International      [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
                   Conference on Computer Vision (ICCV), Oct 2017. 6                     and Li Fei-Fei. Imagenet: A large-scale hierarchical image
               [6] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-                database. In 2009 IEEE conference on computer vision and
                   ing into high quality object detection. In Proceedings of the         pattern recognition, pages 248–255. Ieee, 2009. 5
                   IEEEConference on Computer Vision and Pattern Recogni-          [19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
                   tion, pages 6154–6162, 2018. 6                                        Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
               [7] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han                 Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
                   Hu. Gcnet: Non-localnetworksmeetsqueeze-excitationnet-                vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
                   works and beyond. In Proceedings of the IEEE/CVF Inter-               worth 16x16 words: Transformers for image recognition at
                   nationalConferenceonComputerVision(ICCV)Workshops,                    scale. In International Conference on Learning Representa-
                   Oct 2019. 3, 6, 7                                                     tions, 2021. 1, 2, 3, 4, 5, 6
               [8] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas            [20] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao
                   Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-            Gou, Yong-Lu Li, and Cewu Lu.        Instaboost: Boosting
                   endobjectdetection with transformers. In European Confer-             instance segmentation via probability map guided copy-
                   enceonComputerVision,pages213–229.Springer,2020. 3,                   pasting. In Proceedings of the IEEE/CVF International Con-
                   6                                                                     ference on Computer Vision, pages 682–691, 2019. 6
               [9] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaox-          [21] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhi-
                   iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping                 wei Fang, and Hanqing Lu.      Dual attention network for
                   Shi, Wanli Ouyang, et al. Hybrid task cascade for instance            scene segmentation. In Proceedings of the IEEE Conference
                   segmentation. In Proceedings of the IEEE/CVF Conference               on Computer Vision and Pattern Recognition, pages 3146–
                   on Computer Vision and Pattern Recognition, pages 4974–               3154, 2019. 3
                   4983, 2019. 6                                                   [22] Kunihiko Fukushima. Cognitron: A self-organizing multi-
              [10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu                  layered neural network. Biological cybernetics, 20(3):121–
                   Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,              136, 1975. 3
                   Jiarui Xu, et al. Mmdetection: Open mmlab detection tool-       [23] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-
                   boxandbenchmark.arXivpreprintarXiv:1906.07155,2019.                   Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple
                   6                                                                     copy-pasteisastrongdataaugmentationmethodforinstance
              [11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian               segmentation. arXiv preprint arXiv:2012.07177, 2020. 2, 7
                   Schroff, and Hartwig Adam. Encoder-decoder with atrous          [24] Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, and Jifeng
                   separable convolution for semantic image segmentation. In             Dai. Learning region features for object detection. In Pro-
                   Proceedings of the European conference on computer vision             ceedings of the European Conference on Computer Vision
                   (ECCV), pages 801–818, 2018. 7                                        (ECCV), 2018. 3
              [12] Yihong Chen, Zheng Zhang, Yue Cao, Liwei Wang, Stephen          [25] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,
                   Lin, and Han Hu. Reppoints v2: Veriﬁcation meets regres-              andYunheWang.Transformerintransformer. arXivpreprint
                   sion for object detection. In NeurIPS, 2020. 6, 7                     arXiv:2103.00112, 2021. 3
                                                                                                                                  ´
              [13] Cheng Chi, Fangyun Wei, and Han Hu.         Relationnet++:      [26] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-
                   Bridgingvisualrepresentationsforobjectdetectionviatrans-              shick. Mask r-cnn. In Proceedings of the IEEE international
                   former decoder. In NeurIPS, 2020. 3, 7                                conference on computer vision, pages 2961–2969, 2017. 6
                                                                                9
                                                                               10020
               [27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.               [41] Boris T Polyak and Anatoli B Juditsky.         Acceleration of
                     Deep residual learning for image recognition. In Proceed-                 stochastic approximation by averaging.      SIAM journal on
                     ings of the IEEE conference on computer vision and pattern                control and optimization, 30(4):838–855, 1992. 6
                     recognition, pages 770–778, 2016. 1, 2, 3, 4                        [42] SiyuanQiao,Liang-ChiehChen,andAlanYuille. Detectors:
               [28] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten                 Detecting objects with recursive feature pyramid and switch-
                     Hoeﬂer,andDanielSoudry. Augmentyourbatch: Improving                       able atrous convolution. arXiv preprint arXiv:2006.02334,
                     generalization through instance repetition. In Proceedings of             2020. 2, 7
                     the IEEE/CVF Conference on Computer Vision and Pattern              [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
                     Recognition, pages 8129–8138, 2020. 6                                     Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
               [29] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen                    Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
                     Wei. Relation networks for object detection. In Proceed-                  Krueger, and Ilya Sutskever. Learning transferable visual
                     ingsoftheIEEEConferenceonComputerVisionandPattern                         models from natural language supervision, 2021. 1
                     Recognition, pages 3588–3597, 2018. 3, 5                            [44] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,
                                                                                                                             ´
               [30] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local                    Kaiming He, and Piotr Dollar. Designing network design
                     relation networks for image recognition. In Proceedings of                spaces.   In Proceedings of the IEEE/CVF Conference on
                     the IEEE/CVFInternationalConferenceonComputerVision                       Computer Vision and Pattern Recognition, pages 10428–
                     (ICCV), pages 3464–3473, October 2019. 2, 3, 5                            10436, 2020. 6
               [31] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-              [45] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
                     ian Q Weinberger.     Densely connected convolutional net-                Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
                     works. In Proceedings of the IEEE conference on computer                  Peter J. Liu. Exploring the limits of transfer learning with a
                     vision and pattern recognition, pages 4700–4708, 2017. 1, 3               uniﬁed text-to-text transformer. Journal of Machine Learn-
               [32] David H Hubel and Torsten N Wiesel.          Receptive ﬁelds,              ing Research, 21(140):1–67, 2020. 5
                     binocular interaction and functional architecture in the cat’s      [46] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan
                     visual cortex. The Journal of physiology, 160(1):106–154,                 Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self-
                     1962. 3                                                                   attention in vision models. In Advances in Neural Informa-
               [33] Diederik P Kingma and Jimmy Ba. Adam: A method for                         tion ProcessingSystems, volume32.CurranAssociates,Inc.,
                     stochastic optimization.   arXiv preprint arXiv:1412.6980,                2019. 2, 3
                     2014. 5                                                             [47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
               [34] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan                      net: Convolutional networks for biomedical image segmen-
                     Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.                tation. In International Conference on Medical image com-
                     Big transfer (bit): General visual representation learning.               puting and computer-assisted intervention, pages 234–241.
                     arXiv preprint arXiv:1912.11370, 6(2):8, 2019. 6                          Springer, 2015. 2
               [35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.              [48] K. Simonyan and A. Zisserman. Very deep convolutional
                     Imagenet classiﬁcation with deep convolutional neural net-                networks for large-scale image recognition. In International
                     works. In Advances in neural information processing sys-                  Conference on Learning Representations, May 2015. 3, 4
                     tems, pages 1097–1105, 2012. 1, 2                                   [49] Bharat Singh and Larry S Davis. An analysis of scale in-
                                     ´                                                         variance in object detection snip.     In Proceedings of the
               [36] Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner,                   IEEE conference on computer vision and pattern recogni-
                     et al. Gradient-based learning applied to document recog-                 tion, pages 3578–3587, 2018. 2
                     nition. Proceedings of the IEEE, 86(11):2278–2324, 1998.            [50] Bharat Singh, Mahyar Najibi, and Larry S Davis. Sniper:
                     2                                                                         Efﬁcient multi-scale training. In Advances in Neural Infor-
                                                     ´
               [37] YannLeCun,PatrickHaffner,LeonBottou,andYoshuaBen-                          mation Processing Systems, volume 31. Curran Associates,
                     gio.  Object recognition with gradient-based learning.      In            Inc., 2018. 2
                     Shape,contourandgroupingincomputervision,pages319–                  [51] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon
                     345. Springer, 1999. 3                                                    Shlens, Pieter Abbeel, and Ashish Vaswani.             Bottle-
               [38] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,                     neck transformers for visual recognition.      arXiv preprint
                     Bharath Hariharan, and Serge Belongie. Feature pyramid                    arXiv:2101.11605, 2021. 3
                     networks for object detection.     In The IEEE Conference           [52] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chen-
                     on Computer Vision and Pattern Recognition (CVPR), July                   feng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan
                     2017. 2                                                                   Yuan, Changhu Wang, et al.        Sparse r-cnn: End-to-end
               [39] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,                   object detection with learnable proposals.     arXiv preprint
                                                               ´
                     Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence                 arXiv:2011.12450, 2020. 3, 6
                     Zitnick. Microsoft coco: Common objects in context. In              [53] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
                     European conference on computer vision, pages 740–755.                    Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
                     Springer, 2014. 5                                                         Vanhoucke, and Andrew Rabinovich. Going deeper with
               [40] Ilya Loshchilov and Frank Hutter. Decoupled weight de-                     convolutions.   In Proceedings of the IEEE conference on
                     cay regularization. In International Conference on Learning               computer vision and pattern recognition, pages 1–9, 2015.
                     Representations, 2019. 6                                                  3
                                                                                     10
                                                                                     10021
              [54] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model         [67] Yuhui Yuan, Xilin Chen, and Jingdong Wang.         Object-
                   scaling for convolutional neural networks. In International          contextual representations for semantic segmentation.   In
                   ConferenceonMachineLearning,pages6105–6114.PMLR,                     16th European Conference Computer Vision (ECCV 2020),
                   2019. 3                                                              August 2020. 7
              [55] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,            [68] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-
                   Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian            work for scene parsing. arXiv preprint arXiv:1809.00916,
                   Ruder, and Donald Metzler. Long range arena : A bench-               2018. 3
                   mark for efﬁcient transformers. In International Conference     [69] SergeyZagoruykoandNikosKomodakis. Wideresidualnet-
                   onLearning Representations, 2021. 8                                  works. In BMVC, 2016. 1
              [56] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-        [70] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi
                   cas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,           Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R
                   Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario              Manmatha, et al. Resnest: Split-attention networks. arXiv
                   Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp ar-             preprint arXiv:2004.08955, 2020. 7
                   chitecture for vision, 2021. 2                                  [71] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and
              [57] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco               Stan Z Li.   Bridging the gap between anchor-based and
                                                            ´ ´
                   Massa, Alexandre Sablayrolles, and Herve Jegou. Training             anchor-free detection via adaptive training sample selection.
                   data-efﬁcient image transformers & distillation through at-          In Proceedings of the IEEE/CVF Conference on Computer
                   tention. arXiv preprint arXiv:2012.12877, 2020. 2, 3, 5, 6           Vision and Pattern Recognition, pages 9759–9768, 2020. 6
              [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-         [72] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-
                   reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia           ing self-attention for image recognition. In Proceedings of
                   Polosukhin. Attention is all you need. In Advances in Neural         the IEEE/CVF Conference on Computer Vision and Pattern
                   Information Processing Systems, pages 5998–6008, 2017. 1,            Recognition, pages 10076–10085, 2020. 3
                   2, 4                                                            [73] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,
              [59] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,                  Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao
                   Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui                Xiang, Philip HS Torr, et al. Rethinking semantic segmen-
                   Tan, Xinggang Wang, et al. Deep high-resolution represen-            tation from a sequence-to-sequence perspective with trans-
                   tation learning for visual recognition. IEEE transactions on         formers. arXiv preprint arXiv:2012.15840, 2020. 2, 3, 6,
                   pattern analysis and machine intelligence, 2020. 3                   7
              [60] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao          [74] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-
                   Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.                  dler, Adela Barriuso, and Antonio Torralba. Semantic under-
                   Pyramid vision transformer:     A versatile backbone for             standing of scenes through the ade20k dataset. International
                   dense prediction without convolutions.     arXiv preprint            Journal on Computer Vision, 2018. 5, 7
                   arXiv:2102.12122, 2021. 3                                       [75] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. De-
              [61] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-               formable convnets v2: More deformable, better results. In
                   ing He. Non-local neural networks. In IEEE Conference                Proceedings of the IEEE Conference on Computer Vision
                   on Computer Vision and Pattern Recognition, CVPR 2018,               and Pattern Recognition, pages 9308–9316, 2019. 1, 3
                   2018. 3                                                         [76] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
              [62] Ross     Wightman.             Pytorch     image     mod-            and Jifeng Dai. Deformable {detr}: Deformable transform-
                   els.            https://github.com/rwightman/                        ers for end-to-end object detection. In International Confer-
                   pytorch-image-models,2019. 6                                         ence on Learning Representations, 2021. 3
              [63] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and
                   Jian Sun. Uniﬁed perceptual parsing for scene understand-
                   ing. In Proceedings of the European Conference on Com-
                   puter Vision (ECCV), pages 418–434, 2018. 7
                                                         ´
              [64] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and
                   Kaiming He. Aggregated residual transformations for deep
                   neural networks. In Proceedings of the IEEE Conference
                   on Computer Vision and Pattern Recognition, pages 1492–
                   1500, 2017. 1, 2, 3
              [65] MinghaoYin,ZhuliangYao,YueCao,XiuLi,ZhengZhang,
                   Stephen Lin, and Han Hu. Disentangled non-local neural
                   networks. In Proceedings of the European conference on
                   computer vision (ECCV), 2020. 3, 7
              [66] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
                   Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-
                   to-token vit: Training vision transformers from scratch on
                   imagenet. arXiv preprint arXiv:2101.11986, 2021. 3
                                                                              11
                                                                               10022
