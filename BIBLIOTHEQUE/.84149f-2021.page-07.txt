                      Table 3: The effect of progressively adding regularisation (each                                                 Table 4: The effect of spatial resolution on the performance of
                      row includes all methods above it) on Top-1 action accuracy on                                                   ViViT-L/16x2 and spatio-temporal attention on Kinetics 400.
                      Epic Kitchens. We use ViViT-B/16x2 Factorised Encoder.                                                                                  Crop size            224          288           320
                                                                                        Top-1 accuracy                                                        Accuracy            80.3          80.7          81.0
                                  Randomcrop,ﬂip, colour jitter                                 38.4                                                          GFLOPs              1446         2919          3992
                                  +Kinetics 400 initialisation                                  39.6                                                          Runtime             58.9         147.6         238.8
                                  +Stochastic depth [30]                                        40.2
                                  +Randomaugment[12]                                            41.1                                                                 32 stride 2          64 stride 2           128 stride 2
                                  +Labelsmoothing[60]                                           43.1
                                  +Mixup[81]                                                    43.7                                              80
                           Spatio-temporal       Factorised encoder       Factorised self-attention    Factorised dot-product
                           80.0                                              0.4                                                                  78
                           77.5                                                                                                                 Top-1 Accuracy
                           75.0                                             TFLOPs0.2                                                             76
                          Top-1 Accuracy72.5
                                16x8               16x4              16x2        16x8               16x4               16x2                               1            2           3           4           5           6           7
                                            Input tubelet size                               Input tubelet size                                                                     Number of views
                                       (a) Accuracy                                        (b) Compute                                 Figure 7: The effect of varying the number of frames input to
                      Figure 6: The effect of varying the number of temporal tokens on                                                 the network and increasing the number of tokens proportionally.
                      (a) accuracy and (b) computation on Kinetics 400, for different                                                  AKinetics video contains 250 frames (10 seconds sampled at 25
                      variants of our model with a ViViT-B backbone.                                                                   fps) and the accuracy for each model saturates once the number of
                      explored them for training ViT for image classiﬁcation.                                                          equidistant temporal views is sufﬁcient to “see” the whole video
                            Each row of Tab. 3 includes all the methods from the                                                       clip. Observe how models processing more frames (and thus more
                      rows above it, and we observe progressive improvements                                                           tokens) achieve higher single- and multi-view accuracy.
                      from adding each regulariser. Overall, we obtain a substan-                                                           Figure 7 shows that as we increase the number of frames
                      tial overall improvement of 5.3% on Epic Kitchens. We                                                            input to the network, the accuracy from processing a single
                      also achieve a similar improvement of 5%, from 60.4% to                                                          viewincreases, as the network incorporates longer temporal
                      65.4%, on SSv2 by using all the regularisation in Tab. 3.                                                        context. However, commonpractice on datasets such as Ki-
                      Note that the Kinetics-pretrained models that we initialise                                                      netics [20, 74, 41] is to average results over multiple, shorter
                      from are from Tab. 2, and that all Epic Kitchens models in                                                       “views”ofthesamevideoclip. Figure7alsoshowsthatthe
                      Tab. 2 were trained with all the regularisers in Tab. 3. For                                                     accuracy saturates once the number of views is sufﬁcient to
                      larger datasets like Kinetics and Moments in Time, we do                                                         cover the whole video. As a Kinetics video consists of 250
                      not use these additional regularisers (we use only the ﬁrst                                                      frames, and we sample frames with a stride of 2, our model
                      row of Tab. 3), as we obtain state-of-the-art results without                                                    which processes 128 frames requires just a single view to
                      them. The supplementary contains hyperparameter values                                                           “see” the whole video and achieve its maximum accuarcy.
                      and additional details for all regularisers.                                                                          Note that we used ViViT-L/16x2 Factorised Encoder
                      Varying the number of tokens                                 Weﬁrst analyse the per-                             (Model 2) here. As this model is more efﬁcient it can pro-
                      formance as a function of the number of tokens along the                                                         cess more tokens, compared to the unfactorised Model 1
                      temporaldimensioninFig.6. Weobservethatusingsmaller                                                              which runs out of memory after 48 frames using tubelet
                      input tubelet sizes (and therefore more tokens) leads to con-                                                    length t = 2 and a “Large” backbone. Models processing
                      sistent accuracy improvements across all of our model ar-                                                        more frames (and thus more tokens) consistently achieve
                      chitectures.            At the same time, computation in terms of                                                higher single- and multi-view accuracy, in line with our ob-
                      FLOPs increases accordingly, and the unfactorised model                                                          servations in previous experiments (Tab. 4, Fig. 6). Mo-
                      (Model 1) is impacted the most.                                                                                  roever, observe that by processing more frames (and thus
                            We then vary the number of tokens fed into the model                                                       more tokens) with Model 2, we are able to achieve higher
                      by increasing the spatial crop-size from the default of 224                                                      accuracy than Model 1 (with fewer total FLOPs as well).
                      to 320 in Tab. 4. As expected, there is a consistent increase                                                         Finally, we observed that for Model 2, the number of
                      in both accuracy and computation. We note that when com-                                                         FLOPs effectively increases linearly with the number of
                      paring to prior work we consistently obtain state-of-the-art                                                     input frames as the overall computation is dominated by
                      results (Sec. 4.3) using a spatial resolution of 224, but we                                                     the initial spatial encoder.                      As a result, the total number
                      also highlight that further improvements can be obtained at                                                      of FLOPs for the number of temporal views required to
                      higher spatial resolutions.                                                                                      achieve maximum accuracy is constant across the models.
                      Varyingthenumberofinputframes Inourexperiments                                                                   In other words, ViViT-L/16x2 FE with 32 frames requires
                      so far, we have kept the number of input frames ﬁxed at 32.                                                      995.3GFLOPsperview,and4viewstosaturatemulti-view
                      Wenowincrease the number of frames input to the model,                                                           accuracy. The 128-frame model requires 3980.4 GFLOPs
                      thereby increasing the number of tokens proportionally.                                                          but only a single view. As shown by Fig. 7, the latter model
                                                                                                                                  6842
