0 Os 15 2

1
time x 10”

Figure 3.3. The stationary probability (for figure 3.2) of
state j vs. time (on a 10” scale).

(a1; J) = .9), learning at state i reduces the learning
at state j leading to an an extremely flat plateau of
improvement at 1 average reward shown in Figure 3.2.
Figure 3.3 shows that this problem is so severe that
p(j) drops as low as 10~7 from it’s initial probability
of .2. As in example 1, to obtain a nonzero estimate
of the gradient it is necessary to visit state j. The
situation could be even worse with a few extra states
in a chain as in figure 3.1.

Although asymptotically a good policy might be
found, these results do not bode well for the answer
to question 3, which is concerned with how fast such a
policy can be found. These results suggest that in any
reasonable number of steps, a gradient method could
end up being trapped at plateaus where estimating the
gradient direction has an unreasonably large sample
complexity. Answering question 3 is crucial to under-
stand how well gradient methods perform, and (to our
knowledge) no such analysis exists.

4 Approximately Optimal RL

The fundamental problem with policy gradients is that
np, which is what we ultimately seek to optimize, is
insensitive to policy improvement at unlikely states
though policy improvement at these unlikely states
might be necessary in order for the agent to achieve
near optimal payoff. We desire an alternative perfor-
mance measure that does not down weight advantages
at unlikely states or unlikely actions. A natural can-
didate for a performance measure is to weight the im-
provement from all states more uniformly (rather than
by D), such as

nu (7) = Es~p [V,(s)]

where pu is some “exploratory” restart distribution. Un-
der our assumption of having access to a p-restart dis-
tribution, we can obtain estimates of n, (7).

Any optimal policy simultaneously maximizes both ny,
and np. Unfortunately, the policy that maximizes n,
within some restricted class of policies may have poor

performance according to 7p, so we must ensure that
maximizing 7, results in a good policy under np.

Greedy policy iteration updates the policy to some 7’
based on some approximate state-action values. In-
stead, let us consider the following more conservative
update rule:

(4.1) Tnew(@; 8) = (1 — a)m(a; 8) + an'(a;s),

for some 7’ and a € [0,1]. To guarantee improve-
ment with a = 1, x’ must choose a better action at
every state, or else we could suffer the penalty shown
in equation 3.1.

In the remainder of this section, we describe a more
conservative policy iteration scheme using a < 1 and
state the main theorems of this paper. In subsection
4.1, we show that 7, can improve under the much less
stringent condition in which 7’ often, but not always,
chooses greedy actions. In subsection 4.2, we assume
access to a greedy policy chooser that outputs “ap-
proximately” greedy policies x’ and then bound the
performance of the policy found by our algorithm in
terms of the quality of this greedy policy chooser.

4.1 Policy Improvement

A more reasonable situation is one in which we are
able to improve the policy with some a > 0 using a 7’
that chooses better actions at most but not all states.
Let us define the policy advantage A,,,,(7') of some
policy z' with respect to a policy 7 and a distribution
pu to be

Anu (x') = Enda [Eann’(ajs) [Ax(s,a)]] .

The policy advantage measures the degree to which 7’
is choosing actions with large advantages, with respect
to the set of states visited under 7 starting from a state
8s ~ p. Note that a policy found by one step of policy
improvement maximizes the policy advantage.

It is straightforward to show that Me | 0 = Anu
(using equation 3.2), so the change in n, is:

Agu (m) + O(a”).

a
1-¥
Hence, for sufficiently small a, policy improvement oc-
curs if the policy advantage is positive, and at the other
extreme of a = 1, significant degradation could occur.
We now connect these two regimes to determine how
much policy improvement is possible.

(4.2)

Anu =

Theorem 4.1. Let A be the policy advantage of
a’ with respect to m and w. and let e¢ =
maxs |Eqvn'(a;s) [Ax ($,@)]|. For the update rule 4.1
and for all a € [0,1]:

2aye
1—7(1-a)

Ny (Tnew) Nut) = (A )-

Qa
1-y¥
