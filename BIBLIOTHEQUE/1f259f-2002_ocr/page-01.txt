2 Preliminaries

A finite Markov decision process (MDP) is defined
by the tuple (S,D, A, R, {P(s';s,a)}) where: S is a
finite set of states, D is the starting state distribu-
tion, A is a finite set of actions, R is a reward function
R:Sx A- [0,R], and {P(s';s,a)} are the transi-
tion probabilities, with P(s'; s,a) giving the next-state
distribution upon taking action a in state s.

Although ultimately we desire an algorithm which uses
only the given MDP M, we assume access to a restart
distribution, defined as follows:

Definition 2.1. A yu restart distribution draws the
next state from the distribution wp.

This restart distribution is a slightly weaker version of
the generative model in [5]. As in [5], our assumption
is considerably weaker than having knowledge of the
full transition model. However, it is a much stronger
assumption than having only “irreversible” experience,
in which the agent must follow a single trajectory, with
no ability to reset to obtain another trajectory from a
state. If js is chosen to be a relatively uniform distri-
bution (not necessarily D), then this w restart distri-
bution can obviate the need for explicit exploration.

The agent’s decision making procedure is characterized
by a stochastic policy (a; 8), which is the probability
of taking action @ in state s (where the semi-colon is
used to distinguish the parameters from the random
variables of the distribution). We only consider the
case where the goal of the agent is to maximize the 7-
discounted average reward from the starting state dis-
tribution D, though this has a similar solution to max-
imizing the average reward for processes that “mix” on
a reasonable timescale [4]. Given 0 < 7 < 1, we define
the value function for a given policy 7 as

oo
V,(s) =(1-y)E b Risen

+=0
where s; and a, are random variables for the state
and action at time ¢ upon executing the policy 7 from
the starting state s (see [7] for a formal definition of
this expectation). Note that we are using normalized
values so V,,(s) € [0, R]. For a given policy 7, we define
the state-action value as

Qx(s, a) = (1 — Rs, a) + YE sw P(s!;8,0) [Vx-(s')]

and as in [8] (much as in [1]), we define the advantage
as

Ax(8,4) = Qx(s,a) — Vr(s »
Again, both Q,(s,a) € [0,R] and A,(
due to normalization.

a) € [-R,R]

It is convenient to define the y-discounted future
state distribution (as in [8]) for a starting state dis-
tribution p as

(2.1) (1 yyy Pr(s, = 5377, 1)

t=0

Ts bb (s)=

where the 1 — ¥ is necessary for normalization. We
abuse notation and write d,,, for the discounted
future-state distribution with respect to the distribu-
tion which deterministically starts from state s. Note
that Vir(s) = Evar,s')wndy,. [R(s',@’)]. This distribu-
tion is analogous to the stationary distribution in the
undiscounted setting, since as y > 1, d,,, tends to the
stationary distribution for all s, if one such exists.

The goal of the agent is to maximize the discounted
reward from the start state distribution D,

np(m) = Es~D [Vx (s)] .

Note that np(7) = Eva,s)wndz,n [R(s,@)]. A well
known result is that a policy exists which simultane-
ously maximizes V,(s) for all states.

3 The Problems with Current Methods

We now examine in more detail the problems with ap-
proximate value function methods and policy gradient
methods. There are three questions to which we desire
answers to:

(1) Is there some performance measure that is
guaranteed to improve at every step?

(2) How difficult is it to verify if a particular up-
date improves this measure?

(3) After a reasonable number of policy updates,
what performance level is obtained?

We now argue that both greedy dynamic programming
and policy gradient methods give unsatisfactory an-
swers to these questions. Note that we did not ask is
“What is the quality of the asymptotic policy?”. We
are only interested in policies that we can find in a rea-
sonable amount of time. Understanding the problems
with these current methods gives insight into our new
algorithm, which addresses these three questions.

3.1 Approximate Value Function Methods

Exact value function methods, such as policy iteration,
typically work in an iterative manner. Given a pol-
icy 7, policy iteration calculates the state-action value
Q,(s,a), and then creates a new deterministic policy
a'(a; 8) such that 7'(a;s) = 1 iff a € argmax,Q,(s, a).
This process is repeated until the state-action values
converge to their optimal values. These exact value
function methods have strong bounds showing how
fast the values converge to optimal (see [7]).

Approximate value function methods typically use ap-
proximate estimates of the state-action values in an
