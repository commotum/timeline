with function approximation. Neural Information Pro-
cessing Systems, 13, 2000.

[9] R. S. Sutton and A. G. Barto. Reinforcement Learn-
ing: An Introduction. MIT Press, 1998.

[10] S. B. Thrun. Efficient exploration in reinforcement
learning. Technical report, Carnegie Mellon Univer-
sity, 1992.

[11] S. D. Whitehead. Complexity and cooperation in q-
learning. Proc. 8th International Conf. on Machine
Learning, pages 363-367, 1991.

8 Appendix: Proofs

The intuition for the proof of theorem 4.1 is that a de-
termines the probability of choosing an action from 7’.
If the current state distribution is d,,,, when an action
from 7’ is chosen, then the performance improvement
is proportional to the policy advantage. The proof in-
volves bounding the performance decrease due to the
state distribution not being exactly d,,,,, when an ac-
tion from 7’ is chosen.

Proof. Throughout the proof, the 4. dependence is not
explicitly stated. For any state s,

Yo tnew (8; a) Ax(s, a)
= Ya — a)n(a; 8) + an' (a; 8))A,(s, a)

=a > (a; 8)Az(s,a).
where we have used }°, 1(a; 8)Ax(s,a) = 0.

For any timestep, the probability that we choose an
action according to 7’ is a. Let c; be the random vari-
able indicating the number of actions chosen from 7’
before time ¢. Hence, Pr(c; = 0) = (1—)!. Defin-
ing pp = Pr(q, > 1) = 1- (1-2)! and P(s:;7) to
be distribution over states at time t while following 7
starting from s ~ yz, it follows that

Ev P(sestmew) b Mnew(; oto)
= OE, P(sc:men) b Hisan(o)
= a(l— Pt) Es~ P(s¢ \cr=0;r new) Is (a; Hato

tapi Es~P(s:\ce>1itnew) b a (a; nate
a

IV

OE 5x P(64\c=0; new) = a! (a; oto)
a

—2apre

= abyvp(sin) b a! (a; oto) — 2apre
a

where we have used the definition of ¢ and P(s;|c; =
0; Tew) = P(s13 7).

By substitution and lemma 6.1, we have:

Ny (new) — ye (x)

= SOY B.nr(eeimnew) b» Tnew (45 nto)
t=0 a
a> Es~P(sen) Is: m'(a;8)Ax(s, a]
t=0 a
—2ae y( —(1-a)')
t=0
a t
= [ob = (a; 8)An(se, |

1 1
~2ae(— - T=y(1 2a)’ .

The result follows from simple algebra.

IV

The proof of theorem 4.4 follows.

Proof. During step (2), we need enough samples such
that |A—A| < § for every loop of the algorithm and, as

proved below, we need to consider at most = ne loops.
If we demand that the probability of failure is is less than
6, then by the union bound and inequality 5. 1, we have

for k trajectories P(failure) < BE oe ~ ame < 6,
where we have taken A = § since the bias in our esti-

mates is at most 3. Thus, we require O (= log #5)

trajectories.

Thus, if A > 2s, then A > 5 > 0. By corollary

4.2, step 4 guarantees improvement of n, by at leas
As)? A_£
(As) > a using a = ots) which proves i.
Since 0 < m < R, there are at most “= 7p steps be-
fore n, becomes larger than R, so the algorithm mus
cease in this time, which proves ii. In order to cease
and return 7, on the penultimate loop, the G; mus
have returned some 7’ such that A < 2, which im-
plies A, (7) < €. By definition of Gz, it follows thai
OPT(A,) < 2e, which proves iii.

