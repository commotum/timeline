Proof. The optimal policy advantage is OPT(A;,,) =

do, dr u(8) max Az (s,a). Therefore,
é > rac) dy (8) max A,(s, a)
> ne i oath) Dd als) )max A, (s,a)
> |e2|" _ Deel 9) Ana)
d eal
= = y|S2] omer) —mcm
TH loo

where the last step follows from lemma 6.1. The sec-
ond inequality is due to dz,,(s) < (1 — y)u(s).

Note that oun

using ys rather than the future-state distribution of an
optimal policy. The interpretation of each factor of
7 is important. In particular, one factor of _& 5
is due to the fact that difference between the perfor-
mance of 7 and optimal is wh times the average ad-
vantage under dy» i(8) (see lemma 6.1) and another
factor of 7=— is due to the inherent non-uniformity of

dap (since deals < (1-77) u(s)).

is a measure of the mismatch in

7 Discussion

We have provided an algorithm that finds an “approx-
imately” optimal solution that is polynomial in the
approximation parameter ¢, but not in the size of the
state space. We discuss a few related points.

7.1 The Greedy Policy Chooser

The ability to find a policy with a large policy advan-
tage can be stated as a regression problem though we
don’t address the sample complexity of this problem.
Let us consider the error given by:
Esndy,, Max |Ax(s, a) — fr(s,a)]-

This loss is an average loss over the state space (though
it is an oo-loss over actions). It is straightforward to
see that if we can keep this error below 5, then we
can construct an é-greedy policy chooser by choos-
ing a greedy policy based on these approximation f,.
This 1, condition for the regression problem is a much
weaker constraint than minimizing an /,.-error over
the state-space, which is is the relevant error for greedy
dynamic programming (see equation 3.1 and [3]).

Direct policy search methods could also be used to
implement this greedy policy chooser.

7.2 What about improving 7p?

Even though we ultimately seek to have good perfor-
mance measure under 7p, we show that it is importan
to improve the policy under a somewhat uniform mea-
sure. An important question is “Can we improve the
policy according to both np and n, at each update?”
In general the answer is “no”, but consider improving
the performance under ji = (1 — 3) + BD instead o'
just ys. This metric only slightly changes the quality o
the asymptotic policy. However by giving weight to D,
the possibility of improving 7p is allowed if the optimal
policy has large advantages under D, though we do no
formalize this here. The only situation where joint im-
provement with np is not possible is when OPT(A,,p
is small. However, this is the problematic case where,
under D, the large advantages are not at states visited
frequently.

7.3 Implications of the mismatch

The bounds we have presented directly show the im-
portance of ensuring the agent starts in states where
the optimal policy tends to visit. It also suggests that
certain optimal policies are easier to learn in large state
spaces — namely those optimal policies which tend to
visit a significant fraction of the state space. An inter-
esting suggestion for how to choose yz, is to use prior
knowledge of which states an optimal policy tends to
visit.

Acknowledgments

We give warm thanks to Peter Dayan for numerous
critical comments.

References

[1] L. C. Baird. Advantage updating. Technical report,
Wright Laboratory, 1993.

[2] P. Bartlett and J. Baxter. Estimation and approxima-
tion bounds for gradient-based reinforcement learn-
ing. Technical report, Australian National University,
2000.

[3] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic
Programming. Athena Scientific, 1996.

[4] S. Kakade. Optimizing average reward using dis-
counted rewards. In Proc. of Computational Learning
Theory, 2001.

[5] M. Kearns, Y. Mansour, and A. Y. Ng. A sparse
sampling algorithm for near-optimal planning in large
markov decision processes. IJCAI, pages 1324-1231,
1999.

[6] N. Meuleau, L. Peshkin, and K. Kim. Exploration in
gradient-based reinforcement learning. Technical re-
port, Massachusetts Institute of Technology, 2001.

[7] M. Puterman. Markov decision processes : Discrete
stochastic dynamic programming. John Wiley and
Sons, 1994.

[8] R. Sutton, D. McAllester, S. Singh, and Y. Mansour.
Policy gradient methods for reinforcement learning

