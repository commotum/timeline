Approximately Optimal Approximate Reinforcement Learning

Sham Kakade

SHAM@GATSBY.UCL.AC.UK

Gatsby Computational Neuroscience Unit, UCL, London WC1N 3AR, UK

John Langford

JCL@CS.CMU.EDU

Computer Science Department, Carnegie-Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15217

Abstract

In order to solve realistic reinforce-
ment learning problems, it is critical
that approximate algorithms be used.
In this paper, we present the conser-
vative policy iteration algorithm which
finds an “approximately” optimal pol-
icy, given access to a restart distri-
bution (which draws the next state
from a particular distribution) and
an approximate greedy policy chooser.
Crudely, the greedy policy chooser
outputs a policy that usually chooses
actions with the largest state-action
values of the current policy, ie it out-
uts an “approximate” greedy policy.
This greedy policy chooser can be im-
lemented using standard value func-
ion approximation techniques. Un-

der these assump
1) is guarantee
ormance metric

imesteps and (3)
imately” optimal

ions, our algorithm:
to improve a per-
(2) is guaranteed

o terminate in a “small” number of

returns an “approx-
policy. The quanti-

fied statements of (2) and (3) depend
on the quality of the greedy policy
chooser, but not explicitly on the the
size of the state space.

1 Introduction

The two standard approaches, greedy dynamic pro-
gramming and policy gradient methods, have enjoyed
many empirical successes on reinforcement learning
problems. Unfortunately, both methods can fail to ef-
ficiently improve the policy. Approximate value func-
tion methods suffer from a lack of strong theoretical
performance guarantees. We show how policy gradient
techniques can require an unreasonably large number

of samples in order to determine the gradient accu-
rately. This is due to policy gradient method’s fun-
damental intertwining of “exploration” and “exploita-
tion”.

In this paper, we consider a setting in which our al-
gorithm is given access to a restart distribution an
an “approximate” greedy policy chooser. Informally,
the restart distribution allows the agent to obtain
its next state from a fixed distribution of our choos-
ing. Through a more uniform restart distribution,
the agent can gain information about states that i
wouldn’t necessarily visit otherwise. Also informally,
the greedy policy chooser is a “black box” that outputs
a policy that on average chooses actions with large
advantages with respect to the current policy, ie i
provides an “approximate” greedy policy. This “black
box” algorithm can be implemented using one of the
well-studied regression algorithms for value functions
(see [9, 3]). The quality of the resulting greedy pol-
icy chooser is then related to the quality of this “black
box”.

Drawing upon the strengths of the standard ap-
proaches, we propose the conservative policy iteration
algorithm. The key ingredients of this algorithm are:
(1) the policy is improved in a more uniform manner
over the state-space and (2) a more conservative policy
update is performed in which the new policy is a mix-
ture distribution of the current policy and the greedy
policy. Crudely, the importance of (1) is to incorpo-
rate “exploration” and the importance of (2) is to avoid
the pitfalls of greedy dynamic programming methods
which can suffer from significant policy degradation
by directly using “approximate” greedy policies. Our
contribution is in proving that such an algorithm con-
verges in a “small” number of steps and returns an
“approximately” optimal policy, where the quantified
claims do not explicitly depend on the the size of the
state space. We first review the problems with stan-
dard approaches, then state our algorithm.

