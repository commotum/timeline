exact method. These methods suffer from a paucity
of theoretical results on the performance of a policy
based on the approximate values. This leads to weak
answers to all three questions.

Let us consider some function approximator V(s) with
the J,-error:

e = max|V(s) — Vr(s)|

where 7 is some policy. Let 7’ be a greedy policy
based on this approximation. We have the following
guarantee (see [3]) for all states s:
QE

Vai(s) 2 Vi(8) — 3 a

In other words, the performance is guaranteed to not
decrease by more than pe. Question 2 is not applica-
ble since these methods don’t guarantee improvement,
and a performance measure to check isn’t well defined.

(3.1)

For approximate methods, the time required to obtain
some performance level is not well understood. Some
convergence and asymptotic results exist (see [3]).

3.2 Policy Gradients Methods

Direct policy gradient methods attempt to find a good
policy among some restricted class of policies, by fol-
lowing the gradient of the future reward. Given some
parameterized class {79|9 € R™}, these methods com-
pute the gradient

(3.2) Va = > dx,p(s)Vr(a; 8) Qx(s, a)

8,0

(as shown in [8]).

For policy gradient techniques, question 1 has the ap-
pealing answer that the performance measure of inter-
est is guaranteed to improve under gradient ascent. We
now address question 2 by examining the situations in
which estimating the gradient direction is difficult. We
show that the lack of exploration in gradient methods
translates into requiring a large number of samples in
order to accurately estimate the gradient direction.

Consider the simple MDP shown in Figure 3.1
(adapted from [10]). Under a policy that gives equal
probability to all actions, the expected time to the
goal from the left most state is 3(2” —n —1), and with
n = 50, the expected time to the goal is about 1015.
This MDP falls in the class of MDPs in which random
actions are more likely than not to increase the dis-
tance to the goal state. For these classes of problems
(see [11]), the expected time to reach the goal state
using undirected exploration, ie random walk explo-
ration, is exponential in the size of the state space.
Thus, any "on-policy" method has to run for at least
this long before any policy improvement can occur. In

n states

Figure 3.1. Example 1: Two actions move agent to the left
and one actions moves agent to the right.

7”
i

eo
fi
YW’:
average reward

= 0 05 1 1.5 2
Re? time x 10”

Figure 3.2. Example 2: A two state MDP and the aver-
age reward vs. time (on a 10” scale) of a policy under
standard gradient descent in the limit of an infinitesimally
small learning rate (initial conditions stated in text).

online value function methods, this problem is seen as
a lack of exploration.

Any sensible estimate of the gradient without reaching
the goal state would be zero, and obtaining non-zero
estimates requires exponential time with "on-policy"
samples. Importance sampling methods do exist (see
[6]), but are not feasible solutions for this class of prob-
lems. The reason is that if the agent could follow some
“off-policy” trajectory to reach the goal state in a rea-
sonable amount of time, the importance weights would
have to be exponentially large.

Note that a zero estimate is a rather accurate esti-
mate of the gradient in terms of magnitude, but this
provides no information about direction, which is the
crucial quantity of interest. The analysis in [2] sug-
gests a relatively small sample size is needed to ac-
curately estimate the magnitude (within some e toler-
ance), though this does not imply an accurate direction
if the gradient is small. Unfortunately, the magnitude
of the gradient can be very small when the policy is
far from optimal.

Let us give an additional example demonstrating the
problems for the simple two state MDP shown in Fig-
ure 3.2, which uses the common Gibbs table-lookup
distributions, {7g : m(a;s) « exp(@sq)}. Increasing
the chance of a self-loop at i decreases the stationary
probability of j, which hinders the learning at state j.
Under an initial policy that has the stationary distri-
bution p(i) = .8 and p(j) = .2 (using 7(a1;7) = .8 and
