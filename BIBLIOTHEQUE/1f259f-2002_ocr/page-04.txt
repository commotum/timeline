The proof of this theorem is given in the appendix. It
is possible to construct a two state example showing
this bound is tight for all a though we do not provide
this example here.

The first term is analogous to the first order increase
specified in equation 4.2, and the second term is a
penalty term. Note that if a = 1, the bound reduces
to

QE
~1-y 1-7
and the penalty term has the same form as that of
greedy dynamic programming, where ¢, as defined
here, is analogous to the J. error in equation 3.1.

Ny (anew) Nw (x) >

The following corollary shows that the greater the pol-
icy advantage the greater the guaranteed performance
increase.

Corollary 4.2. Let R be the maximal possible reward
and A be the policy advantage of x' with respect to 7
and y. If A> 0, then using a = a spe guarantees the
following policy improvement:

2
Nu (new) — N(m) = BR°

Proof. Using the previous theorem, it is aie
ward to show the change is bounded by -— ~(A-a Ze

The corollary follows by choosing the a that maximizes
this bound.

4.2 Answering question 3

We address question 3 by first addressing how fast we
converge to some policy then bounding the quality o
this policy. Naively, we expect our ability to obtain
policies with large advantages to affect the speed of im-
provement and the quality of the final policy. Insteas
of explicitly suggesting algorithms that find policies
with large policy advantages, we assume access to an
é-greedy policy chooser that solves this problem. Le
us call this e-good algorithm G,.(7, 4), which is define
as:

Definition 4.3. An eé-greedy policy chooser,
G-(x,p), is a function of a policy 7 and a state
distribution which returns a policy x’ such thai
Arum’) > OPT(Az,,) — €, where OPT(A;,,) =
maxzy Ax,y (7).

In the discussion, we show that a regression algorithm
that fits the advantages with an § average error is
sufficient to construct such a G,.

The “breaking point” at which policy improvement is
no longer guaranteed occurs when the greedy policy
chooser is no longer guaranteed to return a policy with
a positive policy advantage, ie when OPT(A;,,,) < €.
A crude outline of the Conservative Policy Itera-
tion algorithm is:

(1) Call G.(a, 4) to obtain some 7’

(2) Estimate the policy advantage A,,,(7’)

(3) If the policy advantage is small (less than e),
STOP and return 7.

(4) Else, update the policy and go to (1).

where, for simplicity, we assume ¢ is known. This algo-
rithm ceases when an € small policy advantage (with
respect to 7) is obtained. By definition of the greedy
policy chooser, it follows that the optimal policy ad-
vantage of 7 is less than 2¢. The full algorithm is spec-
ified in the next section. The following theorem shows
that in polynomial time, the full algorithm finds a pol-
icy 7 that is close to the “break point” of the greedy
policy chooser.

Theorem 4.4. With probability at least 1—6, conser-
vative policy iteration: i) improves n, with every pol-
icy update, ii) ceases in at most 2 calls to G. (7, 4),
and iii) returns a policy 7 such that OPT(Az,,) < 2e.

The proof is in the appendix.

To complete the answer to question 3, we need to ad-
dress the quality of the policy found by this algorithm.
Note that the bound on the time until our algorithm
ceases does not depend on the restart distribution
though the performance of the policy 7 that we find
does depend on ys since OPT(A,,,) < 2e. Crudely,
for a policy to have near optimal performance then all
advantages must be small. Unfortunately, if d,,,, is
highly non-uniform, then a small optimal policy ad-
vantage does not necessarily imply that all advantages
are small. The following corollary (of theorem 6.2)
bounds the performance of interest, 7p, for the policy
found by the algorithm. We use the standard defini-
tion of the /,.-norm, ||f||oo = max, f(s).

Corollary 4.5. Assume that for some policy x,
OPT(Az,y) < €. Let x* be an optimal policy. Then

é dy*,D
n mT < 2
np(n*) — np(m) < ay arw Ie
€ dy*,D
<a le
The factor of |S, represents a mismatch be-

tween the distribution of states of the current pol-
icy and that of the optimal policy and elucidates the
problem of using the given start-state distribution D
instead of a more uniform distribution. Essentially,
a more uniform d,,,, ensures that the advantages are
small at states that an optimal policy visits (deter-
mined by d,«,p). The second inequality follows since
dyy(s) > (1 — y)u(s), and it shows that a uniform
measure prevents this mismatch from becoming arbi-
trarily large.

We now discuss and prove these theorems.
