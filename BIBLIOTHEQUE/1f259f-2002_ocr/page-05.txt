5 Conservative Policy Iteration

For simplicity, we assume knowledge of ¢. The conser-
vative policy iteration algorithm is:

(1) Call G.(z, u) to obtain some 7’

(2) Use OE log B) prrestarts to obtain an §-
accurate estimate A of A,,,(z’).

(3) If A < 22, STOP and return 7.

(4) fA> ae, then update policy 7 according to
equation 4.1 using Cota) and return to
step 1.

where 6 is the failure probability of the algorithm.
Note that the estimation procedure of step (2) allows
us to set the learning rate a.

We now specify the estimation procedure
of step (2) to obtain 4-accurate estimates
of A,(z'). It is straightforward to show
that the policy advantage can be written as
Aryl!) = Eyvds,, [Ca(t(a;8) — 1(a;8))Qx(8,4)]-

We can obtain a nearly unbiased estimate x; of
Ax,u(x') using one call to the y-restart distribution
(see definition 2.1). To obtain a sample s from from
dz, we obtain a trajectory from s9 ~ p and accept
the current state s, with probability (1 — y) (see
equation 2.1). Then we chose an action a from the
uniform distribution, and continue the trajectory
from s to obtain a nearly unbiased estimate Q,(s, a)
of Qx(s,a). Using importance sampling, the nearly
unbiased estimate of the policy advantage from the
i-th sample is 7; = naQ;(s, a)(a' (a; 8) — 7 (a; 8)) where
Nq is the number of actions. We assume that each
trajectory is run sufficiently long such that the bias in
2; is less than 5.

Since 0: € [0,R], our samples satisfy x; €
[-n.R,n~R]. Using Hoeffding’s inequality for k inde-
pendent, identically distributed random variables, we
have:
(5.1) Pr(|A~A|>A) < 26 BER,

where A = ¢ an x; (and A here is § biased). Hence,
the number of trajectories required to obtain a A ac-

2 p2

curate sample with a fixed error rate is O (a) :
The proof of theorem 4.4, which guarantees the sound-

ness of conservative policy iteration, is straightforward
and in the appendix.

6 How Good is The Policy Found?

Recall that the bound on the speed with which our
algorithm ceases does not depend on the restart dis-
tribution used. In contrast, we now show that the

quality of the resulting policy could strongly depend
on this distribution.
The following lemma is useful:

Lemma 6.1. For any policies 7 and x and any start-
ing state distribution ju,

na) — Mul) = <= Blonds An]

Proof. Let P,(s') = P(s¢ = 8';7,80 = 8). By defini-
tion of Vz(s),

Vi(s)
= (1-7) Yo Flas na? [R(se, az)]
t=0
= S07 Eesynar, (tL — 1) R(s¢, a8)
t=0

+V;(st) — Vi(sz)]

ES
= So Ears 5141) °F Ps P(se41381001) [(1 — yR(st, ae)

t=0

+7Vi(st41) — Vir(se))] + Va(s)

oo
= Vals) + 0) 7! Blar.s:)n#P [Ax(S1,4%)]
t=0
. 1
= V,(s)+ Tay Maeda [Ax(s', @)]
and the result follows by taking the expectation of this
equation with respect to p.

This lemma elucidates a fundamental measure mis-
match. The performance measure of interest, 7p(7)
changes in proportion the policy advantage A,,p(x'
for small a (see equation 4.2), which is the average
advantage under the state distribution d;,p. How-
ever, for an optimal policy 7*, the difference between
np(x*) and np(m) is proportional to the average ad-
vantage under the state distribution d,+,p. Thus, even
if the optimal policy advantage is small with respect
to a and D, the advantages may not be small under
d,*,p- This motivates the use of the more uniform
distribution wp.

After termination, our algorithm returns a policy 7
which has small policy advantage with respect to p.
We now quantify how far from optimal 7 is, with re-
spect to an arbitrary measure ji.

Theorem 6.2. Assume that for a policy 7,
OPT(Az,y) < €. Let x* be an optimal policy. Then
for any state distribution ji,

(et) € || dae
nim") — nal) < G—) Il dew Ul
€ da
< — || Oxf
<a le

