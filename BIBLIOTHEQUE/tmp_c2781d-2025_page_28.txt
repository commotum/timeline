                      Preprint, Under Review.
                      Appendix A), which were sampled uniformly. The planning frequency for trajectories in the dataset
                      wasalso sampled uniformly from the range [2, 12]. The Llama 3.1 8B instruct model served as the
                      baseline for all SFT experiments, with no Low-Rank Adaptation (LoRA) adapters applied during this
                      stage (Hu et al., 2022). We used AdamW (Loshchilov & Hutter, 2019) as the optimizer throughout.
                      Weconsideredtwoobjectivefunctions: directactionpredictionandfullworldmodeling. Ourfindings,
                      presented in Figures 7 and 8, indicate that omitting world modeling yields superior results in terms of
                      both performance and reduced catastrophic forgetting. While we initially tested world modeling as a
                      potential regularization technique to mitigate overfitting, it ultimately proved to be a distractor for the
                      agents. To additionally model the environment dynamics, the weights of the model had to be changed
                      moresignificantly, which is reflected in the higher KL divergence, and consequently led to increased
                      forgetting and reduced model steerability. It is worth noting that for the agent to plan dynamically
                      during SFT, it must also model the plans themselves, exposing it to a greater number of tokens during
                      this stage. Training was performed using the DeepSpeed ZeRO Stage 3 optimizer (Aminabadi et al.,
                      2022) to effectively manage memory and scale operations across multiple GPUs.
                      Figure 7: SFT Training Metrics. Comparison of (left) total loss, (center) accuracy, and (right) KL
                      divergence across training steps for four SFT configurations. Models incorporating world modeling
                      (green, red) exhibit lower training loss and higher accuracy but also show higher KL divergence.
                      Amongtheconfigurations without world modeling, ’SFT plan dynamically’ (orange) demonstrates
                      highest total loss as modeling plans is much harder then modeling the world or just actions.
                      Figure 8: SFT Evaluation Metrics. Comparison of (left) normalized game score, (center) total
                      numberofplansgenerated, and (right) average plan length across training steps for the same four SFT
                      configurations as in Figure 7. The configurations without world modeling (orange, blue) consistently
                      achieve better task performance in terms of normalized score then configurations which additionally
                      model the world (green, red).
                      TheSFThyperparameters are shown in Table 1.
                      C.2  RLFT
                      During the Reinforcement Learning Fine-Tuning (RLFT) phase, we applied RSLoRA adapters
                      (Kalajdzievski, 2023) separately to actor and critic models. Data collection involved using vLLM
                      (Kwonetal., 2023) to host model weights and BALROG (Paglieri et al., 2025a) integrated with Ray
                      (Moritz et al., 2018) to process outputs and handle agent-environment interactions efficiently. A
                                                             28
