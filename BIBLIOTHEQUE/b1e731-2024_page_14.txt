                                                         Inference                                                Inference
                               Training       Mean         GA10         GA100           Training      Mean        GA10         GA100
                               Mean        30.2 (15.7)   72.8 (29.5)  82.2 (21.3)       Mean        7.6 (5.6)   51.3 (35.7)  62.8 (38.3)
                               GA1          26.6 (7.4)   98.0 (0.6)    99.2 (0.6)       GA1         7.4 (4.4)   93.1 (4.3)    97.7 (2.2)
                               GA2          14.4 (3.5)   97.2 (1.6)    98.9 (1.2)       GA2         3.0 (2.6)   86.1 (6.0)    95.9 (2.1)
                               GA3          1.0 (0.7)    85.7 (6.3)    98.3 (1.9)       GA3         0.0 (0.0)   55.0 (7.8)    93.9 (3.8)
                               GA1(g)       10.6 (6.4)   93.8 (5.1)    97.4 (1.9)       GA1(g)      1.3 (1.0)   81.7 (13.3)   91.5 (5.5)
                                          Training distribution                                 Weaklyout-of-distribution
                                                                                     Inference
                                                           Training      Mean        GA10         GA100
                                                           Mean         0.3 (0.5)  18.8 (14.5)   41.1 (29.6)
                                                           GA1          0.0 (0.0)  59.9 (11.6)   88.0 (5.3)
                                                           GA2          0.0 (0.0)  38.5 (13.0)   81.8 (10.9)
                                                           GA3          0.0 (0.0)   11.3 (9.3)   72.0 (14.0)
                                                           GA1(g)       0.0 (0.0)  40.9 (19.8)   71.1 (14.3)
                                                                   Strongly out-of-distribution
                             Table 2: Study of the out-of-distribution (OOD) performance on the Pattern task. Models are
                             trained on patterns that have a density of 50% (half black, half colored), then evaluated on the same
                             distribution, on a density of 75% (weakly OOD) and 100% (strongly OOD). Performance is averaged
                             over 3 training runs with different seeds, with standard deviation in parentheses.
                             generators from Hodel [2024] to generate as many input-output pairs as needed during training in an
                             online fashion. These 400 generators solely implement the programs from the ARC-AGI training set.
                             Being able to perfectly execute this set of 400 programs is a necessary yet not sufficient condition
                             for solving ARC-AGI. By limiting the training set to only the core knowledge priors shown in the
                             training dataset we ensure that there is no data leakage from other sources such as the evaluation
                             datasets. Therefore any performance on the evaluation set we demonstrate in this work is purely due
                             to generalization.
                                                     Training Loss                           ARC-AGI Training Challenges Accuracy
                                                                                     0.25
                                                                                     0.20
                                                                                    Accuracy0.15
                                Loss (log scale)                                     0.10
                                 10 1
                                                                                     0.05
                                                                                     0.00
                                        0     50000    100000  150000   200000           0       50000   100000   150000    200000
                                                     Training Steps                                      Training Steps
                             Figure 10: Training LPN on the re-arc generators from the ARC-AGI training set. The run took 5
                             days on a TPU v3-8 without converging. (Left): training loss in log scale. (Right): top-2 accuracy
                             using mean inference mode (no latent optimization) on the 400 tasks from the ARC-AGI training set.
                             Training    Wetrain an LPN model of 39M parameters with a latent space of dimension 256, using
                             meantraining, i.e. no latent optimization during training. We performed 220k training steps with
                             a batch size of 128 on a TPU v3-8, which took 5 days. We then fine-tuned the model for 2k steps
                                                                                14
