4.1 Process vs Outcome Supervision

We now conduct a direct comparison of outcome and process supervision. We
first sample between 1 and 200 solutions per problem from a small-scale genera-
tor. For each dataset, we provide three forms of supervision: process supervision

from PRMjarge, outcome supervision
from final-answer checking. The cho
between these three series of reward
identical datasets. See Appendix H

rom PRMharge, and outcome supervision

ice of supervision is the only difference

models, which are otherwise trained on
or more details about how PRMharge is

used for outcome and process supervision.

In Figure 4a, we evaluate each reward model by its best-of-500 selection. We
see that process supervision significantly outperforms both forms of outcome
supervision at all data collection scal In Figure 4b, we evaluate the best
reward model from each series by its best-of-N performance across different
values of N. We see that using PRMjarge for outcome supervision is noticeably
more effective than final-answer checking. This can be explained by the fact
that PRMjarge provides better supervision for solutions that reach the correct
final answer using incorrect reasoning.

It is not clear whether supervision by PRMiarge or by final-answer checking
represents the more appropriate outcome supervision baseline. While final-
answer supervision is more explicitly outcome based, its main weakness — the
existence of false positives — is arguably over-emphasized in the MATH dataset.
Outcome supervision by PRMharge better represents outcome supervision in do-
mains that are less susceptible to false positives. We consider outcome supervi-
sion by PRMharge to be the more relevant baseline, but we encourage the reader
to draw their own conclusions.

es.

4.2 Active Learning

Finally, we investigate the impact of active learning. We train a small-scale
reward model, PRMeclector, On a single sample from each problem, and we use
this model to score 1000 samples per problem. To train each of our larger re-
ward models, we select N samples per problem such that 80% are the most
convincing (according to PRMgelector) Wrong-answer samples, and 20% are the
most convincing samples that remain (right- or wrong-answer). We score the
selected samples with PRMjarge and train on those scores. This process ensures
that all samples are relatively convincing under PRMsgelector, that a large frac-
tion are known to contain at least one mistake, and that our overall dataset
is not too heavily biased toward wrong-answer solutions. Performance of this
data labelling scheme is shown in Figure 4a. By comparing the slopes of the
line of best fit with and without active learning, we estimate that this form
of active learning is approximately 2.6x more data efficient than uniform data
labelling. We note that the model trained on the largest active learning dataset
(200 samples per problem) appears to slightly underperform the expected trend
line. Our best explanation for this observation is that 200 samples represents
a significant fraction of the overall selection pool (1000 samples) and that this

