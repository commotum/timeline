                                         x                                             ImageNet test set, and won the 1st place in the ILSVRC
                                                                                       2015 classiﬁcation competition. The extremely deep rep-
                                       weight layer                                    resentations also have excellent generalization performance
                            F(x)             relu            x                         on other recognition tasks, and lead us to further win the
                                       weight layer                                    1st places on: ImageNet detection, ImageNet localization,
                                                          identity
                               F(x)+x                                                COCOdetection, and COCO segmentation in ILSVRC &
                                             relu                                      COCO2015competitions. Thisstrongevidenceshowsthat
                         Figure 2. Residual learning: a building block.                the residual learning principle is generic, and we expect that
                                                                                       it is applicable in other vision and non-vision problems.
               are comparably good or better than the constructed solution
               (or unable to do so in feasible time).                                  2. Related Work
                  In this paper, we address the degradation problem by                 Residual Representations. In image recognition, VLAD
               introducing a deep residual learning framework.                In-
               stead of hoping each few stacked layers directly ﬁt a                   [18] is a representation that encodes by the residual vectors
               desired underlying mapping, we explicitly let these lay-                with respect to a dictionary, and Fisher Vector [30] can be
               ers ﬁt a residual mapping. Formally, denoting the desired               formulated as a probabilistic version [18] of VLAD. Both
               underlying mapping as H(x), we let the stacked nonlinear                of them are powerful shallow representations for image re-
               layers ﬁt another mapping of F(x) := H(x)−x. The orig-                  trieval and classiﬁcation [4, 48]. For vector quantization,
               inal mappingisrecastintoF(x)+x. Wehypothesizethatit                     encoding residual vectors [17] is shown to be more effec-
               is easier to optimize the residual mapping than to optimize             tive than encoding original vectors.
               the original, unreferenced mapping. To the extreme, if an                   In low-level vision and computer graphics, for solv-
               identity mapping were optimal, it would be easier to push               ing Partial Differential Equations (PDEs), the widely used
               the residual to zero than to ﬁt an identity mapping by a stack          Multigrid method [3] reformulates the system as subprob-
               of nonlinear layers.                                                    lems at multiple scales, where each subproblem is respon-
                  TheformulationofF(x)+xcanberealizedbyfeedfor-                        sible for the residual solution between a coarser and a ﬁner
               ward neural networks with “shortcut connections” (Fig. 2).              scale. An alternative to Multigrid is hierarchical basis pre-
               Shortcut connections [2, 34, 49] are those skipping one or              conditioning [45, 46], which relies on variables that repre-
               more layers. In our case, the shortcut connections simply               sent residual vectors between two scales. It has been shown
               perform identity mapping, and their outputs are added to                [3, 45, 46] that these solvers converge much faster than stan-
               the outputs of the stacked layers (Fig. 2). Identity short-             dard solvers that are unaware of the residual nature of the
               cut connections add neither extra parameter nor computa-                solutions. These methods suggest that a good reformulation
               tional complexity. The entire network can still be trained              or preconditioning can simplify the optimization.
               end-to-end by SGD with backpropagation, and can be eas-                 Shortcut Connections. Practices and theories that lead to
               ily implemented using common libraries (e.g., Caffe [19])               shortcut connections[2,34,49]havebeenstudiedforalong
               without modifying the solvers.                                          time. An early practice of training multi-layer perceptrons
                  We present comprehensive experiments on ImageNet                     (MLPs)istoaddalinear layer connected from the network
               [36] to show the degradation problem and evaluate our                   input to the output [34, 49]. In [44, 24], a few interme-
               method. Weshowthat: 1) Our extremely deep residual nets                 diate layers are directly connected to auxiliary classiﬁers
               are easy to optimize, but the counterpart “plain” nets (that            for addressing vanishing/exploding gradients. The papers
               simply stack layers) exhibit higher training error when the             of [39, 38, 31, 47] propose methods for centering layer re-
               depth increases; 2) Our deep residual nets can easily enjoy             sponses, gradients, and propagated errors, implemented by
               accuracy gains from greatly increased depth, producing re-              shortcut connections. In [44], an “inception” layer is com-
               sults substantially better than previous networks.                      posed of a shortcut branch and a few deeper branches.
                  Similar phenomena are also shown on the CIFAR-10 set                     Concurrent with our work, “highway networks” [42, 43]
               [20], suggesting that the optimization difﬁculties and the              present shortcut connections with gating functions [15].
               effects of our methodarenotjustakintoaparticulardataset.                These gates are data-dependent and have parameters, in
               Wepresentsuccessfully trained models on this dataset with               contrast to our identity shortcuts that are parameter-free.
               over 100 layers, and explore models with over 1000 layers.              When a gated shortcut is “closed” (approaching zero), the
                  On the ImageNet classiﬁcation dataset [36], we obtain                layers in highway networks represent non-residual func-
               excellent results by extremely deep residual nets. Our 152-             tions.    On the contrary, our formulation always learns
               layer residual net is the deepest network ever presented on             residual functions; our identity shortcuts are never closed,
               ImageNet, while still having lower complexity than VGG                  and all information is always passed through, with addi-
               nets [41].   Our ensemble has 3.57% top-5 error on the                  tional residual functions to be learned. In addition, high-
                                                                                   2
