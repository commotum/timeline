                           Published as a conference paper at ICLR 2024
                           since an engineer working on addressing an issue may not know a priori which files need to be
                           modified. In addition, this setting is also not necessarily comprehensive since edited files alone may
                           not include all the required context to understand exactly how software will behave when interacting
                           with unseen parts of the code.
                           We compare the BM25 retrieval results with those of the “oracle” retrieval setting, as shown in
                           Table 3. We observe that in approximately 40% of instances, BM25 retrieves a superset of the
                           oracle files for the 27,000-token context limit. However, in almost half of the instances with the
                           27,000-token limit, it retrieves none of the files from the “oracle” context.
                           4.2   INPUT FORMAT
                           Once the retrieved files are selected using one of the two methods above, we construct the input
                           to the model consisting of task instructions, the issue text, retrieved files and documentation, and
                           finally an example patch file and prompt for generating the patch file. Examples of instances and
                           further details on this formulation are provided in Appendix D.
                           4.3   MODELS
                           Due to the need to process long sequence lengths, there are only a few models that are currently
                           suitable for SWE-bench. Thus we evaluate ChatGPT-3.5 (gpt-3.5-turbo-16k-0613), GPT-4
                           (gpt-4-32k-0613), Claude 2, and SWE-Llama with their context limits shown in Table 4.
                           Table 2: Model resolve rates with BM25 re-        Table3: BM25recallwithrespecttooraclefiles
                           trieval, with different maximumcontextlengths.    for different maximum context lengths.
                                                     Max. Content                                BM25Recall
                               Model              13k    27k     50k                         13k      27k     50k
                               Claude 2           1.96   1.87   1.22                 Avg.   29.58    44.41   51.06
                               SWE-Llama7b        0.70   0.31   0.00                 All    26.09    39.83   45.90
                               SWE-Llama13b       0.70   0.48   0.00                 Any    34.77    51.27   58.38
                           Table 4: We compare the different context lengths and proportion of the “oracle” retrieval setting
                           covered. Models with shorter context lengths are thus inherently disadvantaged. Note that descrip-
                           tions of token-lengths is a relative non-standard measure (e.g. Llama-tokenized sequences are 42%
                           longer on average than the equivalent sequence tokenized for GPT-4).
                                                           ChatGPT-3.5     GPT-4    Claude 2    SWE-Llama
                                          Max. Tokens         16,385      32,768    100,000      ≥100,000
                                          %ofInstances        58.1%        84.1%     96.4%        ≥94.8%
                           5   RESULTS
                           Wereportresultsformodelsusingdifferentretrievalmechanismsandpromptingstyles,thenprovide
                           some analysis and insight into model performance and difficulty. We summarize models’ perfor-
                           mance using BM25 retrieval in Table 5. Across the board, models struggle significantly to resolve
                           issues. The best performing model, Claude 2, is only able to resolve 1.96% of the issues.
                           To analyze the importance of the retriever to the overall system results, we present the “oracle”
                           retrieval results in Appendix Table 18. There, Claude 2 is able to resolve 4.8% of issues using the
                           “oracle” retriever. We further analyze the importance of context in the discussion below.
                           Difficulty differs across repositories. When breaking performance down by repository, all models
                           trend similarly across different repositories as show in Figure 4. Despite this, the issues resolved by
                           eachmodeldonotnecessarilyoverlapextensively. Forexample,inthe“oracle”settingClaude2and
                                                                          5
