                        Published as a conference paper at ICLR 2025
                        Unlike conventional models, which scale parameters indiscriminately (Jeon et al., 2021; Lu et al., 2022; Wu
                        et al., 2020), our model adaptively reuses parameters across tasks. By doing so, it balances the inherent tension
                        between the efficient use of parameters across tasks of varied complexity and the challenge of parameter
                        allocation based on input difficulty.
                        Theintrospection network operates as a switch, choosing between the Fixed-Point Iterative (FPI) layers or
                        opting for a no-operation action, based on the input representation in the prediction model (Vaswani et al.,
                        2017; Hochreiter & Schmidhuber, 1995). Notably, given that the introspection network’s decision is based
                        on the input representation produced by the more complex prediction network, it does not need as much
                        complexity as that required for a direct input assessment.
                        In essence, the MIND model effectively manages
                        the allocation of computational efforts, saving
                        complex tasks for more comprehensive process-
                        ing and conserving resources for simpler ones.
                        Figure 1 outlines the architecture of our proposed
                        model.
                        Todemonstratetheeffectivenessofthisapproach,
                        we show how the MIND model can detect pat-
                        terns of varying difficulty using an Ising model
                        (Cipra, 2000) as a toy example, illustrating its
                        adaptive computational capabilities. We further
                        validate the MIND model’s effectiveness on stan-
                        dard benchmarks across multiple domains. In
                        language modeling, we evaluate on SQuAD (Ra-
                        jpurkar et al., 2016) and WikiText (Gardent et al.,
                        2017). To demonstrate that it is domain-agnostic,
                        wealsovalidate on vision benchmarks including
                        CIFAR-100 (Krizhevsky, 2009) and ImageNet
                        (Deng et al., 2009). Remarkably, the MIND
                        model surpasses ResNet-50 (He et al., 2016) and
                        EfficientNet (Tan & Le, 2021) on ImageNet, and     Figure 1: Architecture of the MIND model and the order
                        Transformer (Vaswani et al., 2017) on SQuAD, of operation. The introspection network decides on the
                        while using only a three-layer predictive network  computational branch to follow, optimizing efficiency by
                        and 5 to 12 times fewer parameters.                allocating more resources to harder inputs and less to
                        Theremainder of this paper is structured as fol-   easier ones. FPIs reuse parameters and dynamically adapts
                        lows: Section 2 provides an overview of back- to the input difficulty.
                        ground work related to adaptive computation and
                        introspection mechanisms in deep learning; Section 3 details the architecture of the MIND model, explaining
                        the roles of the primary prediction network and the auxiliary introspection network; Section 4 presents our
                        experimental methodology, the datasets used, and the results obtained from our evaluations.
                        2    RELATED WORK
                        2.1   FIXED-POINT METHODS IN NEURAL NETWORKS
                        Fixed-point iteration (FPI) methods have a rich history in neural network optimization and architecture design.
                        Both Almeida (1987) and Pineda (1987) independently introduced the concept of fixed points by extending
                        backpropagation to recurrent neural networks. Recent advances, such as the modernized formulation of Liao
                        et al. (2018) and the Deep Equilibrium Models (DEQs) proposed by Bai et al. (2019), build on these early
                        insights. DEQs implicitly define infinitely deep networks through a fixed-point formulation, continuing the
                        tradition of using FPI methods in neural networks. Our approach integrates these developments, incorporating
                        FPI layers to enable adaptive computation depth.
                                                                          2
