                                  Published as a conference paper at ICLR 2024
                                  Table 5: We compare models against each other using the BM25 retriever as described in Section 4.
                                  ∗Duetobudgetconstraints we evaluate GPT-4 on a 25% random subset of SWE-bench.
                                                                       Model                  %Resolved        %Apply
                                                                       Claude 2                   1.96           43.07
                                                                       ChatGPT-3.5                0.17           26.33
                                                                       GPT-4∗                     0.00           14.83
                                                                       SWE-Llama7b                0.70           51.74
                                                                       SWE-Llama13b               0.70           53.62
                                      15                                                                                                  ChatGPT-3.5
                                      10                                                                                                  Claude 2
                                     % Resolved5                                                                                          SWE-Llama 13b
                                       0
                                          astropy django         seaborn flask requests xarray  pylint pytest         sphinx sympy
                                                        matplotlib                                           scikit-learn
                                  Figure 4: Resolution rate for three models across the 12 repositories represented in SWE-bench in
                                  the “Oracle” retrieval setting.
                                  SWE-Llama13bperformcomparably,witheachmodelresolving110and91instancesrespectively.
                                  Yet of these instances, Claude 2 only solves 42% of the instances solved by SWE-Llama.
                                  This may also be related to the presence of images in issues, which can be encoded into the is-
                                  sue markdown with embedded image links (i.e. ![image][https://...]). Some repositories
                                  naturally feature more instances with images; for example 32% of matplotlib and 10% of seaborn
                                  instances contain embedded images in their issue text compared to just 2% of all instances. Solving
                                  these instances may require multi-modal LMs or some kind of external tool use to process images.
                                  Difficulty correlates with context length. Chat models may be pre-trained on long sequences of
                                  code but are typically asked to generate shorter coder snippets with limited context provided to
                                  frame the question. As shown in Figure 5, we see that as total context length increases, Claude 2’s
                                  performance drops considerably; behavior that is also observed in other models. In our evaluation
                                  settings, models see a lot of code that may not be directly related to solving the issue at hand, and
                                  they seem to frequently struggle with localizing problematic code needing to be updated. This result
                                  corroborates other studies showing that models become distracted by additional context and may be
                                  sensitive to the relative location of target sequences (Liu et al., 2023b). Even when increasing the
                                  maximumcontextsizeforBM25wouldincreaserecallwithrespect to the oracle files, performance
                                  drops, as shown in Table 2, as models are simply ineffective at localizing problematic code.
                                       50                                                                           Table 6: We show the results for
                                       40                                                                           the “Oracle”-collapsed retrieval
                                       30                                                              Status       setting, which uses oracle files but
                                       20                                                               Resolved    collapses code that isn’t directly
                                      % of Tasks10                                                      Applied     modified by the PR ±15 lines.
                                        0
                                           <20k  20k-50k50k-100k >100k    <500  500-1k 1k-2k  >2k                      Model            “Oracle”-collapsed
                                              # of Input Tokens              # of Issue Tokens                                         Resolved     Applied
                                                                                                                       ChatGPT-3.5        1.09       40.93
                                  Figure 5: We compare the performance of Claude 2 on tasks                            Claude 2           5.93       68.18
                                  partitioned by total input length and by only the issue length.                      GPT-4              3.40       48.65
                                  Further investigating this, we provide an input ablation on the “oracle” retrieval context, “oracle”-
                                  collapsed, where retrieved files are collapsed entirely, except for the lines actually edited by the
                                  true pull request (with ±15 lines of buffer) shown in Table 6. In this setting, we see increases in
                                  performance, with GPT-4 jumping from 1.3% to 3.4% and Claude 2 from 4.8% to 5.9%.
                                                                                                6
