                                                                        ARTICLE IN PRESS
                                                                 Neurocomputing 71 (2008) 1180–1190
                                                                                                                                       www.elsevier.com/locate/neucom
                                                                   Natural Actor-Critic
                                                               Jan Petersa,b,, Stefan Schaalb,c
                                                     aMax-Planck-Institute for Biological Cybernetics, Tuebingen, Germany
                                                        b
                                                         University of Southern California, Los Angeles, CA 90089, USA
                                                    c
                                                     ATRComputational Neuroscience Laboratories, Kyoto 619-0288, Japan
                                                                        Available online 1 February 2008
           Abstract
             In this paper, we suggest a novel reinforcement learning architecture, the Natural Actor-Critic. The actor updates are achieved using
           stochastic policy gradients employing Amari’s natural gradient approach, while the critic obtains both the natural policy gradient and
           additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy
           gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be
           estimated more efﬁciently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by
           the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the
           original Actor-Critic and Bradtke’s Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations
           illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning
           control on an anthropomorphic robot arm.
           r2008Elsevier B.V. All rights reserved.
           Keywords: Policy-gradient methods; Compatible function approximation; Natural gradients; Actor-Critic methods; Reinforcement learning; Robot
           learning
           1. Introduction                                                                   suchunfortunatebehavior can be found in many well-known
                                                                                             greedy reinforcement learning algorithms [6,8].
              Reinforcement learning algorithms based on value func-                            Asanalternative to greedy reinforcement learning, policy-
           tion approximation have been highly successful with discrete                      gradient methods have been suggested. Policy gradients have
           lookup table parameterization. However, when applied with                         rather strong convergence guarantees, even when used in
           continuous function approximation, many of these algo-                            conjunction with approximate value functions, and recent
           rithms failed to generalize, and few convergence guarantees                       results created a theoretically solid framework for policy-
           could be obtained [24]. The reason for this problem can                           gradient estimation from sampled data [25,15].However,
           largely be traced back to the greedy or -greedy policy                           even when applied to simple examples with rather few states,
           updates of most techniques, as it does not ensure a policy                        policy-gradient methods often turn out to be quite inefﬁcient
           improvement when applied with an approximate value                                [14], partially caused by the large plateaus in the expected
           function [8]. During a greedy update, small errors in the                         return landscape where the gradients are small and often do
           value function can cause large changes in the policy which in                     not point directly towards the optimal solution. A simple
           return can cause large changes in the value function. This                        examplethatdemonstratesthisbehaviorisgiveninFig. 1.
           process, when applied repeatedly, can result in oscillations or                      Similar as in supervised learning, the steepest ascent with
           divergence of the algorithms. Even in simple toy systems,                         respect to the Fisher information metric [3], called the
                                                                                             ‘natural’ policy gradient, turns out to be signiﬁcantly more
                                                                                             efﬁcient than normal gradients. Such an approach was ﬁrst
                                                                                            suggested for reinforcement learning as the ‘average
              Corresponding author at: Max-Planck-Institute for Biological Cybernetics,      natural policy gradient’ in [14], and subsequently shown
           Department of Empirical Inference, Spemannstr. 38, 72076 Tuebingen,
           Germany.                                                                          in preliminary work to be the true natural policy gradient
              E-mail address: jan.peters@tuebingen.mpg.de (J. Peters).                       [21,4]. In this paper, we take this line of reasoning one step
           0925-2312/$-see front matter r 2008 Elsevier B.V. All rights reserved.
           doi:10.1016/j.neucom.2007.11.026
                                                                    ARTICLE IN PRESS
                                                        J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190                                       1181
                                                                                                                                            p            T
                                                                                       mated with linear function approximation V ðxÞ¼/ðxÞ v.
                                                                                       The general goal is to optimize the normalized expected
                                                                                       return
                                                                                                                      
                                                                                                   ()
                                                                                                              1       
                                                                                       JðhÞ¼E ð1gÞXgtrh
                                                                                                  t                  t
                                                                                             ¼ Z      p    Z t¼0      
                                                                                                  Xd ðxÞ UpðujxÞrðx;uÞdxdu,
                                                                                       where
            Fig. 1. Whenplottingtheexpectedreturnlandscapeforsimpleproblemas                              1
            1d linear-quadratic regulation, the differences between (a) ‘vanilla’ and  dpðxÞ¼ð1gÞXgtpðxt ¼xÞ
            (b) natural policy gradients becomes apparent [21].                                          t¼0
                                                                                       is the discounted state distribution.
            further in Section 2.2 by introducing the ‘Natural Actor-Critic
            (NAC)’ which inherits the convergence guarantees from                      2.2. Actor improvement with natural policy gradients
            gradient methods. Furthermore, in Section 3, we show
            that several successful previous reinforcement learning                       Actor-Critic and many other policy iteration architec-
            methods can be seen as special cases of this more general                  tures consist of two steps, a policy evaluation step and a
            architecture. The paper concludes with empirical evalua-                   policy improvement step. The main requirements for the
            tions that demonstrate the effectiveness of the suggested                  policy evaluation step are that it makes efﬁcient usage of
            methods in Section 4.                                                      experienced data. The policy improvement step is required
                                                                                       to improve the policy on every step until convergence while
            2. Natural Actor-Critic                                                    being efﬁcient.
                                                                                          The requirements on the policy improvement step rule
            2.1. Markov decision process notation and assumptions                      out greedy methods as, at the current state of knowledge, a
                                                                                       policy improvement for approximated value functions
               For this paper, we assume that the underlying control                   cannot be guaranteed, even on average. ‘Vanilla’ policy
            problem is a Markov decision process (MDP) in discrete                     gradient improvements (see e.g. [25,15]) which follow the
                                                          n                            gradient = JðhÞ of the expected return function JðhÞ (where
            time with continuous state set X ¼ R , and a continuous                                h
                                 m                                                     = f ¼½qf=qy ;...;qf=qy ) denotes the derivative of
            action set U ¼ R       [8]. The assumption of an MDP comes                   h             1             N
            with the limitation that very good state information and                   function f with respect to parameter vector (h) often get
            Markovian environment are assumed. However, similar as                     stuck in plateaus as demonstrated in [14]. Natural gradients
                                                                                       e
            in [1], the results presented in this paper might extend to                = JðhÞ avoid this pitfall as demonstrated for supervised
                                                                                         h
            problems with partial state information.                                   learning problems [3], and suggested for reinforcement
               The system is at an initial state x0 2 X at time t ¼ 0                  learning in [14]. These methods do not follow the steepest
            drawn from the start-state distribution pðx0Þ. At any state                direction in parameter space but the steepest direction with
            xt 2 X at time t, the actor will choose an action ut 2 U                   respect to the Fisher metric given by
            by drawing it from a stochastic, parameterized policy                      e             1
                                                                                       = JðhÞ¼G ðhÞ= JðhÞ,                                               (1)
            pðu jx Þ¼pðu jx ;hÞ with parameters h 2 RN, and the                          h                   h
                t  t        t  t
            system transfers to a new state xtþ1 drawn from the state                  where GðhÞ denotes the Fisher information matrix. It is
            transfer distribution pðxtþ1jxt;utÞ. The system yields a                   guaranteed that the angle between natural and ordinary
            scalar reward rt ¼ rðxt;utÞ2R after each action. We                        gradient is never larger than 90, i.e., convergence to the
            assume that the policy ph is continuously differentiable                   next local optimum can be assured. The ‘vanilla’ gradient is
            with respect to its parameters h, and for each considered                  given by the policy-gradient theorem (see e.g. [25,15])
            policy p , a state-value function VpðxÞ, and the state-action                         Z          Z
                      h                                                                                 p                     p           p
                                p                                                      = JðhÞ¼        d ðxÞ      = pðujxÞðQ ðx;uÞb ðxÞÞdudx,
            value function Q ðx;uÞ exist and are given by                                h                         h
                                                                                                   X         U
                          ()
                             1       
            VpðxÞ¼E Xgtrx ¼x ,                                                                                                                          (2)
                         t          t 0                                                        p
                            t¼0                                                       where b ðxÞ denotes a baseline. Refs. [25,15] demonstrated
                                                                                      that in Eq. (2), the term Qpðx;uÞbpðxÞ can be replaced by
                            ()
                               1       
            Qpðx;uÞ¼E         Xgtrx ¼x;u ¼u ,                                         a compatible function approximation
                           t          t 0        0                                     p                           T        p           p
                               t¼0                                                    f wðx;uÞ¼ð=hlogpðujxÞÞ w  Q ðx;uÞb ðxÞ,                         (3)
            where g 2ð0;1Þ denotes the discount factor, and t a                        parameterized by the vector w, without affecting the
            trajectory. It is assumed that some basis functions /ðxÞ                   unbiasedness of the gradient estimate and irrespective of
            are given so that the state-value function can be approxi-                 the choice of the baseline bpðxÞ. However, as mentioned in
                                                                                  ARTICLE IN PRESS
            1182                                                  J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190
            [25], the baseline may still be useful in order to reduce the                                Dhofthepolicy parameters. As we are interested in natural
            variance of the gradient estimate when Eq. (2) is approxi-                                   policy gradient updates Dh ¼ aw, we wish to employ the
            mated from samples. Based on Eqs. (2) and (3), we derive                                     compatible function approximation fpðx;uÞ from Eq. (3) in
                                                                                                                                                                 w
            an estimate of the policy gradient as                                                        this context. At this point, a most important observation is
                          Z            Z                                                                 that the compatible function approximation fpðx;uÞ is
            = JðhÞ¼            dpðxÞ       pðujxÞ= logpðujxÞ= logpðujxÞTdudxw                                                                                                      w
              h                                       h                h                                 mean-zero w.r.t. the action distribution, i.e.,
                            X           U                                                                Z                                     Z
                           ¼Fhw                                                                (4)                       p                   T
            as = pðujxÞ¼pðujxÞ= logpðujxÞ. Since pðujxÞ is chosen by                                       UpðujxÞfwðx;uÞdu ¼ w                  U=hpðujxÞdu ¼ 0,                            (7)
                  h                          h                                                                            R
            the user, even in sampled data, the integral                                                 since from            pðujxÞdu ¼ 1, differention w.r.t. to y results
                          Z                                                                                   R             U                          p
                                                                             T                           in    U=hpðujxÞdu ¼ 0. Thus, fwðx;uÞ represents an advan-
            Fðh;xÞ¼            pðujxÞ=hlogpðujxÞ=hlogpðujxÞ du                                 (5)                               p                p               p
                            U                                                                            tage function A ðx;uÞ¼Q ðx;uÞV ðxÞ in general. The
                                                                                                         essential differences between the advantage function and
            can be evaluated analytically or empirically without                                         the state-action value function is demonstrated in Fig. 2.
            actually executing all actions. It is also noteworthy that                                   The advantage function cannot be learned with TD-like
            the baseline does not appear in Eq. (4) as it integrates out,                                bootstrapping without knowledge of the value function as
            thus eliminating the need to ﬁnd an optimal selection of                                     the essence of TD is to compare the value VpðxÞ of the two
            this open parameter. Nevertheless, the estimation of Fh ¼                                    adjacent states—but this value has been subtracted out in
            R dpðxÞFðh;xÞdx is still expensive since dpðxÞ is not                                           p
             X                                                                                           A ðx;uÞ. Hence, a TD-like bootstrapping using exclusively
            known. However, Eq. (4) has more surprising implications                                     the compatible function approximator is impossible.
            for policy gradients, when examining the meaning of the                                          As an alternative, [25,15] suggested to approximate
            matrix Fh in Eq. (4). Kakade [14] argued that Fðh;xÞ is the                                    p                                                   ^p
            point Fisher information matrix for state x, and that                                        f wðx;uÞ from unbiased estimates Q ðx;uÞ of the action
                       R     p                                                                           value function, e.g., obtained from rollouts and using least-
            FðhÞ¼ d ðxÞ Fðh;xÞdx, therefore, denotes a weighted                                                                                                    ^p
                         X                                                                               squares minimization between fw and Q . While possible in
            ‘average Fisher information matrix’ [14]. However, going                                     theory, one needs to realize that this approach implies a
            one step further, we demonstrate in Appendix A that Fh is                                    function approximation problem where the parameteri-
            indeed the true Fisher information matrix and does not                                       zation of the function approximator only spans a much
            have to be interpreted as the ‘average’ of the point Fisher                                  smaller subspace of the training data—e.g., imagine
            information matrices. Eqs. (4) and (1) combined imply that                                   approximating a quadratic function with a line. In practice,
            the natural gradient can be computed as                                                      the results of such an approximation depends crucially on
            e                1                                                                          the training data distribution and has thus unacceptably
            = JðhÞ¼G ðhÞF w¼w,                                                                 (6)
              h                        h                                                                 high variance—e.g., ﬁt a line to only data from the right
            since Fh ¼ GðhÞ (cf. Appendix A). Therefore we only need                                     branch of a parabula, the left branch, or data from both
            estimate w and not GðhÞ. The resulting policy improvement                                    branches.
            step is thus hiþ1 ¼ hi þ aw where a denotes a learning rate.                                     Furthermore, in continuous state-spaces a state (except
            Several properties of the natural policy gradient are                                        for single start-states) will hardly occur twice; therefore, we
            worthwhile highlighting:                                                                                                                               ^p                  p
                                                                                                         can only obtain unbiased estimates Q ðx;uÞ of Q ðx;uÞ.
                                                                                                                                                                           ^p
             Convergence to a local minimum guaranteed as for                                           This means the state-action value estimates Q ðx;uÞ have to
                ‘vanilla gradients’ [3].
             By choosing a more direct path to the optimal solution
                in parameter space, the natural gradient has, from
                empirical observations, faster convergence and avoids
                premature convergence of ‘vanilla gradients’ (cf. Fig. 1).
             The natural policy gradient can be shown to be covariant,
                i.e.,  independent of the coordinate frame chosen for
                expressing the policy parameters (cf. Section 3.1).
             As the natural gradient analytically averages out the
                inﬂuence of the stochastic policy (including the baseline of
                the function approximator), it requires fewer data point
                for a good gradient estimate than ‘vanilla gradients’.                                   Fig. 2. The state-action value function in any stable linear-quadratic
                                                                                                         Gaussian regulation problems can be shown to be a bowl (a). The
                                                                                                         advantage function is always a saddle as shown in (b); it is straightforward
            2.3. Critic estimation with compatible policy evaluation                                     to show that the compatible function approximation can exactly represent
                                                                                                         the advantage function—but projecting the value function onto the
               The critic evaluates the current policy p in order to                                     advantage function is non-trivial for continuous problems. This ﬁgure
                                                                                                         shows the value function and advantage function of the system described
            provide the basis for an actor improvement, i.e., the change                                 in the caption of Fig. 1.
                                                                                  ARTICLE IN PRESS
                                                                   J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190                                                          1183
               be projected onto the advantage function Apðx;uÞ. This                                   Table 1
               projection would have to average out the state-value offset                              Natural Actor-Critic Algorithm with LSTD-Q(l)
               VpðxÞ. For example, for linear-quadratic regulation, it is
               straightforward to show that the advantage function is                                   Input: Parameterized policy pðujxÞ¼pðujx;hÞ with initial parameters
                                                                                                        h ¼ h , its derivative = logpðujxÞ and basis functions /ðxÞ for the value
                                                                                                              0                   h
               saddle while the state-action value function is bowl—we                                  function VpðxÞ
               therefore would be projecting a bowl onto a saddle; both
                                                                                                        1: Draw initial state x pðx Þ, and select parameters
               are illustrated in Fig. 2. In this case, the distribution of the                                                  0      0
                                                                                                             A     ¼0;b       ¼z ¼0.
               data has a drastic impact on the projection.                                                    tþ1        tþ1     tþ1
                  To remedy this situation, we observe that we can write                                2: For t ¼ 0;1;2;...do
                                                                                                        3:    Execute: Draw action u pðu jx Þ, observe next state
                                                                                                                                         t     t  t
               the Bellman equations (e.g., see [5]) in terms of the                                         x    pðx     jx ;u Þ, and reward r ¼ rðx ;u Þ.
                                                                                                               tþ1      tþ1  t  t                  t      t  t
               advantage function and the state-value function                                          4:    Critic Evaluation (LSTD-Q(l)): Update
                                                                                                                                     e              T   T T
                                                                                                        4.1:      basis functions: / ¼½/ðxtþ1Þ ;0  ,
                 p               p              p                                                                                      t
               Q ðx;uÞ¼A ðx;uÞþV ðxÞ                                                                                  b           T                 T T
                                                                                                                     / ¼½/ðx Þ ;= logpðu jx Þ  ,
                                             Z                                                                         t        t     h         t t
                                                                                                                                            b                              e T
                                                                                                        4.2:      statistics: z    ¼lz þ/;A          ¼A þz ð/ g/Þ ;
                           ¼rðx;uÞþg             pðx0jx;uÞVpðx0Þdx0.                          (8)                              tþ1      t     t   tþ1     t    tþ1   t      t
                                                                                                                     b     ¼b þz r;
                                              X                                                                        tþ1     t    tþ1 tT     T   T      1
                                                                                                        4.3:      critic parameters: ½v     ; w     ¼A b .
                                p              p                                                                                         tþ1   tþ1        tþ1 tþ1
               Inserting A ðx;uÞ¼fwðx;uÞ and an appropriate basis                                       5:    Actor: If gradient estimate is accurate, ,ðw ;w          Þp, update
               functions representation of the value function as VpðxÞ¼                                 5.1:      policy parameters: h       ¼h þaw , t t1
                     T                                                                                                                    tþ1     t      tþ1
               /ðxÞ v, we can rewrite the Bellman Equation, Eq. (8), as a                               5.2:      forget statistics: z     bz ;A  bA ;b  bb .
                                                                                                        6: end.                       tþ1        tþ1   tþ1        tþ1  tþ1        tþ1
               set of linear equations
                                  T              T
               =hlogpðutjxtÞ wþ/ðxtÞ v
                   ¼rðx;uÞþg/ðx ÞTvþðx;u;x Þ,                                                (9)       deterministic or weekly noisy state transitions and arbi-
                           t   t           tþ1              t   t   tþ1                                 trary stochastic policies. As all previous LSTD suggestions,
               where ðx ;u ;x          Þ denotes an error term which mean-zero
                            t   t   tþ1                                                                 it   loses accuracy with increasing noise in the state
               as can be observed from Eq. (8). These equations enable us                                                          e
                                                                                                        transitions since /             becomes a random variable. The
               to formulate some novel algorithms in the next sections.                                                              t
                  The linear appearance of w and v hints at a least squares                             complete LSTD-Q(l) algorithm is given in the Critic
               to obtain. Thus, we now need to address algorithms that                                  Evaluation (lines 4.1–4.3) of Table 1.
               estimate the gradient efﬁciently using the sampled equa-                                    Once LSTD-Q(l) converges to an approximation of
                                                                                                           p                p
               tions (such as Eq. (9)), and how to determine the additional                             A ðxt;utÞþV ðxtÞ, we obtain two results: the value function
               basis functions /ðxÞ for which convergence of these                                      parameters v, and the natural gradient w. The natural
               algorithms is guaranteed.                                                                gradient w serves in updating the policy parameters
                                                                                                        Dht ¼ awt. After this update, the critic has to forget at least
               2.3.1. Critic evaluation with LSTD-Q(l)                                                  parts of its accumulated sufﬁcient statistics using a forgetting
                  Using Eq. (9), a solution to Eq. (8) can be obtained by                               factor b 2½0;1 (cf. Table 1). For b ¼ 0, i.e., complete
               adapting the LSTD(l) policy evaluation algorithm [9]. For                                resetting, and appropriate basis functions fðxÞ,convergence
               this purpose, we deﬁne                                                                   to the true natural gradient can be guaranteed. The complete
                                                                                                        NACalgorithm is shown in Table 1.
               b              T                      T T                                                   However, it becomes fairly obvious that the basis
               / ¼½/ðxtÞ ;=hlogpðutjxtÞ  ,
                 t
               e                 T    T T                                                               functions can have an inﬂuence on our gradient estimate.
               / ¼½/ðxtþ1Þ ;0  ,                                                            (10)
                 t                                                                                      Whenusing the counterexample in [7] with a typical Gibbs
               as new basis functions, where 0 is the zero vector. This                                 policy, we will realize that the gradient is affected for lo1;
               deﬁnition of basis function reduces bias and variance of the                             for l ¼ 0 the gradient is ﬂipped and would always worsen
               learning process in comparison to SARSA and previous                                     the policy. However, unlike in [7], we at least could
               LSTD(l) algorithms for state-action value functions [9] as                               guarantee that we are not affected for l ¼ 1.
                                           e
               the basis functions /t do not depend on stochastic future
               actions utþ1, i.e., the input variables to the LSTD re-                                  2.3.2. Episodic NAC
               gression are not noisy due to utþ1 (e.g., as in [10])—such                                  Given the problem that the additional basis functions
               input noise would violate the standard regression model                                  fðxÞ determine the quality of the gradient, we need
               that only takes noise in the regression targets into account.                            methods which guarantee the unbiasedness of the natural
               Alternatively, Bradtke et al. [10] assume VpðxÞ¼Qpðx;uÞ                                  gradient estimate. Such method can be determined by
               where u is the average future action, and choose their basis                             summing up Eq. (9) along a sample path, we obtain
               functions accordingly; however, this is only given for                                    N1
               deterministic policies, i.e., policies without exploration and                            X t p
               not applicable in our framework. LSTD(l) with the basis                                         g A ðxt;utÞ
               functions in Eq. (10), called LSTD-Q(l) from now on, is                                   t¼0
                                                                                                                              N1
               thus currently the theoretically cleanest way of applying                                    ¼Vpðx0ÞþXgtrðxt;utÞgNVpðxNÞ.                                             (11)
               LSTD to state-value function estimation. It is exact for                                                       t¼0
                                                                    ARTICLE IN PRESS
          1184                                         J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190
          Table 2                                                                      proof at this point showing that the natural policy gradient
          Episodic Natural Actor-Critic Algorithm (eNAC)                               is in fact covariant under certain conditions, and clarify
          Input: Parameterized policy pðujxÞ¼pðujx;hÞ with initial parameters          why [14] experienced these difﬁculties.
          h ¼ h , and derivative = logpðujxÞ.
               0                  h                                                    Theorem 1. Natural policy-gradients updates are covariant
          For u ¼ 1;2;3;...do                                                          for two policies ph parameterized by h and ph parameterized
              For e ¼ 1;2;3;...do                                                      by h if (i) for all parameters yi there exists a function
                Execute Rollout: Draw initial state x pðx Þ.
                                                    0     0                            y ¼f ðh ;...;h Þ,(ii) the derivative = h and its inverse
                For t ¼ 1;2;3;...;N do                                                  i     i  1        k                             h
                                                                                       = h1.
                  Draw action u pðu jx Þ, observe next state x   pðx   jx ;u Þ,        h
                                 t    t  t                     tþ1     tþ1 t  t
                  and reward r ¼ rðx ;u Þ.
                               t      t  t                                                For the proof see Appendix B. Practical experiments
                end.
              end.                                                                     show that the problems occurred for Gaussian policies in
              Critic Evaluation (Episodic): Determine value function                   [14] are in fact due to the selection the stepsize a which
              J ¼ Vpðx Þ, compatible function approximation fpðx ;u Þ:
                       0                               P       w  t  t                 determines the length of Dh. As the linearization Dh ¼
                                                          N   t             T   T          T
              Update: Determine basis functions: / ¼½        g = logpðu jx Þ ;1 ;
                                                   t      t¼0   h       t t            = h Dh does not hold for large Dh, this can cause
                                       P                                                 h
                                         N   t
                reward statistics: Rt ¼  t¼0 g r;                                      divergence between the algorithms even for analytically
              Actor-Update: When the natural gradient is converged,                    determined natural policy gradients which can partially
              ,ðw ;w Þp,update the policy parameters: h          ¼h þaw .
          6: end. tþ1   tt                                    tþ1   t      tþ1        explain the difﬁculties occurred by Kakade [14].
                                                                                       3.2. NAC’s relation to previous algorithms
          It is fairly obvious that the last term disappears for N !1                     Original Actor-Critic. Surprisingly, the original Actor-
          or episodic tasks (where rðxN1;uN1Þ is the ﬁnal reward);                   Critic algorithm [24] is a form of the NAC. By choosing
          therefore each rollout would yield one equation. If we                                                                       P
          furthermore assume a single start-state, an additional                       a Gibbs policy pðutjxtÞ¼expðyxuÞ=                  b expðyxbÞ,    with
          scalar value function of fðxÞ¼1 sufﬁces. We therefore                        all parameters yxu lumped in the vector h (denoted as
          get a straightforward regression problem:                                    h ¼½yxu) in a discrete setup with tabular representations
          N1                                N1                                       of transition probabilities and rewards. A linear function
          Xgt=logpðut;xtÞTwþJ ¼ X gtrðxt;utÞ                                 (12)      approximation VpðxÞ¼/ðxÞTv with v ¼½vx and unit basis
          t¼0                                 t¼0                                      functions /ðxÞ¼ux was employed. Sutton et al. online
          with exactly dimyþ1 unknowns. This means that for non-                       update rule is given by
          stochastic tasks we can obtain a gradient after dimyþ1                       ytþ1 ¼ yt þa1ðrðx;uÞþgvx0 vxÞ,
                                                                                         xu      xu
          rollouts. The complete algorithm is shown in Table 2.                         tþ1      t
                                                                                       vx    ¼vxþa2ðrðx;uÞþgvx0 vxÞ,
          3. Properties of NAC                                                         where a1, a2 denote learning rates. The update of the critic
                                                                                       parameters vt equals the one of the NAC in expectation as
                                                                                                      x
             In this section, we will emphasize certain properties of                  TD(0) critics converges to the same values as LSTD(0) and
          the NAC. In particular, we want to give a simple proof of                    LSTD-Q(0) for discrete problems [9]. Since for the Gibbs
          covariance of the natural policy gradient, and discuss [14]                  policy we have qlogpðbjaÞ=qyxu ¼ 1pðbjaÞ if a ¼ x and
          observation that in his experimental settings the natural                    b ¼ u, qlogpðbjaÞ=qyxu ¼pðbjaÞ if a ¼ x and bau, and
          policy gradient was non-covariant. Furthermore, we will                      qlogpðbjaÞ=qyxu ¼ 0 otherwise, and as P pðbjxÞAðx;bÞ¼
                                                                                                                                           b
          discuss another surprising aspect about the NAC which is                     0; we can evaluate the advantage function and derive
          its relation to previous algorithms. We brieﬂy demonstrate                   Aðx;uÞ¼Aðx;uÞX pðbjxÞAðx;bÞ
          that established algorithms like the classic Actor-Critic                                             b
          [24], and Bradtke’s Q-Learning [10] can be seen as special                                XqlogpðbjxÞ
          cases of NAC.                                                                         ¼            qyxu      Aðx;bÞ.
                                                                                                     b
          3.1. On the covariance of natural policy gradients                           Since the compatible function approximation represents
                                                                                       the advantage function, i.e., fpðx;uÞ¼Aðx;uÞ, we realize
                                                                                                                             w
             When [14] originally suggested natural policy gradients,                  that the advantages equal the natural gradient, i.e.,
          he came to the disappointing conclusion that they were not                   w¼½Aðx;uÞ. Furthermore, the TD(0) error of a state-
          covariant. As counterexample, he suggested that for two                      action pair ðx;uÞ equals the advantage function in
          different linear Gaussian policies (one in the normal form,                  expectation, and therefore the natural gradient update
          and the other in the information form) the probability                       w ¼Aðx;uÞ¼E 0frðx;uÞþgVðx0ÞVðxÞjx;ug;                          corre-
                                                                                         xu                  x
          distributions represented by the natural policy gradient                     sponds to the average online updates of Actor-Critic. As
          would be affected differently, i.e., the natural policy                      both update rules of the Actor-Critic correspond to the
          gradient would be non-covariant. We intend to give a                         ones of NAC, we can see both algorithms as equivalent.
                                                                  ARTICLE IN PRESS
                                                     J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190                                    1185
              SARSA. SARSA with a tabular, discrete state-action                   Wecompare them in optimization tasks such as Cart-Pole
            value function Qpðx;uÞ and an -soft policy improvement                Balancing and simple motor primitive evaluations and
            pðutjxtÞ¼expðQpðx;uÞ=Þ=XexpðQpðx;uÞ=Þ                                compare them only with episodic NAC. Furthermore, we
                                                                                   apply the combination of episodic NAC and the motor
                                            ^
                                            u                                      primitive framework to a robotic task on a real robot, i.e.,
            canalsobeseenasanapproximationofNAC.Whentreating                       ‘hitting a T-ball with a baseball bat’.
            the table entries as parameters of a policy h     ¼Qpðx;uÞ,we
                                                            xu
            realize that the TD update of these parameters corresponds             4.1. Cart-Pole Balancing
            approximately to the natural gradient update since w            ¼
                                                                          xu
                            0                0  0
            Aðx;uÞExfrðx;uÞþgQðx;uÞQðx;uÞjx;ug.However,                           Cart-Pole Balancing is a well-known benchmark for
            the SARSA-TD error equals the advantage function only for              reinforcement learning. We assume the cart as shown in
            policies where a single action u	 has much better action values        Fig. 3(a) can be described by
            Qðx;u	Þ than all other actions; for such special cases, -soft
            SARSAcan be seen as an approximation of NAC. This also                     €            2€
            corresponds to Kakade’s [14] observation that greedy update            mlxcosyþml ymglsiny¼0,
            step (such as the -soft greedy update), approximates the                                €            _2
                                                                                   ðmþm €
            natural policy gradient.                                                       cÞx þ mlycosymly siny ¼ F,
              Bradtke’s Q-Learning. Bradtke [10] proposed an algo-                                                                     2
            rithm with policy pðu jx Þ¼Nðu jkTx ;s2Þ and parameters                with l ¼ 0:75m, m ¼ 0:15kg, g ¼ 9:81m=s                and mc ¼
                                    t  t         t  i  t  i                                                                                  _ T
                    T    T                                                                                                              _
            h ¼½k ;s (where s denotes the exploration, and i the                  1:0kg. The resulting state is given by x ¼½x;x;y;y , and
             i      i  i             i                                             the action u ¼ F. The system is treated as if it was sampled
            policy update time step) in a linear control task with linear
            state transitions x      ¼Ax þbu, and quadratic rewards                at a rate of h ¼ 60Hz, and the reward is given by rðx;uÞ¼
                         T       tþ1 2     t      t            p                   xTQxþuTRu with Q ¼diagð1:25;1;12;0:25Þ, R ¼ 0:01.
            rðxt;utÞ¼xt Hxt þRut. They evaluated Q ðxt;utÞ with                       Thepolicy is speciﬁed as pðujxÞ¼NðKx;s2Þ. In order to
            LSTD(0) using a quadratic polynomial expansion as basis
            functions, and applied greedy updates:                                 ensure that the learning algorithm cannot exceed an
                                                                                   acceptable parameter range, the variance of the policy is
            kBradtke ¼ arg max Qpðx ;u ¼ kT x Þ
              iþ1                       t  t     iþ1 t                             deﬁned as s¼0:1þ1=ð1þexpðZÞÞ. Thus, the policy
                           kiþ1                                                                                               T   T
                     ¼ðRþgbTPbÞ1gbPA,                                            parameter vector becomes h ¼½K ;Z                 and has the
                                       i        i                                  analytically computable optimal solution K ½5:71;11:3;
                                                                                   82:1;21:6T,ands ¼ 0:1, corresponding to Z !1.As
            where Pi denotes policy-speciﬁc value function parameters              Z !1ishardtovisualize, we show s in Fig. 3(b) despite
            related to the gain k ; no update the exploration s was
                                    i                                   i          the fact that the update takes place over the parameter Z.
            included. Similarly, we can obtain the natural policy                     For each initial policy, samples ðx ;u ;r           ; x   Þ are
            gradient w ¼½w ;w T, as yielded by LSTD-Q(l) analyti-                                                               t  t  tþ1   tþ1
                              k   s                                                being generated using the start-state distributions, transi-
            cally using the compatible function approximation and the              tion probabilities, the rewards and the policy. The samples
            same quadratic basis functions. As discussed in detail in              arrive at a sampling rate of 60Hz, and are immediately sent
            [21], this gives us                                                    to the NAC module. The policy is updated when
                      T                T        T 2                                ,ðw ;wÞp¼ p=180. At the time of update, the true
            w ¼ðgA PbþðRþgb PbÞkÞ s ,                                                    tþ1   t
              k          i                i       i
                                                                                   ‘vanilla’ policy gradient, which can be computed analyti-
                                                                                   cally,1 is used to update a separate policy. The true ‘vanilla’
            w ¼0:5ðRþgbTPbÞs3.
              s                  i    i                                            policy   gradients    these   serve   as a baseline for the
            Similarly, it can be derived that the expected return is               comparison. If the pole leaves the acceptable region
            Jðh Þ¼ðRþgbTPbÞs2 forthistypeofproblems,see[21].                      of p=6pfpp=6, and 1:5mpxpþ1:5m, it is reset
                i                 i    i
            For a learning rate ai ¼ 1=kJðhiÞk, we see                             to a new starting position drawn from the start-state
                                                                                   distribution.
            k    ¼k þaw ¼k ðk þðRþgbTPbÞ1gATPbÞ
              iþ1     i    t  k     i     i               i          i                Results are illustrated in Fig. 3.InFig. 3(b), a sample
                      Bradtke                                                      run is shown: the NAC algorithms estimates the optimal
                 ¼k         ,
                      iþ1                                                          solution within less than 10min of simulated robot trial
            which demonstrates that Bradtke’s Actor Update is a                    time.   The analytically obtained policy gradient for
            special case of the NAC. NAC extends Bradtke’s result as it            comparison takes over 2h of robot experience to get to
            gives an update rule for the exploration—which was not                 the true solution. In a real world application, a signiﬁcant
            possible in Bradtke’s greedy framework.                                amount of time would be added for the vanilla policy
                                                                                   gradient as it is more unstable and leaves the admissible
            4. Evaluations and applications                                        area more often. The policy gradient is clearly outperformed
                                                                                     1The true natural policy gradient can also be computed analytically.
              In this section, we present several evaluations comparing            However, it is not shown as the difference in performance to the Natural
            the episodic NAC architectures with previous algorithms.               Actor-Critic gradient estimate is negligible.
                                                       ARTICLE IN PRESS
        1186                                J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190
        Fig. 3. This ﬁgure shows the performance of Natural Actor-Critic in the Cart-Pole Balancing framework. In (a), you can see the general setup of the pole
        mounted on the cart. In (b), a sample learning run of the both Natural Actor-Critic and the true policy gradient is given. The dashed line denotes the
        Natural Actor-Critic performance while the solid line shows the policy gradients performance. In (c), the expected return of the policy is shown. Thisisan
        average over 100 randomly picked policies as described in Section 4.1.
        by the NAC algorithm. The performance difference              4.2. Motor primitive learning for baseball
        between the true natural gradient and the NAC algorithm
        is negligible and, therefore, not shown separately. By the       This section will turn towards optimizing nonlinear
        time of the conference, we hope to have this example          dynamic motor primitives for robotics. In [13], a novel
        implemented on a real anthropomorphic robot. In               form of representing movement plans ðq _
                                                                                                                   d;qdÞ for the
        Fig. 3(c), the expected return over updates is shown          degrees of freedom (DOF) robot systems was suggested in
        averaged over all hundred initial policies.                   terms of the time evolution of the nonlinear dynamical
          In this experiment, we demonstrated that the NAC is         systems
        comparable with the ideal natural gradient, and outperforms
        the ‘vanilla’ policy gradient signiﬁcantly. Greedy policy      _
        improvement methods do not compare easily. Discretized        qd;k ¼ hðqd;k;zk;gk;t;ykÞ,                             (13)
        greedy methods cannot compete due to the fact that the
                                                                                   _
        amount of data required would be signiﬁcantly increased.      where ðqd;k;qd;kÞ denote the desired position and velocity
        The only suitable greedy improvement method, to our           of a joint, zk the internal state of the dynamic system, gk
        knowledge, is Bradtke’s Adaptive Policy Iteration [10].       the goal (or point attractor) state of each DOF, t the
        However, this method is problematic in real-world applica-    movement duration shared by all DOFs, and yk the open
        tion due to the fact that the policy in Bradtke’s method is   parameters of the function h. The original work in [13]
        deterministic: the estimation of the action-value function is demonstrated how the parameters yk can be learned
        an ill-conditioned regression problem with redundant para-    to match a template trajectory by means of supervised
        meters and no explorative noise. Therefore, it can only work  learning—this scenario is, for instance, useful as the ﬁrst
        in simulated environments with an absence of noise in the     step of an imitation learning system. Here we will add the
        state estimates and rewards.                                  ability of self-improvement of the movement primitives in
                                                                     ARTICLE IN PRESS
                                                        J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190                                        1187
            Eq. (13) by means of reinforcement learning, which is the                  shows the analytically optimal solution, which is unac-
            crucial second step in imitation learning. The system in                   hievable for the motor primitives, but nicely approximated
            Eq. (13) is a point-to-point movement, i.e., this task is                  by their best solution, presented by the dark dot-dashed
            rather well suited for episodic NAC.                                       line.  This best solution is reached by both learning
               In Fig. 4, we show a comparison with GPOMDP for                         methods. However, for GPOMDP, this requires approxi-
            simple, single DOF task with a reward of                                   mately 106 learning steps while the NAC takes less than 103
                                                                                       to converge to the optimal solution.
                                N
                               X _2                            2                          We also evaluated the same setup in a challenging
            rkðx0:N;u0:NÞ¼          c1qd;k;i þ c2ðqd;k;N  gkÞ ,                       robot task, i.e., the planning of these motor primitives
                               i¼0                                                     for a seven DOF robot task. The task of the robot is to
            where c1 ¼ 1, c2 ¼ 1000, and gk is chose appropriately.                    hit the ball properly so that it ﬂies as far as possible.
            In Fig. 4(a), we show how the expected cost decreases for                  Initially, it is taught in by supervised learning as can
            both GPOMDP and the episodic NAC. The positions                            be seen in Fig. 5(b); however, it fails to reproduce the
            of the motor primitives are shown in Fig. 4(b) and in                      behavior as shown in Fig. 5(c); subsequently, we improve
            Fig. 4(c) the accelerations are given. In 4(b,c), the dashed               the performance using the episodic NAC which yields
            line shows the initial conﬁgurations, which is accomplished                the performance shown in Fig. 5(a) and the behavior in
            by zero parameters for the motor primitives. The solid line                Fig. 5(d).
                                                                                                                   20
                                                                    1
               )1000                                                                                            2]
               θ                     GPOMDP                       0.8
                (
               J 800                                                                                               10
                 600                                              0.6
                 400                                            position [rad]0.4                                    0
               Expected Cost 200                                  0.2                                            acceleration [rad/s
                          Episodic Natural Actor-Gritic
                    0                                               0                                              -10
                      0     200    400    600    800    1000          0            0.5           1                     0            0.5           1
                              Number of Episodes                                      time [s]                                         time [s]
                                  Expected Cost                             Position of motor primitives                      Controls of motor primitives
            Fig. 4. This ﬁgure illustrates the task accomplished in the toy example. In (a), we show how the expected cost decreases for both GPOMDP and the
            episodic Natural Actor-Critic. The positions of the motor primitives are shown in (b) and in (c) the accelerations are given. In (b,c), the dashed line shows
            the initial conﬁgurations, which is accomplished by zero parameters for the motor primitives. The solid line shows the analytically optimal solution, which
            is unachievable for the motor primitives, but nicely approximated by their best solution, presented by the dark dot-dashed line. This best solution is
                                                                                                   6                                             3
            reached by both learning methods. However, for GPOMDP, this requires approximately 10 learning steps while the NAC takes less than 10 to converge
            to the optimal solution.
                         5
                     x 10
                   0
               )  -2
               θ
                (
               J  -4
                  -6
               Performance -8
                 -10
                     0       100      200     300      400
                                   Episodes
            Fig. 5. The ﬁgure shows (a) the performance of a baseball swing task when using the motor primitives for learning. In (b), the learning system is initialized
            by imitation learning, in (c) it is initially failing at reproducing the motor behavior, and (d) after several hundred episodes exhibiting a nicely learned
            batting.
                                                                                                                                                                                ARTICLE IN PRESS
                         1188                                                                                                                 J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190
                         5. Conclusion                                                                                                                                                                                           matrix for the average reward case by
                                In this paper, we have summarized novel developments                                                                                                                                             GðhÞ¼ lim n1E f= logpðsÞ= logpðs ÞTg
                                                                                                                                                                                                                                                           n!1                             s         h                                h                        0:n
                         in policy-gradient reinforcement learning, and based on                                                                                                                                                                                                      1                     2
                         these, we have designed a novel reinforcement learning                                                                                                                                                                  ¼lim n Esf=hlogpðsÞg,                                                                                                                                     (A.2)
                         architecture, the NAC algorithm. This algorithm comes in                                                                                                                                                                                 n!1                                ()
                                                                                                                                                                                                                                                                                                               n
                         (at least) two forms, i.e., the LSTD-Q(l) form which                                                                                                                                                                    ¼lim n1Es X=2logpðutjxtÞ
                                                                                                                                                                                                                                                                  n!1                                                         h
                         depends on sufﬁciently rich basis functions, and the                                                                                                                                                                                     Z                           Z             t¼0
                         Episodic form which only requires a constant as additional                                                                                                                                                                                              p                                              2
                                                                                                                                                                                                                                                 ¼ dðxÞ pðujxÞ= logpðujxÞdudx                                                                                                                              (A.3)
                         basis function. We compare both algorithms and apply the                                                                                                                                                                                                                                               h
                                                                                                                                                                                                                                                           Z X                        Z U
                         latter on several evaluative benchmarks as well as on a                                                                                                                                                                                         p
                         baseball swing robot example.                                                                                                                                                                                           ¼ Xd ðxÞ UpðujxÞ=hlogpðujxÞ
                                Recently, our NAC architecture [19,21] has gained a lot of                                                                                                                                                               = logpðujxÞTdudx ¼ F .                                                                                                                             (A.4)
                         traction in the reinforcement learning community. According                                                                                                                                                                           h                                                                      h
                         to Aberdeen, the NAC is the ‘Current method of choice’ [2].
                         Additional to our work presented at ESANN 2007 in [19]                                                                                                                                                  This proves that the all-action matrix is indeed the Fisher
                         and its earlier, preliminary versions (see e.g. [22,21,18,20]),                                                                                                                                         information matrix for the average reward case. For the
                         the algorithm has found a variety of applications in largely                                                                                                                                            discounted case, with a discount factor g we realize that we
                         unmodiﬁed form in the last year. The current range of                                                                                                                                                   can rewrite the problem where the probability of rollout is
                         additional applications includes optimization of constrained                                                                                                                                            given by
                         reaching movements of humanoid robots [12], trafﬁc-light                                                                                                                                                                                                   !
                                                                                                                                                                                                                                                                                             n
                         system optimization [23], multi-agent system optimization                                                                                                                                               pgðs0:nÞ¼pðs0:nÞ X giIx;u
                         [11,28], conditional random ﬁelds [27] and gait optimization                                                                                                                                                                                                                             i    i
                         in robot locomotion [26,17]. All these new developments                                                                                                                                                                                                           i¼0
                         indicate that the NAC is about to become a standard
                         architecture in the area of reinforcement learning as it is                                                                                                                                             and derive that the all-action matrix equals the Fisher
                         among the few approaches which have scaled towards                                                                                                                                                      information matrix by the same kind of reasoning as in
                         interesting applications.                                                                                                                                                                               Eq. (A.4). Therefore, we can conclude that in general, i.e.,
                                                                                                                                                                                                                                 GðhÞ¼Fh.
                         Appendix A. Fisher information property
                                In Section 6, we explained that the all-action matrix Fh                                                                                                                                         Appendix B. Proof of the covariance theorem
                         equals in general the Fisher information matrix GðhÞ.In                                                                                                                                                         For small parameter changes Dh and Dh, we have
                         [16], we can ﬁnd the well-known lemma that by differ-
                         entiating R n pðxÞdx ¼ 1 twice with respect to the para-                                                                                                                                                Dh¼=hhTDh. If the natural policy gradient is a covariant
                                                            R                                                                                                                                                                    update rule, a change Dh along the gradient = JðhÞ would
                         meters h, we can obtain                                                                                                                                                                                                                                                                                                                                      h
                                                                                                                                                                                                                                 result in the same change Dh along the gradient = JðhÞ for
                          Z                                                                                                                                                                                                                                                                                                                                                                     h
                                      pðxÞ=2logpðxÞdx                                                                                                                                                                            the same scalar step-size a: By differentiation, we can
                                  n                     h                                                                                                                                                                        obtain = JðhÞ¼= h= JðhÞ. It is straightforward to show
                              R                  Z                                                                                                                                                                                                            h                              h           h
                                  ¼ pðxÞ= logpðxÞ= logpðxÞTdx                                                                                                                                     (A.1)                         that the Fisher information matrix includes the Jacobian
                                                         n                     h                                h                                                                                                                = h twice as factor
                                                     R                                                                                                                                                                                h
                         for any probability density function pðxÞ. Furthermore, we                                                                                                                                                                        Z             p             Z                                                                                                           T
                                                                                                                                                                                                                                  FðhÞ¼                              d ðxÞ                       pðujxÞ= logpðujxÞ= logpðujxÞ dudx,
                         can rewrite the probability pðs                                                                        Þ of a rollout or trajectory                                                                                                                                                             h                                       h
                                                                                                                         0:n                                                                                                                                   X Z                         U Z
                         s           ¼½x , u , r , x , u , r , ..., x , u , r , x                                                                                            T as
                            0:n                    0          0          0          1          1          1                       n          n          n          nþ1                                                                            ¼=hh                           dpðxÞ                       pðujxÞ=hlogpðujxÞ
                                                                          n                                                                                                                                                                                                X                           U
                         pðs             Þ¼pðx ÞY pðx                                                 jx ;u Þpðu jx Þ                                                                                                                                                                             T                                 T
                                  0:n                           0 t¼0                        tþ1            t        t              t       t                                                                                                             =hlogpðujxÞ dudx=hh ,
                                                                                                                                                                                                                                                  ¼=hFðhÞ= hT.
                                                                                                                                                                                                                                                               h                         h
                         which implies that
                                                                              n                                                                                                                                                  This shows that natural gradient in the h parameterization
                              2                                           X 2
                         = logpðs Þ¼                                                   = logpðu jx Þ.                                                                                                                            is given by
                              h                        0:n                                   h                        t        t
                                                                           t¼0
                                                                                                                                                                                                                                   e                                   1
                                                                                                                                                                                                                                  = JðhÞ¼F ðhÞ= JðhÞ
                         Using Eq. (A.1), and the deﬁnition of the Fisher informa-                                                                                                                                                      h                                                   h
                                                                                                                                                                                                                                                          ¼ð= hFðhÞ= hTÞ1= h= JðhÞ.
                         tion matrix [3], we can determine Fisher information                                                                                                                                                                                            h                         h                        h           h
                                                                                ARTICLE IN PRESS
                                                                 J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190                                                       1189
              This has a surprising implication as it makes it straightfor-                          [16] T. Moon, W. Stirling, Mathematical Methods and Algorithms for
              ward to see that the natural policy is covariant since                                      Signal Processing, Prentice-Hall, Englewood Cliffs, NJ, 2000.
                                                                                                     [17] J. Park, J. Kim, D. Kang, An RLS-based Natural Actor-Critic
                              T               Te                                                          algorithm for locomotion of a two-linked robot arm, in: Proceedings
               Dh¼a= h Dh¼a= h = JðhÞ,
                           h               h      h                                                       of Computational Intelligence and Security: International Conference
                   ¼a= hTð= hFðhÞ= hTÞ1= h= JðhÞ,                                                        (CIS 2005), Xi’an, China, December 2005, pp. 15–19.
                           h       h          h          h    h
                           1                    e                                                   [18] J. Peters, S. Schaal, Policy gradient methods for robotics, in:
                   ¼aF ðhÞ=hJðhÞ¼a=hJðhÞ,                                                                 Proceedings of the IEEE/RSJ International Conference on Intelligent
                                                                                                          Robots and Systems (IROS), Beijing, China, 2006.
              assuming that = h is invertible. This concludes that the                               [19] J.  Peters, S. Schaal, Applying the episodic natural actor-critic
                                      h                                                                   architecture to motor primitive learning, in: Proceedings of the 2007
              natural policy gradient is in fact a covariant gradient                                     European Symposium on Artiﬁcial Neural Networks (ESANN), 2007.
              update rule.                                                                           [20] J. Peters, S. Vijayakumar, S. Schaal, Scaling reinforcement learning
                 The assumptions underlying this proof require that the                                   paradigms for motor learning, in: Proceedings of the 10th Joint
              learning rate is very small in order to ensure a covariant                                  Symposium on Neural Computation (JSNC), Irvine, CA, May 2003.
              gradient descent process. However, single update steps will                            [21] J. Peters, S. Vijaykumar, S. Schaal, Reinforcement learning for
              always be covariant and, thus, this requirement is only                                     humanoid robotics, in: IEEE International Conference on Human-
                                                                                                          doid Robots, 2003.
              formally necessary but barely matters in practice. Similar                             [22] J. Peters, S. Vijayakumar, S. Schaal, Natural Actor-Critic, in:
              as in other gradient descent problems, learning rates can be                                Proceedings of the European Machine Learning Conference
              chosen to optimize the performance without changing the                                     (ECML), Porto, Portugal, 2005.
              fact that the covariance of a single update step direction                             [23] S. Richter, D. Aberdeen, J. Yu, Natural Actor-Critic for road trafﬁc
              will not be affected.                                                                       optimisation, in: Advances in Neural Information Processing
                                                                                                          Systems, 2007.
                                                                                                     [24] R.S. Sutton, A.G. Barto, Reinforcement Learning, MIT Press,
                                                                                                          Cambridge, MA, 1998.
              References                                                                             [25] R.S. Sutton, D. McAllester, S. Singh, Y. Mansour, Policy gradient
                                                                                                          methods for reinforcement learning with function approximation, in:
                [1] D. Aberdeen, Policy-gradient algorithms for partially observable                      Advances in Neural Information Processing Systems, vol. 12, 2000.
                   Markov decision processes, Ph.D. Thesis, Australian National                      [26] T. Ueno, Y. Nakamura, T. Shibata, K. Hosoda, S. Ishii, Fast and
                   Unversity, 2003.                                                                       Stable learning of quasi-passive dynamic walking by an unstable
                [2] D. Aberdeen, POMDPs and policy gradients, in: Proceedings of the                      biped robot based on off-policy Natural Actor-Critic, in: IEEE/RSJ
                   Machine Learning Summer School (MLSS), Canberra, Australia,                            International Conference on Intelligent Robots and Systems (IROS),
                   2006.                                                                                  2006.
                [3] S. Amari, Natural gradient works efﬁciently in learning, Neural                  [27] S.V. N Vishwanathan, X. Zhang, D. Aberdeen, Conditional random
                   Comput. 10 (1998) 251–276.                                                             ﬁelds for reinforcement learning, in: Y. Bengio, Y. LeCun (Eds.),
                [4] J. Bagnell, J. Schneider, Covariant policy search, in: International                  Proceedings of the 2007 Snowbird Learning Workshop, San Juan,
                   Joint Conference on Artiﬁcial Intelligence, 2003.                                      Puerto Rico, March 2007.
                [5] L.C. Baird, Advantage updating, Technical Report WL-TR-93-1146,                  [28] X. Zhang, D. Aberdeen, S.V.N. Vishwanathan, Conditional random
                   Wright Lab., 1993.                                                                     ﬁelds for multi-agent reinforcement learning, in: Proceedings of the
                [6] L.C. Baird, A.W. Moore, Gradient descent for general reinforcement                    24th International Conference on Machine Learning (ICML 2007),
                   learning, in: Advances in Neural Information Processing Systems,                       ACMInternational Conference Proceeding Series, Corvalis, Oregon,
                   vol. 11, 1999.                                                                         2007, pp. 1143–1150.
                [7] P. Bartlett, An introduction to reinforcement learning theory: value
                   function methods, in: Machine Learning Summer School, 2002,
                   pp. 184–202.                                                                                                 Jan Peters heads the Robot Learning Lab
                [8] D.P. Bertsekas, J.N. Tsitsiklis, Neuro-Dynamic Programming,                                                 (RoLL) at the Max-Planck Institute for Biologi-
                   Athena Scientiﬁc, Belmont, MA, 1996.                                                                         cal Cybernetics (MPI) while being an invited
                [9] J. Boyan, Least-squares temporal difference learning, in: Machine                                           researcher at the Computational Learning and
                   Learning: Proceedings of the Sixteenth International Conference,                                             MotorControlLabattheUniversityofSouthern
                   1999, pp. 49–56.                                                                                             California (USC). Before joining MPI, he grad-
              [10] S. Bradtke, E. Ydstie, A.G. Barto, Adaptive Linear Quadratic                                                 uated from University of Southern California
                   Control Using Policy Iteration, University of Massachusetts,                                                 withaPh.D.inComputerScienceinMarch2007.
                   Amherst, MA, 1994.                                                                                           Jan Peters studied Electrical Engineering, Com-
              [11] O. Buffet, A. Dutech, F. Charpillet, Shaping multi-agent systems with                                        puter Science and Mechanical Engineering. He
                   gradient reinforcement learning, Autonomous Agents Multi-Agent                    holds two German M.Sc. degrees in Informatics and in Electrical
                   Syst. 15 (2) (October 2007) 1387–2532.                                            Engineering (Dipl-Informatiker from Hagen University and Diplom-
              [12] F. Guenter, M. Hersch, S. Calinon, A. Billard, Reinforcement                      Ingenieur from Munich University of Technology/TUM) and two M.Sc.
                   learning for imitating constrained reaching movements, RSJ Adv.                   degrees in ComputerScience andMechanicalEngineeringfromUniversity
                   Robotics 21 (13) (2007) 1521–1544.                                                of Southern California (USC). During his graduate studies, Jan Peters has
              [13] A. Ijspeert, J. Nakanishi, S. Schaal, Learning rhythmic movements by              been a visiting researcher at the Department of Robotics at the German
                   demonstration using nonlinear oscillators, in: IEEE International                 Aerospace Research Center (DLR) in Oberpfaffenhofen, Germany, at
                   Conference on Intelligent Robots and Systems (IROS 2002), 2002,                   Siemens Advanced Engineering (SAE) in Singapore, at the National
                   pp. 958–963.                                                                      University of Singapore (NUS), and at the Department of Humanoid
              [14] S.A. Kakade, Natural policy gradient, in: Advances in Neural                      Robotics and Computational Neuroscience at the Advanced Telecommu-
                   Information Processing Systems, vol. 14, 2002.                                    nication Research (ATR) Center in Kyoto, Japan. His research interests
              [15] V. Konda, J. Tsitsiklis, Actor-critic algorithms, in: Advances in                 include robotics, nonlinear control, machine learning, and motor skill
                   Neural Information Processing Systems, vol. 12, 2000.                             learning.
                                                                 ARTICLE IN PRESS
          1190                                       J. Peters, S. Schaal / Neurocomputing 71 (2008) 1180–1190
                                 Stefan Schaal is an Associate Professor at the     and the Artiﬁcial Intelligence Laboratory at MIT, an Invited Researcher
                                 Department of Computer Science and the Neu-        at the ATR Human Information Processing Research Laboratories in
                                 roscience Program at the University of Southern    Japan, and an Adjunct Assistant Professor at the Georgia Institute of
                                 California, and an Invited Researcher at the ATR   Technology and at the Department of Kinesiology of the Pennsylvania
                                 Human Information Sciences Laboratory in           State University. Dr. Schaal’s research interests include topics of statistical
                                 Japan, where he held an appointment as Head        and machine learning, neural networks, computational neuroscience,
                                 of the Computational Learning Group during an      functional brain imaging, nonlinear dynamics, nonlinear control theory,
                                 international ERATO project, the Kawato Dy-        and biomimetic robotics. He applies his research to problems of artiﬁcial
                                 namic Brain Project (ERATO/JST). Before join-      and biological motor control and motor learning, focusing on both
                                 ing USC, Dr. Schaal was a postdoctoral fellow at   theoretical investigations and experiments with human subjects and
                                 the Department of Brain and Cognitive Sciences     anthropomorphic robot equipment.
