                                                       ANEURALPROBABILISTIC LANGUAGEMODEL
                                                              n        c      h     m direct       mix     train.    valid.   test.
                                  MLP1                        5              50    60      yes      no       182       284     268
                                  MLP2                        5              50    60      yes      yes                275     257
                                  MLP3                        5               0    60      yes      no       201       327     310
                                  MLP4                        5               0    60      yes      yes                286     272
                                  MLP5                        5              50    30      yes      no       209       296     279
                                  MLP6                        5              50    30      yes      yes                273     259
                                  MLP7                        3              50    30      yes      no       210       309     293
                                  MLP8                        3              50    30      yes      yes                284     270
                                  MLP9                        5             100    30      no       no       175       280     276
                                  MLP10                       5             100    30      no       yes                265     252
                                  Del. Int.                   3                                               31       352     336
                                  Kneser-Ney back-off         3                                                        334     323
                                  Kneser-Ney back-off         4                                                        332     321
                                  Kneser-Ney back-off         5                                                        332     321
                                  class-based back-off        3     150                                                348     334
                                  class-based back-off        3     200                                                354     340
                                  class-based back-off        3     500                                                326     312
                                  class-based back-off        3   1000                                                 335     319
                                  class-based back-off        3   2000                                                 343     326
                                  class-based back-off        4     500                                                327     312
                                  class-based back-off        5     500                                                327     312
                         Table 1: Comparative results on the Brown corpus. The deleted interpolation trigram has a test per-
                                    plexity that is 33% above that of the neural network with the lowest validation perplexity.
                                    The difference is 24% in the case of the best n-gram (a class-based model with 500 word
                                    classes). n : order of the model. c : number of word classes in class-based n-grams. h :
                                    number of hidden units. m : number of word features for MLPs, number of classes for
                                    class-based n-grams. direct: whether there are direct connections from word features to
                                    outputs. mix: whether the output probabilities of the neural network are mixed with the
                                    output of the trigram (with a weight of 0.5 on each). The last three columns give perplexity
                                    on the training, validation and test sets.
                         probabilities. On the other hand, without those connections the hidden units form a tight bottleneck
                         which might force better generalization.
                             Table 2 gives similar results on the larger corpus (AP News), albeit with a smaller difference
                         in perplexity (8%). Only 5 epochs were performed (in approximately three weeks with 40 CPUs).
                         The class-based model did not appear to help the n-gram models in this case, but the high-order
                         modiÔ¨Åed Kneser-Ney back-off model gave the best results among the n-gram models.
                         5. Extensions and Future Work
                         In this section, we describe extensions to the model described above, and directions for future work.
                                                                                1149
