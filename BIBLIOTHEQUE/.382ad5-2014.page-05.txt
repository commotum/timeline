                              Theorem 1. The global minimum of the virtual training criterion C(G) is achieved if and only if
                              p =p .Atthatpoint,C(G)achievesthevalue−log4.
                                g     data
                              Proof. For p = p        , D∗ (x) = 1, (consider Eq. 2). Hence, by inspecting Eq. 4 at D∗ (x) = 1, we
                                            g     data   G         2                                                       G         2
                              ﬁnd C(G) = log 1 +log 1 = −log4. To see that this is the best possible value of C(G), reached
                                                  2        2
                              only for p = p       , observe that
                                         g     data
                                                           E        [−log2]+E            [−log2] = −log4
                                                             x∼p                   x∼p
                                                                 data                  g
                              and that by subtracting this expression from C(G) = V (D∗ ,G), we obtain:
                                                                                              G
                                                                                                                     
                                                                                p     +p                    p     +p
                                             C(G)=−log(4)+KL p                   data     g    +KL p  data            g               (5)
                                                                            data      2                   g      2
                              whereKListheKullback–Leiblerdivergence. We recognize in the previous expression the Jensen–
                              Shannondivergence between the model’s distribution and the data generating process:
                                                              C(G)=−log(4)+2·JSD(p                   kp )                               (6)
                                                                                                  data  g
                              Since the Jensen–Shannon divergence between two distributions is always non-negative and zero
                              only when they are equal, we have shown that C∗ = −log(4) is the global minimum of C(G) and
                              that the only solution is p  =p ,i.e.,thegenerativemodelperfectlyreplicatingthedatagenerating
                                                         g      data
                              process.
                              4.2   Convergence of Algorithm 1
                              Proposition2. IfGandDhaveenoughcapacity,andateachstepofAlgorithm1,thediscriminator
                              is allowed to reach its optimum given G, and p is updated so as to improve the criterion
                                                                                 g
                                                          E        [logD∗(x)]+E            [log(1 − D∗ (x))]
                                                            x∼p           G           x∼p              G
                                                                data                      g
                              then p converges to p
                                     g                 data
                              Proof. Consider V (G,D) = U(p ,D) as a function of p as done in the above criterion. Note
                                                                    g                          g
                              that U(p ,D) is convex in p . The subderivatives of a supremum of convex functions include the
                                        g                     g
                              derivative of the function at the point where the maximum is attained. In other words, if f(x) =
                              sup       f (x) and f (x) is convex in x for every α, then ∂f (x) ∈ ∂f if β = argsup                  f (x).
                                  α∈A α              α                                           β                            α∈A α
                              This is equivalent to computing a gradient descent update for p at the optimal D given the cor-
                                                                                                     g
                              responding G. sup U(p ,D) is convex in p with a unique global optima as proven in Thm 1,
                                                    D      g                     g
                              therefore with sufﬁciently small updates of p , p converges to p , concluding the proof.
                                                                               g   g                 x
                              In practice, adversarial nets represent a limited family of p distributions via the function G(z;θ ),
                                                                                              g                                         g
                              and we optimize θ rather than p itself. Using a multilayer perceptron to deﬁne G introduces
                                                   g                g
                              multiple critical points in parameter space. However, the excellent performance of multilayer per-
                              ceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical
                              guarantees.
                              5    Experiments
                              Wetrained adversarial nets an a range of datasets including MNIST[23], the Toronto Face Database
                              (TFD)[28],andCIFAR-10[21]. Thegeneratornetsusedamixtureofrectiﬁerlinearactivations[19,
                              9] and sigmoid activations, while the discriminator net used maxout [10] activations. Dropout [17]
                              was applied in training the discriminator net. While our theoretical framework permits the use of
                              dropout and other noise at intermediate layers of the generator, we used noise as the input to only
                              the bottommost layer of the generator network.
                              We estimate probability of the test set data under p by ﬁtting a Gaussian Parzen window to the
                                                                                        g
                              samples generated with G and reporting the log-likelihood under this distribution. The σ parameter
                                                                                    5
