                  Published as a conference paper at ICLR 2025
                  A BACKWARDPASSTHROUGHFIXED-POINTITERATIONS
                  Training the MIND model involves back propagating gradients through fixed-point iterations in the prediction
                  network. Computinggradients through these iterations poses challenges, particularly due to the computational
                  cost of unrolling and the difficulty in computing inverse Jacobians required for implicit differentiation. The
                  PhantomGradientsmethodintroducedbyGengetal.(2021)offersanefficient and stable alternative for
                  training implicit models. Standard methods for backpropagation through fixed-point iterations include:
                       • Unrolling (Backpropagation Through Time—BPTT): Unrolling the iterations and computing gradi-
                         ents at each step. This approach is memory-intensive and computationally expensive, particularly
                         whenthenumberofiterations is large or variable.
                       • Implicit Differentiation: Computing gradients using the implicit function theorem, which involves
                         solving linear systems with the Jacobian matrix. This can be computationally intensive and may
                         suffer from numerical instability.
                  ThePhantomGradientsmethodbypassestheneedforunrolling or computing inverse Jacobians by approxi-
                  mating the gradients through the fixed-point iterations using a surrogate function. Specifically, it treats the
                  fixed-point iteration as a single transformation and defines the gradient to be proportional to the change
                  induced by the last iteration.
                  Let z(K) be the final activation after K iterations:
                                                  z(K) = f(z(K−1);θl).                            (7)
                  ThePhantomGradientapproximates the gradient of the loss L with respect to the parameters θl as:
                                                            
                                                 dL ≈    ∂L   ⊤ ∂z(K).                            (8)
                                                 dθ     ∂z(K)    ∂θ
                                                   l               l
                  Similarly, the gradient with respect to the input z(0) is approximated as:
                                                         ⊤ K      (k) !
                                             dL ≈     ∂L      Y ∂z        .                       (9)
                                            dz(0)    ∂z(K)    k=1 ∂z(k−1)
                  However, instead of computing the full product of Jacobians, which is equivalent to unrolling, the Phantom
                  Gradients method approximates this by using the identity matrix or a simplified estimate. The key idea is to
                  approximate the gradient as if the fixed-point iteration were a single-layer transformation. This approximation
                  assumes that the earlier iterations have a diminishing effect on the final output, which is often the case when
                  the fixed-point iteration converges.
                  Therefore, we can approximate:
                                              dL ≈ ∂L ⊤∂f(z(K);θl).                            (10)
                                              dθ     ∂z(K)       ∂θ
                                                l                  l
                  Similarly, for the input:           
                                           dL       ∂L   ⊤ ∂f(z(K);θ ) ∂z(K−1)
                                               ≈                   l        .                    (11)
                                          dz(0)   ∂z(K)     ∂z(K−1)   ∂z(0)
                  Byrecursively applying this approximation and assuming that ∂z(K−1) ≈ I, where I is the identity matrix,
                  wesimplify the computation.                       ∂z(0)
                  During backpropagation, we approximate the gradients with respect to θl as:
                                                ∂L ≈∂L⊤∂f(z∗;θl).                              (12)
                                                ∂θ     ∂z∗      ∂θ
                                                  l               l
                                                       16
