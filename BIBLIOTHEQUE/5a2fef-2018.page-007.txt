                                    attending to                                          time                                 attention weights
                                    from           input
                                   ing
                                   attendInput Vector Id(a) Reference vector is the last in a sequence, e.g. "Choose the 5th furthest vector from vector 7"
                                                   (b) Reference vector is the ﬁrst in a sequence, e.g. "Choose the 3rd furthest vector from vector 4"
                                        (c) Reference vector comes in the middle of a sequence, e.g. "Choose the 6th furthest vector from vector 6"
                                 Figure 3: Model analysis. Each row depicts the attention matrix at each timestep of a particular
                                 sequence. The text beneath spells out the particular task for the sequence, which was encoded and
                                 provided to the model as an input. We mark in red the vector that is referenced in the task: e.g., if the
                                                              nd
                                 modelis to choose the 2          farthest vector from vector 7, then the time point at which vector 7 was
                                 input to the model is depicted in red. A single attention matrix shows the attention weights from one
                                 particular memory slot (y-axis) to another memory slot (columns), or the input (offset column), with
                                 the numbers denoting the memory slot and “input” denoting the input embedding.
                                 non-auto-regressive fashion - that is, with no teacher forcing during training. This is likely related to
                                 the effect that relaxing the ground truth requirement has on improving model generalization [39] and
                                 hence, performance. It is perhaps more pronounced in these tasks due to the independence of output
                                 token probabilities and also the sharply uni-modal nature of the output distribution (that is, there is
                                 noambiguity in the answer given the program).
                                         Table 1: Test per character Accuracy on Program Evaluation and Memorization tasks.
                                         Model                             Add      Control      Program       Copy      Reverse      Double
                                         LSTM[3,37]                        99.8       97.4         66.1        99.8        99.7         99.7
                                         EntNet [38]                       98.4       98.0         73.4        91.8       100.0         62.3
                                         DNC[5]                            99.4       83.8         69.5        100.0      100.0        100.0
                                         Relational Memory Core            99.9       99.6         79.0        100.0      100.0         99.8
                                   Table 2: Validation and test perplexities on WikiText-103, Project Gutenberg, and GigaWord v5.
                                                                                   WikiText-103          Gutenberg        GigaWord
                                                                                   Valid.     Test     Valid     Test         Test
                                                  LSTM[40]                            -       48.7        -        -            -
                                                  Temporal CNN[41]                    -       45.2        -        -            -
                                                  Gated CNN[42]                       -       37.2        -        -            -
                                                  LSTM[32]                          34.1      34.3      41.8     45.5         43.7
                                                  Quasi-RNN[43]                      32        33         -        -            -
                                                  Relational Memory Core            30.8      31.6      39.2     42.0         38.3
                                                                                            7
