                         Published as a conference paper at ICLR 2021
                                              ¨
                         SeppHochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):1–42, 1997.
                         Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Weinberger. Deep Networks with
                            Stochastic Depth. arXiv e-prints, art. arXiv:1603.09382, March 2016.
                         J. Huang, L. Qu, R. Jia, and B. Zhao. O2u-net: A simple noisy label detection approach for deep
                            neural networks. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp.
                            3325–3333, 2019.
                         Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, Hy-
                            oukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen.         GPipe: Ef-
                            ﬁcient Training of Giant Neural Networks using Pipeline Parallelism.  arXiv e-prints, art.
                            arXiv:1811.06965, November 2018.
                         Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
                            reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
                         Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-
                            son. Averaging Weights Leads to Wider Optima and Better Generalization. arXiv e-prints, art.
                            arXiv:1803.05407, March 2018.
                                                           ´
                         Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gener-
                            alization in neural networks. CoRR, abs/1806.07572, 2018. URL http://arxiv.org/abs/
                            1806.07572.
                         Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun
                            Cho, and Krzysztof Geras. The Break-Even Point on Optimization Trajectories of Deep Neural
                            Networks. arXiv e-prints, art. arXiv:2002.09572, February 2020.
                         Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Regularizing
                            very deep neural networks on corrupted labels. CoRR, abs/1712.05055, 2017. URL http:
                            //arxiv.org/abs/1712.05055.
                         Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond Synthetic Noise: Deep Learning on
                            Controlled Noisy Labels. arXiv e-prints, art. arXiv:1911.09781, November 2019.
                         Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
                            generalization measures and where to ﬁnd them. arXiv preprint arXiv:1912.02178, 2019.
                         Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv e-prints,
                            art. arXiv:1412.6980, December 2014.
                         Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
                            and Neil Houlsby. Big transfer (bit): General visual representation learning, 2020.
                         Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do Better ImageNet Models Transfer Better?
                            arXiv e-prints, art. arXiv:1805.08974, May 2018.
                         John Langford and Rich Caruana. (not) bounding the true error. In Advances in Neural Information
                            Processing Systems, pp. 809–816, 2002.
                         Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-
                            tion. Annals of Statistics, pp. 1302–1338, 2000.
                         Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo Li, and Jinwoo Shin. Robust inference via
                            generative classiﬁers for handling noisy labels, 2019.
                         HaoLi,ZhengXu,GavinTaylor,andTomGoldstein. Visualizing the loss landscape of neural nets.
                            CoRR,abs/1712.09913, 2017. URL http://arxiv.org/abs/1712.09913.
                         Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment.
                            CoRR,abs/1905.00397, 2019. URL http://arxiv.org/abs/1905.00397.
                         James Martens and Roger Grosse. Optimizing Neural Networks with Kronecker-factored Approxi-
                            mate Curvature. arXiv e-prints, art. arXiv:1503.05671, March 2015.
                                                                      11
