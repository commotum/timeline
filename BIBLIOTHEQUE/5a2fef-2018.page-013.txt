                             A Furthertaskdetails, analyses, and model conﬁgurations
                             In the following sections we provide further details on the experiments and the model conﬁgurations. We will
                             sometimes refer to the following terms when describing the model:
                                    • “total units”: The total number of elements in the memory matrix M. Equivalent to the size of each
                                       memorymultiplied by the number of memories.
                                    • “numheads”: The number of attention heads; i.e., the number of unique sets of queries, keys, and
                                       values produced for the memories.
                                    • “memoryslots” or “number of memories”: Equivalent to the number of rows in matrix M.
                                    • “numblocks”: The number of iterations of attention performed at each time-step.
                                    • “gate style”: Gating per unit or per memory slot
                             A.1    Nth Farthest
                             Inputs consisted of sequences of eight randomly sampled, 16-dimensional vectors from a uniform distribution
                             x ∼ U(−1,1), and vector labels l ∼ {1,2,...,8}, encoded as a one-hot vectors and sampled without
                              t                                  t
                             replacement. Labels were sampled and hence did not correspond to the time-points at which the vectors were
                             presented to the model. Appended to each vector-label input was the task speciﬁcation (i.e., the values of n and
                             mforthat sequence), also encoded as one-hot vectors. Thus, an input for time-step t was a 40-dimensional
                             vector (x ;l ;n;m).
                                      t  t
                             For all models (RMC, LSTM, DNC) we used the Adam optimiser [44] with a batch size of 1600, learning rates
                             tuned between 1e−5 and 1e−3, and trained using a softmax cross entropy loss function. All the models had
                             an equivalent 4-layer MLP (256 units per layer with ReLu non-linearities) to process their outputs to produce
                             logits for the softmax. Learning rate did not seem to inﬂuence performance, so we settled on 1e−4 for the ﬁnal
                             experiments.
                             For the LSTM and DNC, architecture parameters seemingly made no difference to model performance. For the
                             LSTMwetriedhiddensizesrangingfrom64upto4096units,andfortheDNCwetried1,8,or16memories,
                             128, 512, or 1024 memory sizes (which we tied to the controller LSTM size), and 1, 2, or 4 memory reads &
                             writes. The DNC used a 2-layer LSTM controller.
                             For the RMC we used 1, 8, or 16 memories with 2048 total units (so, the size of each memory was   2048  ),
                                                                                                                             num_mems
                             1, 8, or 16 heads, 1 block of attention, and both the ‘unit’ and ‘memory’ gating methods. Figure 4 shows the
                             results of a hyperparameter sweep scaled according to wall-clock time (models with more but smaller memories
                             are faster to run than those with fewer but larger memories, and we chose to compare models with equivalent
                             numberoftotal units in the memory matrix M).
                             A.2    ProgramEvaluation
                             Tofurther study the effect of relational structure on working memory and symbolic representation we turned
                             to a set of problems that provided insights into the RMC’s ﬁtness as a generalized computational model. The
                             Learning to Execute (LTE) dataset [25] provided a good starting point for assessing the power of our model
                             over this class of problems. Sample problems are of the form of linear time, constant memory, mini-programs.
                             Training samples were generated in batches of 128 on-the-ﬂy. Each model was trained for 200K iterations using
                                                                      −3
                             an Adamoptimiser and learning rate of 1e    . The samples were parameterized by literal length and nesting
                             depth which deﬁne the length of terminal values in the program snippets and the level of program operation
                             nesting. Within each batch the literal length and nesting value was sampled uniformly up to the maximum value
                             for each - this is consistent with the Mix curriculum strategy from [25]. We evaluated the model against a batch
                             of 12800 samples using the maximum nesting and literal length values for all samples and report the top score.
                             Examples of samples for each task can be found in ﬁgure 6 and ﬁgure 7. It also worth noting that the modulus
                             operation was applied to addition, control, and full program samples so as to bound the output to the maximum
                             literal length in case of longer for-loops.
                             The sequential model consists of an encoder and a decoder which each take the form of a recurrent neural
                             network [45, 25]. Once the encoder has processed the input sequence the state of the encoder is used to initialize
                             the decoder state and subsequently to generate the target sequence (program output). The output from all
                             models is passed through a 4-layer MLP - all layers have size 256 with an output ReLU - to generate an output
                             embedding at each step of the output sequence.
                             In [25] teacher forcing is used for both training and testing in the decode phase. For our experiments, we began
                             byexploring teacher forcing during training but used model predictions from the previous step as input to the
                             the decoder at the next step when evaluating the model [45]. We also considered the potential effect of limiting
                                                                                13
