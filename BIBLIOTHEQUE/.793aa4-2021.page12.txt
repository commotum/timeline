                                               ˆ                    ˆ
                   Choosedp = n > d andletP = I. NowchosingPwithzerosintheﬁrstn−dp columnsandidentity
                                      h
                                       ˆ
                in the last d columns (P = [0       , I    ]) gives
                           p                  d,n−d   d ,d
                                                   p   p p
                                                          0             0        
                                                    >        n−d ,n−d     n−d ,d
                                                ˆ ˆ             p    p       p p
                                                PP =         0            I         .
                                                              d ,n−d       d ,d
                                                               p    p       p p
                Combiningthese two gives us
                                                                          > > ˆˆ>
                                             rank(Ar) = rank(XW W X +PP )
                                                                     Q    K
                                                        =min(d +dp,n)>d .
                                                                 h             h
                   LetX∈Rn×dbetheinputwordembeddingsindimensiondwithsequencelengthn. Wehavetrainable
                position embeddings P ∈ Rn×d, which are added to the input sequence before feeding into the model g.
                For a given input X and label y, the objective for a loss function ` is as follows:
                                                        L=`(g(X+P),y)                                             (5)
                Theorem 2. Let X and P be trainable embedding matrices in Rn×d. Then the gradients of the loss
                function in equation (5), at any point (X,y), and for any differentiable functions ` and g, are same for X
                andP.
                Remarks. This theorem shows us that the gradients are same for the input token embeddings and
                position embeddings. While in standard NLP tasks the inputs X can be different in each step due to
                different input tokens being present in each mini batch, the result still suggests that additive position
                embedding can limit the model from learning the relative importance of position encodings with respect
                to token embeddings based on the training task at hand.
                Proof of Theorem 2. The above theorem follows by just computing the gradients and showing they are
                equal for each step.
                   Gradients of the above objective w.r.t X and P are as follows.
                                                ∇ L=∇ L·∇             g · ∇ (X+P)
                                                  X       g      X+P       X
                                                 =∇L·∇          g
                                                     g      X+P
                                                ∇ L=∇ L·∇            g · ∇ (X+P)
                                                  P       g      X+P      P
                                                 =∇L·∇          g.
                                                     g      X+P
                Theabovecomputationofgradient follows from chain rule. This shows that the gradients of L w.r.t. X
                and P are the same.
                                                                2985
