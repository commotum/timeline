                         Published as a conference paper at ICLR 2025
                         Algorithm 3 Forward Propagation for MIND model
                         Require: Input data x, predition network layers L = {L ,L ,...,L }, introspection network I, maximum
                                                                                     1   2        N
                              iterations K    , tolerance ϵ
                                           max
                         Ensure: Output prediction y
                          1: Initialize: z ← x
                                           0
                          2: Obtain layer selection: S ← I(x)                                                                      ▷ S ⊆ L
                          3: for l = 1 to N do
                          4:      if Ll ∈ S then         (0)                                                ▷ Apply Fixed-Point Iteration
                          5:          Initialize k ← 0, z    ←z
                          6:          repeat             l        l−1
                                           (k+1)        (k)     
                          7:              z      ←f z ;θ
                                           l          l   l     l
                          8:              k ←k+1
                                            ∥z(k)−z(k−1)∥
                          9:          until   l    l       <ϵork≥K
                                                ∥z(k)∥                   max
                                                  l
                         10:          z ←z(k)
                         11:      else l     l                                                           ▷Standard Forward Propagation
                         12:          z ←f (z       ; θ )
                                       l     l   l−1   l
                         13:      endif
                         14: end for
                         15: Output: y ← OutputLayer(z )
                                                             N
                         Theproofofconvergence follows from the contraction mapping principle. Let xn be the n-th iterate of the
                         fixed point iteration. Then:
                                                                            ∗                    ∗
                                                                 d(x     , x ) = d(T(x ),T(x ))
                                                                     n+1                 n
                                                                                             ∗
                                                                               ≤κ·d(x ,x )
                                                                                         n
                                                                                   n          ∗
                                                                               ≤κ ·d(x0,x )
                                        n                                         ∗
                         Asn→∞,κ →0sinceκ<1.Therefore,d(x ,x )→0,provingthatthesequence{x }convergesto
                                                                              n                                            n
                                           ∗
                         the fixed point x (Nisar et al., 2024).
                         Therate of convergence is linear, with an error bound given by:
                                                                                    n
                                                                           ∗       κ
                                                                  ∥x −x ∥≤              ∥x −x ∥                                         (20)
                                                                     n           1−κ 1           0
                         This error bound demonstrates that the convergence rate depends on the contraction constant κ, with smaller
                         values of κ leading to faster convergence (Ansar & Mas’ud, 2023).
                         In the context of our MIND model, the fixed point iteration is applied to the introspection network, ensuring
                         that the model converges to a stable representation of the input data. This convergence property is crucial for
                         the stability and reliability of the model’s predictions.
                         C EXPERIMENTSETUP
                         All experiments were conducted using PyTorch (Paszke et al., 2019) on NVIDIA A40 GPUs with 20GB
                         memory. The MINDmodelwasoptimizedusingtheAdamoptimizer(Kingma&Ba,2014)withaninitial
                         learning rate of 1 × 10−3, decayed by a factor of 0.1 every 30 epochs. The batch size was set to 64.
                         Hyperparameters α, β, γ, and δ in Equation 5 were fine-tuned to 0.5, 0.2, 0.2, and 0.1 respectively, while λ for
                         Lintrospect was set to 0.6. Each model was trained for 100 epochs with early stopping, triggered when validation
                         loss did not improve over 10 epochs. Fixed-point iteration (FPI) tolerance for the MIND architecture was set to
                                                                             19
