                         HowCanSelf-AttentionNetworksRecognizeDyck-nLanguages?
                                                 Javid Ebrahimi, Dhruv Gelda, Wei Zhang
                                                         Visa Research, Palo Alto, USA
                                               {jebrahim,dhgelda,wzhan}@visa.com
                                        Abstract                                          2                          4
                                                                              T                           T
                                                                              (                            (
                       We focus on the recognition of Dyck-n (Dn)             [                           <
                       languages with self-attention (SA) networks,           ]                           >
                       which has been deemed to be a difﬁcult task            )                            [
                                                                              [                           {
                       for these networks. We compare the perfor-             (                           }
                       mance of two variants of SA, one with a start-         (                            ]
                                                                              )                           <
                                        +                         −           )                           >
                       ing symbol (SA ) and one without (SA ).                ]                            )
                                                +
                       Our results show that SA    is able to general-          T ( [ ] ) [ ( ( ) ) ]        T ( < > [ { } ] < > )
                       ize to longer sequences and deeper dependen-
                                                       −
                       cies. For D , we ﬁnd that SA       completely
                                   2                                         Figure 1: Softmax attention scores of the second layer
                       breaks down on long sequences whereas the                                    +
                                                                             of a sufﬁx-masked SA , for a D and a D sequence.
                                      +                                                                       2         4
                       accuracy of SA   is 58.82%. We ﬁnd attention          Therowsandcolumnsdenotequeriesandkeys,respec-
                       maps learned by SA+ to be amenable to in-             tively. The layer produces virtually hard attentions, in
                       terpretation and compatible with a stack-based        which each symbol attends only to one preceding sym-
                       language recognizer. Surprisingly, the perfor-        bol or itself. The attended symbol is either the starting
                       mance of SA networks is at par with LSTMs,            symbol (T) or the last unmatched opening bracket.
                       which provides evidence on the ability of SA
                       to learn hierarchies without recursion.
                  1    Introduction                                          a starting symbol to the vocabulary, a two-layer
                                                                             multi-headed SA network (i.e., the encoder of a
                                                                             Transformer) is able to learn D       languages, and
                  There is a growing interest in using formal lan-                                              n
                  guages to study fundamental properties of neural           generalize to longer sequences, although not per-
                  architectures, which has led to the extraction of in-      fectly. As shown in Figure 1, the network is able to
                  terpretable models (Weiss et al., 2018; Merrill et al.,    identify the corresponding closing bracket for an
                  2020). Recent work (Hao et al., 2018; Suzgun               opening bracket, in what resembles a stack-based
                  et al., 2019; Skachkova et al., 2018) has explored         automaton. For example, the symbol “]” in the
                  the generalized Dyck-n (Dn) languages, a subset            string “([])”, will ﬁrst pop “[” from the stack, then
                  of context-free languages. Dn consists of “well-           it attends to “(”, the last unmatched symbol, which
                  balanced” strings of parentheses with n different          will determine the next valid closing bracket. The
                  types of bracket pairs, and it is the canonical formal     starting symbol (T) enables the model to learn the
                  language to study nested structures (Chomsky and           occurrence of the end of a clause or the end of the
                       ¨                                                     sequence, which can be regarded as a mechanism
                  Schutzenberger, 1959). Weiss et al. (2018) show
                  that LSTMs (Hochreiter and Schmidhuber, 1997)              to represent an empty stack.
                  are a variant of the k-counter machine and can rec-           Ourworkistheﬁrsttoperform an empirical ex-
                  ognize D1 languages. The dynamic counting mech-            ploration of SA on formal languages. We present
                  anisms, however, are not sufﬁcient for Dn>1 as it          detailed comparison between an SA which incorpo-
                  requires emulating a pushdown automata. Hahn               rates a starting symbol (SA+), andonethatdoesnot
                                                                                 −
                  (2020) shows that for a sufﬁciently large length,          (SA ), and demonstrate signiﬁcant differences in
                  Transformers (Vaswani et al., 2017) will fail to           their generalization across the length of sequences
                  transduce the D language.                                  and the depth of dependencies.
                                    2
                     Weempirically show that with the addition of               Recent work has suggested that the ability of
                                                                        4301
                                 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4301–4306
                                                                c
                                        November16-20,2020. 2020AssociationforComputational Linguistics
