# A Length-Extrapolatable Transformer (2023)
Source: 8e24d0-2023.pdf

## Core reasons
- The paper critiques existing positional embeddings for poor length extrapolation and proposes a new relative positional embedding (XPOS) to improve position handling.
- The central contribution is a positional encoding modification (extrapolatable position embedding) aimed at improving attention resolution and extrapolation.

## Evidence extracts
- "Among many relative position embeddings, ROPE (Su et al., 2021) shows better performance and is used to many PLMs such as PaLM (Chowdhery et al., 2022). However, it canâ€™t deal with sequences with exceeding length. Alibi (Press et al., 2021) mitigates the extrapolation problem but sacrifices the general performance." (p. 1)
- "Considering all the properties above, we propose Extrapolatable Position Embedding (XPOS), which is a universal-good design for Transformers." (p. 2)

## Classification
Class name: Positional Encoding Improvement Proposal
Class code: 1

$$
\boxed{1}
$$
