# Rethinking Positional Encoding in Language Pre-Training (2021)
Source: 75e90b-2020.pdf

## Core reasons
- The paper explicitly critiques existing absolute positional encoding, arguing the addition of positional and word embeddings creates mixed correlations and noise.
- It proposes a new positional encoding method (TUPE) that separates word and positional correlations in self-attention, directly modifying positional encoding behavior.

## Evidence extracts
- "First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources." (p. 1)
- "we propose a new positional encoding method called Transformer with Untied Positional Encoding (TUPE)." (p. 1)

## Classification
Class name: Positional Encoding Improvement Proposal
Class code: 1

$$
\boxed{1}
$$
