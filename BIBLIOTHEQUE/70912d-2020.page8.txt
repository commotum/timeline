                                                            Generative Pretraining from Pixels
               skiy et al., 2015), jigsaw puzzle solving (Noroozi & Favaro,    However, our experiments also demonstrate several areas
               2016), and rotation prediction (Gidaris et al., 2018). A clus-  for improvement. We currently model low resolution in-
               ter of similar approaches based on contrastive losses com-      puts with self-attention. By comparison, most other self-
               paring various views and transformations of input images        supervised results use CNN based encoders that easily work
               have recently driven signiﬁcant progress in self-supervised     with high resolution images. It is not immediately obvious
               learning (Hjelm et al., 2018; Bachman et al., 2019; Tian        howtobestbridge the gap between performant autoregres-
               et al., 2019).                                                  sive and discriminative models. Additionally, we observed
               Among contrastive approaches, our work is most similar          that our approach requires large models in order to learn
               to Contrast Predictive Coding (Oord et al., 2018) which         high quality representations. iGPT-L has 2 to 3 times as
               also utilizes a autoregressive prediction objective, but in a   many parameters as similarly performing models on Ima-
               learned latent space, and to Selﬁe (Trinh et al., 2019) which   geNet and uses more compute.
               trains a bidirectional self-attention architecture on top of a  Although dense self-attention was a deliberate choice for
               standard convolutional network to differentiate correct vs      this work due to it being domain agnostic and widely used in
               wrongpatches.                                                   NLP,it becomes very memory and computationally expen-
               Ourworkisdirectly inspired by the success of generative         sive due to its quadratic scaling with sequence length. We
               pre-training methods developed for Natural Language Pro-        mitigated this via the context reduction techniques discussed
               cessing. These methods predict some parts of a piece of text    in section 3.2 but it is still a signiﬁcant limitation. Future
               conditioned on other parts. Our work explores two training      workcould instead address this via architectural changes by
               objectives in this framework, autoregressive prediction as      exploring more efﬁcient self-attention approaches. Several
               originally explored for modern neural sequence models by        promising techniques have recently been developed such as
               Dai&Le(2015),andadenoisingobjective,similartoBERT               local 2D relative attention (Bello et al., 2019; Ramachan-
               (Devlin et al., 2018). The context in-painting approach of      dran et al., 2019), sparse attention patterns (Child et al.,
               Pathak et al. (2016) also explores pre-training by predict-     2019), locality sensitive hashing (Kitaev et al., 2020), and
               ing corruptions but predicts large regions of high-resolution   multiscale modeling (Menick & Kalchbrenner, 2018).
               images.                                                         Finally, our results, considered together with Donahue &
               Kolesnikov et al. (2019); Goyal et al. (2019) conducted         Simonyan(2019),suggestrevisitingtherepresentationlearn-
               rigorous investigations of existing self-supervised methods.    ing capabilities of other families of generative models such
               Several of our ﬁndings are consistent with their results, in-   as ﬂows (Dinh et al., 2014; Kingma & Dhariwal, 2018)
               cluding the beneﬁts of scale and the non-monotonic perfor-      and VAEs in order to study whether they show similarly
               manceofrepresentations with depth in certain architectures.     competitive representation learning capabilities.
               Expressive autoregressive models tractably optimizing like-     References
               lihood were ﬁrst applied to images by Uria et al. (2013)
               and popularized by Oord et al. (2016) serving for the ba-       Ba,J.L.,Kiros,J.R.,andHinton,G.E. Layernormalization.
               sis of several papers similarly adapting transformers to the       arXiv preprint arXiv:1607.06450, 2016.
               problem of generative image modeling (Parmar et al., 2018;      Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning
               Child et al., 2019).                                               representations by maximizing mutual information across
               Keetal.(2018)introduced the pixel-by-pixel CIFAR10 task            views. In Advances in Neural Information Processing
               and ﬁrst benchmarked the performance of a 1D sequence              Systems, pp. 15509–15519, 2019.
               transformer on a competitive image classiﬁcation dataset.       Bello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V.
               Rives et al. (2019) similarly investigates whether the recent      Attention augmented convolutional networks. In Proceed-
               success of unsupervised pre-training in NLP applies to other       ings of the IEEE International Conference on Computer
               domains, observing promising results on protein sequence           Vision, pp. 3286–3295, 2019.
               data.
               6. Discussion and Conclusion                                    Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N.,
                                                                                  Oliver, A., and Raffel, C. A.     Mixmatch: A holistic
               Ourresults suggest that generative image modeling contin-          approach to semi-supervised learning. In Advances in
               ues to be a promising route to learn high-quality unsuper-         Neural Information Processing Systems, pp. 5050–5060,
               vised image representations. Simply predicting pixels learns       2019.
               state of the art representations for low resolution datasets.   Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A
               In high resolution settings, our approach is also competitive      simple framework for contrastive learning of visual rep-
               with other self-supervised results on ImageNet.                    resentations. arXiv preprint arXiv:2002.05709, 2020.
