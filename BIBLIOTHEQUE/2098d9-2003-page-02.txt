                                            BENGIO,DUCHARME,VINCENTANDJAUVIN
                   mated, and when the number of values that each discrete variable can take is large, most observed
                   objects are almost maximally far from each other in hamming distance.
                       Ausefulwaytovisualizehowdifferentlearningalgorithms generalize, inspired fromtheviewof
                   non-parametric density estimation, is to think of how probability mass that is initially concentrated
                   onthetraining points (e.g., training sentences) is distributed in a larger volume, usually in some form
                   of neighborhood around the training points. In high dimensions, it is crucial to distribute probability
                   mass where it matters rather than uniformly in all directions around each training point. We will
                   show in this paper that the way in which the approach proposed here generalizes is fundamentally
                   different from the way in which previous state-of-the-art statistical language modeling approaches
                   are generalizing.
                       Astatistical model of language can be represented by the conditional probability of the next
                   word given all the previous ones, since
                                                      ˆ  T     T ˆ     t−1
                                                     P(w )=      P(w |w   ),
                                                         1    ∏ t 1
                                                              t=1
                   where w is the t-th word, and writing sub-sequence wj =(w ,w   ,···,w    ,w ). Such statisti-
                           t                                          i      i  i+1      j−1  j
                   cal language models have already been found useful in many technological applications involving
                   natural language, such as speech recognition, language translation, and information retrieval. Im-
                   provements in statistical language models could thus have a signiﬁcant impact on such applications.
                       When building statistical models of natural language, one considerably reduces the difﬁculty
                   of this modeling problem by taking advantage of word order, and the fact that temporally closer
                   words in the word sequence are statistically more dependent. Thus, n-gram models construct ta-
                   bles of conditional probabilities for the next word, for each one of a large number of contexts,i.e.
                   combinations of the last n−1 words:
                                                    ˆ     t−1    ˆ     t−1
                                                   P(w |w    ) ≈P(w |w      ).
                                                       t  1         t  t−n+1
                   We only consider those combinations of successive words that actually occur in the training cor-
                   pus, or that occur frequently enough. What happens when a new combination of n words appears
                   that was not seen in the training corpus? We do not want to assign zero probability to such cases,
                   because such new combinations are likely to occur, and they will occur even more frequently for
                   larger context sizes. A simple answer is to look at the probability predicted using a smaller context
                   size, as done in back-off trigram models (Katz, 1987) or in smoothed (or interpolated) trigram mod-
                   els (Jelinek and Mercer, 1980). So, in such models, how is generalization basically obtained from
                   sequences of words seen in the training corpus to new sequences of words? A way to understand
                   how this happens is to think about a generative model corresponding to these interpolated or back-
                   off n-gram models. Essentially, a new sequence of words is generated by “gluing” very short and
                   overlapping pieces of length 1, 2 ... or up to n words that have been seen frequently in the training
                   data. The rules for obtaining the probability of the next piece are implicit in the particulars of the
                   back-off or interpolated n-gram algorithm. Typically researchers have used n = 3, i.e. trigrams,
                   and obtained state-of-the-art results, but see Goodman (2001) for how combining many tricks can
                   yield to substantial improvements. Obviously there is much more information in the sequence that
                   immediately precedes the word to predict than just the identity of the previous couple of words.
                   There are at least two characteristics in this approach which beg to be improved upon, and that we
                                                              1138
