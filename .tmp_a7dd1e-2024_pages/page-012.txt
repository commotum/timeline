                  benchmark for evaluating foundation models. arXiv
                  preprint arXiv:2304.06364.
                Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu,
                  Chao Yang, Wanli Ouyang, and Yu Qiao. 2024a.
                  Emulated disalignment: Safety alignment for large
                  language models may backfire!    arXiv preprint
                  arXiv:2402.12343.
                Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu,
                  Xiangyu Yue, Wanli Ouyang, and Yu Qiao. 2023.
                  Beyond one-preference-for-all: Multi-objective di-
                  rect preference optimization.   arXiv preprint
                  arXiv:2310.03708.
                Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong,
                  Chao Yang, and Yu Qiao. 2024b. Weak-to-strong
                  search: Align large language models via search-
                  ing over small language models.  arXiv preprint
                  arXiv:2405.19262.
                                                               7432
