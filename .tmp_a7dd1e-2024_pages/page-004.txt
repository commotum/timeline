                         Task                         Abbr.    Description
                         Context Memory                CM      Recall early dialogue details to address the user’s current question.
                         Anaphora Resolution           AR      Identify pronoun referents throughout a multi-turn dialogue.
                         Separate Input                 SI     Thefirst turn outlines the task requirements and the following turns specify the task input.
                         Topic Shift                    TS     Recognize and focus on the new topic when users unpredictably switch topics.
                         Content Confusion             CC      Avoid interference from similar-looking queries with distinct meanings in the dialogue’s history.
                         Content Rephrasing            CR      Rephrase the content of the last response according to the user’s newest requirement.
                         Format Rephrasing              FR     Rephrase the format of the last response according to the user’s newest requirement.
                         Self-correction                SC     Recorrect the last response according to the user feedback.
                         Self-affirmation               SA     Preserve the last response against inaccurate user feedback.
                         Mathematical Reasoning        MR      Collaboratively solve complex mathematical problems with users across dialogue turns.
                         General Reasoning             GR      Collaboratively solve complex general reasoning problems with users across dialogue turns.
                         Instruction Clarification      IC     Seek clarification by asking further questions on ambiguous user queries.
                         Proactive Interaction          PI     Propose questions in reaction to user statements to spark their interest to continue the dialogue.
                                                   Table 1: The 13 tasks for multi-turn dialogues within MT-Bench-101.
                           Benchmark        #Dialogues    #Turns   #Tasks    Fine-grained        3.2     DataCollection
                           AlpacaEval          805         805        1            ✗             We tailored unique data generation prompts for
                            MT-Bench            80         160        1            ✗             each task based on its specific characteristics and
                          MT-Bench++            80         640        1            ✗
                             BotChat           547         547        1            ✗             utilized GPT-4 to construct data. In detail, the
                              MINT             568         568        3            ✗             prompts included data generation rules and used
                         MT-Bench-101          1388        4208       13          ✓              manually crafted examples as guidance for GPT-
                                           Table 2: Data statistics.                             4. This ensured the generated data met the spe-
                       ters incorrect feedback from the user. In such cases,                     cific needs of each task. Our benchmark covers 30
                       the chatbot needs to identify the inaccuracies in the                     diverse topics, including health, history, science,
                       user’s feedback and adhere to its original response.                      finance, law, humanities, arts, and others. The Ap-
                                                                                                 pendix A shows a complete list of topics and the
                       Reasoning:          Mathematical Reasoning (MR). Ef-                      prompts used for data generation.
                       fective reasoning across multiple dialogue turns                              For each task, we utilized GPT-4 to generate
                       is essential in solving mathematical problems, as                         over 1000 samples. These samples were then rig-
                       users may introduce new conditions or hypothe-                            orously curated by human annotators to form the
                       ses as the conversation progresses. General Rea-                          final dataset. For each piece of data, it underwent
                       soning (GR) encompasses a variety of reasoning                            screening by five annotators, and we ultimately re-
                       challenges, e.g. puzzles, inductive reasoning, and                        tained only the data that all annotators deemed to
                       deductive reasoning. Chatbots are required to work                        beofhighquality. The primary criteria for curation
                       alongside users through successive dialogue turns                         are shown in the Appendix A.
                       to address these issues.                                                  3.3     DataStatistics
                       3.1.3     Interactivity                                                   Table 2 shows the key statistics of our MT-Bench-
                                                                                                 101. This benchmark features a comprehensive hi-
                       Chatbotsproactivelyproposequestionstoguidethe                             erarchical taxonomy for multi-turn dialogues with
                       dialogue or gather information for better responses                       13 distinct tasks, 1388 dialogues, and 4208 turns.
                       in chatbot-triggered dialogue.                                            Detailed statistics for each task can be found in the
                       Questioning:           Instruction Clarification (IC) tar-                Appendix B. Additionally, we provide a compara-
                       gets scenarios where the user’s initial question is                       tive analysis between MT-Bench-101 and existing
                       unclear. The chatbot needs to ask follow-up ques-                         dialogue evaluation benchmarks. This comparison
                       tions to obtain more information. This iterative                          highlights that MT-Bench-101 is the first dataset
                       intent clarification process may span several turns                       to specifically focus on fine-grained multi-turn dia-
                       to ensure the chatbot fully grasps the user’s intent.                     logue abilities, notable for its extensive volume of
                       Proactive Interaction (PI) assesses the chatbot’s                         data and diversity of tasks.
                       ability to craft suitable follow-up questions or com-                     3.4     Evaluation
                       ments in reaction to user statements, thereby spark-                      In multi-turn dialogues, new turns rely on the
                       ing the user’s interest to continue the dialogue.                         interaction between humans and chatbots in the
                                                                                           7424
