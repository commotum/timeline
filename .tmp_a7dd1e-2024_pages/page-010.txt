                     Alpacafarm: A simulation framework for methods           Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
                     that learn from human feedback. Advances in Neural          sch, Chris Bamford, Devendra Singh Chaplot, Diego
                     Information Processing Systems, 36.                         delasCasas,FlorianBressand,GiannaLengyel,Guil-
                                                                                 laume Lample, Lucile Saulnier, et al. 2023a. Mistral
                  Weixin Feng, Xingyuan Bu, Chenchen Zhang, and                  7b. arXiv preprint arXiv:2310.06825.
                     Xubin Li. 2022.     Beyond bounding box: Multi-
                     modalknowledgelearningforobjectdetection. arXiv          Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun
                     preprint arXiv:2205.04072.                                  Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin
                                                                                 Jiang, Qun Liu, and Wei Wang. 2023b. Follow-
                  Sarah E Finch, James D Finch, and Jinho D Choi. 2022.          bench: A multi-level fine-grained constraints follow-
                     Don’t forget your abc’s: Evaluating the state-of-the-       ing benchmark for large language models. arXiv
                     art in chat-oriented dialogue systems. arXiv preprint       preprint arXiv:2310.20410.
                     arXiv:2212.09180.
                                                                              Tian Lan, Xian-Ling Mao, Wei Wei, and Heyan
                  Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.             Huang. 2020. Which kind is better in open-domain
                     2023. Improving language model negotiation with             multi-turn dialog, hierarchical or non-hierarchical
                     self-play and in-context learning from ai feedback.         models?     an empirical study.      arXiv preprint
                     arXiv preprint arXiv:2305.10142.                            arXiv:2008.02964.
                  Yuan Gao, Xingyuan Bu, Yang Hu, Hui Shen, Ti Bai,           HaonanLi,YixuanZhang,Fajri Koto, Yifei Yang, Hai
                     XubinLi, and Shilei Wen. 2018. Solution for large-          Zhao, Yeyun Gong, Nan Duan, and Timothy Bald-
                     scale hierarchical object detection datasets with in-       win. 2023a. Cmmlu: Measuring massive multitask
                     complete annotation and data imbalance.        arXiv        language understanding in chinese. arXiv preprint
                     preprint arXiv:1810.06208.                                  arXiv:2306.09212.
                  ArnavGudibande,EricWallace, Charlie Snell, Xinyang          Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap,
                     Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and            Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica.
                     Dawn Song. 2023. The false promise of imitating             2024a. From live data to high-quality benchmarks:
                     proprietary llms. arXiv preprint arXiv:2305.15717.          Thearena-hard pipeline.
                  HongchengGuo,JianYang,JiahengLiu,LiqunYang,                 XuechenLi,Tianyi Zhang, Yann Dubois, Rohan Taori,
                     Linzheng Chai, Jiaqi Bai, Junran Peng, Xiaorong Hu,         Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
                     Chao Chen, Dongfeng Zhang, et al. 2023. Owl: A              Tatsunori B. Hashimoto. 2023b. Alpacaeval: An
                     largelanguagemodelforitoperations. arXivpreprint            automatic evaluator of instruction-following models.
                     arXiv:2309.09298.                                           https://github.com/tatsu-lab/alpaca_eval.
                  Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma,             Yizhi Li, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun
                     Tianyu Zheng, Zhouliang Yu, Ding Pan, Yizhi Li,             Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao
                     RuiboLiu,YueWang,etal.2024a. Codeeditorbench:               Ma, Kai Zhang, et al. 2024b. Cif-bench: A chi-
                     Evaluating code editing capability of large language        nese instruction-following benchmark for evaluating
                     models. arXiv preprint arXiv:2404.03543.                    the generalizability of large language models. arXiv
                  Jinyang Guo, Jianyu Wu, Zining Wang, Jiaheng Liu,              preprint arXiv:2402.13109.
                     Ge Yang, Yifu Ding, Ruihao Gong, Haotong Qin,            Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen
                     and Xianglong Liu. 2024b. Compressing large lan-            Zhang, Yu Zhang, Ge Zhang, Jiakai Wang, Haoran
                     guagemodelsbyjointsparsificationandquantization.            Que, Yukang Chen, Wenbo Su, et al. 2024a. E2-
                     ICML.                                                       llm: Efficient and extreme length extension of large
                  Tianxing He, Jingyu Zhang, Tianle Wang, Sachin                 language models. arXiv preprint arXiv:2401.06951.
                     Kumar, Kyunghyun Cho, James Glass, and Yulia             Jie Liu, Zhanhui Zhou, Chao Yang, Han-Sen Zhong,
                     Tsvetkov. 2022. On the blind spots of model-based           and Wanli Ouyang. 2024b. Storm-7b: An empirical
                     evaluation metrics for text generation. arXiv preprint      study of iterative direct preference optimization.
                     arXiv:2212.10020.
                                                                              Weimin Lyu, Xiao Lin, Songzhu Zheng, Lu Pang,
                  DanHendrycks,CollinBurns,StevenBasart,AndyZou,                 HaibinLing,SusmitJha,andChaoChen.2024. Task-
                     Mantas Mazeika, Dawn Song, and Jacob Steinhardt.            agnostic detector for insertion-based backdoor at-
                     2020. Measuring massive multitask language under-           tacks. arXiv preprint arXiv:2403.17155.
                     standing. arXiv preprint arXiv:2009.03300.
                                                                              Carol A Marchel. 2007. Learning to talk/talking to
                  Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei                   learn: Teaching critical dialogue. Teaching Educa-
                     Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,              tional Psychology, 2(1):1–15.
                     Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024.
                     C-eval: A multi-level multi-discipline chinese evalua-   OpenAI. 2022. Introducing chatgpt.
                     tion suite for foundation models. Advances in Neural
                     Information Processing Systems, 36.                      OpenAI. 2023. Gpt-4 technical report. PREPRINT.
                                                                          7430
