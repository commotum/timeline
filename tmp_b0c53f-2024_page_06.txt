                            6      C. He et al.
                            3.1   Linear Attention
                            We begin by revisiting the self-attention introduced by [46]. Given an input
                            matrix x ∈ RN×d, where N denotes the number of tokens and d denotes the
                            dimensionality. Self-attention first linearly projects the input to queries, keys,
                            and values using weight matrices W , W , and W , such that Q = xW ,K =
                                                            Q    K        V                  Q
                            xW ,V = xW . Then it computes a Softmax-based attention map based on
                               K          V                             √
                            queries and keys i.e., A(Q,K) = Softmax(QKT)/ d.Theoutputofself-attention
                            is defined as the weighted sum of N values with the weights corresponding to
                            the attention map, i.e., O = A(Q,K)V. This approach involves calculating the
                            similarity for all query-key pairs, which yields a computational complexity of
                            O(N2).
                               Linear attention [1,17] offers a notable alternative to self-attention by sig-
                            nificantly reducing its computational complexity from O(N2) to O(N). This
                            efÏciency is achieved through the application of kernel functions on the query
                            (Q) and key (K) matrices, which effectively approximates the original attention
                            mapwithoutrelying on Softmax, i.e., A(Q,K) = ϕ(Q)ϕ(K)T. Taking advantage
                            of the associative property of matrix multiplication, the computation shifts from
                            (ϕ(Q)ϕ(K)T)V to ϕ(Q)(ϕ(K)TV). This modification leads to a computational
                            complexity proportional to the number of tokens, maintaining an order of O(N).
                            3.2   Scattered Linear Attention (SLA)
                            Instead of organizing the voxels within a window into fixed-length subsets, we
                            arrange the voxels of the entire scene into a single flattened matrix, as illustrated
                            in Figure 3(a). These voxels are ordered based on their window coordinates,
                            ensuring that voxels from the same window form a sub-matrix in contiguous
                            memory. Initially, we use a shared projection layer to map all the voxels in the
                            scene to query, key, and value representations, denoted Q, K, V . Subsequently,
                            we perform attention computations on these submatrices separately, resulting in
                            the scattered linear attention (SLA) formula as follows:
                                                                      j   j  j
                                           SLA(Q,K,V)=Concat[LA(Q ,K ,V )]j=1:M,                 (2)
                            where M is the number of non-empty window in the current scene and LA is the
                            linear attention formula used in [17]. Based on this formula, the output voxel
                            features Oj in the jth window can be defined as:
                                                                P j
                                                           ϕ(Q)   m ϕ(K)⊤V
                                                 LA:Oj =          i=1    i   i                   (3)
                                                                 P j
                                                            ϕ(Q)   m ϕ(K)⊤
                                                                   i=1    i
                                    j
                            where m is the number of voxel in the window and ϕ(x) is the kernel function.
                            Hardware EfÏcient Implementation. It should be noted that the implemen-
                            tation of Equation 2 is not straightforward, as the sub-matrices have different
                            numbers of rows. One naive implementation is to cache the “kv” matrix of each
