                 removed entirely. Originally we removed entire documents given a single collision, but that overly penalized long
                 documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in
                 whichtheWikipedia article quotes a single line from a book. We ignored 13−grams that matched more than 10 training
                 documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar
                 content that we likely do want the model to learn, rather than undesired speciﬁc overlaps with test sets. Examples for
                                                                            11
                 various frequencies can be found in the GPT-3 release repository .
                 Overlap methodology     For our benchmark overlap analysis in Section 4, we used a variable number of words N to
                 check for overlap for each dataset, where N is the 5th percentile example length in words, ignoring all punctuation,
                 whitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic
                 tasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data
                 markedasdirty are shown in Table C.1. Unlike GPT-2’s use of bloom ﬁlters to compute probabilistic bounds for test
                 contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps
                 between test sets and our full training corpus, even though we only trained on 40% of our ﬁltered Common Crawl
                 documents per Section 2.2.
                 Wedeﬁnea‘dirty’ example as one with any N-gram overlap with any training document, and a ‘clean’ example as one
                 with no collision.
                 Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed
                 bythis analysis, ﬁltering described above failed on long documents such as books. Because of cost considerations it
                 wasinfeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling
                 benchmarks plus the Children’s Book Test showed almost complete overlap, and therefore were not included in this
                 paper. Overlaps are shown in Table C.1
                 Overlap results  To understand how much having seen some of the data helps the model perform on downstream
                 tasks, we ﬁlter every validation and test set by dirtiness. Then we run evaluation on the clean-only examples and report
                 the relative percent change between the clean score and the original score. If the clean score is more than 1% or 2%
                 worse than the overall score, it suggests the model may have overﬁt to the examples it has seen. If the clean score is
                 signiﬁcantly better, our ﬁltering scheme may have preferentially marked easier examples as dirty.
                 This overlap metric tends to show a high rate of false positives for datasets that contain background information (but
                 not answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words
                 long, which we ignored in our ﬁltering process (except for wordscrambling tasks). One instance where this technique
                 seems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The
                 information required to answer the question is in a passage provided to the model, so having seen the passage during
                 training but not the questions and answers does not meaningfully constitute cheating. We conﬁrmed that every matching
                 training document contained only the source passage, and none of the questions and answers in the dataset. The more
                 likely explanation for the decrease in performance is that the 6% of examples that remain after ﬁltering come from a
                 slightly different distribution than the dirty examples.
                 Figure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but
                 there is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive
                 to contamination. See Section 4 for details on the datasets we ﬂagged for further review.
                   11https://github.com/openai/gpt-3/blob/master/overlap_frequency.md
                                                                      44
