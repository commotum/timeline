                              8       H. Wang et al.
                                  Lastly, following Panoptic-DeepLab [19], we adopt exactly the same stem [78]
                              of three convolutions, dual decoders, and prediction heads. The heads produce
                              semantic segmentation and class-agnostic instance segmentation, and they are
                              merged by majority voting [89] to form the ﬁnal panoptic segmentation.
                                  In cases where the inputs are extremely large (e.g., 2177×2177) and memory
                              is constrained, we resort to a large span m = 65 in all our axial-attention blocks.
                              Note that we do not consider the axial span as a hyper-parameter because it is
                              already suﬃcient to cover long range or even global context on several datasets,
                              and setting a smaller span does not signiﬁcantly reduce M-Adds.
                              4    Experimental Results
                              Weconductexperimentsonfourlarge-scale datasets. We ﬁrst report results with
                              our Axial-ResNet on ImageNet [70]. We then convert the ImageNet pretrained
                              Axial-ResNet to Axial-DeepLab, and report results on COCO [56], Mapillary
                              Vistas [62], and Cityscapes [22] for panoptic segmentation, evaluated by panop-
                              tic quality (PQ) [45]. We also report average precision (AP) for instance seg-
                              mentation, and mean IoU for semantic segmentation on Mapillary Vistas and
                              Cityscapes. Our models are trained using TensorFlow [1] on 128 TPU cores for
                              ImageNet and 32 cores for panoptic segmentation.
                                  Training protocol: On ImageNet, we adopt the same training protocol
                              as [65] for a fair comparison, except that we use batch size 512 for Full Axial-
                              ResNetsand1024forallothermodels,withlearningratesscaledaccordingly[29].
                                  For panoptic segmentation, we strictly follow Panoptic-DeepLab [19], except
                              using a linear warm up Radam [58] Lookahead [92] optimizer (with the same
                              learning rate 0.001). All our results on panoptic segmentation use this setting.
                              We note this change does not improve the results, but smooths our training
                              curves. Panoptic-DeepLab yields similar result in this setting.
                              4.1   ImageNet
                              For ImageNet, we build Axial-ResNet-L from ResNet-50 [31]. In detail, we set
                              d    = 128, d    = 2d = 16 for the ﬁrst stage after the ‘stem’. We double
                                in          out      q
                              them when spatial resolution is reduced by a factor of 2 [76]. Additionally, we
                              multiply all the channels   [35,71,34] by 0.5, 0.75, and 2, resulting in Axial-
                              ResNet-{S, M, XL}, respectively. Finally, Stand-Alone Axial-ResNets are further
                              generated by replacing the ‘stem’ with three axial-attention blocks where the
                              ﬁrst block has stride 2. Due to the computational cost introduced by the early
                              layers, we set the axial span m = 15 in all blocks of Stand-Alone Axial-ResNets.
                              We always use N = 8 heads [65]. In order to avoid careful initialization of
                              W ,W ,W ,rq,rk,rv, we use batch normalizations [40] in all attention layers.
                                 Q   K    V
                                  Tab. 1 summarizes our ImageNet results. The baselines ResNet-50 [31] (done
                              by[65]) and Conv-Stem + Attention [65] are also listed. In the conv-stem setting,
                              adding BN to attention layers of [65] slightly improves the performance by 0.3%.
