                                                                                            Axial-DeepLab       5
                                region is extracted to serve as a memory bank for computing the output yo.
                                                                                       2
                                This signiﬁcantly reduces its computation to O(hwm ), allowing self-attention
                                modules to be deployed as stand-alone layers to form a fully self-attentional
                                neural network. Additionally, a learned relative positional encoding term is in-
                                corporated into the aﬃnities, yielding a dynamic prior of where to look at in the
                                receptive ﬁeld (i.e., the local m × m square region). Formally, [65] proposes
                                                   y =      X softmax (qTk +qTr             )v                 (2)
                                                    o                      p  o  p    o p−o p
                                                        p∈Nm×m(o)
                                where Nm×m(o) is the local m × m square region centered around location
                                o = (i,j), and the learnable vector rp−o ∈ Rdq is the added relative positional
                                encoding. The inner product qTrp−o measures the compatibility from location
                                                                o
                                p = (a,b) to location o = (i,j). We do not consider absolute positional encoding
                                qTr , because they do not generalize well compared to the relative counter-
                                 o p
                                part [65]. In the following paragraphs, we drop the term relative for conciseness.
                                    In practice, d  and d     are much smaller than d , and one could extend
                                                  q       out                          in
                                single-head attention in Eq. (2) to multi-head attention to capture a mixture of
                                aﬃnities. In particular, multi-head attention is computed by applying N single-
                                headattentions in parallel on x (with diﬀerent Wn,Wn,Wn,∀n ∈ {1,2,...,N}
                                                               o                  Q    K    V
                                for the n-th head), and then obtaining the ﬁnal output z by concatenating the
                                                                                          o
                                results from each head, i.e., z = concat (yn). Note that positional encodings
                                                               o           n o
                                are often shared across heads, so that they introduce marginal extra parameters.
                                    Position-Sensitivity: We notice that previous positional bias only depends
                                on the query pixel x , not the key pixel x . However, the keys x could also have
                                                     o                    p                      p
                                informationaboutwhichlocationtoattendto.Wethereforeaddakey-dependent
                                positional bias term kTrk    , besides the query-dependent bias qTrq    .
                                                       p p−o                                      o p−o
                                    Similarly, the values v do not contain any positional information in Eq. (2).
                                                          p
                                In the case of large receptive ﬁelds or memory banks, it is unlikely that y
                                                                                                                 o
                                contains the precise location from which v comes. Thus, previous models have
                                                                            p
                                to trade-oﬀ between using smaller receptive ﬁelds (i.e., small m × m regions)
                                and throwing away precise spatial structures. In this work, we enable the output
                                y to retrieve relative positions rv  , besides the content v , based on query-key
                                 o                               p−o                        p
                                aﬃnities qTk . Formally,
                                           o p
                                         y =      X softmax (qTk +qTrq             +kTrk )(v +rv )             (3)
                                          o                      p  o  p    o p−o     p p−o    p    p−o
                                              p∈Nm×m(o)
                                                      k        d                                            v
                                where the learnable r      ∈Rq is the positional encoding for keys, and r       ∈
                                  d                   p−o                                                   p−o
                                R out is for values. Both vectors do not introduce many parameters, since they
                                are shared across attention heads in a layer, and the number of local pixels
                                |N      (o)| is usually small.
                                   m×m
                                    Wecallthisdesignposition-sensitive self-attention, which captures long range
                                interactions with precise positional information at a reasonable computation
                                overhead, as veriﬁed in our experiments.
