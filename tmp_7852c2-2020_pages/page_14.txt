                            14    H. Wang et al.
                                      Table 7. Varying axial-attention span on Cityscapes val set
                                 Backbone        Span    Params   M-Adds   PQ     AP   mIoU
                                 ResNet-101        -     43.8M    530.0B   59.9  31.9   74.6
                                 Axial-ResNet-L  5 × 5   44.9M    617.4B   59.1  31.3   74.5
                                 Axial-ResNet-L  9 × 9   44.9M    622.1B   61.2  31.1   77.6
                                 Axial-ResNet-L 17 × 17  44.9M    631.5B   62.8  34.0   79.5
                                 Axial-ResNet-L 33 × 33  44.9M    650.2B   63.8  35.9   80.2
                                 Axial-ResNet-L 65 × 65  44.9M    687.4B   64.2  36.3   80.6
                               Importance of Axial-Attention Span: In Tab. 7, we vary the span m
                            (i.e., spatial extent of local regions in an axial block), without ASPP. We observe
                            that a larger span consistently improves the performance at marginal costs.
                            5   Conclusion and Discussion
                            In this work, we have shown the eﬀectiveness of proposed position-sensitive axial-
                            attention on image classiﬁcation and segmentation tasks. On ImageNet, our
                            Axial-ResNet, formed by stacking axial-attention blocks, achieves state-of-the-
                            art results among stand-alone self-attention models. We further convert Axial-
                            ResNet to Axial-DeepLab for bottom-up segmentation tasks, and also show
                            state-of-the-art performance on several benchmarks, including COCO, Mapil-
                            lary Vistas, and Cityscapes. We hope our promising results could establish that
                            axial-attention is an eﬀective building block for modern computer vision models.
                               Ourmethodbearsasimilaritytodecoupledconvolution[41], which factorizes
                            a depthwise convolution [75,35,20] to a column convolution and a row convolu-
                            tion. This operation could also theoretically achieve a large receptive ﬁeld, but its
                            convolutional template matching nature limits the capacity of modeling multi-
                            scale interactions. Another related method is deformable convolution [23,96,27],
                            where each point attends to a few points dynamically on an image. However,
                            deformable convolution does not make use of key-dependent positional bias
                            or content-based relation. In addition, axial-attention propagates information
                            densely, and more eﬃciently along the height- and width-axis sequentially.
                               Although our axial-attention model saves M-Adds, it runs slower than con-
                            volutional counterparts, as also observed by [65]. This is due to the lack of
                            specialized kernels on various accelerators for the time being. This might well be
                            improved if the community considers axial-attention as a plausible direction.
                            Acknowledgments
                            We thank Niki Parmar for discussion and support; Ashish Vaswani, Xuhui Jia,
                            Raviteja Vemulapalli, Zhuoran Shen for their insightful comments and sugges-
                            tions; Maxwell Collins and Blake Hechtman for technical support. This work is
                            supported by Google Faculty Research Award and NSF 1763705.
