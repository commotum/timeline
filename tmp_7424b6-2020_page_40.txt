                                  +
             task-speciﬁc [SDCW19, JYS 19, KR16] approaches to distillation of language models. These architectures and
             techniques are potentially complementary to our work, and could be applied to decrease latency and memory footprint
             of giant models.
             As ﬁne-tuned language models have neared human performance on many standard benchmark tasks, considerable
             effort has been devoted to constructing more difﬁcult or open-ended tasks, including question answering [KPR+19,
                 +       +                                +
             IBGC 14,CCE 18,MCKS18],readingcomprehension[CHI 18,RCM19],andadversarially constructed datasets
             designed to be difﬁcult for existing language models [SBBC19, NWD+19]. In this work we test our models on many
             of these datasets.
             Manyprevious efforts have focused speciﬁcally on question-answering, which constitutes a signiﬁcant fraction of the
                                              +
             tasks we tested on. Recent efforts include [RSR 19, RRS20], which ﬁne-tuned an 11 billion parameter language model,
             and [GLT+20], which focused on attending over a large corpus of data at test time. Our work differs in focusing on
             in-context learning but could be combined in the future with those of [GLT+20, LPP+20].
             Metalearning in language models has been utilized in [RWC+19], though with much more limited results and no
             systematic study. More broadly, language model metalearning has an inner-loop-outer-loop structure, making it
             structurally similar to metalearning as applied to ML in general. Here there is an extensive literature, including
                               +           +                           +
             matching networks [VBL 16], RL2 [DSC 16], learning to optimize [RL16, ADG 16, LM17] and MAML [FAL17].
             Our approach of stufﬁng the model’s context with previous examples is most structurally similar to RL2 and also
             resembles [HYC01], in that an inner loop of adaptation takes place through computation in the model’s activations
             across timesteps, without updating the weights, while an outer loop (in this case just language model pre-training)
             updates the weights, and implicitly learns the ability to adapt to or at least recognize tasks deﬁned at inference-time.
                                                            +          +
             Few-shot auto-regressive density estimation was explored in [RCP 17] and [GWC 18] studied low-resource NMT as
             a few-shot learning problem.
             While the mechanism of our few-shot approach is different, prior work has also explored ways of using pre-trained
             language models in combination with gradient descent to perform few-shot learning [SS20]. Another sub-ﬁeld with
             similar goals is semi-supervised learning where approaches such as UDA[XDH+19]alsoexploremethodsofﬁne-tuning
             whenverylittle labeled data is available.
             Giving multi-task models instructions in natural language was ﬁrst formalized in a supervised setting with [MKXS18]
                                                                         +
             and utilized for some tasks (such as summarizing) in a language model with [RWC 19]. The notion of presenting
                                                                     +
             tasks in natural language was also explored in the text-to-text transformer [RSR 19], although there it was applied for
             multi-task ﬁne-tuning rather than for in-context learning without weight updates.
             Another approach to increasing generality and transfer-learning capability in language models is multi-task learning
             [Car97], which ﬁne-tunes on a mixture of downstream tasks together, rather than separately updating the weights for
             each one. If successful multi-task learning could allow a single model to be used for many tasks without updating the
             weights (similar to our in-context learning approach), or alternatively could improve sample efﬁciency when updating
                                                                                  +      +
             the weights for a new task. Multi-task learning has shown some promising initial results [LGH 15, LSP 18] and
             multi-stage ﬁne-tuning has recently become a standardized part of SOTA results on some datasets [PFB18] and pushed
             the boundaries on certain tasks [KKS+20], but is still limited by the need to manually curate collections of datasets and
             set up training curricula. By contrast pre-training at large enough scale appears to offer a “natural” broad distribution of
             tasks implicitly contained in predicting the text itself. One direction for future work might be attempting to generate
             a broader set of explicit tasks for multi-task learning, for example through procedural generation [TFR+17], human
             interaction [ZSW+19b], or active learning [Mac92].
             Algorithmic innovation in language models over the last two years has been enormous, including denoising-based
                                                                          +      +
             bidirectionality [DCLT18], preﬁxLM [DL15] and encoder-decoder architectures [LLG 19, RSR 19], random permu-
                                  +                                             +
             tations during training [YDY 19], architectures that improve the efﬁciency of sampling [DYY 19], improvements in
                                      +                                              +
             data and training procedures [LOG 19], and efﬁciency increases in the embedding parameters [LCG 19]. Many of
             these techniques provide signiﬁcant gains on downstream tasks. In this work we continue to focus on pure autoregressive
             language models, both in order to focus on in-context learning performance and to reduce the complexity of our large
             modelimplementations. However, it is very likely that incorporating these algorithmic advances could improve GPT-3’s
             performance on downstream tasks, especially in the ﬁne-tuning setting, and combining GPT-3’s scale with these
             algorithmic techniques is a promising direction for future work.
             8  Conclusion
             We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and
             benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of
                                                      40
