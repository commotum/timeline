           limited value for training the recognition model. After learning, the system’s dreams are richly
           structured (Fig. 4E), compositionally recombining latent building blocks and motifs acquired from
           the training data in creative ways never seen in its waking experience, but ideal for training a broadly
           generalizable recognition model (29).
              Inspired by the classic AI ‘copy demo’ – where an agent looks at a tower made of toy blocks then
           re-creates it (30) – we next gave DreamCoder 107 tower ‘copy tasks’ (split 50/50 test/train, Fig. 5A),
           where the system observes both an image of a tower and the locations of each of its blocks, and must
           write a program that plans how a simulated hand would build the tower. The system starts with the
           same control ﬂow primitives as with LOGO graphics. Inside its learned library we ﬁnd parametric
           ‘options’ (31) for building blocks towers (Fig. 5B), including concepts like arches, staircases, and
           bridges, which one also sees in the model’s dreams (Fig. 5C-D).
              Next we consider few-shot learning of probabilistic generative concepts, an ability that comes
           naturally to humans, from learning new rules in natural language (32), to learning routines for
           symbols and signs (5), to learning new motor routines for producing words (33). We ﬁrst task
           DreamCoder with inferring a probabilistic regular expression (or Regex, see Fig. 1A for examples)
           from a small number of strings, where these strings are drawn from 256 CSV columns crawled from
           the web (data from (34), tasks split 50/50 test/train, 5 example strings per concept). The system learns
           to learn regular expressions that describe the structure of typically occurring text concepts, such as
           phone numbers, dates, times, or monetary amounts (Fig. S5). It can explain many real-world text
           patterns and use its explanations as a probabilistic generative model to imagine new examples of
           these concepts. For instance, though DreamCoder knows nothing about dollar amounts it can infer an
           abstract pattern behind the examples $5.70, $2.80, $7.60, ..., to generate $2.40 and $3.30 as other
           examples of the same concept. Given patterns with exceptions, such as -4.26, -1.69, -1.622, ..., -1
           it infers a probabilistic model that typically generates strings such as -9.9 and occasionally generates
           strings such as -2. It can also learn more esoteric concepts, which humans may ﬁnd unfamiliar but can
           still readily learn and generalize from a few examples: Given examples -00:16:05.9, -00:19:52.9,
           -00:33:24.7, ..., it infers a generative concept that produces -00:93:53.2, as well as plausible near
           misses such as -00:23=43.3.
              Welastconsiderinferringreal-valuedparametricequationsgeneratingsmoothtrajectories(seeS2.1.6
           and Fig. 1A, ‘Symbolic Regression’). Each task is to ﬁt data generated by a speciﬁc curve – either a
           rational function or a polynomial of up to degree 4. We initialize DreamCoder with addition, multipli-
           cation, division, and, critically, arbitrary real-valued parameters, which we optimize over via inner-loop
           gradient descent. We model each parametric program as probabilistically generating a family of curves,
           and penalize use of these continuous parameters via the Bayesian Information Criterion (BIC) (35).
           OurBayesian machinery learns to home in on programs generating curves that explain the data while
           parsimoniously avoiding extraneous continuous parameters. For example, given real-valued data from
           1.7x2 −2.1x+1.8itinfers a program with three continuous parameters, but given data from 2.3 it
           infers a program with two continuous parameters.                            x−2.8
           Quantitative analyses of DreamCoder across domains
           Tobetter understand how DreamCoder learns, we compared our full system on held out test problems
           with ablations missing either the neural recognition model (the “dreaming” sleep phase) or ability
           to form new library routines (the “abstraction” sleep phase). We contrast with several baselines:
           Exploration-Compression (13), which alternately searches for programs, and then compresses out
           reused components into a learned library, but without our refactoring algorithm; Neural Program
                                                   10
