                                                         Test-Time Learning for Large Language Models
                  0.4                                           1.0                                         0.34
                                                                          Geography                                   Low perplexity samples
                                                  0.319         0.8       Agriculture                       0.32      High perplexity samples
                  0.3                                                     Medicine
                        0.245                                   0.6       Finance
                  0.2                                                                                       0.30
                                                               Output PPL0.4
                  ROUGE-LSUM                                                                                ROUGE-LSUM0.28
                  0.1                0.078                      0.2
                                                                0.0                                         0.26
                  0.0                                              0.0    0.2    0.4   0.6    0.8    1.0          10%   20%   40%   60%   80%  100%
                      Origin LLM  Entropy Min.  Perplexity Min.                  Input PPL                        The Proportion of Samples Involved Training
                (a) Effectiveness of Entropy and Perplexity (b) Trends in perplexity to input and perplex- (c) Effect of different test samples in test-
                Minimization Strategies.                     ity to output under Llama3.1-8B-Instruct.    time perplexity minimization.
                Figure 1. Summary of our exploration and observations: (a) demonstrates that perplexity minimization improves the performance of
                LLMs,whileentropy minimization (Wang et al., 2021) may harm their performance; (b) reveals that the trend of LLM’s perplexity to the
                input P(x) and perplexity to the output P(y|x) is the same (results are normalized), i.e., we can minP(y|x;Θ) by minP(x;Θ); and (c)
                                                                                                               Θ                  Θ
                emphasizes that training on high-perplexity samples makes more contribution than low-perplexity ones.
                4. Test-Time Learning for LLMs                                       Algorithm 1 The pipeline of proposed TLM.
                                                                                                                              M
                                                                                     Input: Test samples D           ={x }        , the trained LLM
                In this paper, we propose a Test-Time Learning (TTL)                                            Test       j j=1
                                                                                          f (·), LoRA ∆Θ with trainable parameters B and A,
                methodforLargeLanguageModels(LLMs)calledTLM,                               Θ
                which dynamically adapts LLMs using only unlabeled test                   batch size B.
                data. ThepipelineofTLM isshowninAlgorithm1,ourpro-                     1: Initialize LoRA parameters ∆Θ.
                                                                                                                                      ˜
                posed TLM is composed of three key components. 1) Input                2: Add LoRAparameters to trained LLM Θ = Θ+∆Θ.
                                                                                                                  B
                                                                                       3: for a batch X = {x }         in D      do
                Perplexity Minimization Objective: Inspired by the strong                                       b b=1       Test
                correlation between input perplexity and output perplexity,            4:    Calculate predictions y˜ for all x ∈ X via fΘ(·).
                weadoptinput perplexity minimization as the optimization               5:    Calculate sample selection score S(x) via Eqn. (6).
                                                                                                             ˜
                objective. This enables LLMs to better fit the target data             6:    Update LLM(Θ)withEqn.(5).
                distribution during test time, as detailed in Sec. 4.1. 2)             7: end for
                                                                                                                   ˜
                Sample-Efficient Learning Strategy: Not all test samples             Output: The final LLM (Θ).
                equally impact model updates. Employing a perplexity-
                based weighting scheme, the model actively selects and
                emphasizes high-perplexity test samples for backpropaga-             ized by Θ. A lower perplexity indicates that the model’s
                tion, thereby enabling efficient parameter updates during            predictions are more confident and closely align with the
                Test-Time Learning (c.f. Sec. 4.2). 3) Lightweight Parame-           true distribution of the data, which implies better model
                ter Updates via LoRA: To mitigate catastrophic forgetting            fitting (Jumelet & Zuidema, 2023). Therefore, for a given
                and reduce computational costs, we integrate LoRA into               question-answer pair {x,y}, minimizing the perplexity of
                TTL.Byupdatingonlyasmallsubsetofmodelparameters,                     the model’s response y can enhance the model’s ability to
                LoRAenableslightweight training and effectively mitigates            fit the target data distribution, leading to improved perfor-
                catastrophic forgetting, making our proposed method suit-            manceonout-of-distribution (OOD) data. Specifically, by
                able for real-world deployment (c.f. Sec. 4.3).                      minimizing the perplexity P(y|x;Θ) of the answer y given
                                                                                     the input x, which can be formulated as:
                4.1. Perplexity Minimization for Test-Time Learning                                               (−1 PT logp(y |x,y        ;Θ))
                                                                                       minP(y|x;Θ) = mine T              t=1       t    1:t−1    . (3)
                Perplexity (Bengio et al., 2000) is a widely used measure                Θ                   Θ
                in language modeling that quantifies how well a model                This minimization process improves the model’s perfor-
                predicts a sequence of tokens (Devlin et al., 2019; Brown            manceinthetarget data distribution. However, during the
                et al., 2020). Given a sequence of tokens {x ,x ,...,x },
                                                                 1   2       T       testing phase, we can only access the user’s input x and not
                the perplexity P is defined as the exponentiation of the             the ground truth output y. To address this limitation, we
                average negative log-likelihood of the predicted tokens:             hypothesize that minimizing the perplexity of the input x,
                                                  P                                  denoted as minP(x;Θ), may reduce the perplexity of the
                                             (−1    T   logp(x |x     ;Θ))
                  P({x ,x ,...,x }) = e         T   t=1       t  1:t−1    ,  (2)                   Θ
                        1   2       T                                                model’s response y. The mathematical justification for this
                where logp(x |x         ; Θ) is the conditional probability of       transformation is based on the assumption that the model
                               t   1:t−1
                predicting token ti given the previous tokens, parameter-            parameters Θ influence both P(y|x;Θ) and P(x;Θ) in a
                                                                                  4
