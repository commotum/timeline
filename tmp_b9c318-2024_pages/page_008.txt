                  iScience                                                                                                                              ll
                  Article                                                                                                                         OPENACCESS
                  Figure 5. Visualization of VSAC detection results on the KITTI test set
                  Colored bounding boxes of cars and their conﬁdence scores are plotted on the point cloud images.
                     Moreover,eachoperationonthenon-emptyvoxelisassignedtoaseparateCUDAthread,allowingallstepstobeexecutedsynchronously
                  ontheGPU.Thisparallel processing greatly reduces the complexity of searching for voxels.
                  Voxel self-attention
                  Voxelself-attention modulecanbeappliedinmostvoxeldetectors.Itconstructslong-rangerelationshipsbetweenvoxelsbyutilizingmulti-
                  head attention. As shown in Figure 1B, voxel self-attention consists of attention-1 and attention-2 modules. The attention-1 preserves the
                  original structural information of the 3D space by solely calculating features of non-empty voxels. Meanwhile, the attention-2 enhances ﬂex-
                  ibility by additionally extracting features from non-empty voxel and a select few empty voxels. This structure allows the model to not only
                  retain detailedspatial information, but also to take into account the wider contextualinformationprovided by thenullvoxels, whichis crucial
                  for overall sceneunderstanding.Comparedtotheclassicaltransformermodule,ourproposedmodulehasthefollowingmaindifferences:(1)
                  weintroduceanewrangeforattentioncalculation,whichallowsattention toobjectrelevant non-empty voxels in a controlled fashion. (2) In
                  order to stabilize the learning process, we replace layer normalization with batch normalization. (3) Considering the small number of non-
                  emptyvoxelsinlearning,weremovealldropoutlayers.(4)Afterthefeedforwardnetwork,aprojectionlayerisaddedtopotentiallyenhance
                  the model’s representational capability.
                     Speciﬁcally, for each query voxel, the U(i) is determined by ripple-spread center-emanating attention. Next, multi-head attention is
                  applied to the voxels participating in the attention process to obtain their features. Let f,f˛ F, respectively, represent the features of the
                                                                                                                i  k
                  queryvoxelandthevoxilesparticipatingintheattention.Firstly,thevoxelindicesi,k˛Varemappedtotheactualvoxelcentero,o ,asfollow:
                                                                                                                                                       i k
                                                                       oi = r 3ði + 0:5Þ; ok = r 3ðk + 0:5Þ                                            (Equation 7)
                  Whererrepresents the voxel size.
                     Thenfor a single attention head, the query embedding Q, key embedding K , and value embedding V are computed:
                                                                                   i                   k                           k
                                                                                  8Q =fW
                                                                                  < i      i  q
                                                                                    K = f W +E                                                         (Equation 8)
                                                                                  : k      k   k   p
                                                                                    V = f W +E
                                                                                      k    k  v    p
                  WhereW ,W ,W arederivedfromthelinearprojections of the query, key, and value, respectively. The computation of the positional en-
                            q    k   v
                  coding E is as follows:
                           p
                                                                                        ð        Þ
                                                                                 Ep = oi  ok Wp                                                       (Equation 9)
                  WhereW isalinearprojection of the position vector.
                            p
                     Sinceemptyvoxelslackeffectivefeatures,theycannotprovidequeryembeddingQ directlyfromf.Therefore,inordertoinvolveempty
                                                                                                              i              i
                  voxels in the attention mechanism andproviderepresentativevaluesfor Q at thepositions of emptyvoxels,weneedtoassignvaluesbased
                                                                                                i
                  onthevoxel features fk that participate in the attention mechanism:
                                                                                   Q = L ðf Þ                                                         (Equation 10)
                                                                                      i   k˛UðiÞ k
                  Where, the function L represents the max pool.
                                                                                                                       iScience 27, 110759, September 20, 2024      7
