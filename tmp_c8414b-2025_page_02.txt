                                   Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective
               Table 1. Performance comparison of related work. We distinguish        Visual Representation of a Task Instance:
               between solutions where the underlying model weights are open-                        Input                              Output
               source or proprietary.
                 Model                                 Public Eval     Open
                 Name                                Accuracy[%] Source            CompactStringFormatofsameInstance:
                 o1-preview (Kamradt, 2024)                21            ✗             <bos> A ... Z a ... z I 2 1 1 0 1 2 1 LF
                 RyanGreenblatt (Greenblatt, 2024)         42            ✗                                             1 2 1 0 2 2 2 LF
                 Jeremy Berman (Berman, 2024)              58.5          ✗                                             2 1 1 0 1 1 1 LF
                 GPTo3(arcprize.org, 2025)                 82.8          ✗                                                      O 1 1 1 LF
                 Avg. Human(LeGriset al., 2024)            60.2          ?                                                         1 3 1 LF
                           ¨                                                                                                       1 1 1 LF
                 TTT(Akyureketal., 2024)                   53.5          ✓
                 BARC(Lietal.,2025)                       56.75          ✓                                                              <eos>
                                   ¨
                 TTT+BARC(Akyureketal.,2024)               62.8          ✓
                 Ours                                      71.6          ✓         Figure 2. Our standard tokenization approach. Note that we use
                                                                                   one token per cell instead of compressing the problem more. We
                                                                                   also try to not include any unnecessary delimiters. The Pre-prompt
                                                                                   (the alphabet in upper then lower case, i.e. “A...Za...z”) is only in-
               the real challenge is creating the conditions under which           cludedforthefirst example. Depending on the model and run there
               these capacities can be reliably expressed.                         might be some small changes to the pre-prompt and input/output
               Buildingontheseinsights,wedevelopedanapproachspecif-                prefix tokens.
               ically tailored to the ARC dataset. Our method achieves             TheOriginal ARC Dataset: The Abstraction and Rea-
               SOTAperformance for open source models of 71.6% (or                 soning Corpus (ARC-AGI) introduced by Chollet (2019)
               points) on the public ARC-AGI evaluation set and surpasses          challenges the idea that language models can efficiently
               average human performance of 60.2%, as measured by                  generalize from a small number of examples, often referred
               LeGris et al. (2024).                                               to as few-shot prompting. The original ARC-AGI dataset
               In particular, we employ a depth-first search (DFS) al-             consists of 900 reasoning tasks, divided into 400 training
               gorithm on LLM predictions to generate diverse, high-               tasks, 400 public evaluation tasks, and 100 private and thus
               probability solutions, and re-use the same LLM also as              unpublished evaluation tasks. Each task involves input and
               a product of experts (see Section 4.1) to select the best can-      output grids of varying sizes, ranging from 1x1 to 30x30
               didate. This dual role allows us to rank candidate solutions        and utilize a palette of ten distinct colors.
               via augmented likelihood estimates, effectively amplifying          Theobjective of each individual ARC task is to discern the
               the model’s latent reasoning abilities. Compared to more            transformation rule from input to output from the examples
               heavily scaled or closed-source systems, our method stands          and apply it to new input grids to generate the correct out-
               out for its transparency, reproducibility, and low inference        puts. A task is considered successfully solved when the
               cost of around 0.02$ per task, in stark comparison to 17$           modelproduces the accurate output within a maximum of
               per task for o3 (arcprize.org, 2025). This demonstrates that        two attempts. Designed to be straightforward for humans
               abstract reasoning on ARC-AGI is not exclusively the do-            yet challenging for machine learning systems, the tasks high-
               mainofmassiveproprietary models.                                    light the current limitations of AI in abstract reasoning. In a
               In the sections that follow, we detail our data modeling            study by LeGris et al. (2024), the average human was able to
               and training strategies, describe our DFS-based solution            correctly solve 60.2% of the evaluation tasks, while 97.8%
               exploration, and provide comprehensive results and ablation         of the tasks were solved by at least one participant using
               studies.                                                            two guesses.
               Ourfinal model, along with the training and inference code,
               is publicly available on GitHub.                                    Competition-driven Progresses:         Since ARC’s introduc-
                                                                                   tion in 2019, several competitions with hundreds of partici-
               2. Related Work                                                     pants have sought to develop solutions with strong perfor-
                                                                                   mance on the dataset. Approaches up to 2024 frequently
               TheAbstraction and Reasoning Corpus (ARC) has played                employed program search over domain-specific languages
               a central role in advancing research on abstract reasoning          (DSLs), and have yielded a score of 39% using Top-3 scor-
               in artificial intelligence, inspiring a wide range of studies       ing (Wind, 2020).
               focused on its dataset, competitive benchmarks, and the             In 2024, ARC-AGI hosted another Kaggle competition,
               development of solutions driven by resource constraints.            where for the first time large language model (LLM) ap-
                                                                                                                                                 1
                                                                                2
