                        (a)                 (b)
             Figure 8: Results on programs of different token lengths on the C dataset. (a) The program token length
             distributions after each retraining iteration. (b) The accuracies on programs of different token lengths.
             6 Related Work
             Programming by example. Programming by example problems have been widely studied with
             various applications, and recent works have developed deep neural networks as program synthesiz-
             ers [20, 40, 17, 9]. Most prior works focus on synthesizing programs in domain-speciﬁc languages,
             such as FlashFill [40, 17, 51] for string transformation, Karel [9, 44, 12, 21] for simulated robot
             navigation, and LISP-style languages for list manipulation [6, 25, 57, 36]. In this work, we make the
             ﬁrst attempt of synthesizing C code in a restricted domain from input-output examples only, and we
             focus on the list manipulation domain.
             Somerecent works investigate the limitations of synthetic datasets and the ambiguity in program
             speciﬁcations for neural program synthesis [45, 13, 46, 28]. These works focus on reducing the bias
             of data distributions and generating more diverse input-output pairs, while our data regeneration
             aims to improve the quality of programs. We consider incorporating both lines of work to further
             improve the dataset quality as future work. In addition, drawing the inspiration from self-training
             and bootstrapping techniques developed for other applications [35, 1, 32, 52] to extend our iterative
             retraining scheme is also another future direction.
             Execution-guided program synthesis. To learn better program representations, some recent works
             incorporate the execution information to guide the synthesis process [47, 57, 44, 12, 18, 48, 7, 21, 38,
             37,31]. Inparticular, leveragingpartialprogramexecutionstatesimprovestheperformanceforseveral
             programsynthesistasks[12,57,18,37]. However,existingapproachesrelyonprograminterpretersto
             provide the intermediate execution results whenever applicable. In contrast, we demonstrate that our
             latent executor learns the latent execution traces (LaET) without such a requirement. Besides program
             synthesis, execution traces have also been utilized for other software engineering applications [2, 33].
             Neuralexecution. Ourlatent executor is related to prior works on learning to execute algorithms [55,
             50, 53] and programs [8]. They focus on predicting execution results for full algorithms and programs,
             but do not utilize them for program synthesis. Latent state prediction has also been studied in other
             applications such as task-oriented dialogue systems [34, 56] and robotics [42].
             7 Conclusion
             Inthiswork,weproposeLaSynth,whichlearnsthelatentrepresentationtoapproximatetheexecution
             of partial programs, even if their semantics are not well-deﬁned. We demonstrate the possibility of
             synthesizing elementary C code from input-output examples only, and leveraging learned execution
             signiﬁcantly improves the prediction performance by around 20%. Meanwhile, compared to the
             randomly generated programs, LaSynth synthesizes more concise programs that resemble human-
             written code, and training on these synthesized programs further improves the prediction performance
             for both Karel and C program synthesis. Our results indicate the promise of leveraging the learned
             program synthesizer to improve the dataset quality for programming by example tasks.
             Weconsider extending our approach to synthesize more complicated real-world code as future work.
             Forexample,wewillintegrateourlatentexecutorintolarge-scale pre-trained language models, which
             could further improve the performance of those program synthesis models taking natural language
             speciﬁcations. We will also study program synthesis problems with unbounded input ranges and
             different type signatures, which could be approached with the usage of subword tokenizers.
                                  10
