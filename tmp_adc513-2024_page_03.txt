              ric for evaluation.                                                   of human annotators during evaluation not only introduces
                 Based on SEED-Bench, we comprehensively evaluate                   bias but also incurs significant costs. LLaVA-Bench [24],
              22 prominent open-source MLLMs. Our evaluation results                LAMM[42]andTouchstone[3]utilizeGPTtoevaluatethe
              yield the following three key findings: (1) Existing MLLMs            answers’ relevance and accuracy to the ground truth. The
              havenotyetreachedtheceilinglevelofcapabilityL1 forthe                 reliance on entity extraction and GPT metrics can impact
              comprehension of fixed-form images and texts, with even               the accuracy and reliability of the evaluation. MME [11]
              the top-ranked model achieving only a 60% accuracy rate.              and MMBench [25] aim to enhance the objective evalu-
              MLLMs, in particular, exhibit poor performance in certain             ation of MLLMs by constructing 2194 True/False Ques-
              dimensions, such as understanding charts and visual math-             tions and 2974 Multiple Choice Questions across a variety
              ematics. (2) MLLMs achieve less satisfactory performance              of ability dimensions respectively. Considering the limited
              at capability L2 than that at L1, which indicates that it is          scale of these benchmarks, their evaluation results may ex-
              morechallenging for MLLMs to comprehend free-form in-                 hibit instability. In this work, we introduce SEED-Bench
              terleaved image-text inputs since most MLLMs are trained              to evaluate the hierarchical capabilities of MLLMs includ-
              on structured image-caption pairs. (3) At present, only a             ing the generation of both texts and images, which contains
              few MLLMscanattain capability L3, which requires mod-                 24K human-annotated multiple-choice questions covering
              els to output content in multiple modalities. A universal             27evaluation dimensions.
              MLLM that unifies the generation of images and texts is               3. SEED-Bench
              currently underexplored. We will launch an evaluation plat-
              form and consistently maintain a leaderboard for assessing            3.1. Hierarchical Capability Levels
              and comparing model performance.
                                                                                    Wecategorize the capabilities of MLLMs into hierarchical
              2. Related Work                                                       levels from L to L4, based on input and output modali-
                                                                                                    0
              Multimodal Large Language Models. With the impres-                    ties, where the higher level encompasses the lower capabil-
              sive success of Large language models (LLM) [7, 10, 37],              ity tiers, as illustrated in Fig. 1. SEED-Bench covers the
                                                                                    assessment of MLLMs up to L . The detailed categoriza-
              recent studies work on generative Multimodal Large Lan-                                                  3
              guage Models (MLLMs) [2, 8, 15–18, 23, 24, 32, 34,                    tion of capability level is illustrated below,
              41, 45, 47] to improve multimodal comprehension through               Level L : Building upon LLMs, the most fundamental ca-
                                                                                             0
              aligning visual features of pre-trained image encoder with            pability of MLLMs generating text based on provided text
              LLMs on image-text datasets.        Some work [19, 27, 28]            inputs, whichdoesnotnecessitatespecificevaluationwithin
              further considers video inputs and leverages the vast ca-             the MLLMbenchmark.
              pabilities of LLMs for video understanding tasks. Recent              Level L : MLLMs at this capability level should possess
              work[9,13,14,21,36,39]takesignificantstridesinequip-                           1
              ping MLLMs with the capacity for generating images be-                the ability to comprehend multimodal inputs in a fixed for-
              yond texts. In SEED-Bench, we provide a comprehensive                 mat, i.e., image or multiple images (video input can be re-
              and objective evaluation of these models to thoroughly as-            garded as multiple images) and then texts. Current MLLM
              sess their hierarchical capabilities.                                 benchmarksonlyevaluate this capability level with a single
              Benchmarks for Multimodal Large Language Models.                      image and text as inputs.
              WiththerapiddevelopmentofMultimodalLargeLanguage                      Level L2: MLLMsatthis capability level should be able to
              Models (MLLMs), some concurrent works [3, 11, 25, 40,                 understand multimodal inputs with open-form interleaved
              42] propose various benchmarks for evaluating MLLMs.                  image-text data, which aligns with the multimodal inputs
              However, they remain limited to assessing only the model’s            encountered in real-life scenarios.
              ability to predict texts given single image-text inputs, failing      Level L : Besides the inherent ability of LLMs to generate
              to keep up with the strides made in multimodal model capa-                     3
              bilities. For example, GVT [38] constructs a benchmark by             texts, MLLMs at this capability level should also be pro-
              aggregating two semantic-level understanding tasks (VQA               ficient in producing images, as advanced MLLMs are ex-
              and Image Captioning) and two fine-grained tasks (Ob-                 pected to process and represent multimodal content on both
              ject Counting and Multi-class Identification). However, its           input and output sides.
              evaluation is constrained to limited aspects of visual un-            Level L4: MLLMs at the highest capability level should
              derstanding. LVLM-eHub [40] combines multiple existing                possess the ability to process and generate interleaved
              computer vision benchmarks and develops an online plat-               image-text content in an open-form format, which is an es-
              form, where two models are prompted to answer a question              sential step towards achieving general artificial intelligence.
              related to an image and human annotators are employed                 We will incorporate evaluations of this capability level in
              to compare the predictions of models. The involvement                 our future work.
                                                                                13301
