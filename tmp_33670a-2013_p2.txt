                                                                      φ          z          θ
                                                                                 x
                                                                                   N
                             Figure1: Thetypeofdirectedgraphicalmodelunderconsideration. Solidlinesdenotethegenerative
                             model p (z)p (x|z), dashed lines denote the variational approximation q (z|x) to the intractable
                                      θ     θ                                                               φ
                             posterior p (z|x). The variational parameters φ are learned jointly with the generative model pa-
                                         θ
                             rameters θ.
                             straightforward to extend this scenario to the case where we also perform variational inference on
                             the global parameters; that algorithm is put in the appendix, but experiments with that case are left to
                             future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming
                             data, but here we assume a ﬁxed dataset for simplicity.
                             2.1   Problemscenario
                                                                       (i) N
                             Let us consider some dataset X = {x          }     consisting of N i.i.d. samples of some continuous
                                                                           i=1
                             or discrete variable x. We assume that the data are generated by some random process, involving
                             an unobserved continuous random variable z. The process consists of two steps: (1) a value z(i)
                             is generated from some prior distribution p ∗(z); (2) a value x(i) is generated from some condi-
                                                                            θ
                             tional distribution p ∗(x|z). We assume that the prior p ∗(z) and likelihood p ∗(x|z) come from
                                                  θ                                       θ                       θ
                             parametric families of distributions p (z) and p (x|z), and that their PDFs are differentiable almost
                                                                    θ           θ
                             everywherew.r.t. both θ and z. Unfortunately, a lot of this process is hidden from our view: the true
                             parameters θ∗ as well as the values of the latent variables z(i) are unknown to us.
                             Very importantly, we do not make the common simplifying assumptions about the marginal or pos-
                             terior probabilities. Conversely, we are here interested in a general algorithm that even works efﬁ-
                             ciently in the case of:
                                   1. Intractability:    the case where the integral of the marginal likelihood p (x)                =
                                       R                                                                                     θ
                                         p (z)p (x|z)dz is intractable (so we cannot evaluate or differentiate the marginal like-
                                          θ     θ
                                       lihood), where the true posterior density p (z|x) = p (x|z)p (z)/p (x) is intractable
                                                                                      θ            θ       θ       θ
                                       (so the EM algorithm cannot be used), and where the required integrals for any reason-
                                       able mean-ﬁeld VB algorithm are also intractable. These intractabilities are quite common
                                       and appear in cases of moderately complicated likelihood functions p (x|z), e.g. a neural
                                                                                                                 θ
                                       network with a nonlinear hidden layer.
                                   2. A large dataset: we have so much data that batch optimization is too costly; we would like
                                       to make parameter updates using small minibatches or even single datapoints. Sampling-
                                       based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a
                                       typically expensive sampling loop per datapoint.
                             Weareinterested in, and propose a solution to, three related problems in the above scenario:
                                   1. Efﬁcient approximate ML or MAP estimation for the parameters θ. The parameters can be
                                       of interest themselves, e.g. if we are analyzing some natural process. They also allow us to
                                       mimicthehidden random process and generate artiﬁcial data that resembles the real data.
                                   2. Efﬁcient approximate posterior inference of the latent variable z given an observed value x
                                       for a choice of parameters θ. This is useful for coding or data representation tasks.
                                   3. Efﬁcient approximate marginal inference of the variable x. This allows us to perform all
                                       kindsofinferencetaskswhereaprioroverxisrequired. Commonapplicationsincomputer
                                       vision include image denoising, inpainting and super-resolution.
                                                                                 2
