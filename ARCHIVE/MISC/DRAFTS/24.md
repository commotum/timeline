## 2011 — Spreadsheet PBE becomes a real-world “infer the function from examples” benchmark genre

* **Automating String Processing in Spreadsheets Using Input-Output Examples (FlashFill)** (Gulwani, 2011)
  **Target Domain:** rule/function inference from a few **string I/O pairs** (spreadsheet transforms)
  **Resource:** the canonical **FlashFill-style PBE benchmark set** (many real user transforms) + a standard PBE formulation that later datasets explicitly reuse. ([Microsoft][1])

---

## 2012 — “Semantic” PBE: transformations that require table lookups and context

* **Learning Semantic String Transformations from Examples** (Singh, Gulwani, 2012)
  **Target Domain:** infer string transformations that depend on **relational/table context**
  **Resource:** a set of **real-world semantic transformation benchmarks** (a core pillar of spreadsheet PBE evaluation).

---

## 2013–2014 — Standardized synthesis benchmarks at scale (examples + constraints)

* **Syntax-Guided Synthesis (SyGuS) + SyGuS-Comp 2014** (Alur et al., 2013/2015; competition 2014)
  **Target Domain:** program/function synthesis under a grammar (often with examples/constraints)
  **Resource:** the **SyGuS-Comp benchmark suites** (public sets + standardized format), which later expand into explicit **PBE tracks**. ([SyGuS][2])

---

## 2015 — General-purpose I/O-to-program benchmark suite (sorting, list ops, etc.)

* **General Program Synthesis Benchmark Suite (PSB / PSB1)** (Helmuth, Spector, 2015)
  **Target Domain:** “traditional programming” tasks specified by **I/O examples** (sorting, list processing, numeric ops…)
  **Resource:** *PSB*, a curated suite + canonical datasets (widely reused for data-structure/algorithmic transforms). ([Hamilton College][3])

---

## 2016 — PBE at scale in the wild + “PBE tracks” become formalized

* **BlinkFill: Semi-supervised Programming By Example for Syntactic String Transformations** (Singh, 2016)
  **Target Domain:** spreadsheet PBE string transforms with contextual cues
  **Resource:** large **real-world benchmark pool** (often cited as a standard PBE evaluation set). ([VLDB][4])

* **SyGuS-Comp 2016 Results/Benchmarks (PBE track)** (Alur et al., 2016)
  **Target Domain:** standardized PBE evaluation (strings + other theories)
  **Resource:** **hundreds of new PBE benchmarks** added for the competition tracks (institutionalized evaluation corpora). ([arXiv][5])

---

## 2016–2017 — I/O-to-program in small DSLs (algorithmic/data-structure transforms)

* **DeepCoder: Learning to Write Programs** (Balog et al., 2016/2017)
  **Target Domain:** infer a short program from **multiple I/O examples**, in a DSL with list/array ops (e.g., SORT/TAKE/SUM)
  **Resource:** a **DSL + program/task generator** (competition-style list problems expressed as I/O sets), heavily reused as an inductive synthesis benchmark. ([OpenReview][6])

* **RobustFill: Neural Program Learning under Noisy I/O** (Devlin et al., 2017)
  **Target Domain:** FlashFill-like program induction from I/O examples (robust to noise)
  **Resource:** a large synthetic **program→I/O generation protocol** + evaluation on FlashFill-style benchmarks; became a standard recipe for PBE dataset generation. ([GitHub][7])

---

## 2017 — Karel “grid-world program induction” becomes a flagship I/O→program dataset

* **Dataset for Learning Karel Programs (MSR Karel Dataset)** (released with the MSR work; widely used 2017+)
  **Target Domain:** infer a Karel program / rule from a few **grid I/O demonstrations**
  **Resource:** the **Karel dataset** of synthetically generated programs paired with multiple I/O examples (a foundational I/O program synthesis benchmark). ([msr-redmond.github.io][8])

---

## 2018 — Neural synthesis papers that “anchor” Karel as standardized infrastructure

* **Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis** (Bunel et al., 2018)
  **Target Domain:** I/O program synthesis for Karel with loops/conditionals
  **Resource:** popularizes the **full-scale Karel benchmark regime** (and points to the public dataset used in many follow-ups). ([arXiv][9])

---

## 2019 — Few-shot rule induction on grids (pure I/O “infer the transformation”)

* **On the Measure of Intelligence (ARC)** (Chollet, 2019)
  **Target Domain:** infer a latent transformation/program from **2–4 I/O grid examples**
  **Resource:** *ARC*, 800 tasks that are basically “few-shot function inference” in a symbolic-ish grid world. ([Cava Lab][10])

* **SyGuS-Comp “PBE tracks” mature (e.g., PBE-Strings, PBE-BV)** (track taxonomy visible by 2019+)
  **Target Domain:** PBE in **strings** and **bit-vectors** (very close to your ADD-CARRY vibe)
  **Resource:** competition tracks and associated benchmark corpora formalize PBE for different theories. ([sygus-org.github.io][11])

---

## 2020 — Symbolic regression as “universal function finding” from examples

* **AI Feynman (and AI Feynman 2.0)** (Udrescu & Tegmark, 2020)
  **Target Domain:** infer closed-form mathematical functions from sampled (x→y) examples (physics-style)
  **Resource:** the widely used **Feynman equation set** (100 equations) as a de facto standardized symbolic regression testbed. ([Science][12])

* **DreamCoder (program induction across multiple domains)** (Ellis et al., 2020) — **borderline as “benchmark,” but very relevant**
  **Target Domain:** infer programs from examples across list-processing, symbolic graphics, etc.
  **Resource:** a widely reused **multi-domain task suite** for program induction experiments (not just a single dataset). ([Courses at UW][13])

---

## 2021 — Naturalistic I/O induction at scale (from real code/tests) + refreshed PSB suite

* **PROGRES: A large-scale benchmark for few-shot program induction and synthesis** (Alet et al., 2021)
  **Target Domain:** infer the “sub-program” / rule from I/O examples derived from real programs
  **Resource:** *PROGRES*, a **large meta-dataset of program induction tasks** harvested via unit tests + execution traces (explicitly designed as infrastructure). ([Proceedings of Machine Learning Research][14])

* **PSB2: The Second Program Synthesis Benchmark Suite** (Helmuth et al., 2021)
  **Target Domain:** general I/O-based program synthesis (harder, more modern)
  **Resource:** *PSB2*, updated suite + datasets (again: sorting, data-structure transforms, classic CS-style problems). ([arXiv][15])

---

## 2022–2023 — Ongoing “living” benchmark infrastructure for symbolic regression

* **SRBench (living benchmark framework for symbolic regression)** (Cava Lab; continually updated)
  **Target Domain:** symbolic regression evaluation (function inference) across many datasets
  **Resource:** *SRBench*, benchmark framework + datasets + standardized comparisons used broadly in the SR community. ([GitHub][16])

---

## 2024–2025 — Benchmarking the benchmark (measurement critique + stronger SR protocols)

* **SRBench++: principled benchmarking of symbolic regression** (de França et al., 2024/2025 publication trail)
  **Target Domain:** symbolic regression evaluation methodology (not just another model)
  **Resource:** *SRBench++*, improved evaluation protocol (adds interpretability/diagnostic dimensions). ([PubMed][17])

* **Towards the Next Generation of Symbolic Regression Benchmarks (SRBench update)** (2025)
  **Target Domain:** updated evaluation infrastructure for SR methods
  **Resource:** updated *SRBench* with expanded method coverage and refined evaluation/reporting. ([arXiv][18])

---

### “Closest matches” to what you described (heap/sort/tree ops, discrete search, Taelin-adjacent vibe)

If you want **data-structure / algorithmic** I/O→function tasks (heapify, sorting variants, list transforms), the best “plug-and-play” benchmark families are:

* **PSB / PSB2** (explicitly includes classic CS-style tasks; lots of list/array transforms) ([Hamilton College][3])
* **DeepCoder DSL tasks** (competition-style list problems, many essentially “infer the algorithmic pipeline from examples”) ([OpenReview][6])
* **SyGuS PBE-BV / PBE-Strings** (for “bitvector logic / string logic from examples,” very compatible with discrete search) ([sygus-org.github.io][11])
* **Karel dataset** (grid transformation/program induction; rich control flow) ([msr-redmond.github.io][8])
* **ARC** (few-shot rule induction from I/O grids; often treated as “program induction without the DSL”)
* **Symbolic regression benchmarks (AI Feynman, SRBench)** (continuous “universal function finder” analogue) ([Science][12])

If you want, I can do a **third pass** that’s even more literal to your examples: benchmarks that are explicitly **bit-level circuits / boolean functions / add-carry / small algorithms** (often housed under SyGuS PBE-BV and related bitvector synthesis suites), plus any newer “I/O → code” datasets that are designed for **discrete program search** rather than just neural mapping.

[1]: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/popl11-synthesis.pdf "Automating String Processing in Spreadsheets Using Input ..."
[2]: https://sygus.org/comp/2014/ "SyGuS-Comp 2014"
[3]: https://www.cs.hamilton.edu/~thelmuth/Pubs/2015-GECCO-benchmark-suite.pdf "General Program Synthesis Benchmark Suite"
[4]: https://www.vldb.org/pvldb/vol9/p816-singh.pdf "BlinkFill: Semi-supervised Programming By Example for ..."
[5]: https://arxiv.org/pdf/1611.07627 "SyGuS-Comp 2016: Results and Analysis"
[6]: https://openreview.net/pdf?id=ByldLrqlx&utm_source=chatgpt.com "DEEPCODER: LEARNING TO WRITE PROGRAMS"
[7]: https://github.com/thelmuth/program-synthesis-benchmark-datasets "General Program Synthesis Benchmark Suite Datasets"
[8]: https://msr-redmond.github.io/karel-dataset/ "Data Format | Dataset for Learning Karel Programs"
[9]: https://arxiv.org/abs/1805.04276 "Leveraging Grammar and Reinforcement Learning for ..."
[10]: https://cavalab.org/srbench/ "SRBench - Cava Lab"
[11]: https://sygus-org.github.io/comp/2019/ "SyGuS-Comp 2019"
[12]: https://www.science.org/doi/10.1126/sciadv.aay2631 "AI Feynman: A physics-inspired method for symbolic ..."
[13]: https://courses.cs.washington.edu/courses/cse599j1/22sp/papers/dreamcoder.pdf "DreamCoder: Growing generalizable, interpretable ..."
[14]: https://proceedings.mlr.press/v139/alet21a.html "A large-scale benchmark for few-shot program induction and ..."
[15]: https://arxiv.org/pdf/2106.06086 "PSB2: The Second Program Synthesis Benchmark Suite"
[16]: https://github.com/cavalab/srbench "GitHub - cavalab/srbench: A living benchmark framework ..."
[17]: https://pubmed.ncbi.nlm.nih.gov/40761553/ "SRBench++ : principled benchmarking of symbolic regression ..."
[18]: https://arxiv.org/html/2505.03977v1 "Towards the Next Generation of Symbolic Regression ..."
