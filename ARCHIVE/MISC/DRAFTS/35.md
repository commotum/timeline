- **Uni3DL: Unified Model for 3D and Language Understanding** (2023)
  **Improves On:** vision-language models that primarily operate on 2D images.
  **Adaptation:** unify 3D representations with language understanding for joint 3D + text reasoning. ([arXiv][35-1])
- **Evolutionary Test-Time Compute (write-up)** (2024)
  **Missing capability:** variable test-time compute and search to solve hard reasoning tasks.
  **Mechanism:** evolutionary-style test-time adaptation/selection over candidate solutions to boost performance. ([Substack][35-2])
- **Fixed Point Diffusion Models** (2024)
  **Contribution Type:** generative modeling framework.
  **Reason:** reframes diffusion sampling/training via fixed-point structure to stabilize or improve generation. ([CVF][35-3])
- **Solving olympiad geometry without human demonstrations (AlphaGeometry)** (2024)
  **Missing capability:** reliable multi-step geometry proof search without curated human solution traces.
  **Mechanism:** neuro-symbolic pipeline for generating and verifying formal geometry proofs at olympiad level. ([Nature][35-4])
- **Gaussian Adaptive Attention Is All You Need** (2024)
  **Contribution Type:** attention mechanism variant.
  **Reason:** replace/augment standard attention with a Gaussian adaptive form to improve efficiency and/or inductive bias. ([arXiv][35-5])
- **Beyond A*: Planning with Transformers** (2024)
  **Missing capability:** scalable planning/search that goes beyond classical A* heuristics.
  **Mechanism:** Transformer-based planning that learns to guide or replace components of traditional search. ([arXiv][35-6])
- **Rotary Position Embedding for Vision Transformer (RoPE-Mixed / 2D RoPE study)** (2024)
  **Critique:** common 2D positional schemes (or naive RoPE transfers) can be suboptimal for images.
  **Improvement:** study + variants for applying rotary positional encoding in ViTs (e.g., mixed/2D formulations). ([arXiv][35-7])
- **VG4D: Vision-Language Model Goes 4D Video Recognition** (2024)
  **Improves On:** image-centric VLMs that don’t model space-time well.
  **Adaptation:** extend VLMs to 4D (spatiotemporal) video recognition with space-time token modeling. ([arXiv][35-8])
- **DAPE: Data-Adaptive Positional Encoding for Length Extrapolation** (2024)
  **Critique:** fixed positional encodings can extrapolate poorly to longer contexts.
  **Improvement:** data-adaptive positional encoding designed to improve length extrapolation. ([NeurIPS Proceedings][35-9])
- **What matters when building vision-language models? (Idefics2)** (2024)
  **Contribution Type:** empirical study / model-building guidance.
  **Reason:** analyze design and training choices that most affect VLM performance and efficiency. ([arXiv][35-10])
- **Base of RoPE Bounds Context Length** (2024)
  **Critique:** RoPE’s base/frequency parameterization imposes constraints on usable context length.
  **Improvement:** theory/analysis (and implications) linking RoPE base to context-length limits. ([arXiv][35-11])
- **RoTHP: Rotary Position Embedding-based Transformer Hawkes Process** (2024)
  **Improves On:** standard Transformer sequence models not tailored to continuous-time event data.
  **Adaptation:** combine Hawkes process modeling with RoPE-parameterized Transformer representations for temporal point processes. ([arXiv][35-12])
- **LieRE: Lie Rotational Positional Encodings** (2024)
  **Critique:** conventional positional encodings can lack principled geometric structure.
  **Improvement:** positional encoding built from Lie-group rotational structure for more principled position handling. ([arXiv][35-13])
- **Learning Iterative Reasoning through Energy Diffusion** (2024)
  **Missing capability:** iterative refinement dynamics for multi-step reasoning beyond single-pass inference.
  **Mechanism:** energy/diffusion-inspired iterative reasoning procedure to refine solutions over steps. ([arXiv][35-14])
- **Simultaneous Instance Pooling & Bag Selection for MIL using ViTs** (2024)
  **Contribution Type:** architecture/training method for multiple instance learning (MIL).
  **Reason:** integrate instance pooling and bag selection within ViT-based MIL to improve performance and efficiency. ([Springer][35-15])
- **Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters** (2024)
  **Missing capability:** principled allocation of extra compute at inference time.
  **Mechanism:** strategy/analysis for optimally scaling test-time compute to outperform parameter scaling in some regimes. ([arXiv][35-16])
- **Length Extrapolation of Causal Transformers without Position Encoding (NoPE)** (2024)
  **Critique:** explicit positional encodings can be a bottleneck for length extrapolation in causal Transformers.
  **Improvement:** remove positional encoding (and analyze/enable extrapolation behavior without it). ([ACL Anthology][35-17])
- **H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark** (2024)
  **Target Domain:** human baseline measurement on ARC-style abstract reasoning tasks.
  **Resource:** a robust estimate/protocol for human performance on ARC, for fair model comparison. ([arXiv][35-18])
- **Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding** (2024)
  **Target Domain:** length extrapolation methods and failure modes in Transformers.
  **Resource:** taxonomy/survey of approaches framed through positional encoding choices and alternatives. ([ACL Anthology][35-19])
- **Beyond Position: The Emergence of Wavelet-like Properties in Transformers** (2024)
  **Contribution Type:** theoretical/interpretability analysis.
  **Reason:** characterize emergent wavelet-like behavior in Transformers beyond explicit positional mechanisms. ([arXiv][35-20])
- **ARC-Heavy / ARC-Potpourri (dataset description embedded in Cornell report)** (2024)
  **Target Domain:** harder and more varied ARC-style abstract reasoning evaluation.
  **Resource:** dataset variants (ARC-Heavy / ARC-Potpourri) described as infrastructure for improved measurement. ([Cornell][35-21])
- **Combining Induction and Transduction for Abstract Reasoning** (2024)
  **Contribution Type:** modeling/training approach for abstract reasoning.
  **Reason:** combine inductive generalization with transductive/test-time adaptation to improve ARC-like performance. ([arXiv][35-22])
- **Searching Latent Program Spaces** (2024)
  **Missing capability:** structured search over program-like solution spaces that is hard for pure next-token prediction.
  **Mechanism:** search procedure over latent program representations to solve reasoning tasks. ([arXiv][35-23])
- **The Surprising Effectiveness of Test-Time Training for Few-Shot Learning** (2024)
  **Missing capability:** on-the-fly adaptation when few labeled examples are available.
  **Mechanism:** test-time training procedure that updates/optimizes at inference to improve few-shot performance. ([Project Page][35-24])
- **Towards Efficient Neurally-Guided Program Induction for ARC-AGI** (2024)
  **Missing capability:** efficient program induction/search guided by neural priors for ARC-AGI tasks.
  **Mechanism:** neurally-guided program induction pipeline to search programs more efficiently for ARC solutions. ([arXiv][35-25])

[35-1]: https://arxiv.org/pdf/2312.03026.pdf "Uni3DL (2023) — arXiv"
[35-2]: https://jeremyberman.substack.com/p/how-i-got-a-record-536-on-arc-agi "Evolutionary Test-Time Compute (2024) — Substack"
[35-3]: https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.pdf "Fixed Point Diffusion Models (2024) — CVF / CVPR"
[35-4]: https://www.nature.com/articles/s41586-023-06747-5.pdf "AlphaGeometry (2024) — Nature"
[35-5]: https://arxiv.org/pdf/2401.11143.pdf "Gaussian Adaptive Attention (2024) — arXiv"
[35-6]: https://arxiv.org/pdf/2402.14083.pdf "Beyond A*: Planning with Transformers (2024) — arXiv"
[35-7]: https://arxiv.org/pdf/2403.13298.pdf "RoPE for ViT / RoPE-Mixed 2D study (2024) — arXiv"
[35-8]: https://arxiv.org/pdf/2404.11605.pdf "VG4D (2024) — arXiv"
[35-9]: https://proceedings.neurips.cc/paper_files/paper/2024/file/2f050fa9f0d898e3f265d515f50ae8f9-Paper-Conference.pdf "DAPE (2024) — NeurIPS Proceedings"
[35-10]: https://arxiv.org/pdf/2405.02246.pdf "Idefics2 study (2024) — arXiv"
[35-11]: https://arxiv.org/pdf/2405.14591.pdf "Base of RoPE Bounds Context Length (2024) — arXiv"
[35-12]: https://arxiv.org/pdf/2405.06985.pdf "RoTHP (2024) — arXiv"
[35-13]: https://arxiv.org/pdf/2406.10322.pdf "LieRE (2024) — arXiv"
[35-14]: https://arxiv.org/pdf/2406.11179.pdf "Energy Diffusion Iterative Reasoning (2024) — arXiv"
[35-15]: https://doi.org/10.1007/s00521-024-09417-3 "MIL with ViTs (2024) — Springer"
[35-16]: https://arxiv.org/pdf/2408.03314.pdf "Scaling LLM Test-Time Compute Optimally (2024) — arXiv"
[35-17]: https://aclanthology.org/2024.findings-acl.834.pdf "NoPE (2024) — ACL Anthology"
[35-18]: https://arxiv.org/pdf/2409.01374.pdf "H-ARC (2024) — arXiv"
[35-19]: https://aclanthology.org/2024.findings-emnlp.582.pdf "Length Extrapolation Survey (2024) — ACL Anthology"
[35-20]: https://arxiv.org/pdf/2410.18067.pdf "Wavelet-like Properties in Transformers (2024) — arXiv"
[35-21]: https://www.cs.cornell.edu/~ellisk/documents/arc_induction_vs_transduction.pdf "ARC-Heavy / ARC-Potpourri (2024) — Cornell"
[35-22]: https://arxiv.org/pdf/2411.02272.pdf "Combining Induction and Transduction (2024) — arXiv"
[35-23]: https://arxiv.org/pdf/2411.08706.pdf "Searching Latent Program Spaces (2024) — arXiv"
[35-24]: https://ekinakyurek.github.io/papers/ttt.pdf "Test-Time Training for Few-Shot Learning (2024) — Project Page"
[35-25]: https://arxiv.org/pdf/2411.17708.pdf "Neurally-Guided Program Induction for ARC-AGI (2024) — arXiv"
