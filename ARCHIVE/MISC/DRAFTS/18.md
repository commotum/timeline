Below is a **timeline-form** “infrastructure for progress” list: papers whose *main* contribution is **data / benchmark / evaluation protocol / measurement taxonomy or critique**, with modeling (if any) clearly secondary.

I’m aiming for **broad, high-signal coverage across vision, NLP, reasoning, long-context, multimodal, code, safety/alignment, and eval methodology**—the places where “measurement” has most shaped the field.

---

## 2009 — Large-scale vision data as a scaling catalyst

* **ImageNet: A Large-Scale Hierarchical Image Database** (Deng et al., 2009)
  **Target Domain:** large-scale object recognition + hierarchical visual concepts
  **Resource:** *ImageNet*, a WordNet-structured, large-scale labeled image database. ([image-net.org][1])

---

## 2014 — Scene understanding and rich annotation

* **Microsoft COCO: Common Objects in Context** (Lin et al., 2014)
  **Target Domain:** object recognition in complex scenes (detection/segmentation/captioning)
  **Resource:** *MS COCO*, large-scale images with dense instance-level labels in context. ([arXiv][2])

---

## 2016 — Reading comprehension becomes standardized at scale

* **SQuAD: 100,000+ Questions for Machine Comprehension of Text** (Rajpurkar et al., 2016)
  **Target Domain:** extractive QA / reading comprehension
  **Resource:** *SQuAD*, 100k+ crowd-sourced QA pairs grounded in Wikipedia passages. ([arXiv][3])

---

## 2018 — “Harder-than-SQuAD” reasoning diagnostics for QA + multi-hop supervision

* **Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge** (Clark et al., 2018)
  **Target Domain:** grade-school science QA with harder reasoning requirements
  **Resource:** *ARC* (Easy + Challenge splits) + *ARC Corpus* for science reasoning evaluation. ([arXiv][4])

* **HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering** (Yang et al., 2018)
  **Target Domain:** multi-hop QA + explainability
  **Resource:** *HotpotQA*, 113k QA pairs with supporting-fact supervision across multiple docs. ([arXiv][5])

* **GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding** (Wang et al., 2018)
  **Target Domain:** general NLU as *benchmark suite + diagnostics*
  **Resource:** *GLUE*, standardized multi-task evaluation + diagnostic set + leaderboard. ([arXiv][6])

---

## 2019 — Measuring “general intelligence” + discrete reasoning over text + adversarial commonsense

* **On the Measure of Intelligence** (Chollet, 2019)
  **Target Domain:** abstract reasoning / generalization beyond training distribution
  **Resource:** *ARC (ARC-AGI)*, 800 symbolic grid I/O tasks as a generalization benchmark. ([arXiv][7])

* **DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs** (Dua et al., 2019)
  **Target Domain:** numerical/discrete operations grounded in text (counting, addition, sorting, etc.)
  **Resource:** *DROP*, adversarially created RC benchmark requiring discrete reasoning. ([arXiv][8])

* **HellaSwag: Can a Machine Really Finish Your Sentence?** (Zellers et al., 2019)
  **Target Domain:** commonsense inference
  **Resource:** *HellaSwag*, a challenge dataset built with adversarial filtering. ([arXiv][9])

* **SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems** (Wang et al., 2019)
  **Target Domain:** tougher general NLU evaluation
  **Resource:** *SuperGLUE*, harder task suite + toolkit + leaderboard, positioned as “beyond GLUE.” ([arXiv][10])

---

## 2020 — Broad capability measurement + safety-focused evaluation data

* **Measuring Massive Multitask Language Understanding (MMLU)** (Hendrycks et al., 2020; ICLR 2021)
  **Target Domain:** broad academic/professional knowledge + problem solving across 57 subjects
  **Resource:** *MMLU*, multi-domain multiple-choice evaluation for general LMs. ([arXiv][11])

* **RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models** (Gehman et al., 2020)
  **Target Domain:** toxicity risk / safety measurement for generation
  **Resource:** *RealToxicityPrompts*, 100k naturally occurring prompts + toxicity scores to probe degeneration. ([arXiv][12])

---

## 2021 — Math + code evaluation become “standard instruments”

* **Measuring Mathematical Problem Solving With the MATH Dataset** (Hendrycks et al., 2021)
  **Target Domain:** competition-level math reasoning (with full solutions)
  **Resource:** *MATH*, 12,500 challenging problems + step-by-step solutions (and auxiliary data). ([arXiv][13])

* **Training Verifiers to Solve Math Word Problems (GSM8K)** (Cobbe et al., 2021)
  **Target Domain:** multi-step arithmetic word problems
  **Resource:** *GSM8K*, 8.5k high-quality grade-school math problems (train/test split). ([arXiv][14])

* **TruthfulQA: Measuring How Models Mimic Human Falsehoods** (Lin, Hilton, Evans, 2021)
  **Target Domain:** truthfulness / imitation-of-misconceptions failure mode
  **Resource:** *TruthfulQA*, 817 questions designed to elicit common false beliefs. ([arXiv][15])

* **Evaluating Large Language Models Trained on Code (HumanEval)** (Chen et al., 2021)
  **Target Domain:** code generation functional correctness
  **Resource:** *HumanEval*, docstring-to-code tasks evaluated by unit tests + pass@k protocol. ([arXiv][16])

---

## 2022 — Multimodal reasoning benchmarks + “holistic” evaluation framing

* **LAION-5B: An open large-scale dataset for training next generation image-text models** (Schuhmann et al., 2022)
  **Target Domain:** large-scale vision–language pretraining
  **Resource:** *LAION-5B*, 5.85B CLIP-filtered image–text pairs from Common Crawl. ([LAION][17])

* **Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering (ScienceQA)** (Lu et al., 2022)
  **Target Domain:** multimodal science QA + explanation-conditioned evaluation
  **Resource:** *ScienceQA*, ~21k multimodal MCQs with lectures and explanations. ([arXiv][18])

* **Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models (BIG-bench)** (Srivastava et al., 2022)
  **Target Domain:** broad “capability frontier” tasks
  **Resource:** *BIG-bench*, 200+ diverse tasks intended to stress LMs beyond “standard” NLP. ([arXiv][19])

* **Holistic Evaluation of Language Models (HELM)** (Liang et al., 2022)
  **Target Domain:** taxonomy + measurement design across capability/risk axes
  **Resource:** *HELM*, scenario+metric taxonomy + standardized evaluation as a living benchmark. ([arXiv][20])

---

## 2023 — Long-context, real-world coding tasks, instruction-following, and multimodal “exam-style” eval

* **LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding** (Bai et al., 2023)
  **Target Domain:** long-context understanding across task types (EN/ZH)
  **Resource:** *LongBench*, unified-format long-context benchmark across 21 datasets / 6 categories. ([arXiv][21])

* **SWE-bench: Can Language Models Resolve Real-World GitHub Issues?** (Jimenez et al., 2023; ICLR 2024)
  **Target Domain:** software engineering as an evaluation frontier
  **Resource:** *SWE-bench*, 2,294 GitHub issue→PR tasks requiring code edits validated by tests. ([arXiv][22])

* **Instruction-Following Evaluation for Large Language Models (IFEval)** (Zhou et al., 2023)
  **Target Domain:** instruction-following with *verifiable constraints*
  **Resource:** *IFEval*, ~500 prompts spanning 25 verifiable instruction types (objective scoring). ([arXiv][23])

* **Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena** (Zheng et al., 2023)
  **Target Domain:** open-ended assistant evaluation + measurement critique of LLM-judging biases
  **Resource:** *MT-Bench* + methodology for LLM-as-judge evaluation (with bias analysis + mitigations). ([arXiv][24])

* **MMBench: Is Your Multi-modal Model an All-around Player?** (Liu et al., 2023)
  **Target Domain:** general-purpose multimodal understanding
  **Resource:** *MMBench*, systematically designed bilingual multiple-choice evaluation for VLMs. ([arXiv][25])

* **MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts** (Lu et al., 2023)
  **Target Domain:** visual math reasoning
  **Resource:** *MathVista*, benchmark spanning visual+math skills across curated sources. ([arXiv][26])

* **MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI** (Yue et al., 2023; CVPR 2024)
  **Target Domain:** college-level multimodal “exam” reasoning across disciplines
  **Resource:** *MMMU*, 11.5k multi-discipline multimodal questions from exams/textbooks. ([arXiv][27])

* **GPQA: A Graduate-Level Google-Proof Q&A Benchmark** (Rein et al., 2023)
  **Target Domain:** hard science QA requiring expert knowledge + reasoning
  **Resource:** *GPQA*, 448 expert-written multiple-choice questions (bio/chem/phys). ([arXiv][28])

---

## 2024 — “Measurement of measurement”: long-context realism, chat eval at scale, and debiasing protocols

* **RULER: What’s the Real Context Size of Your Long-Context Language Models?** (Hsieh et al., 2024)
  **Target Domain:** long-context capability beyond simple “needle” retrieval
  **Resource:** *RULER*, configurable synthetic tasks (retrieval + multi-hop tracing + aggregation) to test real context use. ([arXiv][29])

* **Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference** (Chiang et al., 2024)
  **Target Domain:** human preference evaluation for chat assistants “in the wild”
  **Resource:** *Chatbot Arena*, large-scale pairwise voting + statistical treatment (Elo-style evaluation). ([arXiv][30])

* **Length-Controlled AlpacaEval** (Dubois et al., 2024)
  **Target Domain:** instruction-following evaluation + measurement bias
  **Resource:** *LC-AlpacaEval*, protocol/paper focused on reducing **length bias** in LLM-judge evaluation. ([arXiv][31])

* **SWE-bench (ICLR 2024 version)** (Jimenez et al., 2024)
  **Target Domain:** standardized *real-world* coding evaluation
  **Resource:** formalizes framework + reporting for SWE-bench as an evaluation ecosystem. ([ICLR Proceedings][32])

* **A Survey on Evaluation of Large Language Models** (Chang et al., 2024)
  **Target Domain:** evaluation taxonomy + measurement landscape
  **Resource:** a broad **evaluation survey/taxonomy** (useful as an “index” of benchmarks and metrics). ([ACM Digital Library][33])

* **MT-Bench-101** (Bai et al., 2024)
  **Target Domain:** multi-turn dialogue evaluation at finer granularity
  **Resource:** *MT-Bench-101*, expanded benchmark for complex multi-turn chat evaluation. ([ACL Anthology][34])

* **LTD-Bench: Evaluating Large Language Models by Letting Them Draw** (Lin et al., 2024)
  **Target Domain:** visual/spatial reasoning diagnostics for LLMs
  **Resource:** *LTD-Bench*, prompts forcing observable drawing/code outputs to reveal spatial failures. ([OpenReview][35])

---

## 2025 — New structured-domain and multilingual long-context benchmarks

* **TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution** (2025)
  **Target Domain:** table understanding + deep table reasoning
  **Resource:** *TReB*, 26 subtasks + evaluation framework with multiple inference modes (e.g., CoT/PoT variants). ([arXiv][36])

* **ONERULER: Benchmarking multilingual long-context language models** (Kim et al., 2025)
  **Target Domain:** multilingual long-context evaluation
  **Resource:** *ONERULER*, multilingual adaptation of RULER across many languages and tasks. ([arXiv][37])

---

### If you want an even more “complete” version

This list is already long, but “full list” in this category is basically unbounded (every subfield has its own benchmark line). If you tell me which **slice** you care about most, I can expand that slice to near-exhaustive coverage without changing the format, e.g.:

* **Reasoning-heavy text benchmarks** (multi-hop, math, logic, planning, proofs)
* **Multimodal reasoning** (charts/diagrams/geometry/science labs)
* **Long-context** (retrieval vs aggregation vs multi-doc reasoning vs codebase tasks)
* **Code** (unit-test correctness, repo-level issue fixing, security, tool-use)
* **Safety / truthfulness / toxicity / bias / robustness** (and measurement critiques)

If you pick one slice, I’ll do a “second-pass aggressive hunt” for **2024–2025** in that slice (similar to what we did for positional encoding).

[1]: https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf "A Large-Scale Hierarchical Image Database"
[2]: https://arxiv.org/abs/1405.0312 "[1405.0312] Microsoft COCO: Common Objects in Context"
[3]: https://arxiv.org/abs/1606.05250 "SQuAD: 100,000+ Questions for Machine Comprehension of Text"
[4]: https://arxiv.org/abs/1803.05457 "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
[5]: https://arxiv.org/abs/1809.09600 "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering"
[6]: https://arxiv.org/abs/1804.07461 "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"
[7]: https://arxiv.org/abs/1911.01547 "[1911.01547] On the Measure of Intelligence"
[8]: https://arxiv.org/abs/1903.00161 "DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs"
[9]: https://arxiv.org/abs/1905.07830 "HellaSwag: Can a Machine Really Finish Your Sentence?"
[10]: https://arxiv.org/abs/1905.00537 "SuperGLUE: A Stickier Benchmark for General-Purpose ..."
[11]: https://arxiv.org/abs/2009.03300 "Measuring Massive Multitask Language Understanding"
[12]: https://arxiv.org/abs/2009.11462 "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"
[13]: https://arxiv.org/abs/2103.03874 "Measuring Mathematical Problem Solving With the MATH Dataset"
[14]: https://arxiv.org/abs/2110.14168 "Training Verifiers to Solve Math Word Problems"
[15]: https://arxiv.org/abs/2109.07958 "TruthfulQA: Measuring How Models Mimic Human Falsehoods"
[16]: https://arxiv.org/abs/2107.03374 "Evaluating Large Language Models Trained on Code"
[17]: https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/ "LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI ..."
[18]: https://arxiv.org/abs/2209.09513 "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"
[19]: https://arxiv.org/abs/2206.04615 "Beyond the Imitation Game: Quantifying and extrapolating ..."
[20]: https://arxiv.org/abs/2211.09110 "[2211.09110] Holistic Evaluation of Language Models"
[21]: https://arxiv.org/abs/2308.14508 "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding"
[22]: https://arxiv.org/abs/2310.06770 "SWE-bench: Can LLMs Resolve Real-World GitHub Issues?"
[23]: https://arxiv.org/abs/2311.07911 "Instruction-Following Evaluation for Large Language Models"
[24]: https://arxiv.org/abs/2306.05685 "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena"
[25]: https://arxiv.org/abs/2307.06281 "MMBench: Is Your Multi-modal Model an All-around Player?"
[26]: https://arxiv.org/abs/2310.02255 "MathVista: Evaluating Mathematical Reasoning of ..."
[27]: https://arxiv.org/abs/2311.16502 "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"
[28]: https://arxiv.org/abs/2311.12022 "GPQA: A Graduate-Level Google-Proof Q&A Benchmark"
[29]: https://arxiv.org/abs/2404.06654 "RULER: What's the Real Context Size of Your Long-Context Language Models?"
[30]: https://arxiv.org/pdf/2403.04132 "Chatbot Arena: An Open Platform for Evaluating LLMs by ..."
[31]: https://arxiv.org/abs/2404.04475 "[2404.04475] Length-Controlled AlpacaEval: A Simple Way ..."
[32]: https://proceedings.iclr.cc/paper_files/paper/2024/file/edac78c3e300629acfe6cbe9ca88fb84-Paper-Conference.pdf "SWE-BENCH: CAN LANGUAGE MODELS RESOLVE ..."
[33]: https://dl.acm.org/doi/full/10.1145/3641289 "A Survey on Evaluation of Large Language Models"
[34]: https://aclanthology.org/2024.acl-long.401.pdf "MT-Bench-101: A Fine-Grained Benchmark for Evaluating ..."
[35]: https://openreview.net/forum?id=TG5rvKyEbu&utm_source=chatgpt.com "Evaluating Large Language Models by Letting Them Draw"
[36]: https://arxiv.org/html/2506.18421v1 "TReB: A Comprehensive Benchmark for Evaluating Table ..."
[37]: https://arxiv.org/abs/2503.01996 "Benchmarking multilingual long-context language models"
