You’re right — I accidentally **left out the whole references section pass**. Here’s the **complete timeline** for *every paper (and the two Su blog posts) referenced in “Rotary Embeddings: A Relative Revolution”* — same format, each with **canonical arXiv/PDF links**.

---

## 2017 — Transformer baseline

* **Attention Is All You Need** (Vaswani et al., 2017)
  **Critique:** self-attention is permutation-invariant; order must be injected.
  **Improvement:** introduces Transformer + **sinusoidal absolute positional encoding**.
  **Links (arXiv / PDF):**

  ````text
  arXiv: https://arxiv.org/abs/1706.03762
  PDF:   https://arxiv.org/pdf/1706.03762
  ``` :contentReference[oaicite:0]{index=0}
  ````

---

## 2018 — Relative PE + a long-sequence domain exemplar

* **Self-Attention with Relative Position Representations** (Shaw, Uszkoreit, Vaswani, 2018)
  **Critique:** absolute PE is not the only route; attention scoring can directly incorporate relative distance.
  **Improvement:** classic **relative position representations** integrated into attention.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1803.02155
  PDF:   https://arxiv.org/pdf/1803.02155
  ```

* **Music Transformer** (Huang et al., 2018)
  **Critique:** music’s long-range structure stresses naive positional handling.
  **Improvement:** uses relative-position ideas for music sequence modeling; cited as a motivating multi-dimensional setting.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1809.04281
  PDF:   https://arxiv.org/pdf/1809.04281
  ```

---

## 2019 — Logit-bias style relative positions

* **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)** (Raffel et al., 2019)
  **Critique:** content/position interactions can be simplified; position can act as a bias term.
  **Improvement:** **relative positional bias** added directly to attention logits (bucketing in practice).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1910.10683
  PDF:   https://arxiv.org/pdf/1910.10683
  ```

---

## 2020 — Scaling LMs, efficient attention, and “fix position handling” work

* **Language Models are Few-Shot Learners (GPT-3)** (Brown et al., 2020)
  **Critique:** scaling highlights practical long-context issues; learned absolute PEs can be brittle under context packing/truncation.
  **Improvement:** large autoregressive LM baseline (cited as the learned-absolute PE default).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2005.14165
  PDF:   https://arxiv.org/pdf/2005.14165
  ```

* **Rethinking Attention with Performers** (Choromanski et al., 2020)
  **Critique:** full softmax attention is quadratic; many positional methods that require an explicit N×N attention matrix don’t fit efficient attention.
  **Improvement:** **Performer / FAVOR+** kernelized attention to approximate softmax efficiently.
  **Links (arXiv / PDF):**

  ````text
  arXiv: https://arxiv.org/abs/2009.14794
  PDF:   https://arxiv.org/pdf/2009.14794
  ``` :contentReference[oaicite:1]{index=1}

  ````

* **Rethinking Positional Encoding in Language Pre-training (TUPE)** (Ke, He, Liu, 2020)
  **Critique:** naive “token + position add” can entangle correlations in a suboptimal way.
  **Improvement:** **untied / decoupled** positional treatment in attention.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2006.15595
  PDF:   https://arxiv.org/pdf/2006.15595
  ```

* **Shortformer: Better Language Modeling using Shorter Inputs** (Press, Smith, Lewis, 2020)
  **Critique:** training/inference setups make some positional choices inefficient or poorly behaved; seek a robust alternative.
  **Improvement:** inject position by adding absolute position embeddings to **queries/keys** rather than token embeddings (position-infused attention variant).
  **Links (arXiv / PDF):**

  ````text
  arXiv: https://arxiv.org/abs/2012.15832
  PDF:   https://arxiv.org/pdf/2012.15832
  ``` :contentReference[oaicite:2]{index=2}

  ````

* **Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision** (Tan, Bansal, 2020)
  **Critique:** pure text-only self-supervision is limited; training practices and supervision shape what position mechanisms “mean” in practice.
  **Improvement:** adds visually grounded supervision (“vokens”) mapped to language tokens.
  **Links (arXiv / PDF):**

  ````text
  arXiv: https://arxiv.org/abs/2010.06775
  PDF:   https://arxiv.org/pdf/2010.06775
  ``` :contentReference[oaicite:3]{index=3}
  ````

---

## 2021 — Transferability + datasets + systems + RoPE itself

* **The Pile: An 800GB Dataset of Diverse Text for Language Modeling** (Gao et al., 2021)
  **Critique:** evaluation of PE methods needs broad, diverse corpora at scale.
  **Improvement:** large multi-source dataset used in the blog’s large-model experiments.
  **Links (arXiv / PDF):**

  ````text
  arXiv: https://arxiv.org/abs/2101.00027
  PDF:   https://arxiv.org/pdf/2101.00027
  ``` :contentReference[oaicite:4]{index=4}

  ````
* **Do Transformer Modifications Transfer Across Implementations and Applications?** (Narang et al., 2021)
  **Critique:** many transformer “improvements” don’t transfer robustly across codebases/tasks.
  **Improvement:** systematic study motivating “robust” architectural changes (used in the post’s conclusion framing).
  **Links (arXiv / PDF):**

  ````text
  arXiv: https://arxiv.org/abs/2102.11972
  PDF:   https://arxiv.org/pdf/2102.11972
  ``` :contentReference[oaicite:5]{index=5}

  ````
* **Learning Transferable Visual Models From Natural Language Supervision (CLIP)** (Radford et al., 2021)
  **Critique:** large-scale practice changes how representations are used and transferred (cited among “common practices”).
  **Improvement:** large-scale contrastive vision–language pretraining (CLIP).
  **Links (arXiv / PDF):**

  ````text
  arXiv: https://arxiv.org/abs/2103.00020
  PDF:   https://arxiv.org/pdf/2103.00020
  ``` :contentReference[oaicite:6]{index=6}

  ````
* **Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM** (Narayanan et al., 2021)
  **Critique:** systems constraints drive practical sequence/context decisions; efficient training is part of the long-context story.
  **Improvement:** scalable distributed training (pipeline/tensor/data parallelism + schedules).
  **Links (arXiv / PDF):**

  ````text
  arXiv: https://arxiv.org/abs/2104.04473
  PDF:   https://arxiv.org/pdf/2104.04473
  ``` :contentReference[oaicite:7]{index=7}

  ````
* **RoFormer: Enhanced Transformer with Rotary Position Embedding** (Su et al., 2021)
  **Critique:** many relative methods require explicit attention matrices (hard for efficient attention); sinusoidal absolute PE only partially yields relative structure.
  **Improvement:** **RoPE** rotates Q/K so dot-products depend on relative offsets; broadly compatible across attention variants.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2104.09864
  PDF:   https://arxiv.org/pdf/2104.09864
  ```

---

## 2021 — Referenced source posts (not papers, but explicitly cited)

* **The Transformer Positional Encodings That Have Researchers Racking Their Brains** (Su Jianlin, blog, 2021-02-03)
  **Critique:** surveys absolute vs relative PE families and oddball hybrids.
  **Improvement:** synthesizes perspective that motivates the fused/rotational approach.
  **Link:**

  ```text
  https://kexue.fm/archives/8130
  ```

* **The Path to Upgrading Transformers: 2. Rotary Position Embedding, a Combination of Strengths** (Su Jianlin, blog, 2021-03-23)
  **Critique:** sinusoidal PE is “absolute trying to be relative” but not cleanly.
  **Improvement:** introduces RoPE derivation and properties (linear-attention compatibility emphasis).
  **Link:**

  ```text
  https://kexue.fm/archives/8265
  ```

* **Rotary Embeddings: A Relative Revolution** (Biderman et al., blog, 2021-04-20)
  **Critique:** existing PE methods trade off simplicity vs efficiency; many don’t work with efficient attention.
  **Improvement:** popularizes RoPE in broader open-source LLM tooling; provides experiments and implementations.
  **Link:**

  ```text
  https://blog.eleuther.ai/rotary-embeddings/
  ```

---

If you want, next we can do the exact same thing for the *“full details, see here”* links inside the Eleuther post (those sometimes hide additional referenced writeups/notes that don’t appear in the numbered references).
