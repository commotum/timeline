## 2021 — “4D point cloud video” becomes a first-class Transformer domain (XYZT)

* **Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos (P4Transformer)** (Fan et al., CVPR 2021)
  **Improves On:** 1D Transformers (language) and single-frame 3D point models
  **Adaptation:** treat point cloud sequences as **4D (x,y,z,t)** and model them directly with **spatiotemporal point attention** (explicitly avoiding point tracking) ([CVF Open Access][1])

---

## 2022 — “4D” in robotics/autonomy: sensor × time fusion and sparse spatiotemporal sampling

* **LIFT: Learning 4D LiDAR Image Fusion Transformer for 3D Object Detection** (Zeng et al., CVPR 2022)
  **Improves On:** 1D Transformer success + 3D perception fusion that underuses **cross-sensor temporal context**
  **Adaptation:** a **LiDAR–camera fusion Transformer** that explicitly models **cross-sensor interactions over time** (4D sequential fusion as the core design) ([CVF Open Access][2])

* **Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion** (Lin et al., 2022)
  **Improves On:** BEV-style 3D detection and heavy dense view transforms / global attention for multi-view sequences
  **Adaptation:** “Sparse **4D sampling**”: assign **4D keypoints** per 3D anchor and fuse features across **view/scale/timestamp** via sparse sampling + hierarchical fusion (spatiotemporal 4D representation is central) ([arXiv][3])

---

## 2023 — 4D panoptic segmentation: unify segmentation + tracking over LiDAR sequences with Transformer-style queries

* **Mask4D: End-to-End Mask-Based 4D Panoptic Segmentation for LiDAR Sequences** (Marcuzzi et al., RA-L 2023)
  **Improves On:** pipelines that do 3D panoptic per-frame then use **hand-crafted temporal association**
  **Adaptation:** an end-to-end **mask + query** formulation that reuses/updates queries over time to keep consistent IDs (4D = segmentation + tracking across frames) ([IPB Uni Bonn][4])

* **4D-Former: Multimodal 4D Panoptic Segmentation** (Athar et al., CoRL 2023)
  **Improves On:** LiDAR-only 4D panoptic systems that lack strong appearance cues and struggle with temporal consistency
  **Adaptation:** a **query-based Transformer** that fuses **LiDAR sequences + RGB** and iteratively refines semantic + temporally consistent instance masks (explicit 4D panoptic transformerization) ([arXiv][5])

* **Mask4Former: Mask Transformer for 4D Panoptic Segmentation** (Yilmaz et al., arXiv 2023)
  **Improves On:** non-learned association strategies and “segmentation then tracking” decomposition
  **Adaptation:** unify **semantic + instance segmentation + tracking** with **spatio-temporal instance queries** in a single transformer-style model for LiDAR sequences ([arXiv][6])

---

## 2024 — 4D “world modeling” as representation learning over time (attention-centric pretraining for dynamic scenes)

* **DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving** (Min et al., CVPR 2024)
  **Improves On:** 2D-pretraining-centric perception and weaker temporal scene representations for driving
  **Adaptation:** a **world-model 4D representation learning** framework that pretrains on multi-camera sequences in a spatiotemporal (4D) fashion for downstream 3D tasks (4D representation is the centerpiece) ([CVF Open Access][7])

* **OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving** (Zheng et al., ECCV 2024) — **borderline (Transformer involvement depends on implementation details)**
  **Improves On:** modeling dynamics via tracked boxes (limited scene-level fidelity)
  **Adaptation:** models **4D occupancy evolution** (3D space over time) as a world model for forecasting/planning; strongly “4D representation” driven, but the paper is sometimes framed as world-modeling rather than “Transformer-first.” ([ECVA][8])

---

## 2025 — 4D panoptic segmentation becomes “zero-shot” (foundation signals lifted into 4D)

* **Zero-Shot 4D LiDAR Panoptic Segmentation (SAL-4D)** (Zhang et al., CVPR 2025)
  **Improves On:** supervised 4D panoptic segmentation bottlenecked by labels; limited transfer from 2D/video foundation models to LiDAR sequences
  **Adaptation:** a pipeline that lifts multimodal foundation signals (e.g., video object segmentation + VLM features) into **4D LiDAR** and trains a model for **4D panoptic segmentation** without labeled 4D data (4D adaptation is central) ([CVF Open Access][9])

* **Streaming 4D Panoptic Segmentation via Dual Threads** (arXiv 2025) — **borderline (streaming setup is central; confirm transformer core if you want this kept “strict”)**
  **Improves On:** offline 4D panoptic methods that aren’t designed for real-time sequential deployment
  **Adaptation:** explicitly targets **streaming 4D** inference over point cloud sequences under strict time budgets (the domain shift is “4D + streaming”). ([arXiv][10])

---

If you want this to be *maximally strict* to your criterion (attention-as-core and clearly “Transformer adaptation”), tell me whether you want to **exclude borderline world-model papers** like OccWorld unless the paper is unambiguously transformer-centric. I can then produce a “strict-only” 4D timeline (and a separate “4D-adjacent world-model” appendix) in the same format.

[1]: https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_Point_4D_Transformer_Networks_for_Spatio-Temporal_Modeling_in_Point_Cloud_CVPR_2021_paper.pdf "Point 4D Transformer Networks for Spatio-Temporal ..."
[2]: https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_LIFT_Learning_4D_LiDAR_Image_Fusion_Transformer_for_3D_Object_CVPR_2022_paper.pdf "LIFT: Learning 4D LiDAR Image Fusion Transformer for 3D ..."
[3]: https://arxiv.org/abs/2211.10581 "Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion"
[4]: https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/marcuzzi2023ral-meem.pdf "End-to-End Mask-Based 4D Panoptic Segmentation for ..."
[5]: https://arxiv.org/abs/2311.01520 "4D-Former: Multimodal 4D Panoptic Segmentation"
[6]: https://arxiv.org/abs/2309.16133 "Mask4Former: Mask Transformer for 4D Panoptic Segmentation"
[7]: https://openaccess.thecvf.com/content/CVPR2024/papers/Min_DriveWorld_4D_Pre-trained_Scene_Understanding_via_World_Models_for_Autonomous_CVPR_2024_paper.pdf "DriveWorld: 4D Pre-trained Scene Understanding via World ..."
[8]: https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02024.pdf "OccWorld: Learning a 3D Occupancy World Model for ..."
[9]: https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Zero-Shot_4D_Lidar_Panoptic_Segmentation_CVPR_2025_paper.pdf "Zero-Shot 4D Lidar Panoptic Segmentation - CVF Open Access"
[10]: https://arxiv.org/html/2510.17664v1 "Streaming 4D Panoptic Segmentation via Dual Threads"
