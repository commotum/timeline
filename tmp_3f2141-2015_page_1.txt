                          Learning to Transduce with Unbounded Memory
                           EdwardGrefenstette    KarlMoritzHermann            MustafaSuleyman
                            Google DeepMind         Google DeepMind            Google DeepMind
                           etg@google.com          kmh@google.com         mustafasul@google.com
                                                         Phil Blunsom
                                              Google DeepMind and Oxford University
                                                   pblunsom@google.com
                                                          Abstract
                              Recently, strong results have been demonstrated by Deep Recurrent Neural Net-
                              works on natural language transduction problems. In this paper we explore the
                              representational power of these models using synthetic grammars designed to ex-
                              hibit phenomena similar to those found in real transduction problems such as ma-
                              chinetranslation. These experiments lead us to propose new memory-based recur-
                              rent networks that implement continuously differentiable analogues of traditional
                              data structures such as Stacks, Queues, and DeQues. We show that these architec-
                              turesexhibitsuperiorgeneralisationperformancetoDeepRNNsandareoftenable
                              to learn the underlying generating algorithms in our transduction experiments.
                      1   Introduction
                      Recurrent neural networks (RNNs) offer a compelling tool for processing natural language input in
                      a straightforward sequential manner. Many natural language processing (NLP) tasks can be viewed
                      as transduction problems, that is learning to convert one string into another. Machine translation is
                      a prototypical example of transduction and recent results indicate that Deep RNNs have the ability
                      to encode long source strings and produce coherent translations [1, 2]. While elegant, the appli-
                      cation of RNNs to transduction tasks requires hidden layers large enough to store representations
                      of the longest strings likely to be encountered, implying wastage on shorter strings and a strong
       arXiv:1506.02516v3  [cs.NE]  3 Nov 2015dependency between the number of parameters in the model and its memory.
                      In this paper we use a number of linguistically-inspired synthetic transduction tasks to explore the
                      ability of RNNs to learn long-range reorderings and substitutions. Further, inspired by prior work on
                      neural network implementations of stack data structures [3], we propose and evaluate transduction
                      models based on Neural Stacks, Queues, and DeQues (double ended queues). Stack algorithms are
                      well-suitedtoprocessingthehierarchicalstructuresobservedinnaturallanguageandwehypothesise
                      that their neural analogues will provide an effective and learnable transduction tool. Our models
                      provide a middle ground between simple RNNs and the recently proposed Neural Turing Machine
                      (NTM) [4] which implements a powerful random access memory with read and write operations.
                      Neural Stacks, Queues, and DeQues also provide a logically unbounded memory while permitting
                      efÔ¨Åcient constant time push and pop operations.
                      Our results indicate that the models proposed in this work, and in particular the Neural DeQue, are
                      able to consistently learn a range of challenging transductions. While Deep RNNs based on long
                          This version of the paper is identical to the version found in the proceedings of Advances in Neural Infor-
                      mationProcessingSystems,2015,withtheadditionofsomemissingreferences. Figureshavebeenmadelarger
                      for increased legibility.
                                                              1
