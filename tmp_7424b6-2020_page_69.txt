               [DGM06] Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment
                      challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classiﬁcation,
                      andrecognising textual entailment, pages 177–190. Springer, 2006.
              [DGV+18] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal
                      transformers. Arxiv, 2018.
              [DHKH14] NadirDurrani,BarryHaddow,PhilippKoehn,andKennethHeaﬁeld. Edinburgh’s phrase-based machine
                      translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation,
                      pages 97–104, 2014.
                [DL15] AndrewM.DaiandQuocV.Le. Semi-supervisedsequencelearning. In Advances in neural information
                      processing systems, 2015.
              [DMST19] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank: Investigat-
                      ing projection in naturally occurring discourse. 2019. To appear in proceedings of Sinn und Bedeutung
                      23. Data can be found at https://github.com/mcdm/CommitmentBank/.
              [DSC+16] Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast
                      reinforcement learning via slow reinforcement learning. ArXiv, abs/1611.02779, 2016.
                  +
             [DWD 19] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
                      Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint
                      arXiv:1903.00161, 2019.
              [DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.
                      Transformer-xl: Attentive language models beyond a ﬁxed-length context. Arxiv, 2019.
              [EOAG18] SergeyEdunov,MyleOtt,MichaelAuli,andDavidGrangier. Understanding back-translation at scale.
                      arXiv preprint arXiv:1808.09381, 2018.
               [FAL17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
                      deep networks. ArXiv, abs/1703.03400, 2017.
                [Fyo00] Yaroslav Fyodorov. A natural logic inference system, 2000.
                [GG19] HilaGonenandYoavGoldberg. Lipstickonapig: Debiasingmethodscoverupsystematicgenderbiases
                      in word embeddings but do not remove them. arXiv preprint arXiv:1903.03862, 2019.
              [GLT+20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-
                      augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020.
             [GMDD07] DaniloGiampiccolo,BernardoMagnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing
                      textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and
                      paraphrasing, pages 1–9. Association for Computational Linguistics, 2007.
                [Gra16] Alex Graves. Adaptive computation time for recurrent neural networks. Arxiv, 2016.
                  +
              [GSL 18] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A
                      Smith. Annotation artifacts in natural language inference data. arXiv preprint arXiv:1803.02324, 2018.
               [GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. Gltr: Statistical detection and visualiza-
                      tion of generated text. arXiv preprint arXiv: 1906.04043, 2019.
                  +
             [GWC 18] Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor OK Li. Meta-learning for low-resource
                      neural machine translation. arXiv preprint arXiv:1808.08437, 2018.
                [HB20] Daniel Hernandez and Tom Brown. Ai and efﬁciency, May 2020.
              [HBFC19] AriHoltzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.
                      CoRR,abs/1904.09751, 2019.
              [HLW+20] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.
                      Pretrained transformers improve out of distribution robustness. arXiv preprint arXiv:2004.06100, 2020.
                                                  69
