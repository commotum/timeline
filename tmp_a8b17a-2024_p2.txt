             Inspired by Kazemnejad et al. (2023), we conduct a thorough investigation of the extrapolation
             problembydesigningaspecificTransformermodelforthispurposeandpresentingdetailedtheoretical
             analysis. To the best of our knowledge, this is the first theoretical endeavor to understand the inner
             workings of extrapolation. We first elucidate the cause of NoPE’s failure when input exceeds the
             effective window length in Theorem 3.1, and analyse PE’s failure in 3.2. Building upon Theorem 3.2,
             weproveinTheorem3.3thatthroughmeticulousweaveposition, (i.e., weave PE), it is feasible to
             achieve extrapolation beyond the effective window length.
             Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based
             triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of
             length 13 is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to
             weavethe relative position in Mesa-Extrapolation.
             Wefurther introduce a novel weave-PE-based method, Mesa-Extrapolation, which demonstrates
             substantial extrapolation prowess without the need for additional training. Specifically, we use
             chunk-based triangular attention matrix, aiming at memory-friendly resource consumption (Figure
             1left), and we employ Stair PE to handle the last chunk (Figure 1 right below), effectively making
             Mesa-Extrapolation a completely free plug-in for LLMs.
             Ourcontributions are summarized as follows:
               1. Theoretical Analysis We provide a comprehensive theoretical analysis on the challenges
                 encountered by NoPE beyond its effective window length, and investigate the effect of PE
                 in this context. Our theorems prove that through meticulous weave position, PE can be
                 effectively extrapolated beyond the effective window length. This theoretical foundation
                 establishes a clear understanding of the extrapolation problem and potential solutions for
                 LLMsutilizing PE.
               2. Introduction of Mesa-Extrapolation We introduce a novel extrapolation approach called
                 "Mesa-Extrapolation." Based on triangular attention matrix, this method strategically orga-
                 nizes input tokens into chunks, with the final chunk employing a weave PE method (e.g.,
                 Stair PE) to integrate all token states. Mesa-Extrapolation provides a practical and effective
                 solution to the extrapolation problem.
               3. EmpiricalValidation Comprehensiveexperimentsdemonstratethatourapproachachieves
                 competitive performance, offering the benefits of extremely low memory usage and the
                 fastest inference speed compared to existing methods. Importantly, it is an easy plug-
                 in and can enhance extrapolation performance of LLMs without any additional resource
                 consumption.
             2 Background
             Since the self-attention mechanism itself does not contain position information, PE components have
             becomeanintegral part of the Transformer architecture. Cmmon choices for PE are either absolute,
             where each absolute position (e.g., 1,2,3,...) is directly represented, or relative, where the distance
             between tokens is used as positional information. Absolute Position Embedding (APE) embeds
                                   2
