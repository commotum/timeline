                                                     Test-Time Learning for Large Language Models
               Chen, G., Niu, S., Chen, D., Zhang, S., Li, C., Li, Y., and    Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,
                 Tan, M. Cross-device collaborative test-time adaptation.        A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,
                 In The Thirty-eighth Annual Conference on Neural Infor-         A., et al. The llama 3 herd of models. arXiv preprint
                 mation Processing Systems, 2024a.                               arXiv:2407.21783, 2024.
               Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O.,   Fan, W., Ding, Y., Ning, L., Wang, S., Li, H., Yin, D.,
                 Kaplan,J.,Edwards,H.,Burda,Y.,Joseph,N.,Brockman,               Chua, T.-S., and Li, Q. A survey on rag meeting llms:
                 G., et al. Evaluating large language models trained on          Towards retrieval-augmented large language models. In
                 code. arXiv preprint arXiv:2107.03374, 2021.                    Proceedings of the 30th ACM SIGKDD Conference on
                                                                                 KnowledgeDiscovery and Data Mining, pp. 6491–6501,
               Chen, Y., Niu, S., Wang, Y., Xu, S., Song, H., and Tan, M.        2024.
                 Towards robust and efficient cloud-edge elastic model        Fleuret, F. et al. Test time adaptation through perturbation
                 adaptation via selective entropy distillation.    In The        robustness. In NeurIPS 2021 Workshop on Distribution
                 Twelfth International Conference on Learning Represen-          Shifts: Connecting Methods and Applications, 2021.
                 tations, 2024b.
               Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and   Gandelsman, Y., Sun, Y., Chen, X., and Efros, A. Test-time
                 Jia, J. Longlora: Efficient fine-tuning of long-context         training with masked autoencoders. Advances in Neural
                 large language models. In The Twelfth International             Information Processing Systems, 35:29374–29385, 2022.
                 Conference on Learning Representations, 2024c.               Gao, J., Zhang, J., Liu, X., Darrell, T., Shelhamer, E., and
                                                                                 Wang,D. Backtothesource: Diffusion-drivenadaptation
               Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,         to test-time corruption. In Proceedings of the IEEE/CVF
                 G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,          Conference on Computer Vision and Pattern Recognition,
                 Gehrmann, S., et al. Palm: Scaling language modeling            pp. 11786–11796, 2023.
                 with pathways. Journal of Machine Learning Research,         Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu,
                 24(240):1–113, 2023.                                            X., Naumann, T., Gao, J., and Poon, H. Domain-specific
               Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fe-        language model pretraining for biomedical natural lan-
                 dus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al.     guage processing. ACM Transactions on Computing for
                 Scaling instruction-finetuned language models. Journal          Healthcare (HEALTH), 3(1):1–23, 2021.
                 of Machine Learning Research, 25(70):1–53, 2024.             Hardt, M. and Sun, Y. Test-time training on nearest neigh-
               Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,         bors for large language models. In The Twelfth Interna-
                 Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,       tional Conference on Learning Representations, 2024.
                 R., et al. Training verifiers to solve math word problems.   He, P., Liu, X., Gao, J., and Chen, W. Deberta: Decoding-
                 arXiv preprint arXiv:2110.14168, 2021.                          enhanced bert with disentangled attention. In Interna-
               Dettmers,T.,Pagnoni,A.,Holtzman,A.,andZettlemoyer,L.              tional Conference on Learning Representations, 2021.
                 Qlora: Efficient finetuning of quantized llms. Advances      Hong, J., Lyu, L., Zhou, J., and Spranger, M.        Mecta:
                 in Neural Information Processing Systems, 36, 2024.             Memory-economiccontinual test-time model adaptation.
                                                                                 In 2023 International Conference on Learning Represen-
               Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:        tations, 2023.
                 Pre-training of deep bidirectional transformers for lan-     Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
                 guage understanding, 2019. URL https://arxiv.                   L., Chen, W., et al. Lora: Low-rank adaptation of large
                 org/abs/1810.04805.                                             language models. In International Conference on Learn-
               Du, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z.,           ing Representations, 2022.
                 and Tang, J. GLM: General language model pretrain-           Hu, J., Wang, Y., Zhang, S., Zhou, K., Chen, G., Hu, Y.,
                 ing with autoregressive blank infilling.    In Muresan,         Xiao, B., and Tan, M. Dynamic ensemble reasoning for
                 S., Nakov, P., and Villavicencio, A. (eds.), Proceed-           llm experts. Proceedings of the Thirty-Fourth Interna-
                 ings of the 60th Annual Meeting of the Association for          tional Joint Conference on Artificial Intelligence, 2025a.
                 Computational Linguistics (Volume 1: Long Papers),
                 pp. 320–335, Dublin, Ireland, May 2022. Association          Hu, J., Zhang, W., Wang, Y., Hu, Y., Xiao, B., Tan, M.,
                 for Computational Linguistics. doi: 10.18653/v1/2022.           and Du, Q. Dynamic compressing prompts for effi-
                 acl-long.26. URL https://aclanthology.org/                      cient inference of large language models. arXiv preprint
                 2022.acl-long.26/.                                              arXiv:2504.11004, 2025b.
                                                                           10
