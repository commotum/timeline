- **Feedforward and Recurrent Processing in Vision** (2000)
  **Contribution Type:** neuroscience perspective on visual computation.
  **Reason:** analyze complementary roles of feedforward and recurrent processing in visual perception. ([PubMed][38-1])
- **The Hidden Logic of Sudoku (Second Edition)** (2007)
  **Contribution Type:** reasoning and constraint-satisfaction exposition.
  **Reason:** formalize logical structures and strategies underlying Sudoku as a model reasoning system. ([Book][38-2])
- **Core Knowledge** (2007)
  **Contribution Type:** cognitive science foundations.
  **Reason:** propose innate core knowledge systems shaping human cognition and learning. ([Harvard LDS][38-3])
- **Canonical Microcircuits for Predictive Coding** (2012)
  **Contribution Type:** neuroscience theory.
  **Reason:** describe cortical microcircuits implementing predictive coding principles. ([PubMed][38-4])
- **Hierarchy of Intrinsic Timescales in Cortex** (2014)
  **Contribution Type:** neuroscience systems analysis.
  **Reason:** identify hierarchical temporal processing scales across cortical regions. ([NYU][38-5])
- **Neural Turing Machines** (2014)
  **Missing capability:** persistent, addressable memory in neural networks.
  **Mechanism:** differentiable read/write operations over an external memory matrix. ([arXiv][38-6])
- **Adaptive Computation Time** (2016)
  **Missing capability:** variable-depth computation per input.
  **Mechanism:** learned halting mechanism allowing networks to adapt computation steps dynamically. ([arXiv][38-7])
- **Attention Is All You Need** (2017)
  **Contribution Type:** architectural unification.
  **Reason:** introduce the Transformer, replacing recurrence and convolution with attention mechanisms. ([arXiv][38-8])
- **Recurrent Relational Networks** (2018)
  **Missing capability:** iterative relational reasoning.
  **Mechanism:** recurrent message passing over relational structures for constraint satisfaction. ([NeurIPS Proceedings][38-9])
- **Universal Transformers** (2018)
  **Missing capability:** iterative refinement with shared parameters.
  **Mechanism:** recurrent application of a Transformer block with adaptive depth. ([arXiv][38-10])
- **Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset** (2018)
  **Target Domain:** vision–language pretraining.
  **Resource:** large-scale cleaned and hypernymed image–text dataset for VLM training. ([arXiv][38-11])
- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (2018)
  **Contribution Type:** pretraining methodology.
  **Reason:** introduce masked language modeling and bidirectional Transformer pretraining. ([arXiv][38-12])
- **LXMERT: Learning Cross-Modality Encoder Representations** (2019)
  **Contribution Type:** multimodal Transformer architecture.
  **Reason:** dual-stream cross-attention model for vision–language reasoning tasks. ([arXiv][38-13])
- **ViLBERT: Pretraining Task-Agnostic Vision-and-Language Representations** (2019)
  **Contribution Type:** multimodal pretraining framework.
  **Reason:** two-stream Transformer with co-attention for transferable V–L representations. ([arXiv][38-14])
- **VisualBERT: A Simple and Performant Baseline for Vision and Language** (2019)
  **Contribution Type:** unified multimodal baseline.
  **Reason:** single-stream Transformer over image regions and text tokens for V–L tasks. ([arXiv][38-15])
- **Deep Equilibrium Models** (2019)
  **Missing capability:** infinite-depth/implicit computation.
  **Mechanism:** define models by fixed points of implicit layers trained via equilibrium solvers. ([arXiv][38-16])
- **VideoBERT: A Joint Model for Video and Language Representation Learning** (2019)
  **Contribution Type:** video–language pretraining.
  **Reason:** learn joint representations by tokenizing video and text sequences. ([arXiv][38-17])
- **HowTo100M: Learning a Text-Video Embedding by Watching Narrated Videos** (2019)
  **Target Domain:** large-scale video–language learning.
  **Resource:** 100M narrated video clips for weakly supervised V–L embedding learning. ([arXiv][38-18])
- **On the Measure of Intelligence** (2019)
  **Target Domain:** abstract and spatial reasoning.
  **Resource:** Abstraction and Reasoning Corpus (ARC) as a benchmark for general intelligence. ([arXiv][38-19])
- **UNITER: Universal Image-Text Representation Learning** (2020)
  **Contribution Type:** unified VLM pretraining.
  **Reason:** single-stream Transformer with multiple pretraining objectives for strong V–L transfer. ([arXiv][38-20])
- **OSCAR: Object-Semantics Aligned Pre-training for Vision-Language Tasks** (2020)
  **Contribution Type:** VLM pretraining with object tags.
  **Reason:** align object-level semantics with text to improve cross-modal grounding. ([arXiv][38-21])
- **End-to-End Object Detection with Transformers (DETR)** (2020)
  **Improves On:** CNN-based object detection pipelines.
  **Adaptation:** global attention with set prediction and bipartite matching loss for detection. ([arXiv][38-22])
- **An Image is Worth 16×16 Words: Vision Transformer (ViT)** (2020)
  **Improves On:** CNN-based image classification.
  **Adaptation:** tokenize images into patches and process with a standard Transformer encoder. ([arXiv][38-23])
- **Deformable DETR: Deformable Transformers for End-to-End Object Detection** (2020)
  **Improves On:** DETR’s global attention inefficiency.
  **Adaptation:** sparse, multi-scale deformable attention for efficient detection. ([arXiv][38-24])
- **Training data-efficient image transformers & distillation through attention** (2020)
  **Contribution Type:** training methodology for ViTs.
  **Reason:** use distillation tokens and teacher supervision to improve data efficiency. ([arXiv][38-25])

[38-1]: https://pubmed.ncbi.nlm.nih.gov/10925037/ "Feedforward and Recurrent Processing in Vision (2000) — PubMed"
[38-2]: # "The Hidden Logic of Sudoku (2007) — Book (local copy)"
[38-3]: https://www.harvardlds.org/wp-content/uploads/2017/01/SpelkeKinzler07-1.pdf "Core Knowledge (2007) — Harvard LDS"
[38-4]: https://pubmed.ncbi.nlm.nih.gov/23238495/ "Canonical Microcircuits for Predictive Coding (2012) — PubMed"
[38-5]: https://www.cns.nyu.edu/wanglab/publications/pdf/murray.nn2014.pdf "Hierarchy of Intrinsic Timescales in Cortex (2014) — NYU"
[38-6]: https://arxiv.org/pdf/1410.5401.pdf "Neural Turing Machines (2014) — arXiv"
[38-7]: https://arxiv.org/pdf/1603.08983.pdf "Adaptive Computation Time (2016) — arXiv"
[38-8]: https://arxiv.org/pdf/1706.03762.pdf "Attention Is All You Need (2017) — arXiv"
[38-9]: https://proceedings.neurips.cc/paper_files/paper/2018/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf "Recurrent Relational Networks (2018) — NeurIPS"
[38-10]: https://arxiv.org/pdf/1807.03819.pdf "Universal Transformers (2018) — arXiv"
[38-11]: https://arxiv.org/pdf/1803.10137.pdf "Conceptual Captions (2018) — arXiv"
[38-12]: https://arxiv.org/pdf/1810.04805.pdf "BERT (2018) — arXiv"
[38-13]: https://arxiv.org/pdf/1908.07490.pdf "LXMERT (2019) — arXiv"
[38-14]: https://arxiv.org/pdf/1908.02265.pdf "ViLBERT (2019) — arXiv"
[38-15]: https://arxiv.org/pdf/1908.03557.pdf "VisualBERT (2019) — arXiv"
[38-16]: https://arxiv.org/pdf/1909.01377.pdf "Deep Equilibrium Models (2019) — arXiv"
[38-17]: https://arxiv.org/pdf/1904.01766.pdf "VideoBERT (2019) — arXiv"
[38-18]: https://arxiv.org/pdf/1906.03327.pdf "HowTo100M (2019) — arXiv"
[38-19]: https://arxiv.org/pdf/1911.01547.pdf "On the Measure of Intelligence (2019) — arXiv"
[38-20]: https://arxiv.org/pdf/1909.11740.pdf "UNITER (2020) — arXiv"
[38-21]: https://arxiv.org/pdf/2004.06165.pdf "OSCAR (2020) — arXiv"
[38-22]: https://arxiv.org/pdf/2005.12872.pdf "DETR (2020) — arXiv"
[38-23]: https://arxiv.org/pdf/2010.11929.pdf "Vision Transformer (ViT) (2020) — arXiv"
[38-24]: https://arxiv.org/pdf/2010.04159.pdf "Deformable DETR (2020) — arXiv"
[38-25]: https://arxiv.org/pdf/2012.12877.pdf "DeiT (2020) — arXiv"
