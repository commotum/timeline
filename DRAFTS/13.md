## 2014 — External, persistent memory as a first-class compute primitive

* **Neural Turing Machines (NTM)** (Graves et al., 2014)
  **Missing capability:** persistent, addressable memory + algorithmic state beyond a fixed hidden vector
  **Mechanism:** differentiable **read/write heads** over an external memory matrix (end-to-end trainable “computer-like” inference) ([arXiv][1])

---

## 2015 — Multi-hop “pondering”, structured memory (stacks), and program induction

* **End-To-End Memory Networks (MemN2N)** (Sukhbaatar et al., 2015)
  **Missing capability:** multi-step retrieval / multi-hop reasoning rather than single-pass inference
  **Mechanism:** differentiable **external memory** + repeated **attention “hops”** before producing an answer ([arXiv][2])

* **Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets** (Joulin & Mikolov, 2015)
  **Missing capability:** counting + hierarchical/algorithmic behavior hard for standard RNNs
  **Mechanism:** attach a differentiable **stack data structure** to a recurrent controller ([arXiv][3])

* **Learning to Transduce with Unbounded Memory (Neural Stack / Queue / DeQue)** (Grefenstette et al., 2015)
  **Missing capability:** unbounded structured memory for transduction-like tasks
  **Mechanism:** differentiable **stack/queue/deque** operations integrated into the model’s computation loop ([arXiv][4])

* **Neural Programmer-Interpreters (NPI)** (Reed & de Freitas, 2015)
  **Missing capability:** program induction + compositional execution with reusable subroutines
  **Mechanism:** a recurrent core + persistent **program memory** that **calls subprograms** (learned program execution) ([arXiv][5])

* **Neural Programmer: Inducing Latent Programs with Gradient Descent** (Neelakantan et al., 2015)
  **Missing capability:** arithmetic/logic operations that pure sequence models struggle to learn
  **Mechanism:** differentiable selection/composition of **built-in operators** across multiple steps (latent program execution) ([arXiv][6])

* **Neural GPUs Learn Algorithms** (Kaiser & Sutskever, 2015)
  **Missing capability:** learning algorithms that generalize to much longer inputs than training
  **Mechanism:** a highly-parallel **convolutional recurrent** computation grid that performs iterative algorithmic updates ([arXiv][7])

---

## 2016 — Variable compute (learned halting) and richer external memory

* **Adaptive Computation Time (ACT)** (Graves, 2016)
  **Missing capability:** variable depth / “think longer on hard cases”
  **Mechanism:** differentiable **halting** that learns how many internal steps to run per input/time step ([arXiv][8])

* **Differentiable Neural Computer (DNC)** (Graves et al., 2016)
  **Missing capability:** learning and manipulating data structures (graphs, lists) with long-term memory
  **Mechanism:** NTM-like external memory + differentiable **allocation and temporal linkage** for structured reads/writes ([Nature][9])

* **Sparse Differentiable Neural Computer (SDNC)** (Rae et al., 2016)
  **Missing capability:** scaling memory-augmented computation to larger memories
  **Mechanism:** sparse/differentiable approximations enabling large external memory access efficiently ([arXiv][10])

* **Neural Module Networks (NMN)** (Andreas et al., 2016)
  **Missing capability:** compositional reasoning pipelines (dynamic per-question computation graphs)
  **Mechanism:** dynamically assemble a network from reusable **modules** conditioned on input structure ([CVF Open Access][11])

---

## 2017 — Differentiable theorem proving and “memory access as an algorithm”

* **End-to-End Differentiable Proving (Neural Theorem Provers)** (Rocktäschel & Riedel, 2017)
  **Missing capability:** symbolic-style multi-hop logical inference (backward chaining) in neural form
  **Mechanism:** recursively construct computation inspired by **Prolog-style backward chaining** with differentiable unification ([arXiv][12])

* **Lie-Access Neural Turing Machines (LANTM)** (Yang, 2016/2017)
  **Missing capability:** flexible memory addressing beyond standard content-based lookup
  **Mechanism:** alternative differentiable **memory access geometry** (Lie group actions) for algorithmic tasks ([OpenReview][13])

---

## 2018 — Iteration as a core primitive: recurrent relational inference + recurrent Transformers

* **Recurrent Relational Networks (RRN)** (Palm et al., 2018)
  **Missing capability:** iterative constraint satisfaction / multi-step relational inference
  **Mechanism:** recurrent **message passing** over a relational graph for many inference steps ([NeurIPS Papers][14])

* **Universal Transformers (UT)** (Dehghani et al., 2018)
  **Missing capability:** iterative refinement per token position (beyond a fixed-depth feedforward stack)
  **Mechanism:** apply a **recurrent** transformer block repeatedly (often with ACT-style halting) to revise representations ([arXiv][15])

* **Neural Ordinary Differential Equations (Neural ODEs)** (Chen et al., 2018)
  **Missing capability:** continuous-depth / adaptive computation tied to solver precision
  **Mechanism:** replace discrete layers with an ODE-defined hidden-state dynamics solved by a **black-box ODE solver** ([arXiv][16])

---

## 2019 — Differentiable solvers, implicit depth, and adaptive attention compute

* **SATNet: Differentiable Satisfiability Solver** (Wang et al., 2019)
  **Missing capability:** constraint solving (e.g., Sudoku) that standard nets struggle with
  **Mechanism:** integrate a differentiable (smoothed) **MAXSAT solver** into end-to-end learning ([arXiv][17])

* **Deep Equilibrium Models (DEQ)** (Bai et al., 2019)
  **Missing capability:** effectively infinite-depth computation / equilibrium dynamics with constant memory
  **Mechanism:** define the network by a **fixed point** and compute it via root-finding; backprop via implicit differentiation ([arXiv][18])

* **Adaptive Attention Span in Transformers** (Sukhbaatar et al., 2019)
  **Missing capability:** dynamic allocation of compute/memory over long contexts
  **Mechanism:** each head learns its **effective attention span** (soft mask) to trade accuracy vs compute ([arXiv][19])

---

## 2020 — Retrieval and modular “knowledge access” as computation (not weights)

* **REALM: Retrieval-Augmented Language Model Pre-Training** (Guu et al., 2020)
  **Missing capability:** updatable knowledge without baking everything into parameters
  **Mechanism:** learned **retriever + reader**; retrieval is in the computation loop during pretrain and inference ([arXiv][20])

* **Depth-Adaptive Transformers (adaptive layer usage)** (Elbayad et al., 2020)
  **Missing capability:** variable compute at inference time for easy vs hard inputs
  **Mechanism:** dynamically decide how many transformer layers to execute per input (ACT-inspired) ([Jiatao Gu][21])

---

## 2021 — Better learned halting and explicit “think longer” objectives

* **PonderNet: Learning to Ponder** (Banino et al., 2021)
  **Missing capability:** stable, learnable variable compute / iterative refinement
  **Mechanism:** probabilistic halting over repeated computation steps with an explicit tradeoff between accuracy and compute ([arXiv][22])

---

## 2022 — Test-time memory and tool-interacting computation loops

* **Memorizing Transformers** (Wu et al., 2022)
  **Missing capability:** acquire new facts at inference time without weight updates
  **Mechanism:** approximate **kNN retrieval** over stored key/value representations inside a transformer layer ([arXiv][23])

* **ReAct: Synergizing Reasoning and Acting in Language Models** (Yao et al., 2022/2023)
  **Missing capability:** grounded multi-step problem solving that can query the world / reduce hallucinations
  **Mechanism:** interleave **reasoning traces** with **actions** (tool/API calls) as part of the inference procedure ([arXiv][24])

---

## 2023 — Retrieval at scale + explicit search over “thoughts” + self-taught tool use

* **Toolformer: Language Models Can Teach Themselves to Use Tools** (Schick et al., 2023)
  **Missing capability:** reliable arithmetic/lookup/planning subskills via external tools
  **Mechanism:** self-supervised training that inserts/filters **API calls** to improve future token prediction ([arXiv][25])

* **Tree of Thoughts (ToT)** (Yao et al., 2023)
  **Missing capability:** exploration, lookahead, backtracking—beyond left-to-right greedy decoding
  **Mechanism:** explicit **tree search** over intermediate “thought” states, with self-evaluation and backtracking ([arXiv][26])

---

## 2024 — (This category explodes; here are the “mechanism-first” directions that clearly match)

* **RETRO-style retrieval-augmented pretraining and variants** (RETRO originally 2021; many 2024 follow-ons compare/extend the compute pattern) — **included for continuity**
  **Missing capability:** parameter-efficient scaling via external memory and cross-attention to retrieved chunks
  **Mechanism:** retrieval + chunked cross-attention in the forward pass ([arXiv][27])

---

## 2025 — Continued “adaptive thinking” / dynamic depth research threads (mechanism framing)

* **Adaptive Thinking Using Dynamic Computation** (Mathur et al., ICLR 2025)
  **Missing capability:** allocate “thinking steps” dynamically (variable compute)
  **Mechanism:** dynamic computation policies building on halting/dynamic-depth ideas (ACT/PonderNet lineage) ([ICLR Proceedings][28])

---

### Notes (so you can keep the taxonomy clean)

* I **excluded** papers whose main contribution is *positional encoding* or *lifting Transformers into 2D/3D/4D* (per your rules), even if they mention “reasoning.”
* I **included** “prompting-as-algorithm” papers (ReAct, ToT) because their core claim is explicitly about **standard feedforward decoding missing search/interaction**, and their contribution is a **new inference-time computation procedure** rather than a new dataset prompt trick. ([arXiv][26])

If you want the next round in the same timeline style: I can do an **aggressive sweep of 2019–2025** for “**test-time compute / search / planning**” and “**implicit layers / equilibrium / solver-in-the-loop**” papers that are easy to miss because they’re marketed as *efficiency*, *alignment*, or *agents* rather than “computation mechanisms.”

[1]: https://arxiv.org/abs/1410.5401 "Neural Turing Machines"
[2]: https://arxiv.org/abs/1503.08895 "End-To-End Memory Networks"
[3]: https://arxiv.org/abs/1503.01007 "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"
[4]: https://arxiv.org/abs/1506.02516 "Learning to Transduce with Unbounded Memory"
[5]: https://arxiv.org/abs/1511.06279 "Neural Programmer-Interpreters"
[6]: https://arxiv.org/abs/1511.04834 "Neural Programmer: Inducing Latent Programs with Gradient Descent"
[7]: https://arxiv.org/abs/1511.08228 "Neural GPUs Learn Algorithms"
[8]: https://arxiv.org/abs/1603.08983 "Adaptive Computation Time for Recurrent Neural Networks"
[9]: https://www.nature.com/articles/nature20101 "Hybrid computing using a neural network with dynamic ..."
[10]: https://arxiv.org/pdf/1610.09027 "Scaling Memory-Augmented Neural Networks with Sparse ..."
[11]: https://openaccess.thecvf.com/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html "Neural Module Networks - CVPR 2016 Open Access Repository"
[12]: https://arxiv.org/abs/1705.11040 "End-to-End Differentiable Proving"
[13]: https://openreview.net/pdf?id=Byiy-Pqlx&utm_source=chatgpt.com "LIE-ACCESS NEURAL TURING MACHINES"
[14]: https://papers.neurips.cc/paper/7597-recurrent-relational-networks.pdf "Recurrent Relational Networks"
[15]: https://arxiv.org/abs/1807.03819 "[1807.03819] Universal Transformers"
[16]: https://arxiv.org/abs/1806.07366 "Neural Ordinary Differential Equations"
[17]: https://arxiv.org/abs/1905.12149 "SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver"
[18]: https://arxiv.org/abs/1909.01377 "Deep Equilibrium Models"
[19]: https://arxiv.org/abs/1905.07799 "[1905.07799] Adaptive Attention Span in Transformers"
[20]: https://arxiv.org/abs/2002.08909 "REALM: Retrieval-Augmented Language Model Pre-Training"
[21]: https://jiataogu.me/papers/elbayad2020depth.pdf "arXiv:1910.10073v4 [cs.CL] 14 Feb 2020"
[22]: https://arxiv.org/abs/2107.05407 "[2107.05407] PonderNet: Learning to Ponder"
[23]: https://arxiv.org/abs/2203.08913 "Memorizing Transformers"
[24]: https://arxiv.org/abs/2210.03629 "ReAct: Synergizing Reasoning and Acting in Language Models"
[25]: https://arxiv.org/abs/2302.04761 "Toolformer: Language Models Can Teach Themselves to Use Tools"
[26]: https://arxiv.org/abs/2305.10601 "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
[27]: https://arxiv.org/abs/2112.04426 "Improving language models by retrieving from trillions of tokens"
[28]: https://proceedings.iclr.cc/paper_files/paper/2025/file/955499a8e2860ed746717c1374224c43-Paper-Conference.pdf "ADAPTIVE THINKING USING DYNAMIC COMPUTATION"
