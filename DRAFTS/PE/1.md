Below is a best-effort **timeline** of papers that match your “**positional encoding improvement proposal**” classification (Transformer attention as the core setting; paper explicitly critiques prior PE/RPE; core contribution is a **change to positional encoding / position injection**, not “just add more dims”). Where something is **borderline** (e.g., PE is important but not the paper’s main thrust), I mark it.

---

## 2018 — Relative position inside attention (the modern “RPE” lineage)

* **Self-Attention with Relative Position Representations** (Shaw, Uszkoreit, Vaswani, 2018)
  **Critique:** absolute position added to inputs is an awkward fit for attention’s permutation-invariance; poor inductive bias for distance.
  **Improvement:** inject **relative distance embeddings** directly into attention (key/value side). ([arXiv][1])

---

## 2019 — Relative PE designed for long-context reuse

* **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context** (Dai et al., 2019)
  **Critique:** absolute PE breaks “state reuse” across segments and causes temporal confusion in recurrence/memory.
  **Improvement:** a **relative positional encoding formulation** compatible with segment-level recurrence for long-context LM. ([arXiv][2])

---

## 2020 — “Content vs position” disentangling / untying

* **DeBERTa: Decoding-enhanced BERT with Disentangled Attention** (He et al., 2020)
  **Critique:** standard position injection entangles content/position too early and too tightly.
  **Improvement:** **disentangled attention** that separately models content and (relative) position in attention scoring. ([arXiv][3])

* **Rethinking Positional Encoding in Language Pre-training (TUPE)** (Ke et al., ICLR-era OpenReview, ~2020/2021)
  **Critique:** mixing token and position correlations (and treating `[CLS]` position like ordinary tokens) is noisy / suboptimal.
  **Improvement:** **untie** positional and token projections; separate positional correlation path (TUPE variants). ([OpenReview][4])

---

## 2021 — Big year: RoPE, bias-only PE, and vision-focused RPE

* **RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE)** (Su et al., 2021)
  **Critique:** absolute PE is inflexible; many relative schemes don’t mesh cleanly with some attention variants.
  **Improvement:** **rotary** position embedding that makes relative offsets emerge naturally in dot-products. ([arXiv][5])

* **Train Short, Test Long: Attention with Linear Biases (ALiBi)** (Press, Smith, Lewis, 2021)
  **Critique:** common PEs extrapolate poorly to longer lengths than trained.
  **Improvement:** remove token-additive PE; add a **distance-proportional linear bias** directly to attention logits. ([arXiv][6])

* **Rethinking and Improving Relative Position Encoding for Vision Transformer (iRPE)** (Wu et al., ICCV 2021)
  **Critique:** “RPE works well in NLP” but is **unclear/controversial in vision**; existing methods have tradeoffs when flattened to 2D images.
  **Improvement:** **2D image-aware RPE variants** (directional distance modeling + attention interaction design). ([arXiv][7])

* **A Simple and Effective Positional Encoding for Transformers** (Chen et al., EMNLP 2021)
  **Critique:** absolute PE doesn’t directly express relative relations; prior RPE designs have practical issues.
  **Improvement:** proposes a simplified, effective PE/RPE scheme (language setting). ([ACL Anthology][8])

* **Swin Transformer (relative position bias)** (Liu et al., 2021) — **borderline** (PE isn’t the main thesis, but it is a deliberate PE choice)
  **Critique:** absolute position embeddings in ViTs are less suited to shifted-window attention and spatial generalization.
  **Improvement:** **learnable relative position bias** within windows. ([arXiv][9])

---

## 2022 — Formalizing extrapolatable relative PE + fixing RoPE drift

* **KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation** (Chi et al., NeurIPS 2022)
  **Critique:** existing RPE variants extrapolate inconsistently and lack a unifying principle.
  **Improvement:** **kernelize** positional differences via CPD/PD kernel framework to derive extrapolatable RPEs. ([arXiv][10])

* **XPos / “Length-Extrapolatable Transformer” line (RoPE + decay)** (Sun et al., circulated 2022; widely cited 2023)
  **Critique:** RoPE can become unstable / degrade for long extrapolation.
  **Improvement:** add an **exponential decay** to RoPE rotation (XPos) to stabilize long-range behavior. ([arXiv][11])

---

## 2023 — Context extension via “position remapping” and RoPE scaling

* **Extending Context Window of Large Language Models via Positional Interpolation (PI)** (Chen et al., 2023)
  **Critique:** direct extrapolation past trained context can blow up attention scores and fail catastrophically.
  **Improvement:** **down-scale/interpolate position indices** so inference stays in-range for RoPE; minimal finetune. ([arXiv][12])

* **YaRN: Efficient Context Window Extension of Large Language Models** (Peng et al., 2023 preprint; later ICLR)
  **Critique:** RoPE models fail to generalize beyond training length; existing extension methods are compute-hungry.
  **Improvement:** **frequency-aware RoPE interpolation** + related scaling tricks for efficient long-context finetuning. ([arXiv][13])

* **A Length-Extrapolatable Transformer** (Sun et al., ACL 2023)
  **Critique:** length extrapolation is tied to attention’s positional “resolution.”
  **Improvement:** design changes including **relative position embedding** targeted at improving extrapolation indicators. ([ACL Anthology][14])

---

## 2024 — Refining RoPE extension + vision-specific rotary fixes + directed-attention PE replacements

* **Resonance RoPE: Improving Context Length Generalization of Large Language Models** (Wang et al., 2024)
  **Critique:** RoPE interpolation/remapping still leaves an OOD gap for positions.
  **Improvement:** refine RoPE feature interpolation for OOD positions (“resonance” shaping). ([arXiv][15])

* **Rotary Position Embedding for Vision Transformer (RoPE-Mixed / improved 2D RoPE)** (Heo et al., ECCV 2024)
  **Critique:** common **axial 2D RoPE** misses diagonal structure important in vision.
  **Improvement:** **mixed-axis frequency** 2D RoPE variant for images. ([ECVA][16])

* **LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate** (Fuller et al., NeurIPS 2024)
  **Critique:** standard patch position encodings cause a distribution shift when changing patch counts / resolution.
  **Improvement:** “drop-in replacement” position handling using **directed attention heads + 2D masks / distance-penalized bias** to improve extrapolation. ([arXiv][17])

* **(RoPE scaling analysis + new scaling)** EMNLP 2024 paper on RoPE scaling methods (Wu et al., 2024)
  **Critique:** many RoPE scaling methods are empirical and poorly grounded in RoPE’s internal distribution.
  **Improvement:** proposes a more principled scaling approach (within the RoPE-scaling family). ([ACL Anthology][18])

* **Exploring Context Window of LLMs via … (NTK-aware / scaled RoPE discussion)** (Dong et al., NeurIPS 2024) — **borderline** (more analysis-heavy, but includes PE scaling focus)
  ([NeurIPS Proceedings][19])

---

## 2025 — Explicit “what vs where” separation + context-aware biases

* **Decoupling the “What” and “Where” With Polar Coordinate Positional Embeddings (PoPE)** (Gopalakrishnan et al., 2025)
  **Critique:** RoPE entangles content (“what”) and position (“where”), harming tasks needing independent matches.
  **Improvement:** **PoPE** polar-coordinate positional embedding removing the confound; improves zero-shot length extrapolation. ([arXiv][20])

* **Context-aware Biases for Length Extrapolation (CABLE)** (Veisi et al., EMNLP 2025)
  **Critique:** fixed-form biases/PEs can be too rigid for long-context generalization.
  **Improvement:** learns **context-aware additive relative positional biases**. ([ACL Anthology][21])

---

### References

[1]: https://arxiv.org/abs/1803.02155?utm_source=chatgpt.com "Self-Attention with Relative Position Representations"
[2]: https://arxiv.org/abs/1901.02860?utm_source=chatgpt.com "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
[3]: https://arxiv.org/abs/2006.03654?utm_source=chatgpt.com "DeBERTa: Decoding-enhanced BERT with Disentangled ..."
[4]: https://openreview.net/pdf?id=09-528y2Fgf&utm_source=chatgpt.com "RETHINKING POSITIONAL ENCODING IN LANGUAGE ..."
[5]: https://arxiv.org/abs/2104.09864?utm_source=chatgpt.com "RoFormer: Enhanced Transformer with Rotary Position Embedding"
[6]: https://arxiv.org/abs/2108.12409?utm_source=chatgpt.com "Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation"
[7]: https://arxiv.org/abs/2107.14222?utm_source=chatgpt.com "Rethinking and Improving Relative Position Encoding for Vision Transformer"
[8]: https://aclanthology.org/2021.emnlp-main.236.pdf?utm_source=chatgpt.com "A Simple and Effective Positional Encoding for Transformers"
[9]: https://arxiv.org/pdf/2103.14030?utm_source=chatgpt.com "arXiv:2103.14030v2 [cs.CV] 17 Aug 2021"
[10]: https://arxiv.org/abs/2205.09921?utm_source=chatgpt.com "KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"
[11]: https://arxiv.org/pdf/2212.10554?utm_source=chatgpt.com "arXiv:2212.10554v1 [cs.CL] 20 Dec 2022"
[12]: https://arxiv.org/abs/2306.15595?utm_source=chatgpt.com "Extending Context Window of Large Language Models via Positional Interpolation"
[13]: https://arxiv.org/abs/2309.00071?utm_source=chatgpt.com "YaRN: Efficient Context Window Extension of Large Language Models"
[14]: https://aclanthology.org/2023.acl-long.816.pdf?utm_source=chatgpt.com "A Length-Extrapolatable Transformer"
[15]: https://arxiv.org/abs/2403.00071?utm_source=chatgpt.com "[2403.00071] Resonance RoPE: Improving Context Length ..."
[16]: https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01584.pdf?utm_source=chatgpt.com "Rotary Position Embedding for Vision Transformer"
[17]: https://arxiv.org/abs/2405.13985?utm_source=chatgpt.com "LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate"
[18]: https://aclanthology.org/2024.emnlp-main.414.pdf?utm_source=chatgpt.com "Extending Context Window of Large Language Models ..."
[19]: https://proceedings.neurips.cc/paper_files/paper/2024/file/1403ab1a427050538ec59c7f570aec8b-Paper-Conference.pdf?utm_source=chatgpt.com "Exploring Context Window of Large Language Models via ..."
[20]: https://arxiv.org/abs/2509.10534?utm_source=chatgpt.com "Decoupling the \"What\" and \"Where\" With Polar Coordinate Positional Embeddings"
[21]: https://aclanthology.org/2025.emnlp-main.1545.pdf?utm_source=chatgpt.com "Context-aware Biases for Length Extrapolation"
