## 2017 — The baseline problem statement: attention needs position

* **Attention Is All You Need** (Vaswani et al., 2017)
  **Critique:** self-attention is permutation-invariant; you must inject order information.
  **Improvement:** introduces the original Transformer + **sinusoidal absolute positional embeddings** as a default solution.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1706.03762
  PDF:   https://arxiv.org/pdf/1706.03762
  ```

---

## 2018 — “Relative position inside attention” (and an application domain where it matters)

* **Self-Attention with Relative Position Representations** (Shaw, Uszkoreit, Vaswani, 2018)
  **Critique:** adding absolute position to inputs is not the only way; attention scoring can directly use relative distances.
  **Improvement:** classic **relative position representations** integrated into attention (key/value modifications).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1803.02155
  PDF:   https://arxiv.org/pdf/1803.02155
  ```

* **Music Transformer** (Huang et al., 2018)
  **Critique:** long-range structure (music) stresses naive absolute position schemes.
  **Improvement:** uses a relative-position approach tailored for music sequence modeling (a motivating example in the RoPE post).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1809.04281
  PDF:   https://arxiv.org/pdf/1809.04281
  ```

---

## 2019 — Learned relative-position bias as a “matrix-level” mechanism

* **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)** (Raffel et al., 2019)
  **Critique:** positional effects can be simplified; learn position influence as a bias term.
  **Improvement:** **relative positional bias** added to attention logits (with bucketing in the implementation).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1910.10683
  PDF:   https://arxiv.org/pdf/1910.10683
  ```

---

## 2020 — The “efficient attention” pressure test + common long-context training realities

* **Language Models are Few-Shot Learners (GPT-3)** (Brown et al., 2020)
  **Critique:** scaling exposes practical issues in positional handling and long-context training choices.
  **Improvement:** large-scale autoregressive LM baseline using **learned absolute positional embeddings** in practice (as compared against RoPE in the post).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2005.14165
  PDF:   https://arxiv.org/pdf/2005.14165
  ```

* **Rethinking Attention with Performers** (Choromanski et al., 2020)
  **Critique:** full softmax attention is quadratic; efficient attention variants often can’t use methods that require building the full N×N attention matrix.
  **Improvement:** **Performer** (FAVOR+) kernelized attention to approximate softmax efficiently.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2009.14794
  PDF:   https://arxiv.org/pdf/2009.14794
  ```

* **Rethinking Positional Encoding in Language Pre-training (TUPE)** (Ke et al., 2020)
  **Critique:** standard “token + position add” entangles correlations in ways that can be suboptimal.
  **Improvement:** **untied / decoupled** treatment of positional effects in attention.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2006.15595
  PDF:   https://arxiv.org/pdf/2006.15595
  ```

* **Shortformer: Better Language Modeling using Shorter Inputs** (Press, Smith, Lewis, 2020)
  **Critique:** standard absolute position injection can interact poorly with training setups / long contexts; look for alternatives that behave better.
  **Improvement:** modifies *where/how* position information is introduced (position-aware design aimed at better LM behavior under constrained contexts).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2012.15832
  PDF:   https://arxiv.org/pdf/2012.15832
  ```

* **Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision** (Tan, Bansal, 2020)
  **Critique:** standard LM supervision can be limited; also cited in the RoPE post in the context of common preprocessing/training practices around contexts.
  **Improvement:** adds **visual grounding supervision** (“vokens”) to language learning.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2010.06775
  PDF:   https://arxiv.org/pdf/2010.06775
  ```

---

## 2021 — RoPE enters as a “works in vanilla + efficient attention” positional mechanism

* **The Pile: An 800GB Dataset of Diverse Text for Language Modeling** (Gao et al., 2021)
  **Critique:** evaluations of PE methods should be stress-tested on large, diverse corpora.
  **Improvement:** provides a large-scale dataset used in the post’s billion-parameter experiments.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2101.00027
  PDF:   https://arxiv.org/pdf/2101.00027
  ```

* **Do Transformer Modifications Transfer Across Implementations and Applications?** (Narang et al., 2021)
  **Critique:** many “improvements” to Transformers don’t transfer robustly across codebases/tasks.
  **Improvement:** systematic transferability study used in the post as motivation for focusing on robust architectural changes (like RoPE).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2102.11972
  PDF:   https://arxiv.org/pdf/2102.11972
  ```

* **Learning Transferable Visual Models From Natural Language Supervision (CLIP)** (Radford et al., 2021)
  **Critique:** (as invoked in the post’s context list) large-scale training pipelines often reshape how sequences/contexts are packed and consumed.
  **Improvement:** contrastive vision–language pretraining at scale (cited as an example of large-scale practice).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2103.00020
  PDF:   https://arxiv.org/pdf/2103.00020
  ```

* **Efficient Large-Scale Language Model Training on GPU Clusters** (Narayanan et al., 2021)
  **Critique:** scaling LMs introduces system constraints that influence sequence handling (and thus positional schemes in practice).
  **Improvement:** large-scale distributed training methods (pipeline/tensor/data parallelism) supporting very large Transformers.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2104.04473
  PDF:   https://arxiv.org/pdf/2104.04473
  ```

* **RoFormer: Enhanced Transformer with Rotary Position Embedding** (Su et al., 2021)
  **Critique:** many relative position methods require the full attention matrix (hard for efficient attention); sinusoidal absolute PE only “sort of” behaves like relative.
  **Improvement:** **RoPE** rotates Q/K so dot-products depend on relative offsets; works in vanilla attention and adapts to efficient/linear-like settings.
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2104.09864
  PDF:   https://arxiv.org/pdf/2104.09864
  ```

---

If you drop the URL (or the raw “References” block) for the *next* RoPE-related writeup you want to mine, I’ll keep this exact timeline style and do the same extraction again.
