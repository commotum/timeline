Here’s a **second-pass (aggressive) sweep focused on 2024–2025**, prioritizing papers that **explicitly** do “*critique → propose new/modified positional encoding (or attention-logit positional bias)*,” even when the title markets itself as **context extension**.

I’m listing only items that *strongly* match your criteria; a few “analysis-forward” papers that are often confused for PE proposals are marked **(borderline / analysis)**.

---

## 2024

* **LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens** (Ding et al., 2024)
  **Critique:** naïve RoPE scaling / long-context extension causes non-uniformity and mismatch issues as length grows.
  **Improvement:** a **RoPE-extension scheme** (non-uniform / optimized adjustments) explicitly framed as a PE-side fix for very long contexts. ([arXiv][1])

* **HiRoPE: Length Extrapolation for Code Models Using Hierarchical Rotary Position Embedding** (Zhang et al., ACL 2024)
  **Critique:** “flat” RoPE doesn’t respect code’s hierarchical structure; long-code completion stresses standard position handling.
  **Improvement:** **Hierarchical RoPE** (structure-aware rotary positions) as the core mechanism for length extrapolation in code LMs. ([ACL Anthology][2])

* **Resonance RoPE: Improving Context Length Generalization of Large Language Models** (Wang et al., 2024)
  **Critique:** existing RoPE interpolation/remapping approaches still show distribution shift / generalization loss at longer lengths.
  **Improvement:** a **refined RoPE modification** (“resonance”-style shaping) aimed at stronger length generalization. ([ACL Anthology][3])

* **Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs** (Ma et al., NeurIPS 2024)
  **Critique:** explains why **NoPE** and standard PE fail beyond an “effective range” and argues PE can be extended with the right design.
  **Improvement:** introduces **weave PE** + **Stair PE** and the **Mesa-Extrapolation** method (chunk/triangular attention + weave PE) as a PE-centric route to extrapolation. ([arXiv][4])

* **Rotary Position Embedding for Vision Transformer** (Heo et al., 2024)
  **Critique:** RoPE is underexplored in vision; 2D usage is non-trivial and common implementations have gaps.
  **Improvement:** **practical 2D RoPE implementations/variants** with analysis and recommended design choices for ViTs. ([arXiv][5])

* **Length Generalization of Causal Transformers without Position Encoding** (Wang et al., Findings ACL 2024) — **borderline (NoPE-centric, but still “position mechanism”)**
  **Critique:** explicit PEs aren’t the only way; NoPE can generalize but has failure modes tied to attention distribution.
  **Improvement:** proposes **head temperature tuning** to expand NoPE’s usable context (not a PE per se, but a direct critique+fix for position handling). ([arXiv][6])

---

## 2025

* **HARPE: Head-Adaptive Rotary Position Encoding** (Lian et al., COLING 2025)
  **Critique:** multi-stage long-context training + manual RoPE base tuning is brittle; single-stage training with one big base can be suboptimal.
  **Improvement:** **per-head RoPE base frequencies** (head-adaptive RoPE) trained directly toward target context length. ([ACL Anthology][7])

* **ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices** (Yu et al., 2025; CVPR 2025 version exists)
  **Critique:** RoPE’s fixed, hand-defined rotation matrices limit the transformation space and flexibility/robustness.
  **Improvement:** generalizes RoPE using **trainable commuting angle matrices**, framing commutativity as key for offset-consistent behavior. ([arXiv][8])

* **CABLE: Context-aware Biases for Length Extrapolation** (Veisi et al., 2025)
  **Critique:** static/additive RPE biases (or fixed PE schemes) can be too rigid; long-context behavior benefits from context-conditioning.
  **Improvement:** **context-conditioned positional bias scores** added to attention logits (a dynamic PE/bias mechanism). ([ACL Anthology][9])

* **Wavelet-based Positional Representation for Long Context** (Oka et al., ICLR 2025)
  **Critique:** frames **ALiBi** as behaving like windowed attention and argues it struggles with deep dependencies due to receptive-field limitations.
  **Improvement:** a **wavelet-transform-based positional representation** to capture multiple scales without restricting attention’s field. ([OpenReview][10])

* **Understanding the RoPE Extensions of Long-Context LLMs** (Zhong et al., COLING 2025) — **borderline / analysis**
  **Critique:** many RoPE extensions are used in practice without a clear attention-perspective explanation.
  **Contribution:** primarily **analysis/understanding**, not a brand-new PE (useful as a map of the space, but not itself a PE proposal). ([ACL Anthology][11])

* **CoPE: A Lightweight Complex Positional Encoding** (Amballa, 2025)
  **Critique:** traditional PE methods have limitations (especially for long sequences), and some exhibit long-term decay or incompatibilities.
  **Improvement:** replaces PE with **complex-valued encoding** (real=content, imag=position) + **phase-aware attention** in early layers. ([arXiv][12])

* **TAPA: Positional Encoding via Token-Aware Phase Attention** (Wang et al., 2025)
  **Critique:** argues RoPE introduces an intrinsic distance-dependent bias limiting long-context modeling; many RoPE extensions are post-hoc retuning.
  **Improvement:** inserts a **learnable phase function into attention** (token-aware phase attention) as a new PE mechanism for extrapolation. ([arXiv][13])

---

### Quick “what you might have missed” buckets (2024–2025)

* **“RoPE variants that change the *generator* of rotation”**: ComRoPE ([arXiv][8])
* **“RoPE variants that change *which heads* get which frequencies”**: HARPE ([ACL Anthology][7])
* **“Non-RoPE PE that rewires *how position enters attention*”**: TAPA ([arXiv][13]), CoPE ([arXiv][12])
* **“Bias/logit-level position that becomes *context-dependent*”**: CABLE ([ACL Anthology][9])
* **“Multi-scale / signal-processing inspired position”**: Wavelet-based PR ([arXiv][14])
* **“Chunk/weave PE for extrapolation without full retraining”**: Mesa-Extrapolation + Stair/Weave PE ([arXiv][4])

[1]: https://arxiv.org/pdf/2402.13753?utm_source=chatgpt.com "LongRoPE: Extending LLM Context Window Beyond 2 ..."
[2]: https://aclanthology.org/2024.acl-long.735/?utm_source=chatgpt.com "Length Extrapolation for Code Models Using Hierarchical ..."
[3]: https://aclanthology.org/2024.findings-acl.32.pdf?utm_source=chatgpt.com "Resonance RoPE: Improving Context Length ..."
[4]: https://arxiv.org/abs/2410.15859?utm_source=chatgpt.com "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs"
[5]: https://arxiv.org/abs/2403.13298?utm_source=chatgpt.com "Rotary Position Embedding for Vision Transformer"
[6]: https://arxiv.org/abs/2404.12224?utm_source=chatgpt.com "Length Generalization of Causal Transformers without Position Encoding"
[7]: https://aclanthology.org/2025.coling-main.326/?utm_source=chatgpt.com "A Novel Single-Stage Approach to Long Context Extension ..."
[8]: https://arxiv.org/abs/2506.03737?utm_source=chatgpt.com "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices"
[9]: https://aclanthology.org/2025.emnlp-main.1545.pdf?utm_source=chatgpt.com "Context-aware Biases for Length Extrapolation"
[10]: https://openreview.net/forum?id=OhauMUNW8T&utm_source=chatgpt.com "Wavelet-based Positional Representation for Long Context"
[11]: https://aclanthology.org/2025.coling-main.600.pdf?utm_source=chatgpt.com "Understanding the RoPE Extensions of Long-Context LLMs"
[12]: https://arxiv.org/abs/2508.18308?utm_source=chatgpt.com "CoPE: A Lightweight Complex Positional Encoding"
[13]: https://arxiv.org/abs/2509.12635?utm_source=chatgpt.com "Positional Encoding via Token-Aware Phase Attention"
[14]: https://arxiv.org/html/2502.02004v1?utm_source=chatgpt.com "Wavelet-based Positional Representation for Long Context"
