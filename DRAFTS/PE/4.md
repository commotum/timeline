Got it — same timeline format, but now it’s **every paper/model Su mentions or references in the RoPE post**, each with **canonical arXiv / PDF links** (URLs only inside code blocks).

---

## 2017 — Sinusoidal PE as the “absolute PE that wants to be relative”

* **Attention Is All You Need** (Vaswani et al., 2017)
  **Critique:** self-attention is permutation-invariant; order must be injected.
  **Improvement:** **sinusoidal absolute positional encoding** (closed-form; enables relative structure algebraically). ([Scientific Spaces][1])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1706.03762
  PDF:   https://arxiv.org/pdf/1706.03762
  ```

---

## 2018 — The “absolute-position baseline” in the RoFormer comparison table

* **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (Devlin et al., 2018)
  **Critique:** (as framed by Su) absolute PE is simple but typically not extrapolatable without tricks.
  **Improvement:** uses **trainable absolute positional embeddings** (learned lookup).
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1810.04805
  PDF:   https://arxiv.org/pdf/1810.04805
  ```

---

## 2019 — “Classic relative position” baseline named in the RoFormer comparison table

* **NEZHA: Neural Contextualized Representation for Chinese Language Understanding** (Wei et al., 2019)
  **Critique:** (as used in Su’s table) absolute PE is not ideal; relative PE can be a better inductive bias.
  **Improvement:** introduces/uses **Functional Relative Positional Encoding** in a BERT-like Chinese PLM. ([arXiv][2])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1909.00204
  PDF:   https://arxiv.org/pdf/1909.00204
  ```

---

## 2019 — The long-text evaluation task referenced in the RoFormer results section

* **CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain** (Xiao et al., 2019)
  **Critique:** (implicit) long-text semantics need benchmarks where length matters.
  **Improvement:** provides **a legal-domain similar-case matching dataset** used as Su’s long-text evaluation target. ([arXiv][3])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1911.08962
  PDF:   https://arxiv.org/pdf/1911.08962
  ```

---

## 2021 — Linear-attention reference example (named explicitly in the RoPE linear-attention section)

* **Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention** (Xiong et al., 2021)
  **Critique:** vanilla attention is quadratic; need efficient approximations for long sequences.
  **Improvement:** approximates attention via **Nyström landmark-based low-rank reconstruction** (linear-ish attention variant). ([arXiv][4])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2102.03902
  PDF:   https://arxiv.org/pdf/2102.03902
  ```

---

## 2021 — The paper version of Su’s RoPE/RoFormer (explicitly linked in the post)

* **RoFormer: Enhanced Transformer with Rotary Position Embedding** (Su et al., 2021)
  **Critique:** sinusoidal absolute PE “almost” yields relative structure, but not cleanly; many relative schemes can’t be used in linear attention because they act on the attention matrix.
  **Improvement:** **RoPE**: rotate Q/K by position-dependent orthogonal transforms so dot-products depend on relative offsets; enables a RoFormer model that extrapolates better to longer contexts and is compatible with linear-attention variants. ([arXiv][5])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2104.09864
  PDF:   https://arxiv.org/pdf/2104.09864
  ```

---

If you paste the next Su post, I’ll keep exactly this structure again: **timeline → critique → improvement → code-block links** (and only include what’s actually mentioned/referenced there).

[1]: https://kexue.fm/archives/8180?utm_source=chatgpt.com "Nyströmformer：基于矩阵分解的线性化Attention方案- 科学空间"
[2]: https://arxiv.org/abs/1909.00204?utm_source=chatgpt.com "NEZHA: Neural Contextualized Representation for Chinese Language Understanding"
[3]: https://arxiv.org/abs/1911.08962?utm_source=chatgpt.com "[1911.08962] CAIL2019-SCM: A Dataset of Similar Case ..."
[4]: https://arxiv.org/abs/2102.03902?utm_source=chatgpt.com "A Nyström-Based Algorithm for Approximating Self-Attention"
[5]: https://arxiv.org/abs/2104.09864?utm_source=chatgpt.com "Enhanced Transformer with Rotary Position Embedding"
