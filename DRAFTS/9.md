## 2018 — “3D as unordered sets” becomes attention-friendly

* **Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks** (Lee et al., 2018/2019)
  **Improves On:** 1D ordered-sequence Transformers for language
  **Adaptation:** redesign attention blocks to be **permutation-invariant** (set encoders/aggregators), which naturally fits **point sets** (a common 3D representation)

---

## 2020 — Geometry-first 3D Transformers (equivariance as the adaptation)

* **SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks** (Fuchs et al., NeurIPS 2020)
  **Improves On:** generic attention that ignores 3D symmetry structure
  **Adaptation:** make self-attention **SE(3)-equivariant** for 3D point clouds / 3D graphs, so the model respects rotations and translations by construction ([NeurIPS Proceedings][1])

---

## 2021 — Point-cloud Transformers “as backbones” (local neighborhoods + attention)

* **PCT: Point Cloud Transformer** (Guo et al., 2021)
  **Improves On:** applying 1D attention to irregular, unordered point sets
  **Adaptation:** a transformer-style framework for **unstructured point clouds** with **offset-attention** and point-set processing built around attention as the main operator ([Springer][2])

* **Point Transformer** (Zhao et al., ICCV 2021)
  **Improves On:** standard Transformers assuming token order / grid structure
  **Adaptation:** define self-attention layers specifically for **point clouds** (neighborhood-based point attention) and build full 3D backbones for segmentation/part tasks ([CVF Open Access][3])

* **Voxel Transformer for 3D Object Detection (VoTr)** (Mao et al., ICCV 2021)
  **Improves On:** voxel CNN backbones with limited receptive field for 3D detection
  **Adaptation:** transform voxel features with **sparse voxel self-attention** to capture long-range 3D context efficiently (attention is the core backbone operator) ([arXiv][4])

* **3DETR: An End-to-End Transformer Model for 3D Object Detection** (Misra et al., ICCV 2021)
  **Improves On:** complex 3D detection pipelines with many hand-designed 3D operators
  **Adaptation:** DETR-style **set prediction** for **3D point clouds**: transformer encoder over points + decoder producing 3D boxes from (nonparametric) queries ([arXiv][5])

* **DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries** (Wang et al., 2021; published 2022 venues)
  **Improves On:** 2D-only detection from images / depth-heavy 3D pipelines
  **Adaptation:** query objects in **3D space** and use camera geometry to **index multi-view 2D features** (DETR-like transformer adapted to 3D reasoning) ([arXiv][6])

---

## 2022 — “Bring BERT/MAE to 3D” + spatiotemporal transformers for 3D perception (BEV)

* **Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling** (Yu et al., CVPR 2022)
  **Improves On:** BERT-style masked modeling success in 1D language
  **Adaptation:** tokenize point clouds into **local 3D patches** and pretrain a transformer with **masked point modeling** (BERT paradigm adapted to 3D sets/patches) ([arXiv][7])

* **Masked Autoencoders for Point Cloud Self-supervised Learning (Point-MAE)** (Pang et al., ECCV 2022)
  **Improves On:** MAE’s 2D success and generic SSL not handling point-cloud properties well
  **Adaptation:** masked autoencoding redesigned around **point cloud patching/masking** challenges; transformer encoder/decoder scheme tailored to irregular 3D data ([arXiv][8])

* **BEVFormer: Learning Bird’s-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers** (Li et al., ECCV 2022)
  **Improves On:** 1D sequence Transformers and 2D perception pipelines that don’t unify 3D space + time well
  **Adaptation:** build a unified **3D perception representation (BEV grid)** using **spatiotemporal transformer** attention: BEV queries attend across camera views and time ([arXiv][9])

* **TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers** (Bai et al., CVPR 2022)
  **Improves On:** brittle multi-sensor fusion for 3D detection
  **Adaptation:** transformer-based **fine-grained LiDAR–camera fusion** for 3D detection (attention as the fusion mechanism) ([CVF Open Access][10])

---

## 2023 — Voxel-space Transformers for volumetric 3D understanding from 2D sensors

* **VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion** (Li et al., CVPR 2023)
  **Improves On:** BEV-only 3D perception and dense volumetric completion methods
  **Adaptation:** predict complete **3D voxel semantics** from images using **sparse voxel queries** and transformer attention over voxels/features ([arXiv][11])

* **ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers** (Kulhánek et al., ECCV 2022; heavily used in 3D view synthesis work through 2023)
  **Improves On:** NeRF pipelines requiring many 3D samples + long optimization
  **Adaptation:** a transformer-centric pipeline that maps multi-view context + query pose to novel views without explicit NeRF-style per-ray optimization (a “transformerization” of 3D view synthesis) ([arXiv][12])

* **TransNeRF: Generalizable Neural Radiance Fields for Novel View Synthesis with Transformer** (Wang et al., 2022; cited through 2023)
  **Improves On:** MLP NeRFs that struggle to condition on arbitrary numbers of views / capture view relationships
  **Adaptation:** a **Transformer-based NeRF** that fuses information across observed views via attention (transformer as the conditioning/fusion engine for a 3D field) ([arXiv][13])

---

## 2024 — Scaling 3D point Transformers as “real” foundation backbones

* **Point Transformer V3: Simpler, Faster, Stronger** (Wu et al., CVPR 2024)
  **Improves On:** earlier point Transformers trading speed/scale for accuracy via complex mechanisms
  **Adaptation:** a re-architected **point cloud transformer backbone** optimized for scalability/efficiency while keeping attention central for large 3D scenes ([CVF Open Access][14])

---

## 2025 — Sparse voxel Transformers pushed further (multi-modal, LiDAR-centric)

* **SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection** (2025)
  **Improves On:** BEV-only extraction and inefficiencies in dense voxel processing
  **Adaptation:** operate directly on **sparse 3D voxel features** with a transformer detector (voxel-space attention as the core) ([arXiv][15])

---

If you want the next step while keeping this exact format: I can do an **aggressive 2024–2025 sweep** specifically for (a) **3D point cloud segmentation**, (b) **LiDAR-based 3D detection**, and (c) **neural rendering / radiance-field Transformers**, filtering out papers where “Transformer” is incidental rather than the core adaptation.

[1]: https://proceedings.neurips.cc/paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html "SE(3)-Transformers: 3D Roto-Translation Equivariant ..."
[2]: https://link.springer.com/article/10.1007/s41095-021-0229-5 "PCT: Point cloud transformer | Computational Visual Media"
[3]: https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf "Point Transformer"
[4]: https://arxiv.org/abs/2109.02497 "[2109.02497] Voxel Transformer for 3D Object Detection"
[5]: https://arxiv.org/abs/2109.08141 "An End-to-End Transformer Model for 3D Object Detection"
[6]: https://arxiv.org/abs/2110.06922 "DETR3D: 3D Object Detection from Multi-view Images via ..."
[7]: https://arxiv.org/abs/2111.14819 "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling"
[8]: https://arxiv.org/abs/2203.06604 "[2203.06604] Masked Autoencoders for Point Cloud Self- ..."
[9]: https://arxiv.org/abs/2203.17270 "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers"
[10]: https://openaccess.thecvf.com/content/CVPR2022/papers/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.pdf "Robust LiDAR-Camera Fusion for 3D Object Detection with ..."
[11]: https://arxiv.org/abs/2302.12251 "VoxFormer: Sparse Voxel Transformer for Camera-based ..."
[12]: https://arxiv.org/abs/2203.10157 "ViewFormer: NeRF-free Neural Rendering from Few ..."
[13]: https://arxiv.org/abs/2206.05375 "Generalizable Neural Radiance Fields for Novel View ..."
[14]: https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Point_Transformer_V3_Simpler_Faster_Stronger_CVPR_2024_paper.pdf "Point Transformer V3: Simpler Faster Stronger"
[15]: https://arxiv.org/html/2503.08092v1 "Sparse Voxel-based Transformer for Multi-modal 3D Object ..."
