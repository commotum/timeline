Absolutely — same timeline format, now with **canonical arXiv / PDF links** for each paper Su mentions.

(Per your note: **all of these share the same blog-post reference**, so I’m not repeating the blog citation per item; I’m just attaching the **paper links**.)

---

## 2017 — Trainable absolute PE becomes the default in early seq models

* **Convolutional Sequence to Sequence Learning** (Gehring et al., 2017)
  **Critique:** (context in the post) early seq models still needed an explicit mechanism to inject order.
  **Improvement:** uses **trainable absolute positional embeddings** (as a learnable table) rather than a fixed formula. ([arXiv][1])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1705.03122
  PDF:   https://arxiv.org/pdf/1705.03122
  PMLR:  https://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf
  ```

* **Attention Is All You Need** (Vaswani et al., 2017)
  **Critique:** self-attention is permutation-invariant; without position, token order is lost.
  **Improvement:** proposes **sinusoidal (trigonometric) absolute positional encoding** with a closed-form rule enabling extrapolation in principle. ([arXiv][2])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1706.03762
  PDF:   https://arxiv.org/pdf/1706.03762
  NeurIPS page: https://papers.nips.cc/paper/7181-attention-is-all-you-need
  ```

---

## 2018 — Classic relative position in attention

* **Self-Attention with Relative Position Representations** (Shaw, Uszkoreit, Vaswani, 2018)
  **Critique:** absolute PE added to inputs is not the only (or best) way; attention can be made position-aware directly.
  **Improvement:** inject **relative position representations** into attention (key/value-side modifications), becoming the “classic style” RPE template. ([arXiv][3])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1803.02155
  PDF:   https://arxiv.org/pdf/1803.02155
  ACL Anthology: https://aclanthology.org/N18-2074/
  ```

* **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** (Devlin et al., 2018)
  **Critique:** (implied via Su’s framing) transformers still require PE; the simplest form is learnable absolute tables.
  **Improvement:** uses **trainable absolute positional embeddings** (learned lookup). ([arXiv][4])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1810.04805
  PDF:   https://arxiv.org/pdf/1810.04805
  ACL PDF: https://aclanthology.org/N19-1423.pdf
  ```

* **Improving Language Understanding by Generative Pre-Training (GPT-1)** (Radford et al., 2018)
  **Critique:** same framing as BERT in the post—absolute learned PE is straightforward but not extrapolatable by default.
  **Improvement:** uses **trainable absolute positional embeddings** (learned lookup). ([OpenAI][5])
  **Links (PDF):**

  ```text
  PDF (OpenAI): https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
  ```

---

## 2019 — Relative PE redesigned for segment recurrence + a named “style” emerges

* **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context** (Dai et al., 2019)
  **Critique:** absolute PE clashes with segment-level recurrence/memory (positions don’t stay consistent across segments).
  **Improvement:** the “**XLNet style**” decomposition: relative-position terms in the attention score with trainable vectors (u, v) and a sinusoidal-based relative signal (as described by Su). ([arXiv][6])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1901.02860
  ACL PDF: https://aclanthology.org/P19-1285.pdf
  ```

* **XLNet: Generalized Autoregressive Pretraining for Language Understanding** (Yang et al., 2019)
  **Critique:** (Su’s context) popularized Transformer-XL’s relative-position attention and helped cement the “XLNet style” naming.
  **Improvement:** adopts Transformer-XL’s **relative positional attention formulation** in a strong pretraining regime. ([arXiv][7])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1906.08237
  PDF:   https://arxiv.org/pdf/1906.08237
  ```

* **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)** (Raffel et al., 2019)
  **Critique:** argues for decoupling input content and position interactions; simplifies positional handling.
  **Improvement:** “**T5 style**” relative PE: **learned relative position bias** added directly to attention logits, with **relative-position bucketing**. ([arXiv][8])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1910.10683
  PDF:   https://arxiv.org/pdf/1910.10683
  JMLR PDF: https://jmlr.org/papers/volume21/20-074/20-074.pdf
  ```

---

## 2020 — Learned position dynamics, disentangling schemes, and “unconventional” PE

* **Learning to Encode Position for Transformer with Continuous Dynamical Model (FLOATER)** (Liu et al., ICML 2020)
  **Critique:** fixed-form absolute PE is restrictive; want a learnable generator with extrapolation-friendly behavior.
  **Improvement:** model positional encoding via a **continuous dynamical system / Neural ODE** (recursive/continuous-time PE). ([arXiv][9])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2003.09229
  PDF:   https://arxiv.org/pdf/2003.09229
  ```

* **Rethinking Positional Encoding in Language Pre-training (TUPE)** (Ke et al., 2020; published ICLR 2021)
  **Critique:** addition of word+position embeddings mixes heterogeneous correlations; `[CLS]` shouldn’t be treated like ordinary positions.
  **Improvement:** **untied positional encoding**: separate parameterizations for word-context correlation vs positional correlation, then combine. ([arXiv][10])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2006.15595
  PDF:   https://arxiv.org/pdf/2006.15595
  OpenReview PDF: https://openreview.net/pdf?id=09-528y2Fgf
  ```

* **DeBERTa: Decoding-enhanced BERT with Disentangled Attention** (He et al., 2020; published ICLR 2021)
  **Critique:** different “qk expansion” interaction terms matter; position/content interaction should be structured, not entangled.
  **Improvement:** “**DeBERTa style**” relative PE: disentangled attention that separates content and position vectors and their interactions. ([arXiv][11])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2006.03654
  PDF:   https://arxiv.org/pdf/2006.03654
  ```

* **How Much Position Information Do Convolutional Neural Networks Encode?** (Islam, Jia, Bruce, 2020)
  **Critique:** CNNs appear position-aware without explicit PE—where does that come from?
  **Improvement:** argues a lot of absolute position can be **leaked via padding/boundaries** (Su highlights zero-padding leakage). ([arXiv][12])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/2001.08248
  OpenReview PDF: https://openreview.net/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf
  ```

* **Encoding Word Order in Complex Embeddings** (Wang et al., 2019; published ICLR 2020)
  **Critique:** standard position embeddings capture absolute positions but not richer order relations; real-valued PE isn’t the only route.
  **Improvement:** “**complex number style**” encoding (Complex Order) and a **complex-valued network pipeline** to exploit phase/order structure. ([arXiv][13])
  **Links (arXiv / PDF):**

  ```text
  arXiv: https://arxiv.org/abs/1912.12333
  PDF:   https://arxiv.org/pdf/1912.12333
  OpenReview PDF: https://openreview.net/pdf?id=Hke-WTVtwr
  ```

---

If you want, next round we can do something very mechanical (and satisfying): take this “Su-mentions list” and tag each entry as **(A)** “direct PE proposal,” **(B)** “model that uses PE,” or **(C)** “position analysis/other,” while keeping the exact same timeline look.

[1]: https://arxiv.org/abs/1705.03122 "Convolutional Sequence to Sequence Learning"
[2]: https://arxiv.org/abs/1706.03762 "[1706.03762] Attention Is All You Need"
[3]: https://arxiv.org/abs/1803.02155 "Self-Attention with Relative Position Representations"
[4]: https://arxiv.org/abs/1810.04805 "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
[5]: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf "Improving Language Understanding by Generative Pre- ..."
[6]: https://arxiv.org/abs/1901.02860 "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
[7]: https://arxiv.org/abs/1906.08237 "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
[8]: https://arxiv.org/abs/1910.10683 "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
[9]: https://arxiv.org/abs/2003.09229 "Learning to Encode Position for Transformer with Continuous Dynamical Model"
[10]: https://arxiv.org/abs/2006.15595 "Rethinking Positional Encoding in Language Pre-training"
[11]: https://arxiv.org/abs/2006.03654 "DeBERTa: Decoding-enhanced BERT with Disentangled Attention"
[12]: https://arxiv.org/abs/2001.08248 "How Much Position Information Do Convolutional Neural Networks Encode?"
[13]: https://arxiv.org/abs/1912.12333 "Encoding word order in complex embeddings"
