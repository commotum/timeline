## 2021 — First “4D point cloud video” Transformers (XYZT as the native domain)

* **Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos (P4Transformer)** (Fan et al., CVPR 2021)
  **Improves On:** 1D Transformers (language) and single-frame 3D point cloud models
  **Adaptation:** treat **point cloud sequences as 4D (x,y,z,t)** and model them directly with a **spatiotemporal point transformer**, avoiding explicit point tracking across frames ([CVF Open Access][1])

---

## 2022 — Long-term 4D point video backbones + explicit “4D fusion” Transformers

* **Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding** (Wen et al., ECCV 2022)
  **Improves On:** short-range 4D point transformers that struggle with long-term context
  **Adaptation:** a 4D backbone that builds and attends over **spatiotemporal point primitives** to extend modeling range in 4D sequences ([ECVA][2])

* **LIFT: Learning 4D LiDAR Image Fusion Transformer for 3D Object Detection** (Zeng et al., CVPR 2022)
  **Improves On:** 1D transformer success + 3D detection fusion pipelines that don’t model **sensor × time** interactions well
  **Adaptation:** a **4D spatiotemporal fusion transformer** that explicitly fuses **across sensors and time** (4D = space + time, multi-modal fusion as central transformer adaptation) ([CVF Open Access][3])

* **Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion** (Lin et al., arXiv 2022 / widely cited 2023)
  **Improves On:** BEV-style approaches and naive temporal fusion that doesn’t scale well
  **Adaptation:** “4D sampling” over **(view, scale, timestamp, keypoints)** to sparsely gather and fuse spatiotemporal evidence for detection (a *representation + fusion* adaptation for 4D perception) ([arXiv][4])

---

## 2023 — 4D sequence understanding as a first-class objective (geometry + motion)

* **LeaF: Learning Frames for 4D Point Cloud Sequence Understanding** (Liu et al., ICCV 2023)
  **Improves On:** generic 4D tools that don’t exploit the strong prior that 4D sequences come from structured frames/motion
  **Adaptation:** a 4D point-sequence representation learning framework targeting **geometry + motion descriptors** for point cloud videos (4D understanding as the core) ([CVF Open Access][5])

* **4D-Former: Multimodal 4D Panoptic Segmentation** (Athar et al., CoRL 2023)
  **Improves On:** single-scan 3D panoptic segmentation and non-learned temporal association pipelines
  **Adaptation:** transformer-style **panoptic decoding + learned tracklet association** over **LiDAR sequences (4D)** with **LiDAR+RGB** fusion (4D panoptic + tracking as the centerpiece) ([Proceedings of Machine Learning Research][6])

* **Mask4Former: Mask Transformer for 4D Panoptic Segmentation** (Yilmaz et al., arXiv 2023; updated 2024)
  **Improves On:** hand-crafted temporal association (tracking-by-heuristics) and pipelines that treat 3D segmentation/tracking separately
  **Adaptation:** a **mask-transformer** that unifies **semantic + instance segmentation + tracking** on **sequences of LiDAR point clouds** via **spatio-temporal instance queries** ([arXiv][7])

---

## 2024 — 4D “world models” and task-specific 4D Transformers (lanes, interaction, 4D scene understanding)

* **DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving** (Min et al., CVPR 2024)
  **Improves On:** 2D-pretraining-centric perception (and non-world-model temporal pipelines)
  **Adaptation:** “4D pretraining” that models **dynamic scenes over time** for downstream 3D tasks (world-model framing: learn spatiotemporal dynamics as a core representation) ([CVF Open Access][8])

* **OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving** (ECCV 2024)
  **Improves On:** static / single-frame occupancy and limited temporal modeling
  **Adaptation:** a world-model framework that explicitly models **4D occupancy evolution** (3D space over time) as the central representation for planning/forecasting tasks ([ECVA][9])

* **Interactive4D: Interactive 4D LiDAR Segmentation** (Fradlin et al., 2024)
  **Improves On:** interactive segmentation framed per-scan (ignores the full space-time volume)
  **Adaptation:** interactive segmentation that operates over **multiple LiDAR scans jointly** (space-time volume), enabling multi-object segmentation across **4D LiDAR** in one iteration (4D interaction paradigm is central) ([arXiv][10])

* **Enhanced Scene Understanding on 4D Point Cloud … (Transformer with temporal relationship modeling)** (Jing et al., AAAI 2024)
  **Improves On:** 3D-only perception that fails to leverage temporal cues for dynamic scene understanding
  **Adaptation:** transformer-based fusion of **RGB sequences → 4D point cloud understanding**, emphasizing temporal relationship modeling as the key mechanism ([AAAI Conference Proceedings][11])

* **LLFormer4D: LiDAR-based lane detection … spatio-temporal Transformer** (2024)
  **Improves On:** frame-wise lane detection and limited temporal aggregation
  **Adaptation:** explicit **spatio-temporal transformer** lane detection built around 4D LiDAR signal structure (space + time) ([IET Research Journals][12])

---

## 2025 — 4D panoptic goes “zero-shot” and 4D occupancy generation scales up

* **Zero-Shot 4D LiDAR Panoptic Segmentation (SAL-4D)** (Zhang et al., CVPR 2025)
  **Improves On:** closed-vocabulary 4D panoptic segmentation and training-data bottlenecks
  **Adaptation:** a pipeline with a **Transformer-based instance decoder** operating in **4D LiDAR space**, enabling zero-shot recognition by lifting/aligning external visual-language signals into 4D segmentation+tracking ([CVF Open Access][13])

* **(4D occupancy generation as “world modeling” with transformer backbones)**

  * **OccSora (repo, 2024)** — diffusion-based 4D occupancy generation with a “4D scene tokenizer” (representation adaptation to dynamic 4D scenes) ([GitHub][14])
  * **DynamicCity (arXiv 2025)** — large-scale **dynamic 4D occupancy generation**, using a transformer-style DiT component over compact 4D representations ([arXiv][15])
    *(These are more “4D generative world models” than “segmentation/detection,” but they squarely fit the “increase dimensionality” criterion.)*

---

If you want, I can do the same **aggressive 2024–2025 sweep** specifically for:

* **4D occupancy forecasting** families (OccWorld descendants and related “4D occupancy” transformer/diffusion world-model papers), and
* **4D LiDAR panoptic/segmentation/tracking** families (Mask4Former line + multimodal variants),
  while filtering out anything that’s “just a small tweak” rather than a real 4D adaptation.

[1]: https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_Point_4D_Transformer_Networks_for_Spatio-Temporal_Modeling_in_Point_Cloud_CVPR_2021_paper.pdf?utm_source=chatgpt.com "Point 4D Transformer Networks for Spatio-Temporal ..."
[2]: https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136890018.pdf?utm_source=chatgpt.com "Point Primitive Transformer for Long-Term 4D Point Cloud ..."
[3]: https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_LIFT_Learning_4D_LiDAR_Image_Fusion_Transformer_for_3D_Object_CVPR_2022_paper.pdf?utm_source=chatgpt.com "LIFT: Learning 4D LiDAR Image Fusion Transformer for 3D ..."
[4]: https://arxiv.org/abs/2211.10581?utm_source=chatgpt.com "Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion"
[5]: https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_LeaF_Learning_Frames_for_4D_Point_Cloud_Sequence_Understanding_ICCV_2023_paper.pdf?utm_source=chatgpt.com "Learning Frames for 4D Point Cloud Sequence Understanding"
[6]: https://proceedings.mlr.press/v229/athar23a/athar23a.pdf?utm_source=chatgpt.com "4D-Former: Multimodal 4D Panoptic Segmentation"
[7]: https://arxiv.org/abs/2309.16133?utm_source=chatgpt.com "Mask4Former: Mask Transformer for 4D Panoptic ..."
[8]: https://openaccess.thecvf.com/content/CVPR2024/papers/Min_DriveWorld_4D_Pre-trained_Scene_Understanding_via_World_Models_for_Autonomous_CVPR_2024_paper.pdf?utm_source=chatgpt.com "DriveWorld: 4D Pre-trained Scene Understanding via World ..."
[9]: https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02024.pdf?utm_source=chatgpt.com "OccWorld: Learning a 3D Occupancy World Model for ..."
[10]: https://arxiv.org/abs/2410.08206?utm_source=chatgpt.com "Interactive4D: Interactive 4D LiDAR Segmentation"
[11]: https://ojs.aaai.org/index.php/AAAI/article/view/28045?utm_source=chatgpt.com "Enhanced Scene Understanding on 4D Point Cloud ..."
[12]: https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cvi2.12338?utm_source=chatgpt.com "LLFormer4D: LiDAR‐based lane detection method by ..."
[13]: https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Zero-Shot_4D_Lidar_Panoptic_Segmentation_CVPR_2025_paper.pdf?utm_source=chatgpt.com "Zero-Shot 4D Lidar Panoptic Segmentation - CVF Open Access"
[14]: https://github.com/wzzheng/OccSora?utm_source=chatgpt.com "OccSora: 4D Occupancy Generation Models as World ..."
[15]: https://arxiv.org/html/2410.18084v3?utm_source=chatgpt.com "Large-Scale 4D Occupancy Generation from Dynamic ..."
