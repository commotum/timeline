Below is a **timeline-form** (same format as before) for papers that fit your **“Increasing Transformer’s Dimensions”** classification: Transformers where **attention is central**, the framing acknowledges transformers’ **1D roots** (or highlights the need for non-1D structure), and the core contribution is an **adaptation to higher-dimensional / non-sequential domains** (tokenization/serialization/attention factorization/architecture), **not** primarily a positional-encoding innovation.

---

## 2018 — First “2D as sequence” transformer generalizations (images)

* **Image Transformer** (Parmar et al., 2018)
  **Improves On:** 1D Transformer sequence modeling for text
  **Adaptation:** treats images as sequences for autoregressive image generation; introduces **local 2D-aware attention restrictions** to make attention practical on images ([arXiv][1])

* **Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks** (Lee et al., 2018/2019)
  **Improves On:** 1D ordered sequences → set-structured data
  **Adaptation:** transformer-style attention redesigned to be **permutation-invariant** (set encoder/aggregator) for set-like and unordered domains (incl. point sets as a motivating application) ([arXiv][2])

---

## 2019 — Scaling attention to “big 2D/byte streams” and non-1D domains

* **Generating Long Sequences with Sparse Transformers** (Child et al., 2019)
  **Improves On:** quadratic-cost full attention that limits long sequences
  **Adaptation:** **sparse attention factorization** enabling very long sequences, demonstrated on **images/audio/text from raw bytes** ([arXiv][3])

---

## 2020 — “Vision becomes a Transformer domain” (patching, pixels, detection, 2D factorization)

* **Generative Pretraining from Pixels (iGPT)** (Chen et al., 2020)
  **Improves On:** 1D autoregressive Transformers for text
  **Adaptation:** serialize **2D images into 1D pixel sequences** and train a GPT-style model on pixels ([OpenAI][4])

* **An Image Is Worth 16×16 Words (ViT)** (Dosovitskiy et al., 2020)
  **Improves On:** Transformer encoders for 1D sequence classification
  **Adaptation:** tokenize images into **fixed-size patches** (patch embedding sequence) and run a standard transformer encoder over patch tokens ([arXiv][5])

* **End-to-End Object Detection with Transformers (DETR)** (Carion et al., 2020)
  **Improves On:** CNN pipeline + hand-crafted detection components
  **Adaptation:** use transformer **encoder–decoder** on 2D image features plus **learned object queries** for set prediction (detection as set output) ([arXiv][6])

* **Stand-Alone Axial-Attention for Panoptic Segmentation (Axial-DeepLab)** (Wang et al., 2020)
  **Improves On:** expensive full 2D self-attention and CNN-dominant segmentation
  **Adaptation:** factorize 2D attention into **two 1D attentions along axes** (row/column), enabling global-ish context at manageable cost ([Department of Computer Science][7])

* **SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks** (Fuchs et al., 2020)
  **Improves On:** transformers that ignore 3D symmetry structure
  **Adaptation:** redesign attention to be **SE(3)-equivariant** for 3D point clouds / 3D graphs (geometric domain) ([NeurIPS Proceedings][8])

---

## 2021 — Expansion to video (space-time), dense prediction backbones, graphs, point clouds, audio

### Images: “dense prediction needs pyramids / hierarchy / windows”

* **SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers** (Zheng et al., 2020/2021)
  **Improves On:** FCN encoder–decoder segmentation paradigm
  **Adaptation:** treat segmentation explicitly as **seq2seq**: image → patch-token sequence → transformer encoder + decoder for dense masks ([arXiv][9])

* **Swin Transformer: Hierarchical Vision Transformer using Shifted Windows** (Liu et al., 2021)
  **Improves On:** ViT-style “single-scale global attention” that’s costly for high-res images
  **Adaptation:** hierarchical backbone + **windowed attention with shifting** to scale to high-resolution 2D vision tasks (detection/segmentation) ([CVF Open Access][10])

* **Pyramid Vision Transformer (PVT)** (Wang et al., 2021)
  **Improves On:** “flat token grids” that don’t match multi-scale needs of dense prediction
  **Adaptation:** a **pyramid / multiscale** transformer backbone (spatial reduction attention) for dense prediction in 2D vision ([arXiv][11])

* **SegFormer** (Xie et al., 2021)
  **Improves On:** heavyweight decoders and brittle scaling for segmentation
  **Adaptation:** segmentation framework with **hierarchical transformer encoder + lightweight MLP decoder** (a “dense prediction ready” ViT variant) ([arXiv][12])

* **MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation** (Cheng et al., 2021)
  **Improves On:** per-pixel classification framing for semantic segmentation
  **Adaptation:** reframes segmentation as **set prediction of masks** (DETR-like set outputs) using transformer-style components ([arXiv][13])

### Video: “add time as another axis”

* **TimeSformer: Is Space-Time Attention All You Need for Video Understanding?** (Bertasius et al., 2021)
  **Improves On:** ViT for static images
  **Adaptation:** extend patch tokens across **time**; propose **factorized space–time attention** variants ([arXiv][14])

* **ViViT: A Video Vision Transformer** (Arnab et al., 2021)
  **Improves On:** image transformers / 2D patch tokenization
  **Adaptation:** **spatiotemporal tokenization** and efficient transformer variants by **factorizing spatial vs temporal processing** ([CVF Open Access][15])

* **Multiscale Vision Transformers (MViT)** (Fan et al., 2021)
  **Improves On:** single-scale ViT video models that are expensive
  **Adaptation:** multiscale hierarchy for **video** (token resolution downsampling + channel expansion) ([arXiv][16])

* **Video Swin Transformer** (Liu et al., 2021/2022)
  **Improves On:** globally-attending video transformers
  **Adaptation:** extend Swin’s locality + hierarchy to **spatiotemporal** inputs (windowed attention in space-time) ([arXiv][17])

### Graphs: “sequence ≠ graph”

* **Graphormer: Do Transformers Really Perform Bad for Graph Representation?** (Ying et al., 2021)
  **Improves On:** vanilla transformers lacking graph structural inductive bias
  **Adaptation:** transformer architecture augmented with **graph structural encodings** to operate on graph domains ([arXiv][18])

### 3D point clouds: “unordered 3D sets with geometry”

* **Point Transformer** (Zhao et al., 2020/2021)
  **Improves On:** applying 1D attention to irregular 3D data
  **Adaptation:** self-attention layers designed for **point clouds** (local neighborhoods + point features/coords) ([CVF Open Access][19])

* **PCT: Point Cloud Transformer** (Guo et al., 2020)
  **Improves On:** order sensitivity / irregular-domain difficulty in point clouds
  **Adaptation:** transformer framework tailored to **unordered point sets** (point-cloud learning) ([arXiv][20])

### Audio: “2D time–frequency grids”

* **AST: Audio Spectrogram Transformer** (Gong et al., 2021)
  **Improves On:** CNN-heavy audio pipelines
  **Adaptation:** represent audio as **spectrogram patches** and apply a ViT-style transformer to the 2D time–frequency plane ([arXiv][21])

---

## 2022 — “Arbitrary structured inputs” + large-scale multimodal bridges + point-cloud pretraining

* **Perceiver / Perceiver IO** (Jaegle et al., 2021; Perceiver IO 2021)
  **Improves On:** transformers that scale poorly with very large inputs (e.g., huge 2D/3D arrays)
  **Adaptation:** a general transformer-style architecture handling **arbitrary structured inputs/outputs** via latent bottlenecks and cross-attention ([arXiv][22])

* **Flamingo: a Visual Language Model for Few-Shot Learning** (Alayrac et al., 2022)
  **Improves On:** pure-language transformers (1D text) and rigid multimodal fusion
  **Adaptation:** introduces a **Perceiver Resampler** to compress variable-size **image/video** features into a small set of visual tokens, then interleave with language generation via cross-attention blocks ([arXiv][23])

* **Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling** (Yu et al., 2021/2022)
  **Improves On:** BERT-style pretraining in 1D text
  **Adaptation:** port masked modeling + tokenization ideas to **3D point cloud patches** with a point-cloud tokenizer and transformer backbone ([arXiv][24])

---

## 2023 — (Mostly consolidation / scaling rather than “first adaptation”)

* **Transformer-based 3D point cloud generation networks** (various; example ACM 2023) — **borderline** (domain adaptation is there, but often incremental vs earlier 3D transformer backbones)
  **Improves On:** earlier generative point cloud pipelines
  **Adaptation:** transformer architectures for **3D point cloud generation** in irregular domains ([ACM Digital Library][25])

---

## 2024–2025 — Continued specialization across non-1D domains (mostly variants on the above patterns)

*(These years are huge; many works are “application + refinement.” I’m including only ones where the **higher-dimensional adaptation** is clearly central, not just “we used a ViT backbone.”)*

* **Graph Perceiver IO** (Bae et al., 2025) — **borderline** (adapts a general nD transformer to graphs explicitly)
  **Improves On:** Perceiver IO’s generic recipe for structured inputs
  **Adaptation:** instantiate Perceiver IO specifically for **graph-structured inputs/outputs** ([ScienceDirect][26])

* **POS-BERT: Point cloud one-stage BERT pre-training** (Fu et al., 2024) — **borderline** (pretraining adaptation rather than core backbone invention)
  **Improves On:** Point-BERT’s two-stage tokenizer dependency
  **Adaptation:** simplifies transformer pretraining for **3D point clouds** (still “bring BERT to 3D”) ([ScienceDirect][27])

---

### If you want this to be *even more exhaustive*

Your criteria includes **2D, 3D, 4D, nD, graph-based**, etc., and the literature is *massive* in 2021–2025 (dense prediction backbones, video architectures, medical imaging transformers, remote sensing, molecules/proteins, 3D scenes, robotics). The cleanest way to keep your timeline style *and* be truly comprehensive is to do it in **tracks** (Images / Video / Graphs / 3D geometry / Audio / “General nD & multimodal”), each still timeline-ordered.

If you tell me which tracks you care about most (or if you want *all* tracks), I’ll expand the list with the same formatting—no style changes.

[1]: https://arxiv.org/abs/1802.05751?utm_source=chatgpt.com "Image Transformer"
[2]: https://arxiv.org/abs/1810.00825?utm_source=chatgpt.com "Set Transformer: A Framework for Attention-based ..."
[3]: https://arxiv.org/abs/1904.10509?utm_source=chatgpt.com "Generating Long Sequences with Sparse Transformers"
[4]: https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf?utm_source=chatgpt.com "Generative Pretraining from Pixels"
[5]: https://arxiv.org/abs/2010.11929?utm_source=chatgpt.com "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
[6]: https://arxiv.org/abs/2005.12872?utm_source=chatgpt.com "End-to-End Object Detection with Transformers"
[7]: https://www.cs.jhu.edu/~alanlab/Pubs20/wang2020axial.pdf?utm_source=chatgpt.com "Stand-Alone Axial-Attention for Panoptic Segmentation"
[8]: https://proceedings.neurips.cc/paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html?utm_source=chatgpt.com "SE(3)-Transformers: 3D Roto-Translation Equivariant ..."
[9]: https://arxiv.org/abs/2012.15840?utm_source=chatgpt.com "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"
[10]: https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf?utm_source=chatgpt.com "Hierarchical Vision Transformer Using Shifted Windows"
[11]: https://arxiv.org/abs/2102.12122?utm_source=chatgpt.com "Pyramid Vision Transformer: A Versatile Backbone for ..."
[12]: https://arxiv.org/abs/2105.15203?utm_source=chatgpt.com "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"
[13]: https://arxiv.org/abs/2107.06278?utm_source=chatgpt.com "Per-Pixel Classification is Not All You Need for Semantic Segmentation"
[14]: https://arxiv.org/abs/2102.05095?utm_source=chatgpt.com "Is Space-Time Attention All You Need for Video Understanding?"
[15]: https://openaccess.thecvf.com/content/ICCV2021/papers/Arnab_ViViT_A_Video_Vision_Transformer_ICCV_2021_paper.pdf?utm_source=chatgpt.com "ViViT: A Video Vision Transformer"
[16]: https://arxiv.org/abs/2104.11227?utm_source=chatgpt.com "Multiscale Vision Transformers"
[17]: https://arxiv.org/abs/2106.13230?utm_source=chatgpt.com "Video Swin Transformer"
[18]: https://arxiv.org/abs/2106.05234?utm_source=chatgpt.com "Do Transformers Really Perform Bad for Graph ..."
[19]: https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf?utm_source=chatgpt.com "Point Transformer"
[20]: https://arxiv.org/abs/2012.09688?utm_source=chatgpt.com "[2012.09688] PCT: Point cloud transformer"
[21]: https://arxiv.org/abs/2104.01778?utm_source=chatgpt.com "[2104.01778] AST: Audio Spectrogram Transformer"
[22]: https://arxiv.org/abs/2103.03206?utm_source=chatgpt.com "Perceiver: General Perception with Iterative Attention"
[23]: https://arxiv.org/abs/2204.14198?utm_source=chatgpt.com "Flamingo: a Visual Language Model for Few-Shot Learning"
[24]: https://arxiv.org/abs/2111.14819?utm_source=chatgpt.com "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling"
[25]: https://dl.acm.org/doi/10.1145/3581783.3612226?utm_source=chatgpt.com "Transformer-based Point Cloud Generation Network"
[26]: https://www.sciencedirect.com/science/article/abs/pii/S0031320325005497?utm_source=chatgpt.com "Graph Perceiver IO: A general architecture for ..."
[27]: https://www.sciencedirect.com/science/article/abs/pii/S0957417423030658?utm_source=chatgpt.com "POS-BERT: Point cloud one-stage BERT pre-training"
