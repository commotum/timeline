## 2018 — First serious “images as Transformer domains”

* **Image Transformer** (Parmar et al., 2018)
  **Improves On:** 1D self-attention sequence modeling (text)
  **Adaptation:** recast **images as sequences** for likelihood-based generation; make 2D feasible via **restricted/local self-attention neighborhoods** rather than full global attention ([arXiv][1])

---

## 2019 — Making attention practical for very long “image-like” sequences

* **Generating Long Sequences with Sparse Transformers** (Child et al., 2019)
  **Improves On:** full attention’s quadratic scaling (limits long sequences, including image bytes)
  **Adaptation:** **sparse/factorized attention patterns** enabling extremely long sequences (used for modalities including images/bytes) ([arXiv][2])

---

## 2020 — “Vision becomes a Transformer-native domain” (patching, pixels, detection-as-set)

* **Generative Pretraining from Pixels (iGPT)** (Chen et al., 2020)
  **Improves On:** 1D autoregressive Transformers for language
  **Adaptation:** serialize **2D images into 1D pixel sequences** and train GPT-style causal attention for image understanding/generation ([arXiv][3])

* **An Image Is Worth 16×16 Words (ViT)** (Dosovitskiy et al., 2020)
  **Improves On:** Transformers as primarily 1D NLP models; vision relying on CNN scaffolding
  **Adaptation:** tokenize images into **fixed-size patches** → a patch-token sequence processed by a standard transformer encoder ([arXiv][4])

* **End-to-End Object Detection with Transformers (DETR)** (Carion et al., 2020)
  **Improves On:** CNN detection pipelines with hand-designed components (anchors, NMS, etc.)
  **Adaptation:** transformer **encoder–decoder** with **learned object queries**; detection reframed as **set prediction** via attention ([arXiv][3])

* **Stand-Alone Axial-Attention (Axial-DeepLab)** (Wang et al., 2020)
  **Improves On:** full 2D attention cost and CNN-centric segmentation
  **Adaptation:** factorize 2D attention into **1D axial attentions** (rows/cols) to scale global context for dense prediction ([arXiv][5])

---

## 2021 — Hierarchy, locality, multiscale: the “backbone era” + segmentation as seq2seq

### Image classification backbones (2D scaling and inductive bias)

* **Swin Transformer** (Liu et al., 2021)
  **Improves On:** flat/global ViT that scales poorly to high-res and dense tasks
  **Adaptation:** **hierarchical** vision transformer with **shifted window attention** (local attention + cross-window connectivity) designed for 2D scaling ([arXiv][6])

* **Pyramid Vision Transformer (PVT)** (Wang et al., 2021)
  **Improves On:** difficulty “porting Transformer to dense prediction” with flat tokens
  **Adaptation:** progressive **pyramid** + **spatial-reduction attention** to make transformer features usable for dense 2D prediction ([arXiv][7])

* **CvT: Introducing Convolutions to Vision Transformers** (Wu et al., 2021)
  **Improves On:** pure ViT patch embedding limitations on local inductive bias and efficiency
  **Adaptation:** add **convolutional token embedding** + **convolutional projections** inside transformer blocks (hybrid 2D inductive bias while keeping attention core) ([arXiv][8])

* **Tokens-to-Token ViT (T2T-ViT)** (Yuan et al., 2021)
  **Improves On:** naive patch tokenization that discards local structure
  **Adaptation:** a progressive **tokens-to-token** “re-structurization” step that builds better 2D token sequences before attention ([arXiv][9])

* **CoAtNet: Marrying Convolution and Attention** (Dai et al., 2021)
  **Improves On:** conv-only vs attention-only tradeoffs across data/scale
  **Adaptation:** systematic **stacking of conv + attention stages** to scale image modeling while keeping attention as a core component ([NeurIPS Proceedings][10])

* **DeiT: Data-efficient Image Transformers** (Touvron et al., 2021) — **borderline (more “training adaptation” than “domain adaptation”)**
  **Improves On:** ViT’s dependence on massive pretraining for vision
  **Adaptation:** distillation/token strategy enabling ViTs to work well on ImageNet-only training (still squarely in “make transformers work for 2D images”) ([Proceedings of Machine Learning Research][11])

### Dense prediction (segmentation and universal segmentation)

* **SETR: Rethinking Semantic Segmentation as Seq2Seq with Transformers** (Zheng et al., 2021)
  **Improves On:** FCN encoder–decoder segmentation pipelines
  **Adaptation:** treat segmentation as **sequence-to-sequence**: encode an image as a patch sequence using a **pure transformer** then decode to dense masks ([arXiv][12])

* **SegFormer** (Xie et al., 2021)
  **Improves On:** heavy decoders and resolution sensitivity for transformer segmentation
  **Adaptation:** **hierarchical transformer encoder** producing multiscale features + lightweight MLP decoder for dense 2D outputs ([arXiv][5])

* **MaskFormer** (Cheng et al., 2021)
  **Improves On:** per-pixel classification framing for segmentation
  **Adaptation:** reframes segmentation as **set prediction of masks** using transformer-style decoding (DETR-like idea generalized to masks) ([CVF Open Access][13])

### Detection scaling (2D attention that converges and resolves fine structure)

* **Deformable DETR** (Zhu et al., 2021)
  **Improves On:** DETR’s slow convergence / resolution limits from global attention on feature maps
  **Adaptation:** replace dense attention with **sparse sampling-based (deformable) attention** for 2D feature maps ([arXiv][14])

### Generative (2D codebooks + transformers)

* **Taming Transformers for High-Resolution Image Synthesis (VQGAN + Transformer)** (Esser et al., 2021)
  **Improves On:** direct pixel-sequence transformers being inefficient for high-res
  **Adaptation:** learn a **discrete 2D codebook** (VQGAN) then model token composition with an autoregressive transformer ([arXiv][15])

* **Zero-Shot Text-to-Image Generation (DALL·E)** (Ramesh et al., 2021)
  **Improves On:** text-only autoregressive transformers
  **Adaptation:** a transformer that autoregressively models a **single stream of text tokens + image tokens** (multimodal serialization of 2D images) ([arXiv][16])

---

## 2022 — Plain ViT backbones for detection + “masked modeling” and faster image token generation

* **ViTDet: Exploring Plain ViT Backbones for Object Detection** (Li, Mao, Girshick, He, 2022)
  **Improves On:** need to redesign hierarchical backbones for detection
  **Adaptation:** shows “plain” ViT can be adapted to detection with minimal changes (simple pyramid + window attention blocks) ([arXiv][2])

* **Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation** (Cheng et al., 2022)
  **Improves On:** separate architectures for semantic/instance/panoptic segmentation
  **Adaptation:** a unified transformer mask architecture using **masked attention** to localize cross-attention within predicted mask regions ([arXiv][17])

* **MaskGIT: Masked Generative Image Transformer** (Chang et al., 2022)
  **Improves On:** raster-scan autoregressive image token generation being slow/inefficient
  **Adaptation:** treat images as token grids but generate via **bidirectional transformer** with **iterative refinement** (non-sequential decoding over 2D tokens) ([arXiv][18])

* **Masked Autoencoders Are Scalable Vision Learners (MAE)** (He et al., 2021/2022) — **borderline (pretraining adaptation)**
  **Improves On:** transferring masked modeling from 1D language to 2D patch grids
  **Adaptation:** asymmetric encoder–decoder over **visible patch subsets**; reconstruct masked patches for scalable 2D representation learning ([arXiv][19])

* **BEiT: BERT Pre-Training of Image Transformers** (Bao et al., 2021/2022) — **borderline (pretraining adaptation)**
  **Improves On:** bringing BERT-style masked modeling to image patch tokens
  **Adaptation:** tokenize images into discrete “visual tokens” and run masked prediction with a transformer encoder over 2D patches ([arXiv][20])

* **Scalable Diffusion Models with Transformers (DiT)** (Peebles & Xie, 2022)
  **Improves On:** U-Net dominated diffusion backbones for images
  **Adaptation:** replace U-Net with a **ViT-like transformer** operating on **latent patches** for diffusion image generation ([arXiv][21])

---

## 2023 — Foundation segmentation & new “interfaces” between pixels and sequences

* **Segment Anything (SAM)** (Kirillov et al., 2023)
  **Improves On:** task-specific segmentation pipelines
  **Adaptation:** a transformer-centric system (ViT image encoder + prompt encoder + **transformer mask decoder**) enabling promptable segmentation over 2D images ([CVF Open Access][22])

* **DiffusionDet: Diffusion Model for Object Detection** (Chen et al., 2022/2023) — **borderline (detection via diffusion, but still attention-heavy systems)**
  **Improves On:** standard detection decoding paradigms
  **Adaptation:** detection as a diffusion denoising process over boxes; uses transformer-style components to predict/refine sets ([CVF Open Access][13])

---

## 2024–2025 — Continued consolidation (most work is “new task + refined transformerization”)

*(In images, by 2024–2025 the dominant pattern is: reuse ViT/Swin-style tokenization + attention, then innovate in decoding, scaling, or supervision. The big “dimensional jump” ideas largely landed 2018–2022.)*

* If you want, I can do an **aggressive 2024–2025 sweep** for *image* papers where the **central contribution is a new 2D tokenization / attention layout / decoder interface** (not “we used a ViT backbone”), while keeping this exact format.

If you say which subtrack(s) you want included—**classification backbones**, **detection**, **segmentation**, **generative**, or **all of them**—I’ll expand 2024–2025 accordingly without changing the timeline style.

[1]: https://arxiv.org/abs/1802.05751 "Image Transformer"
[2]: https://arxiv.org/abs/2203.16527 "Exploring Plain Vision Transformer Backbones for Object Detection"
[3]: https://arxiv.org/abs/2005.12872 "End-to-End Object Detection with Transformers"
[4]: https://arxiv.org/abs/2010.11929 "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
[5]: https://arxiv.org/abs/2105.15203 "SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"
[6]: https://arxiv.org/abs/2103.14030 "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"
[7]: https://arxiv.org/abs/2102.12122 "Pyramid Vision Transformer: A Versatile Backbone for ..."
[8]: https://arxiv.org/abs/2103.15808 "CvT: Introducing Convolutions to Vision Transformers"
[9]: https://arxiv.org/abs/2101.11986 "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"
[10]: https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html "CoAtNet: Marrying Convolution and Attention for All Data ..."
[11]: https://proceedings.mlr.press/v139/touvron21a.html "Training data-efficient image transformers & distillation ..."
[12]: https://arxiv.org/abs/2012.15840 "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers"
[13]: https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.pdf "DiffusionDet: Diffusion Model for Object Detection"
[14]: https://arxiv.org/abs/2010.04159 "Deformable Transformers for End-to-End Object Detection"
[15]: https://arxiv.org/abs/2012.09841 "Taming Transformers for High-Resolution Image Synthesis"
[16]: https://arxiv.org/abs/2102.12092 "[2102.12092] Zero-Shot Text-to-Image Generation"
[17]: https://arxiv.org/abs/2112.01527 "Masked-attention Mask Transformer for Universal Image ..."
[18]: https://arxiv.org/abs/2202.04200 "MaskGIT: Masked Generative Image Transformer"
[19]: https://arxiv.org/abs/2111.06377 "Masked Autoencoders Are Scalable Vision Learners"
[20]: https://arxiv.org/abs/2106.08254 "BEiT: BERT Pre-Training of Image Transformers"
[21]: https://arxiv.org/abs/2212.09748 "[2212.09748] Scalable Diffusion Models with Transformers"
[22]: https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf "Segment Anything - CVF Open Access"
