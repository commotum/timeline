## 2015 — Synthetic “skills” as a diagnostic for multi-step reasoning

* **Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks (bAbI)** (Weston et al., 2015)
  **Target Domain:** compositional reasoning skills (coref, induction/deduction, temporal/spatial, counting, etc.)
  **Resource:** *bAbI*, 20 synthetic QA tasks explicitly designed to isolate reasoning capabilities. ([arXiv][1])

---

## 2017–2018 — Multi-hop reading comprehension becomes a benchmark genre

* **Constructing Datasets for Multi-hop Reading Comprehension (WikiHop / MedHop; QAngaroo)** (Welbl et al., 2017/2018)
  **Target Domain:** multi-document, multi-hop evidence integration
  **Resource:** *WikiHop* and *MedHop* induced multi-hop RC datasets. ([arXiv][2])

* **HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering** (Yang et al., 2018)
  **Target Domain:** multi-hop QA with explanation supervision
  **Resource:** *HotpotQA*, 113k QA pairs with **supporting-fact annotations** and comparison questions. ([ACL Anthology][3])

* **Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge** (Clark et al., 2018)
  **Target Domain:** science QA requiring knowledge + reasoning beyond surface cues
  **Resource:** *ARC* (Easy + Challenge) + *ARC Corpus* of science sentences. ([arXiv][4])

* **OpenBookQA: Can a Suit of Armor Conduct Electricity?** (Mihaylov et al., 2018)
  **Target Domain:** “open-book exam” science reasoning with missing commonsense
  **Resource:** *OpenBookQA* + curated “open book” of science facts used in questions. ([arXiv][5])

---

## 2019 — Commonsense + discrete/numeric reasoning + inductive relational diagnostics

* **CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge** (Talmor et al., 2019)
  **Target Domain:** commonsense reasoning (ConceptNet-grounded)
  **Resource:** *CommonsenseQA*, multiple-choice questions designed to force commonsense knowledge. ([ACL Anthology][6])

* **DROP: Discrete Reasoning Over Paragraphs** (Dua et al., 2019)
  **Target Domain:** text-grounded **discrete operations** (add/count/sort) + RC
  **Resource:** *DROP*, ~96k adversarially-built questions requiring discrete reasoning over paragraphs. ([ACL Anthology][7])

* **CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text** (Sinha et al., 2019)
  **Target Domain:** systematic generalization in relational/inductive reasoning
  **Resource:** *CLUTRR*, semi-synthetic family-relation stories with controllable reasoning length. ([ACL Anthology][8])

---

## 2020 — Logic-heavy RC + more structured multi-hop evaluation

* **2WikiMultiHopQA: Comprehensive Evaluation of Reasoning Steps** (Ho et al., 2020)
  **Target Domain:** multi-hop QA with reasoning-path evidence
  **Resource:** *2WikiMultiHopQA*, multi-hop QA with annotated evidence paths for evaluating reasoning steps. ([ACL Anthology][9])

* **ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning** (Yu et al., 2020)
  **Target Domain:** formal-ish logical reasoning in RC (exam-derived)
  **Resource:** *ReClor*, exam-sourced logical RC with EASY/HARD bias split analysis. ([arXiv][10])

* **LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning** (Liu et al., 2020)
  **Target Domain:** deductive logical reasoning over text
  **Resource:** *LogiQA*, 8,678 expert-written logical reasoning QA instances. ([IJCAI][11])

---

## 2021 — “Modern math reasoning” + explicit explanation/proof supervision

* **StrategyQA: A Benchmark with Implicit Reasoning Strategies** (Geva et al., 2021)
  **Target Domain:** implicit multi-step reasoning not stated in the question
  **Resource:** *StrategyQA*, yes/no questions with step decompositions + evidence for each step. ([ACL Anthology][12])

* **GSM8K: Training Verifiers to Solve Math Word Problems** (Cobbe et al., 2021)
  **Target Domain:** multi-step arithmetic word-problem reasoning
  **Resource:** *GSM8K*, 8.5k grade-school math word problems (high-quality, diverse). ([arXiv][13])

* **MATH: Measuring Mathematical Problem Solving** (Hendrycks et al., 2021)
  **Target Domain:** competition-level math reasoning with derivations
  **Resource:** *MATH*, 12,500 challenging problems with **step-by-step solutions**. ([arXiv][14])

* **SVAMP: Simple Variations on Arithmetic Math Word Problems** (Patel et al., 2021)
  **Target Domain:** robustness / invariance in word-problem reasoning
  **Resource:** *SVAMP*, challenge set of perturbed arithmetic MWPs exposing brittleness. ([ACL Anthology][15])

* **EntailmentBank: Explaining Answers with Entailment Trees** (Dalvi et al., 2021)
  **Target Domain:** multi-step reasoning *as structured explanations*
  **Resource:** *EntailmentBank*, multi-step entailment trees for QA-style explanation. ([ACL Anthology][16])

* **ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language** (Tafjord et al., 2021)
  **Target Domain:** rule-based NL reasoning with proof generation / abduction
  **Resource:** *ProofWriter* task suite (incl. RuleTaker variants) emphasizing proofs + abductive completion. ([ACL Anthology][17])

* **miniF2F: a cross-system benchmark for formal Olympiad-level mathematics** (Zheng, Han, Polu, 2021)
  **Target Domain:** formal theorem proving (proof search, formal reasoning)
  **Resource:** *miniF2F*, 488 formalized olympiad/school math statements across systems. ([arXiv][18])

---

## 2022 — Cleaner multi-hop construction + first-order logic annotations + “hard task suites”

* **MuSiQue: Multihop Questions via Single-hop Question Composition** (Trivedi et al., 2022)
  **Target Domain:** multi-hop QA with reduced “cheating” via shortcuts
  **Resource:** *MuSiQue* (Ans/Full variants), 2–4 hop questions with construction controls. ([arXiv][19])

* **FOLIO: Natural Language Reasoning with First-Order Logic** (Han et al., 2022)
  **Target Domain:** logically grounded NL reasoning
  **Resource:** *FOLIO*, NL premises/conclusions paired with **verified FOL annotations**. ([arXiv][20])

* **BIG-Bench Hard (BBH): Challenging BIG-Bench Tasks…** (Suzgun et al., 2022)
  **Target Domain:** multi-step reasoning tasks where standard prompting underestimates ability
  **Resource:** *BBH*, 23 curated “hard” BIG-bench tasks used as a standardized reasoning suite. ([arXiv][21])

---

## 2023 — Harder/cleaner logic sets + “human exam” reasoning batteries

* **LogiQA 2.0 — An Improved Dataset for Logical Reasoning in NLU** (Liu et al., 2023)
  **Target Domain:** improved logical reasoning evaluation (QA + NLI framing)
  **Resource:** *LogiQA 2.0*, refined/expanded logic reasoning dataset correcting issues in earlier sets. ([FRC Chang][22])

* **AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models** (Zhong et al., 2023)
  **Target Domain:** exam-style reasoning across disciplines and difficulty tiers
  **Resource:** *AGIEval*, standardized admission/qualification/competition exams (objective questions). ([arXiv][23])

* **GAIA: a benchmark for General AI Assistants** (Mialon et al., 2023; ICLR 2024)
  **Target Domain:** multi-step real-world reasoning (often planning/tool-use flavored, but still heavily “reasoning over tasks”)
  **Resource:** *GAIA*, curated questions designed to be easy for humans, hard for assistants; strong evaluation focus. ([arXiv][24])

---

## 2024 — Contamination-resistant reasoning eval + “harder MMLU” as measurement infrastructure

* **LiveBench: A Challenging, Contamination-Free LLM Benchmark** (White et al., 2024; ICLR 2025 spotlight)
  **Target Domain:** robust measurement (math/reasoning among core slices) under contamination pressure
  **Resource:** *LiveBench*, frequently-updated tasks scored via objective ground truth (no LLM judges), including reasoning-heavy subsets. ([arXiv][25])

* **MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark** (Wang et al., 2024)
  **Target Domain:** reasoning-focused extension of broad knowledge eval
  **Resource:** *MMLU-Pro*, harder/cleaned MMLU with 10-choice questions and more reasoning-heavy items. ([arXiv][26])

---

## 2025 — Pushing post-saturation reasoning measurement

* **BIG-Bench Extra Hard (BBEH)** (Kazemi et al., 2025)
  **Target Domain:** next-generation reasoning evaluation after BBH saturation
  **Resource:** *BBEH*, replaces each BBH task with a novel, more difficult counterpart probing similar reasoning skills. ([ACL Anthology][27])

---

If you want, I can do the same “aggressive second pass” as before but focused on **2024–2025 reasoning-heavy text benchmarks specifically** (math/logic/proofs/planning) — e.g., newer “hard math” suites, contamination-resistant exam sets, proof datasets, and post-BBH replacements beyond BBEH.

[1]: https://arxiv.org/abs/1502.05698?utm_source=chatgpt.com "Towards AI-Complete Question Answering: A Set of ..."
[2]: https://arxiv.org/abs/1710.06481?utm_source=chatgpt.com "[1710.06481] Constructing Datasets for Multi-hop Reading ..."
[3]: https://aclanthology.org/D18-1259/?utm_source=chatgpt.com "HotpotQA: A Dataset for Diverse, Explainable Multi-hop ..."
[4]: https://arxiv.org/abs/1803.05457?utm_source=chatgpt.com "Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge"
[5]: https://arxiv.org/abs/1809.02789?utm_source=chatgpt.com "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"
[6]: https://aclanthology.org/N19-1421/?utm_source=chatgpt.com "CommonsenseQA: A Question Answering Challenge ..."
[7]: https://aclanthology.org/N19-1246/?utm_source=chatgpt.com "DROP: A Reading Comprehension Benchmark Requiring ..."
[8]: https://aclanthology.org/D19-1458.pdf?utm_source=chatgpt.com "A Diagnostic Benchmark for Inductive Reasoning from Text"
[9]: https://aclanthology.org/2020.coling-main.580/?utm_source=chatgpt.com "Constructing A Multi-hop QA Dataset for Comprehensive ..."
[10]: https://arxiv.org/abs/2002.04326?utm_source=chatgpt.com "ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning"
[11]: https://www.ijcai.org/proceedings/2020/501?utm_source=chatgpt.com "LogiQA: A Challenge Dataset for Machine Reading ..."
[12]: https://aclanthology.org/2021.tacl-1.21/?utm_source=chatgpt.com "Did Aristotle Use a Laptop? A Question Answering ..."
[13]: https://arxiv.org/abs/2110.14168?utm_source=chatgpt.com "Training Verifiers to Solve Math Word Problems"
[14]: https://arxiv.org/abs/2103.03874?utm_source=chatgpt.com "Measuring Mathematical Problem Solving With the MATH Dataset"
[15]: https://aclanthology.org/2021.naacl-main.168/?utm_source=chatgpt.com "Are NLP Models really able to Solve Simple Math Word ..."
[16]: https://aclanthology.org/2021.emnlp-main.585.pdf?utm_source=chatgpt.com "Explaining Answers with Entailment Trees"
[17]: https://aclanthology.org/2021.findings-acl.317.pdf?utm_source=chatgpt.com "ProofWriter: Generating Implications, Proofs, and Abductive ..."
[18]: https://arxiv.org/abs/2109.00110?utm_source=chatgpt.com "MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics"
[19]: https://arxiv.org/abs/2108.00573?utm_source=chatgpt.com "[2108.00573] MuSiQue: Multihop Questions via Single-hop ..."
[20]: https://arxiv.org/abs/2209.00840?utm_source=chatgpt.com "FOLIO: Natural Language Reasoning with First-Order Logic"
[21]: https://arxiv.org/abs/2210.09261?utm_source=chatgpt.com "Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"
[22]: https://frcchang.github.io/pub/An%20Improved%20Dataset%20for%20Logical%20Reasoning%20in%20Natural%20Language%20Understanding.pdf?utm_source=chatgpt.com "LogiQA 2.0 — An Improved Dataset for Logical Reasoning ..."
[23]: https://arxiv.org/pdf/2304.06364?utm_source=chatgpt.com "AGIEval: A Human-Centric Benchmark for Evaluating ..."
[24]: https://arxiv.org/abs/2311.12983?utm_source=chatgpt.com "[2311.12983] GAIA: a benchmark for General AI Assistants"
[25]: https://arxiv.org/abs/2406.19314?utm_source=chatgpt.com "LiveBench: A Challenging, Contamination-Free LLM Benchmark"
[26]: https://arxiv.org/abs/2406.01574?utm_source=chatgpt.com "MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark"
[27]: https://aclanthology.org/2025.acl-long.1285/?utm_source=chatgpt.com "BIG-Bench Extra Hard"
