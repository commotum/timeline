## 2015 — The “default instrument” for vision–language QA

* **VQA: Visual Question Answering** (Antol et al., 2015)
  **Target Domain:** general image–question answering (vision + language + commonsense)
  **Resource:** *VQA v1*, large-scale open-ended QA over COCO + abstract scenes, with standardized accuracy-style evaluation. ([arXiv][1])

---

## 2017 — Measuring and reducing language priors + diagnostic synthetic reasoning

* **VQA v2.0 (“Making the V in VQA Matter”)** (Goyal et al., 2017)
  **Target Domain:** VQA with reduced question→answer priors
  **Resource:** *VQA v2*, balanced complementary-image pairs to expose language-bias shortcuts. ([arXiv][2])

* **CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning** (Johnson et al., 2017)
  **Target Domain:** compositional visual reasoning diagnostics (count, compare, logic, memory)
  **Resource:** *CLEVR*, synthetic 3D scenes + programmatically generated questions + detailed “reasoning type” annotations. ([CVF Open Access][3])

---

## 2018 — Natural, messy, “real user” multimodal evaluation

* **VizWiz Grand Challenge: Answering Visual Questions from Blind People** (Gurari et al., 2018)
  **Target Domain:** goal-oriented VQA “in the wild” (often low-quality images; sometimes unanswerable)
  **Resource:** *VizWiz*, ~31k real visual questions from blind photographers + answerability considerations. ([arXiv][4])

---

## 2019 — Cognition-level vision–language reasoning + multi-image grounding + knowledge-VQA

* **VCR: Visual Commonsense Reasoning (“From Recognition to Cognition…”)** (Zellers et al., 2019)
  **Target Domain:** cognition-level visual commonsense + rationales
  **Resource:** *VCR*, ~290k multiple-choice QA + rationale selection built via adversarial matching to reduce bias. ([arXiv][5])

* **NLVR2: A Corpus for Reasoning about Natural Language Grounded in Photographs** (Suhr et al., 2019)
  **Target Domain:** compositional reasoning with **pairs of real images** and a statement (true/false)
  **Resource:** *NLVR2*, 100k+ examples with each sentence paired to both true and false labels (bias control). ([lil.nlp.cornell.edu][6])

* **GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering** (Hudson & Manning, 2019)
  **Target Domain:** compositional real-image reasoning with tighter bias control and diagnostics
  **Resource:** *GQA*, large-scale questions generated from scene graphs + functional programs + consistency/grounding metrics. ([arXiv][7])

* **OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge** (Marino et al., 2019)
  **Target Domain:** knowledge-intensive VQA where the image alone is insufficient
  **Resource:** *OK-VQA*, questions explicitly requiring outside knowledge (evaluation designed to stress knowledge+vision fusion). ([CVF Open Access][8])

* **TextVQA: Towards VQA Models That Can Read** (Singh et al., 2019)
  **Target Domain:** OCR + reasoning over scene text
  **Resource:** *TextVQA*, questions requiring reading text in images (a large “text-in-image” VQA benchmark). ([arXiv][9])

---

## 2020–2021 — Documents as multimodal reasoning objects

* **DocVQA: A Dataset for VQA on Document Images** (Mathew et al., 2020; WACV 2021)
  **Target Domain:** document understanding for question answering
  **Resource:** *DocVQA*, ~50k questions over 12k+ document images with task categories and baselines. ([arXiv][10])

---

## 2022 — Charts + science QA with explanations + “world knowledge VQA” scaling

* **ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning** (Masry et al., 2022)
  **Target Domain:** chart understanding + visual/logical reasoning
  **Resource:** *ChartQA*, large-scale QA over real-world charts (human-written + generated questions). ([arXiv][11])

* **ScienceQA (“Learn to Explain…”)** (Lu et al., 2022)
  **Target Domain:** multimodal science reasoning + explanations
  **Resource:** *ScienceQA*, ~21k multimodal MCQs with lectures/explanations as supervision + evaluation lens. ([arXiv][12])

* **A-OKVQA: A Benchmark for VQA Using World Knowledge** (Schwenk et al., 2022)
  **Target Domain:** commonsense/world-knowledge VQA beyond simple retrieval
  **Resource:** *A-OKVQA*, ~25k crowdsourced questions requiring broad world knowledge and reasoning. ([arXiv][13])

---

## 2023 — The “MLLM eval suite” era begins + multimodal math reasoning becomes standardized

* **MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models** (Fu et al., 2023)
  **Target Domain:** broad MLLM evaluation across perception + cognition
  **Resource:** *MME*, 14 subtasks to standardize comparison beyond demos. ([arXiv][14])

* **MMBench: Is Your Multi-modal Model an All-around Player?** (Liu et al., 2023)
  **Target Domain:** objective, multi-skill evaluation for VLMs/MLLMs (bilingual)
  **Resource:** *MMBench*, curated multiple-choice benchmark + evaluation protocol (“CircularEval” framing in the paper). ([arXiv][15])

* **MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities** (Yu et al., 2023)
  **Target Domain:** *integrated* capabilities (recognition + OCR + knowledge + spatial + math + generation)
  **Resource:** *MM-Vet*, open-ended evaluation with an LLM-based evaluator to score diverse answer styles. ([arXiv][16])

* **MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts** (Lu et al., 2023; ICLR 2024)
  **Target Domain:** visual-context math reasoning
  **Resource:** *MathVista*, 6,141 examples sourced from many datasets + a taxonomy of task/reasoning types. ([arXiv][17])

* **POPE (Polling-based Object Probing Evaluation) / “Evaluating Object Hallucination in Large Vision-Language Models”** (Li et al., 2023)
  **Target Domain:** hallucinated object claims in LVLMs
  **Resource:** *POPE*, probing-style evaluation protocol/data for object-existence hallucination. ([arXiv][18])

---

## 2024 — Expert-level multimodal exams + hallucination diagnostics + video understanding benchmarks

* **MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI** (Yue et al., 2023; CVPR 2024)
  **Target Domain:** college/exam-style multimodal reasoning across disciplines
  **Resource:** *MMMU*, 11.5K heterogeneous, interleaved text+image questions spanning 30 subjects. ([arXiv][19])

* **SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension** (Li et al., CVPR 2024)
  **Target Domain:** hierarchical evaluation of MLLMs (comprehension + generation)
  **Resource:** *SEED-Bench*, large-scale multiple-choice evaluation with capability levels/dimensions. ([CVF Open Access][20])

* **HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion** (Guan et al., CVPR 2024)
  **Target Domain:** nuanced hallucination/illusion failure modes in LVLMs
  **Resource:** *HallusionBench*, expert-written image–question pairs designed to trigger/diagnose hallucinations. ([arXiv][21])

* **MHaluBench (from “Unified Hallucination Detection…”)** (Chen et al., ACL 2024)
  **Target Domain:** multimodal hallucination detection with fine-grained labeling
  **Resource:** *MHaluBench*, benchmark dataset introduced to evaluate hallucination detection under a unified framing. ([ACL Anthology][22])

* **MM-Vet v2** (Yu et al., 2024)
  **Target Domain:** integrated capabilities **plus** interleaved image–text sequence understanding
  **Resource:** *MM-Vet v2*, expanded format to reflect real-world interleaved multimodal inputs. ([arXiv][23])

* **MVBench: A Comprehensive Multi-modal Video Understanding Benchmark** (Li et al., CVPR 2024)
  **Target Domain:** temporal understanding for multimodal video LLMs
  **Resource:** *MVBench*, broad video QA benchmark focused on temporal reasoning with standardized prompts/protocols. ([CVF Open Access][24])

* **Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis** (Fu et al., 2024 arXiv; CVPR 2025)
  **Target Domain:** “full-spectrum” video analysis evaluation for MLLMs
  **Resource:** *Video-MME*, comprehensive benchmark intended as a standardized suite for video MLLM evaluation. ([arXiv][25])

---

## 2025 — Robustification and “next benchmarks” built on MLLM-era gaps

* **MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding and Reasoning Benchmark** (Yue et al., 2025)
  **Target Domain:** reduce leakage / filter text-only solvable items / harder options
  **Resource:** *MMMU-Pro*, a tightened protocol derived from MMMU to better isolate true multimodal reasoning. ([ACL Anthology][26])

* **VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs** (Li et al., 2025)
  **Target Domain:** figure-based math with **image-based answer choices** (multi-image option reasoning)
  **Resource:** *VisioMath*, benchmark targeting a gap where answer options are images (hard for many LMMs). ([arXiv][27])

* **ChartQAPro: A More Diverse and Challenging Benchmark for Real-World Chart QA** (2025)
  **Target Domain:** chart QA realism (hypothetical, unanswerable, multi-chart, etc.)
  **Resource:** *ChartQAPro*, human-written/verified questions over diverse charts to stress robust chart understanding. ([arXiv][28])

---

If you want the same “**second-pass aggressive hunt**” we did before: tell me whether you mean **(A)** “image+text MLLM benchmarks” (MME/MMBench/SEED/MMMU-family), **(B)** “document+OCR+charts,” or **(C)** “video multimodal.” I can then expand 2024–2025 to a near-exhaustive list *without changing the format*.

[1]: https://arxiv.org/abs/1505.00468?utm_source=chatgpt.com "[1505.00468] VQA: Visual Question Answering"
[2]: https://arxiv.org/abs/1612.00837?utm_source=chatgpt.com "Making the V in VQA Matter: Elevating the Role of Image ..."
[3]: https://openaccess.thecvf.com/content_cvpr_2017/papers/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.pdf?utm_source=chatgpt.com "CLEVR: A Diagnostic Dataset for Compositional Language ..."
[4]: https://arxiv.org/abs/1802.08218?utm_source=chatgpt.com "VizWiz Grand Challenge: Answering Visual Questions from Blind People"
[5]: https://arxiv.org/abs/1811.10830?utm_source=chatgpt.com "From Recognition to Cognition: Visual Commonsense ..."
[6]: https://lil.nlp.cornell.edu/nlvr/?utm_source=chatgpt.com "Natural Language for Visual Reasoning - LIL Lab"
[7]: https://arxiv.org/abs/1902.09506?utm_source=chatgpt.com "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"
[8]: https://openaccess.thecvf.com/content_CVPR_2019/papers/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.pdf?utm_source=chatgpt.com "OK-VQA: A Visual Question Answering Benchmark ..."
[9]: https://arxiv.org/abs/1904.08920?utm_source=chatgpt.com "[1904.08920] Towards VQA Models That Can Read"
[10]: https://arxiv.org/abs/2007.00398?utm_source=chatgpt.com "DocVQA: A Dataset for VQA on Document Images"
[11]: https://arxiv.org/abs/2203.10244?utm_source=chatgpt.com "ChartQA: A Benchmark for Question Answering about ..."
[12]: https://arxiv.org/abs/2209.09513?utm_source=chatgpt.com "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"
[13]: https://arxiv.org/abs/2206.01718?utm_source=chatgpt.com "A-OKVQA: A Benchmark for Visual Question Answering ..."
[14]: https://arxiv.org/abs/2306.13394?utm_source=chatgpt.com "MME: A Comprehensive Evaluation Benchmark for ..."
[15]: https://arxiv.org/abs/2307.06281?utm_source=chatgpt.com "MMBench: Is Your Multi-modal Model an All-around Player?"
[16]: https://arxiv.org/abs/2308.02490?utm_source=chatgpt.com "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"
[17]: https://arxiv.org/abs/2310.02255?utm_source=chatgpt.com "MathVista: Evaluating Mathematical Reasoning of ..."
[18]: https://arxiv.org/abs/2305.10355?utm_source=chatgpt.com "Evaluating Object Hallucination in Large Vision-Language ..."
[19]: https://arxiv.org/abs/2311.16502?utm_source=chatgpt.com "MMMU: A Massive Multi-discipline Multimodal ..."
[20]: https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SEED-Bench_Benchmarking_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf?utm_source=chatgpt.com "SEED-Bench: Benchmarking Multimodal Large Language ..."
[21]: https://arxiv.org/abs/2310.14566?utm_source=chatgpt.com "[2310.14566] HallusionBench: An Advanced Diagnostic ..."
[22]: https://aclanthology.org/2024.acl-long.178.pdf?utm_source=chatgpt.com "Unified Hallucination Detection for Multimodal Large ..."
[23]: https://arxiv.org/abs/2408.00765?utm_source=chatgpt.com "MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models for Integrated Capabilities"
[24]: https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.pdf?utm_source=chatgpt.com "A Comprehensive Multi-modal Video Understanding Benchmark"
[25]: https://arxiv.org/abs/2405.21075?utm_source=chatgpt.com "Video-MME: The First-Ever Comprehensive Evaluation ..."
[26]: https://aclanthology.org/2025.acl-long.736.pdf?utm_source=chatgpt.com "Xiang Yue*, Tianyu Zheng*, Yuansheng Ni*"
[27]: https://arxiv.org/abs/2506.06727?utm_source=chatgpt.com "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs"
[28]: https://arxiv.org/html/2504.05506v1?utm_source=chatgpt.com "ChartQAPro : A More Diverse and Challenging Benchmark ..."
