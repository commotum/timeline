## 2018 ‚Äî Nested synthetic arithmetic with explicit modular structure

* **ListOps: A Diagnostic Dataset for Latent Tree Learning** (Nangia, Bowman, 2018)
  **Target Domain:** hierarchical / compositional computation over sequences
  **Resource:** *ListOps*, prefix-style nested list expressions; includes operations like **SUM mod 10** (i.e., synthetic modular arithmetic embedded in compositional parsing). ([arXiv][1])

---

## 2019 ‚Äî Large synthetic ‚Äúmath task suite‚Äù generator (broad arithmetic ‚Üí algebra ‚Üí number theory)

* **Analysing Mathematical Reasoning Abilities of Neural Models** (Saxton et al., 2019)
  **Target Domain:** algorithmic math reasoning in seq2seq format (arithmetic, algebra, number theory, probability, etc.)
  **Resource:** *DeepMind Mathematics Dataset*, a **programmatic generator** producing many families of synthetic problems (question‚Üíanswer text pairs) used widely as a controlled arithmetic benchmark. ([arXiv][2])

---

## 2022 ‚Äî Modular arithmetic as the ‚Äúfruit fly‚Äù synthetic task family (grokking-era standard)

* **Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets** (Power et al., 2022)
  **Target Domain:** controlled study of algorithm learning + delayed generalization
  **Resource:** a canonical synthetic setup around **modular arithmetic tasks** (esp. modular addition; also links modular addition/multiplication structure via primitive-root representations), now the default ‚Äúgrokking modular arithmetic‚Äù dataset family used in many follow-ups. ([arXiv][3])

---

## 2023 ‚Äî Expanding the family of modular arithmetic toy tasks

* **Grokking Modular Arithmetic** (Gromov, 2023)
  **Target Domain:** modular arithmetic task families as an interpretable controlled benchmark
  **Resource:** modular arithmetic datasets beyond ‚Äújust addition,‚Äù used to study grokking and the feature maps that solve classes of modular tasks. ([arXiv][4])

---

## 2024 ‚Äî ‚ÄúHarder‚Äù modular tasks and scaling regimes (cryptography-motivated)

* **Machine Learning for Modular Multiplication** (Lauter et al., 2024)
  **Target Domain:** modular multiplication as a harder, crypto-relevant synthetic arithmetic task
  **Resource:** experimental task setups / datasets for **modular multiplication** (including seq2seq Transformer framing), aimed at probing learnability limits. ([arXiv][5])

* **Grokking Modular Polynomials** (Doshi et al., 2024)
  **Target Domain:** compositional modular computation beyond binary ops
  **Resource:** modular datasets covering **multi-term modular addition/multiplication** and constructions for **modular polynomials** (designed so models can be tested on systematic compositional generalization). ([arXiv][6])

* **Teaching Transformers Modular Arithmetic at Scale** (Saxena et al., 2024)
  **Target Domain:** scaling modular arithmetic to larger modulus and more terms
  **Resource:** datasets + generation/training setup for **sum of N terms in ùëç_q** with **much larger q and N** than earlier ‚Äútoy‚Äù modular addition; positioned for crypto-motivated regimes. ([arXiv][7])

* **Transformers Can Do Arithmetic with the Right Embeddings** (McLeish et al., NeurIPS 2024) ‚Äî **not modular-only, but very similar ‚Äúalgorithmic arithmetic‚Äù infrastructure**
  **Target Domain:** extreme-length generalization in synthetic arithmetic (addition/multiplication/sorting)
  **Resource:** large controlled arithmetic task suites (not specifically modular, but same ‚Äúalgorithmic toy-data‚Äù spirit; widely used for synthetic arithmetic eval). ([NeurIPS Proceedings][8])

* **Repeated Examples Help Learn Arithmetic** (Charton, Kempe, 2024)
  **Target Domain:** data distribution effects in algorithmic arithmetic learning
  **Resource:** controlled datasets for **GCD** and **modular multiplication**, emphasizing repetition regimes as part of the evaluation/data design. ([OpenReview][9])

---

## 2025 ‚Äî New modular operations (exponentiation) + higher-order modular study settings

* **Learning Modular Exponentiation with Transformers** (Africa et al., 2025)
  **Target Domain:** modular exponentiation (number theory / crypto) as a synthetic reasoning task
  **Resource:** datasets + sampling strategies for training Transformers on **modular exponentiation**, aimed at studying generalization and internal representations. ([arXiv][10])

---

### Practical note (so you can actually ‚Äúget datasets like that‚Äù)

Most of these modular-arithmetic ‚Äúdatasets‚Äù are **programmatic generators** (you set modulus *p/q*, choose operation family, sample pairs/tuples). The most ‚Äúdrop-in, ready-to-run‚Äù dataset codebases tend to be:

* DeepMind Mathematics Dataset generator repo ([GitHub][11])
* Saxena et al.‚Äôs modular arithmetic at scale repo/materials ([GitHub][12])

If you want, I can do the same thing we did earlier: an **aggressive 2024‚Äì2025 second pass** limited strictly to *papers that introduce a new modular/algorithmic arithmetic task family* (not just analysis of grokking), including ones that hide behind titles like ‚Äúarithmetic,‚Äù ‚Äúalgorithmic reasoning,‚Äù ‚Äúlearnability,‚Äù or ‚Äúgeneralization.‚Äù

[1]: https://arxiv.org/abs/1804.06028?utm_source=chatgpt.com "ListOps: A Diagnostic Dataset for Latent Tree Learning"
[2]: https://arxiv.org/abs/1904.01557?utm_source=chatgpt.com "Analysing Mathematical Reasoning Abilities of Neural ..."
[3]: https://arxiv.org/pdf/2201.02177?utm_source=chatgpt.com "grokking: generalization beyond overfit"
[4]: https://arxiv.org/abs/2301.02679?utm_source=chatgpt.com "Grokking modular arithmetic"
[5]: https://arxiv.org/html/2402.19254v1?utm_source=chatgpt.com "Machine Learning for Modular Multiplication"
[6]: https://arxiv.org/abs/2406.03495?utm_source=chatgpt.com "[2406.03495] Grokking Modular Polynomials"
[7]: https://www.arxiv.org/abs/2410.03569v1?utm_source=chatgpt.com "Teaching Transformers Modular Arithmetic at Scale"
[8]: https://proceedings.neurips.cc/paper_files/paper/2024/file/c35986bc1ee29b31c1011481b77fe540-Paper-Conference.pdf?utm_source=chatgpt.com "Transformers Can Do Arithmetic with the Right Embeddings"
[9]: https://openreview.net/pdf?id=qoUHqnE6A0&utm_source=chatgpt.com "Repeated examples help learn arithmetic"
[10]: https://arxiv.org/html/2506.23679v1?utm_source=chatgpt.com "Learning Modular Exponentiation with Transformers"
[11]: https://github.com/google-deepmind/mathematics_dataset?utm_source=chatgpt.com "google-deepmind/mathematics_dataset: This dataset code ..."
[12]: https://github.com/facebookresearch/arithmetic?utm_source=chatgpt.com "facebookresearch/arithmetic"
