## 2011 — Programming-by-example (PBE) string transforms as a canonical “infer the rule from I/O pairs” setting

* **Automating String Processing in Spreadsheets using Input-Output Examples (FlashFill)** (Gulwani, 2011)
  **Target Domain:** function/rule inference from a few **string I/O examples**
  **Resource:** *FlashFill-style PBE benchmark problems* (Excel end-user transformations) that became the de facto real-world PBE evaluation bedrock. ([Microsoft][1])

---

## 2012 — Real-world benchmark sets for semantic + syntactic PBE

* **Learning Semantic String Transformations from Examples** (Singh, Gulwani, 2012)
  **Target Domain:** infer transformations requiring *semantic* table lookups + string operations from I/O pairs
  **Resource:** evaluation over **real-world benchmarks** (Excel forums / product scenarios) for example-driven synthesis. ([VLDB][2])

---

## 2013 — Community benchmark suite for synthesis from examples (formal, solver-oriented)

* **Syntax-Guided Synthesis (SyGuS)** (Alur et al., 2013)
  **Target Domain:** program/function synthesis (often from constraints/examples) with grammar restrictions
  **Resource:** the early **SyGuS benchmark suite + evaluation framing** that seeded SyGuS-Comp and standardized comparisons. ([CIS Penn][3])

---

## 2015 — General “traditional programming” benchmarks expressed as I/O examples

* **General Program Synthesis Benchmark Suite (PSB / “PSB1”)** (Helmuth, Spector, 2015)
  **Target Domain:** general program synthesis from I/O examples (lists, strings, numbers, simple algorithms)
  **Resource:** *PSB*, a **suite of 29 benchmark problems** specified via input/output examples for cross-system comparison. ([Hamilton College][4])

---

## 2016 — Larger, more realistic PBE benchmark pools (still I/O → program/rule)

* **BlinkFill: Semi-supervised Programming By Example for Syntactic String Transformations** (Singh, 2016)
  **Target Domain:** infer spreadsheet string transforms from few examples, leveraging table context
  **Resource:** evaluation on **207 real-world PBE benchmarks** (Excel team + online help forums), a widely-cited benchmark pool. ([VLDB][5])

---

## 2016–2017 — Inductive program synthesis from I/O examples becomes “neural + search”

* **DeepCoder: Learning to Write Programs** (Balog et al., 2016/2017)
  **Target Domain:** infer a small functional program from **multiple I/O examples**
  **Resource:** a standardized **DSL + problem generator** (programs sampled from a DSL; each task given as several I/O pairs) used as a benchmark for inductive synthesis with learned guidance. ([arXiv][6])

* **RobustFill: Neural Program Learning under Noisy I/O** (Devlin et al., 2017)
  **Target Domain:** infer string transformation programs from sets of I/O examples (FlashFill-like)
  **Resource:** **real-world FlashFill benchmarks** + a large **synthetic data generation protocol** (uniformly sample programs from DSL, generate I/O sets), widely reused as PBE infrastructure. ([arXiv][7])

* **Neural Program Meta-Induction** (Devlin et al., 2017)
  **Target Domain:** *program induction* from I/O examples (predict outputs for new inputs)
  **Resource:** leverages the **Karel I/O task distribution** (gridworld I/O pairs generated from programs) as an induction benchmark. ([NeurIPS Papers][8])

---

## 2018 — Karel becomes the flagship “I/O → program (or rule)” benchmark family

* **Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis** (Bunel et al., 2018)
  **Target Domain:** synthesize Karel programs from a small set of I/O demonstrations
  **Resource:** formalizes/uses the **Karel benchmark** at scale (the public Karel dataset and splits became a standard). ([arXiv][9])

---

## 2019 — “Rule inference from a few I/O examples” becomes an explicit generalization benchmark

* **On the Measure of Intelligence (ARC)** (Chollet, 2019)
  **Target Domain:** infer latent rule/program from **a handful of input→output grid examples**
  **Resource:** *ARC*, 800 tasks where each task is essentially “few-shot program induction from examples.” ([arXiv][10])

---

## 2021 — General program synthesis benchmarks get refreshed (harder “classic CS” tasks)

* **PSB2: The Second Program Synthesis Benchmark Suite** (Helmuth, Kelly, 2021)
  **Target Domain:** general synthesis from I/O examples (more difficult than PSB1)
  **Resource:** *PSB2*, **25 new benchmark problems** curated for broad program synthesis evaluation. ([arXiv][11])

* **SRBench (Symbolic Regression Benchmarks)** (living benchmark; widely used and updated)
  **Target Domain:** infer an analytic function from examples (x→y points) — “function finder” framing
  **Resource:** *SRBench*, a standardized benchmark + protocol for symbolic regression methods across datasets (synthetic + real). ([Cava Lab][12])

---

## 2024 — “Benchmarking the benchmark” for function inference (symbolic regression)

* **SRBench++: principled benchmarking of symbolic regression** (de Franca et al., 2024)
  **Target Domain:** symbolic regression as function inference from examples
  **Resource:** *SRBench++*, an expanded/diagnostic benchmarking framework emphasizing systematic evaluation challenges. ([PMC][13])

---

## 2025 — Human-attempt datasets for I/O rule induction (ARC behavioral infrastructure)

* **H-ARC (Human-ARC): A Comprehensive Behavioral Dataset for the Abstraction and Reasoning Corpus** (LeGris et al., 2025)
  **Target Domain:** human program induction behavior on ARC tasks
  **Resource:** *H-ARC*, large-scale dataset of **human solution attempts** on ARC—useful for measuring/diagnosing “function inference” strategies, not just final accuracy. ([Nature][14])

* **SRBench update (“next generation” SRBench)** (2025 update paper)
  **Target Domain:** symbolic regression benchmarking (function inference)
  **Resource:** updated SRBench benchmark framing and coverage (more methods / improved protocol). ([arXiv][15])

---

### How this maps to what you described (Taelin-style “universal function finder”)

There are three closely-related “function inference from examples” benchmark *modes*:

1. **PBE string transforms (I/O strings → program)**: FlashFill/BlinkFill/RobustFill line. ([arXiv][7])
2. **I/O-to-program in small DSLs (Karel / DeepCoder / SyGuS / PSB)**: classic discrete search / enumerative / solver-friendly spaces. ([arXiv][9])
3. **Symbolic regression (points → closed-form function)**: SRBench / SRBench++. ([Cava Lab][12])

If you tell me which of these you care about most (Karel-like grid algorithms? bitvector/boolean circuits like your ADD-CARRY example? heap/sort/tree ops?), I can do the same “aggressive second pass” you liked before—specifically hunting **datasets/benchmark suites** that cover *data-structure transforms and algorithmic tasks from examples* (those exist, but they’re scattered across PL/GP/synthesis venues and easy to miss).

[1]: https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/synasc12.pdf?utm_source=chatgpt.com "Synthesis From Examples: Interaction Models and Algorithms"
[2]: https://vldb.org/pvldb/vol5/p740_rishabhsingh_vldb2012.pdf?utm_source=chatgpt.com "Learning Semantic String Transformations from Examples"
[3]: https://www.cis.upenn.edu/~alur/SyGuS13.pdf?utm_source=chatgpt.com "Syntax-Guided Synthesis - CIS UPenn"
[4]: https://www.cs.hamilton.edu/~thelmuth/Pubs/2015-GECCO-benchmark-suite.pdf?utm_source=chatgpt.com "General Program Synthesis Benchmark Suite"
[5]: https://www.vldb.org/pvldb/vol9/p816-singh.pdf?utm_source=chatgpt.com "BlinkFill: Semi-supervised Programming By Example for ..."
[6]: https://arxiv.org/abs/1611.01989?utm_source=chatgpt.com "[1611.01989] DeepCoder: Learning to Write Programs"
[7]: https://arxiv.org/abs/1703.07469?utm_source=chatgpt.com "RobustFill: Neural Program Learning under Noisy I/O"
[8]: https://papers.nips.cc/paper/6803-neural-program-meta-induction?utm_source=chatgpt.com "Neural Program Meta-Induction"
[9]: https://arxiv.org/abs/1805.04276?utm_source=chatgpt.com "Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"
[10]: https://arxiv.org/abs/1911.01547?utm_source=chatgpt.com "[1911.01547] On the Measure of Intelligence"
[11]: https://arxiv.org/pdf/2106.06086?utm_source=chatgpt.com "PSB2: The Second Program Synthesis Benchmark Suite"
[12]: https://cavalab.github.io/symbolic-regression/?utm_source=chatgpt.com "SRBench: Symbolic Regression Benchmarks - Cava Lab"
[13]: https://pmc.ncbi.nlm.nih.gov/articles/PMC12321164/?utm_source=chatgpt.com "SRBench++ : principled benchmarking of symbolic ..."
[14]: https://www.nature.com/articles/s41597-025-05687-1?utm_source=chatgpt.com "A Comprehensive Behavioral Dataset for the Abstraction ..."
[15]: https://arxiv.org/html/2505.03977v1?utm_source=chatgpt.com "Towards the Next Generation of Symbolic Regression ..."
