## 2014 — Programmatically generated “execute the program / compute the number” tasks

* **Learning to Execute** (Zaremba, Sutskever, 2014)
  **Target Domain:** algorithmic computation from sequences (variables, loops/branching-like structure, arithmetic)
  **Resource:** a **synthetic program-generation** setup where models map short programs (as character sequences) to their execution outputs. ([arXiv][1])

---

## 2018 — Nested arithmetic expressions as a controlled compositional-generalization test

* **ListOps: A Diagnostic Dataset for Latent Tree Learning** (Nangia, Bowman, 2018)
  **Target Domain:** compositional computation requiring correct parsing (deeply nested expressions)
  **Resource:** *ListOps*, prefix-style **synthetic arithmetic expression** dataset with a single correct parse strategy. ([ACL Anthology][2])

---

## 2019 — Large-scale “math task suite” generator (the standard synthetic math workhorse)

* **Analysing Mathematical Reasoning Abilities of Neural Models** (Saxton et al., 2019)
  **Target Domain:** broad algorithmic math reasoning (arithmetic, algebra, calculus-like, probability, number theory, etc.)
  **Resource:** *DeepMind Mathematics Dataset*, a **programmatic generator** producing large numbers of question→answer pairs across many math families (often used as a controlled synthetic benchmark). ([arXiv][3])

---

## 2020 — Synthetic “formal languages” as algorithmic-counting / stack-like computation probes (math-adjacent)

* **Long Range Arena (LRA): A Benchmark for Efficient Transformers** (Tay et al., 2020)
  **Target Domain:** long-context computation stress tests, including synthetic/probing tasks
  **Resource:** *LRA* benchmark suite, notably including **Long ListOps** (synthetic nested arithmetic) as a standardized evaluation. ([arXiv][4])

* **How Can Self-Attention Networks Recognize Dyck-n Languages?** (Ebrahimi et al., 2020) — **math-adjacent (stack/counter structure)**
  **Target Domain:** algorithmic generalization / counting and nesting depth
  **Resource:** standardized **Dyck-n synthetic languages** used as a controllable computation benchmark for attention models. ([ACL Anthology][5])

---

## 2024 — Synthetic arithmetic suites explicitly built for length generalization

* **Transformers Can Do Arithmetic with the Right Embeddings** (McLeish et al., NeurIPS 2024)
  **Target Domain:** algorithmic arithmetic with strong length generalization diagnostics
  **Resource:** controlled **synthetic arithmetic tasks** (e.g., addition/multiplication-style setups) used to test extrapolation and digit-structure handling. ([NeurIPS Proceedings][6])

---

## 2025 — Newer synthetic arithmetic benchmarks (LLM-era “numeracy batteries”)

* **A Benchmark to Evaluate Fundamental Numerical Abilities** (Li et al., 2025)
  **Target Domain:** core numeracy / basic arithmetic competency measurement
  **Resource:** an **Arithmetic Operation Dataset** (constructed pairs across +, −, ×, ÷ with controlled digit-length regimes) designed as a diagnostic benchmark. ([arXiv][7])

* **Arithmetic-Bench: Evaluating Multi-Step Reasoning in LLMs through Basic Arithmetic Operations** (OpenReview, 2025)
  **Target Domain:** multi-step arithmetic reasoning + small algorithmic subtasks (copy/reverse/count/base conversion)
  **Resource:** *Arithmetic-Bench*, a benchmark suite centered on basic arithmetic and related algorithmic transformations. ([OpenReview][8])

* **MathClean: A Benchmark for Synthetic Mathematical Data …** (2025) — **meta-infrastructure (synthetic-math data quality)**
  **Target Domain:** evaluation of **synthetic math data cleaning** (error detection/correction for synthetic math corpora)
  **Resource:** *MathClean* benchmark, framed around measuring quality/control issues in synthetic math generation pipelines. ([arXiv][9])

---

### Notes (to match what you meant by “like grokking modular arithmetic”)

* The closest “drop-in generator” that covers *tons* of algorithmic math families (not just modular) is the **DeepMind Mathematics Dataset** generator. ([GitHub][10])
* Several widely-used “synthetic math-ish” probes show up as **benchmark components** (e.g., **ListOps** inside **LRA**). ([OpenReview][11])
* The **Dyck / counter language** line is not “math problems,” but it’s often used for the same purpose: controlled algorithmic structure (stack depth, counting), and frequently co-analyzed alongside synthetic arithmetic. ([ACL Anthology][5])

If you want, I can do an “aggressive second pass” specifically hunting **synthetic arithmetic / expression-evaluation / algorithmic-number** datasets that are easy to miss because the paper title emphasizes *generalization*, *length extrapolation*, or *formal languages*, rather than “dataset/benchmark.”

[1]: https://arxiv.org/abs/1410.4615 "[1410.4615] Learning to Execute"
[2]: https://aclanthology.org/N18-4013/ "ListOps: A Diagnostic Dataset for Latent Tree Learning"
[3]: https://arxiv.org/pdf/1904.01557 "arXiv:1904.01557v1 [cs.LG] 2 Apr 2019"
[4]: https://arxiv.org/abs/2011.04006 "Long Range Arena: A Benchmark for Efficient Transformers"
[5]: https://aclanthology.org/2020.findings-emnlp.384.pdf "How Can Self-Attention Networks Recognize Dyck-n ..."
[6]: https://proceedings.neurips.cc/paper_files/paper/2024/hash/c35986bc1ee29b31c1011481b77fe540-Abstract-Conference.html "Transformers Can Do Arithmetic with the Right Embeddings"
[7]: https://arxiv.org/pdf/2502.11075 "A Benchmark to Evaluate Fundamental Numerical Abilities ..."
[8]: https://openreview.net/forum?id=ae6bKeffGZ&utm_source=chatgpt.com "Arithmetic-Bench: Evaluating Multi-Step Reasoning in ..."
[9]: https://arxiv.org/html/2502.19058v1 "MathClean: A Benchmark for Synthetic Mathematical Data ..."
[10]: https://github.com/google-deepmind/mathematics_dataset "google-deepmind/mathematics_dataset: This dataset code ..."
[11]: https://openreview.net/forum?id=qVyeW-grC2k&utm_source=chatgpt.com "Long Range Arena : A Benchmark for Efficient Transformers"
