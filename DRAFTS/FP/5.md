## 1847 — Gradient Descent

* **Méthode générale pour la résolution des systèmes d’équations simultanées** (Cauchy, 1847)
  **Core idea:** iteratively reduce an objective by stepping in the **direction of steepest decrease** (gradient-based descent with a step-size choice).
  **Breakthrough:** one of the earliest clear formulations of what becomes **gradient descent** as a general optimization procedure. ([Lemaréchal (historical PDF)][1])

---

## 1908 — Early Backpropagation Analysis

* **(Early gradient/variation method note often attributed to Hadamard’s “méthode” discussions)** (Hadamard, 1908)
  **Core idea:** analyzes optimization via **directional change / steepest improvement** ideas that later align with systematic gradient-based error correction viewpoints.
  **Breakthrough:** commonly cited as an early step toward the **mathematical framing** that ultimately supports gradient-based learning/backprop as “just chain rule + optimization.” ([Schmidhuber (historical pointer)][2])

---

## 1913 — Word Co-Occurrence Modeling

* **An Example of Statistical Investigation of the Text “Eugene Onegin” Concerning the Connection of Samples in Chains** (Markov, 1913)
  **Core idea:** models symbol sequences via **state-dependent transition statistics** (the Markov chain viewpoint, demonstrated on text).
  **Breakthrough:** foundational move from independent counts to **sequential dependence modeling**, a precursor to statistical language modeling concepts. ([PDF][3])

---

## 1948 — Next-Token Prediction

* **A Mathematical Theory of Communication** (Shannon, 1948)
  **Core idea:** formalizes information/entropy and analyzes predictability in sequences (including letter/word prediction style experiments).
  **Breakthrough:** establishes the information-theoretic basis underlying **probabilistic modeling and prediction** of the next symbol/token. ([PDF][4])

---

## 1949 — Context Window

* **Translation (Weaver Memorandum)** (Weaver, 1949)
  **Core idea:** frames machine translation as operating on a **bounded local context** (a “window” of surrounding text) plus statistical/cryptographic analogies.
  **Breakthrough:** catalyzes modern MT thinking and helps normalize the idea that limited context can still yield useful translation—an early ancestor of the “context window” concept. ([PDF][5])

---

## 1970 — Autodiff and Backpropagation

* **Algoritmin kumulatiivinen pyöristysvirhe yksittäisten pyöristysvirheiden Taylor-kehitelmänä (The representation of the cumulative rounding error …)** (Linnainmaa, 1970)
  **Core idea:** computes derivatives efficiently via what we now call **reverse-mode automatic differentiation** (backpropagation as systematic chain-rule accumulation).
  **Breakthrough:** one of the earliest explicit descriptions of **reverse-mode AD**, the core engine that makes large-scale neural training practical. ([PDF][6])

---

## 2003 — Word Embeddings: Probabilistic Models

* **A Neural Probabilistic Language Model** (Bengio, Ducharme, Vincent, Jauvin, 2003)
  **Core idea:** learn **distributed word representations** jointly with a probabilistic language model to fight the curse of dimensionality.
  **Breakthrough:** a key foundation for neural LMs: embeddings + neural scoring as a general recipe for language modeling. ([JMLR PDF][7])

---

## 2013 — Word Embeddings: Vector Space Models

* **Efficient Estimation of Word Representations in Vector Space** (Mikolov, Chen, Corrado, Dean, 2013)
  **Core idea:** learn high-quality word vectors with efficient objectives (CBOW/Skip-gram style training).
  **Breakthrough:** makes embeddings **fast and scalable**, accelerating widespread adoption of vector-space word representations. ([arXiv PDF][8])

---

## 2014 — Adam Optimization Algorithm

* **Adam: A Method for Stochastic Optimization** (Kingma & Ba, 2014)
  **Core idea:** adaptive learning rates via **biased-corrected first/second moment estimates** of gradients.
  **Breakthrough:** becomes a default optimizer for deep learning due to strong **robustness and ease of use** across architectures and tasks. ([arXiv PDF][9])

---

## 2017 — Transformer Architecture

* **Attention Is All You Need** (Vaswani et al., 2017)
  **Core idea:** replace recurrence/convolutions with **self-attention** + feedforward blocks for sequence transduction.
  **Breakthrough:** establishes the **Transformer** as the core scalable architecture behind modern LLMs. ([NeurIPS PDF][10])

---

[1]: https://ems.press/content/book-chapter-files/27368?nt=1 "Lemaréchal — Cauchy and the Gradient Method (discusses Cauchy 1847)"
[2]: https://people.idsia.ch/~juergen/who-invented-backpropagation-2014.html "Schmidhuber — historical pointers on early gradient/backprop precursors (incl. Hadamard mention)"
[3]: https://nessie.ilab.sztaki.hu/~kornai/2021/KalmanCL/markov_1913.pdf "Markov (1913 lecture) — Eugene Onegin Markov chain text (PDF)"
[4]: https://ia803209.us.archive.org/27/items/bstj27-3-379/bstj27-3-379_text.pdf "Shannon (1948) — A Mathematical Theory of Communication (BSTJ PDF)"
[5]: https://www.mt-archive.net/50/Weaver-1949.pdf "Weaver (1949) — Translation memorandum (PDF)"
[6]: https://www.idsia.ch/~juergen/linnainmaa1970thesis.pdf "Linnainmaa (1970) — MSc thesis (PDF)"
[7]: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf "Bengio et al. (2003) — A Neural Probabilistic Language Model (JMLR PDF)"
[8]: https://arxiv.org/pdf/1301.3781 "Mikolov et al. (2013) — Efficient Estimation of Word Representations in Vector Space (arXiv PDF)"
[9]: https://arxiv.org/pdf/1412.6980 "Kingma & Ba (2014) — Adam: A Method for Stochastic Optimization (arXiv PDF)"
[10]: https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf "Vaswani et al. (2017) — Attention Is All You Need (NeurIPS PDF)"
