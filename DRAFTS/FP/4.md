## 2016 — Differentiable logic constraints as a learning signal

* **Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge** (Serafini & d’Avila Garcez, 2016)
  **Missing capability:** neural nets don’t naturally enforce **first-order logic constraints** during learning/inference.
  **Mechanism:** introduce **Real Logic** (truth values in [0,1]) and optimize neural parameters under differentiable **logic satisfiability-style losses** (constraints as soft differentiable objectives). ([arXiv][1])

---

## 2017 — Neural theorem proving (differentiable backward chaining)

* **End-to-End Differentiable Proving** (Rocktäschel & Riedel, 2017)
  **Missing capability:** dense embedding models struggle with **explicit multi-hop symbolic proof structure** (unification, rule application).
  **Mechanism:** **Neural Theorem Provers (NTPs)**: continuous relaxation of Prolog-style **backward chaining**, replacing unification with differentiable similarity kernels over embeddings. ([arXiv][2])

---

## 2018 — Learning SAT solving behavior with message passing + differentiable ILP

* **Learning a SAT Solver from Single-Bit Supervision (NeuroSAT)** (Selsam et al., 2018)
  **Missing capability:** classical SAT solvers have strong heuristics but aren’t learnable end-to-end; neural nets lack structured clause–variable reasoning.
  **Mechanism:** a message-passing network over the SAT factor graph that can **run more iterations at test time** to solve harder instances (iterative constraint propagation learned from data). ([arXiv][3])

* **Learning Explanatory Rules from Noisy Data / Differentiable ILP (∂ILP)** (Evans & Grefenstette, 2018)
  **Missing capability:** symbolic ILP is brittle to noise; neural models don’t induce **explicit logic programs**.
  **Mechanism:** make ILP **end-to-end differentiable** by relaxing rule selection/unification into soft, trainable weights—learning logic programs via gradient descent. ([Jair][4])

* **DeepProbLog: Neural Probabilistic Logic Programming** (Manhaeve et al., 2018)
  **Missing capability:** integrate neural perception with **probabilistic logical inference** in one training loop.
  **Mechanism:** extend ProbLog with **neural predicates**, enabling joint learning where symbolic reasoning (probabilistic logic) and neural modules co-train. ([arXiv][5])

---

## 2019 — “Solvers as layers”: differentiable MaxSAT and convex optimization as CSP engines

* **SATNet: Bridging Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver** (Wang et al., 2019)
  **Missing capability:** standard nets can’t reliably satisfy global discrete constraints; discrete SAT is non-differentiable.
  **Mechanism:** a differentiable (smoothed) **MAXSAT** solver layer (SDP/coordinate-descent flavored) that can be embedded into neural systems and trained end-to-end. ([arXiv][6])

* **OptNet: Differentiable Optimization as a Layer in Neural Networks** (Amos & Kolter, 2017; widely used in this thread of work)
  **Missing capability:** feedforward layers struggle to enforce **hard global constraints** that are naturally expressed as optimization problems.
  **Mechanism:** embed a constrained **QP** solver as a differentiable layer via implicit differentiation (demonstrated on constraint-heavy tasks like mini-Sudoku). ([arXiv][7])

* **Differentiable Convex Optimization Layers (CVXPYLayers / DPP)** (Agrawal et al., NeurIPS 2019)
  **Missing capability:** constraint satisfaction / optimization priors are hard to integrate broadly; custom differentiable solvers are brittle.
  **Mechanism:** general method + tooling for differentiating through **disciplined convex programs**, turning a wide class of convex CSP-like modules into plug-in layers. ([arXiv][8])

* **Neural Logic Machines (NLM)** (Dong et al., 2019)
  **Missing capability:** robust **lifted logical reasoning** (quantifiers, connectives) and generalization to larger object sets.
  **Mechanism:** neural–symbolic blocks that implement differentiable approximations to logical operations over relations, enabling rule-like, size-generalizing reasoning. ([arXiv][9])

---

## 2019 — Neural guidance for *classical* SAT solvers (hybrid neuro-symbolic search)

* **Guiding SAT Solvers with Unsat-Core Predictions (NeuroCore)** (Selsam et al., 2019)
  **Missing capability:** CDCL SAT solvers rely on hand-designed branching heuristics that can miss structure in hard industrial instances.
  **Mechanism:** a neural module predicts **unsat cores** to bias variable branching inside a CDCL solver—tight integration of learning into symbolic search. ([arXiv][10])

---

## 2021 — Neural SAT solving as goal-directed algorithm design

* **Goal-Aware Neural SAT Solver (QuerySAT / goal-aware guidance)** (Ozolins et al., 2021)
  **Missing capability:** purely learned solvers can be inefficient; classical solvers lack learned guidance tuned to goal/structure.
  **Mechanism:** “goal-aware” neural guidance integrated into SAT solving workflow (positioned as a learned improvement to SAT search/heuristics). ([arXiv][11])

* **diff-SAT – Sampling and Probabilistic Reasoning for SAT and Answer Set Programming** (Nickles, 2021)
  **Missing capability:** standard SAT/ASP doesn’t natively optimize against probabilistic constraints or gradient-like objectives.
  **Mechanism:** a SAT/ASP solver variant supporting **differentiable / probabilistic constraints** to steer solution sampling/optimization. ([arXiv][12])

---

## 2024–2025 — New differentiable SAT layers and “SAT-as-differentiable-search”

* **DiffSAT: Differential MaxSAT Layer for SAT Solving** (Zhang et al., ICCAD 2024 / ACM 2025 listings)
  **Missing capability:** end-to-end pipelines still struggle with discrete satisfaction; existing neural SAT solvers often learn predictors rather than a differentiable solving mechanism.
  **Mechanism:** a **differential MaxSAT layer** (with structured initialization) that enables progressive search toward satisfying assignments in a differentiable framework. ([CUHK Computer Science and Engineering][13])

* **(Survey/bridge work) Enhancing Modern SAT Solver With Machine Learning… (NeuroCore context)** (2025) — **borderline (more “integration & evaluation” than a new core solver)**
  **Missing capability:** practical recipe for reliably injecting ML into CDCL pipelines.
  **Mechanism:** consolidates/extends the NeuroCore-style idea of learning-guided heuristics in modern SAT workflows. ([ACM Digital Library][14])

---

### If you want to tighten the scope

Right now this list includes **three intertwined “solver mechanisms” families**:

1. **Differentiable logic / ILP / proving** (LTN, NTP, ∂ILP, DeepProbLog, NLM)
2. **Solvers-as-layers** for constraint satisfaction/optimization (OptNet, CVXPYLayers, SATNet, DiffSAT)
3. **Hybrid neural-guided symbolic SAT search** (NeuroSAT/NeuroCore/goal-aware solvers)

If you tell me which of these you want to be the *center* (e.g., “only SAT/CSP solvers, exclude probabilistic logic programming”), I can do a second pass in the **same timeline format** that’s even more exhaustive within that slice.

[1]: https://arxiv.org/abs/1606.04422?utm_source=chatgpt.com "Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge"
[2]: https://arxiv.org/abs/1705.11040?utm_source=chatgpt.com "End-to-End Differentiable Proving"
[3]: https://arxiv.org/abs/1802.03685?utm_source=chatgpt.com "Learning a SAT Solver from Single-Bit Supervision"
[4]: https://jair.org/index.php/jair/article/view/11172/26376?utm_source=chatgpt.com "View of Learning Explanatory Rules from Noisy Data"
[5]: https://arxiv.org/abs/1805.10872?utm_source=chatgpt.com "DeepProbLog: Neural Probabilistic Logic Programming"
[6]: https://arxiv.org/pdf/1905.12149?utm_source=chatgpt.com "SATNet: Bridging deep learning and logical reasoning ..."
[7]: https://arxiv.org/abs/1703.00443?utm_source=chatgpt.com "OptNet: Differentiable Optimization as a Layer in Neural Networks"
[8]: https://arxiv.org/abs/1910.12430?utm_source=chatgpt.com "Differentiable Convex Optimization Layers"
[9]: https://arxiv.org/abs/1904.11694?utm_source=chatgpt.com "Neural Logic Machines"
[10]: https://arxiv.org/pdf/1903.04671?utm_source=chatgpt.com "arXiv:1903.04671v7 [cs.NE] 19 Jul 2019"
[11]: https://arxiv.org/pdf/2106.07162?utm_source=chatgpt.com "Goal-Aware Neural SAT Solver"
[12]: https://arxiv.org/abs/2101.00589?utm_source=chatgpt.com "diff-SAT -- A Software for Sampling and Probabilistic ..."
[13]: https://www.cse.cuhk.edu.hk/~byu/papers/C237-ICCAD2024-DiffSAT.pdf?utm_source=chatgpt.com "DiffSAT: Differential MaxSAT Layer for SAT Solving"
[14]: https://dl.acm.org/doi/full/10.1145/3716368.3735251?utm_source=chatgpt.com "Enhancing Modern SAT Solver With Machine Learning ..."
