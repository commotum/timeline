## 1957 — Modern RL’s control substrate: MDPs + DP recursion

* **A Markovian Decision Process** (Bellman, 1957)
  **Core idea:** formalizes sequential decision-making under uncertainty as an **MDP** and derives Bellman-style recursions.
  **Breakthrough:** establishes the optimization backbone behind value functions, policy improvement, and dynamic programming. ([JSTOR][1])

---

## 1983 — Actor–critic as a reusable learning/control template

* **Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems** (Barto, Sutton, Anderson, 1983)
  **Core idea:** learn control by combining a learned **critic** (value prediction) with an **actor** (policy parameters).
  **Breakthrough:** early, influential framing of **actor–critic** as a general mechanism for RL control with function approximation. ([incompleteideas.net][2])

---

## 1988 — Temporal-difference learning (bootstrapping) becomes a field cornerstone

* **Learning to Predict by the Methods of Temporal Differences** (Sutton, 1988)
  **Core idea:** learn predictions by bootstrapping from **successive predictions** (TD error), not only from terminal returns.
  **Breakthrough:** establishes **TD learning** as a distinct foundation for prediction/control bridging Monte Carlo and DP. ([incompleteideas.net][3])

---

## 1991 — Model-based RL made practical: learn + plan in one loop

* **Dyna, an Integrated Architecture for Learning, Planning, and Reacting** (Sutton, 1991)
  **Core idea:** integrate real experience with simulated experience from a learned model.
  **Breakthrough:** codifies the **Dyna** paradigm (interleaving model learning, planning updates, and acting) as a general blueprint for model-based RL. ([ACM Digital Library][4])

---

## 1992 — Two pillars: off-policy value control + policy-gradient RL

* **Technical Note: Q-learning** (Watkins & Dayan, 1992; based on Watkins 1989)
  **Core idea:** **off-policy control** via max backup on action-values.
  **Breakthrough:** simple, general **model-free control** algorithm with broad downstream influence. ([Springer][5])

* **Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning (REINFORCE)** (Williams, 1992)
  **Core idea:** optimize expected return directly via **stochastic policy gradients**.
  **Breakthrough:** a canonical foundation for modern **policy gradient** and actor–critic methods. ([Springer][6])

* **TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play** (Tesauro, 1992/1993)
  **Core idea:** **self-play + TD(λ) + function approximation** to improve play without explicit search.
  **Breakthrough:** iconic early demonstration that TD methods can yield strong performance at scale. ([AAAI][7])

---

## 1994 — On-policy control formalized in practice (SARSA)

* **On-line Q-learning Using Connectionist Systems** (Rummery & Niranjan, 1994)
  **Core idea:** online on-policy action-value learning (widely associated with **SARSA**).
  **Breakthrough:** establishes a practical, reusable **on-policy TD control** recipe used throughout deep RL and beyond. ([ResearchGate][8])

---

## 1999 — Temporal abstraction: actions that last (options framework)

* **A Framework for Temporal Abstraction in Reinforcement Learning** (Sutton, Precup, Singh, 1999)
  **Core idea:** extend primitive actions to **options** (policies over variable time spans).
  **Breakthrough:** foundational framework for **hierarchical RL** and semi-MDP reasoning. ([ScienceDirect][9])

---

## 1999–2000 — Actor–critic becomes theory-grounded and reusable

* **Actor–Critic Algorithms** (Konda & Tsitsiklis, 1999/2000)
  **Core idea:** two-timescale stochastic approximation: critic learns (TD), actor follows approximate gradient using critic signal.
  **Breakthrough:** convergence-oriented, general formulation of **actor–critic** as a scalable family. ([NeurIPS Papers][10])

---

## 2015 — Deep RL template (value-based) + stable policy optimization + modern replay

* **Human-level Control through Deep Reinforcement Learning (DQN)** (Mnih et al., 2015)
  **Core idea:** deep Q-learning stabilized by **experience replay** and **target networks**.
  **Breakthrough:** launches the modern deep RL era and a reusable value-learning template. ([Stanford University][11])

* **Trust Region Policy Optimization (TRPO)** (Schulman et al., 2015)
  **Core idea:** constrain policy updates (trust region) for stable improvement.
  **Breakthrough:** foundational stability principle for modern on-policy optimization methods. ([arXiv][12])

* **Prioritized Experience Replay** (Schaul et al., 2015)
  **Core idea:** replay important transitions more often (e.g., by TD error).
  **Breakthrough:** replay becomes an explicit, general **data-selection** mechanism in off-policy RL. ([arXiv][13])

* **Continuous Control with Deep Reinforcement Learning (DDPG)** (Lillicrap et al., 2015/ICLR 2016)
  **Core idea:** deterministic actor–critic for continuous actions with off-policy learning.
  **Breakthrough:** anchors deep continuous-control RL around deterministic policy gradients + replay. ([arXiv][14])

---

## 2016 — Scaling and generality: asynchronous training + AlphaGo’s modern recipe

* **Asynchronous Methods for Deep Reinforcement Learning (A3C)** (Mnih et al., 2016)
  **Core idea:** parallel actor-learners stabilize and speed training.
  **Breakthrough:** establishes a scalable, widely reused paradigm for deep **actor–critic**. ([arXiv][15])

* **Mastering the Game of Go with Deep Neural Networks and Tree Search (AlphaGo)** (Silver et al., 2016)
  **Core idea:** combine policy/value networks with **MCTS**, trained with supervised + **RL self-play**.
  **Breakthrough:** breakthrough system where RL is central and the policy/value + search template becomes a reusable paradigm. ([Nature][16])

---

## 2017 — Practical policy gradients + distributional value learning

* **Proximal Policy Optimization (PPO)** (Schulman et al., 2017)
  **Core idea:** a clipped surrogate objective enabling multiple epochs of minibatch updates stably.
  **Breakthrough:** becomes a default, general-purpose deep RL workhorse (including RLHF pipelines later). ([arXiv][17])

* **A Distributional Perspective on Reinforcement Learning** (Bellemare, Dabney, Munos, 2017)
  **Core idea:** learn the **distribution of returns**, not just the expectation.
  **Breakthrough:** establishes **distributional RL** as a foundational viewpoint + algorithmic family. ([arXiv][18])

---

## 2018 — Self-play RL as a general engine + distributed RL + modern “best-of-DQN” stacks

* **Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (AlphaZero)** (Silver et al., 2017/2018)
  **Core idea:** tabula rasa **self-play RL** + MCTS with a unified algorithm across games.
  **Breakthrough:** “AlphaZero pattern” becomes a reusable scalable paradigm for search + RL. ([arXiv][19])

* **IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures** (Espeholt et al., 2018)
  **Core idea:** decouple acting and learning; stabilize with off-policy correction (V-trace).
  **Breakthrough:** foundational **distributed RL architecture** for high-throughput, multi-task training. ([arXiv][20])

* **Soft Actor-Critic (SAC)** (Haarnoja et al., 2018)
  **Core idea:** maximum-entropy objective: maximize return **and** entropy (robust exploration / stability).
  **Breakthrough:** cements max-entropy RL as a general foundation for strong off-policy continuous control. ([arXiv][21])

* **Rainbow: Combining Improvements in Deep Reinforcement Learning** (Hessel et al., 2018)
  **Core idea:** systematically combine complementary DQN improvements.
  **Breakthrough:** popularizes a modular “building-block” stack for scalable value-based RL. ([arXiv][22])

---

## 2020 — Planning with learned models (model-based RL + search unified)

* **Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero)** (Schrittwieser et al., 2019/2020)
  **Core idea:** learn an internal model sufficient for planning (predict reward/value/policy), then do tree search.
  **Breakthrough:** unifies model-based RL and search into a general scalable recipe without known dynamics. ([Nature][23])

---

## 2025 — RL beyond control: reasoning as an RL objective

* **DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning** (Guo et al., 2025)
  **Core idea:** treat sustained multi-step reasoning behavior as an **RL optimization target**, including a pure-RL (no-SFT) variant.
  **Breakthrough:** high-profile demonstration that RL fine-tuning can reliably induce strong reasoning behaviors in LMs (RL objective is central). ([arXiv][24])

---

If you want the same “second pass” treatment as before, I can do an **aggressive 2019–2025 sweep** for *foundational* deep RL building blocks that often slip through “foundation lists” (e.g., specific stability principles, off-policy corrections, scalable replay/training architectures) while still meeting your criterion #2.

[1]: https://www.jstor.org/stable/24900506?utm_source=chatgpt.com "A Markovian Decision Process"
[2]: https://incompleteideas.net/papers/barto-sutton-anderson-83.pdf?utm_source=chatgpt.com "Neuronlike Adaptive Elements That Can Solve"
[3]: https://incompleteideas.net/papers/sutton-88-with-erratum.pdf?utm_source=chatgpt.com "Learning to predict by the methods of temporal differences"
[4]: https://dl.acm.org/doi/abs/10.1145/122344.122377?utm_source=chatgpt.com "Dyna, an integrated architecture for learning, planning, and ..."
[5]: https://link.springer.com/article/10.1023/A%3A1022676722315?utm_source=chatgpt.com "Technical Note: Q-Learning | Machine Learning - Springer Link"
[6]: https://link.springer.com/article/10.1007/BF00992696?utm_source=chatgpt.com "Simple statistical gradient-following algorithms for ..."
[7]: https://cdn.aaai.org/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf?utm_source=chatgpt.com "TD-Gammon, A Self-teaching Backgammon Program, ..."
[8]: https://www.researchgate.net/profile/Mahesan-Niranjan/publication/2500611_On-Line_Q-Learning_Using_Connectionist_Systems/links/5438d5db0cf204cab1d6db0f/On-Line-Q-Learning-Using-Connectionist-Systems.pdf?utm_source=chatgpt.com "On-Line-Q-Learning-Using-Connectionist-Systems. ..."
[9]: https://www.sciencedirect.com/science/article/pii/S0004370299000521?utm_source=chatgpt.com "A framework for temporal abstraction in reinforcement ..."
[10]: https://papers.nips.cc/paper/1786-actor-critic-algorithms?utm_source=chatgpt.com "Actor-Critic Algorithms"
[11]: https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf?utm_source=chatgpt.com "Human level control through deep reinforcement learning"
[12]: https://arxiv.org/abs/1502.05477?utm_source=chatgpt.com "Trust Region Policy Optimization"
[13]: https://arxiv.org/abs/1511.05952?utm_source=chatgpt.com "Prioritized Experience Replay"
[14]: https://arxiv.org/abs/1509.02971?utm_source=chatgpt.com "Continuous control with deep reinforcement learning"
[15]: https://arxiv.org/abs/1602.01783?utm_source=chatgpt.com "Asynchronous Methods for Deep Reinforcement Learning"
[16]: https://www.nature.com/articles/nature16961?utm_source=chatgpt.com "Mastering the game of Go with deep neural networks and ..."
[17]: https://arxiv.org/abs/1707.06347?utm_source=chatgpt.com "Proximal Policy Optimization Algorithms"
[18]: https://arxiv.org/abs/1707.06887?utm_source=chatgpt.com "A Distributional Perspective on Reinforcement Learning"
[19]: https://arxiv.org/abs/1712.01815?utm_source=chatgpt.com "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"
[20]: https://arxiv.org/abs/1802.01561?utm_source=chatgpt.com "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures"
[21]: https://arxiv.org/abs/1812.05905?utm_source=chatgpt.com "[1812.05905] Soft Actor-Critic Algorithms and Applications"
[22]: https://arxiv.org/abs/1710.02298?utm_source=chatgpt.com "Combining Improvements in Deep Reinforcement Learning"
[23]: https://www.nature.com/articles/s41586-020-03051-4?utm_source=chatgpt.com "Mastering Atari, Go, chess and shogi by planning with a ..."
[24]: https://arxiv.org/abs/2501.12948?utm_source=chatgpt.com "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs ..."
