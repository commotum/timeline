## 2020 — Deformable Attention for Detection

* **Deformable DETR: Deformable Transformers for End-to-End Object Detection** (2020)
  **Improves On:** DETR’s global attention for dense 2D detection.
  **Adaptation:** sparse, multi-scale *deformable attention* for efficient set-based object detection. ([arXiv][33-1])



## 2020 — Data-Efficient ViTs via Distillation

* **Training data-efficient image transformers & distillation through attention** (2020)
  **Contribution Type:** training methodology for Vision Transformers.
  **Reason:** make ViTs work well with less data via a *distillation token* + teacher supervision. ([arXiv][33-2])



## 2020 — Discrete Latents for High-Res Image Generation

* **Taming Transformers for High-Resolution Image Synthesis** (2020)
  **Improves On:** pixel-space autoregressive image Transformers.
  **Adaptation:** learn a discrete image codebook (VQ-style) then model code sequences with a Transformer for high-res synthesis. ([arXiv][33-3])



## 2020 — Pixel-Space Autoregressive Image Modeling

* **Generative Pretraining from Pixels** (2020)
  **Improves On:** 1D autoregressive Transformers for text.
  **Adaptation:** serialize 2D images into long 1D pixel sequences for causal Transformer pretraining. ([OpenAI][33-4])



## 2021 — Zero-Shot Text-to-Image via Joint Token Modeling

* **Zero-Shot Text-to-Image Generation** (2021)
  **Improves On:** separate text-only or image-only sequence modeling.
  **Adaptation:** jointly model text + discrete image tokens to generate images from text prompts (zero-shot). ([arXiv][33-5])



## 2021 — Pyramid Feature Hierarchies for Dense Vision

* **Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions** (2021)
  **Improves On:** single-scale ViT backbones for dense prediction.
  **Adaptation:** multi-stage pyramid token hierarchies to support detection/segmentation-style features. ([arXiv][33-6])



## 2021 — Cross-Attention Between Multi-Scale Token Streams

* **CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification** (2021)
  **Improves On:** single-patch-scale ViT classification.
  **Adaptation:** dual (multi) patch scales with cross-attention fusion between token streams. ([arXiv][33-7])



## 2021 — Convolutions Inside ViTs

* **CvT: Introducing Convolutions to Vision Transformers** (2021)
  **Improves On:** pure ViT patch embedding + attention blocks.
  **Adaptation:** convolutional token embedding/projection to inject locality while keeping attention central. ([arXiv][33-8])



## 2021 — Hierarchical “Transformer-in-Transformer” Vision Modeling

* **Transformer in Transformer** (2021)
  **Improves On:** standard ViT tokenization and global attention.
  **Adaptation:** inner Transformers within local regions plus an outer Transformer across regions (hierarchical). ([arXiv][33-9])



## 2021 — Efficient Spatial Attention Design for High Resolution

* **Twins: Revisiting the Design of Spatial Attention in Vision Transformers** (2021)
  **Improves On:** full global attention cost at high resolution.
  **Adaptation:** mix local window attention with global (subsampled) attention for scalable 2D backbones. ([arXiv][33-10])



## 2021 — Simple Transformer Segmentation Backbone + Lightweight Head

* **SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers** (2021)
  **Improves On:** heavyweight segmentation decoders and non-unified pipelines.
  **Adaptation:** Transformer encoder with a lightweight MLP decode head for efficient dense prediction. ([arXiv][33-11])



## 2021 — Masked-Token Pretraining for Vision

* **BEiT: BERT Pre-Training of Image Transformers** (2021)
  **Contribution Type:** self-supervised pretraining objective.
  **Reason:** adapt BERT-style masked prediction to image tokens to improve ViT representations. ([arXiv][33-12])



## 2021 — Hybrid Conv + Attention Scaling Backbone

* **CoAtNet: Marrying Convolution and Attention for All Data Sizes** (2021)
  **Improves On:** purely conv or purely attention vision stacks.
  **Adaptation:** stage-wise hybrid backbone combining conv layers with attention layers for strong scaling. ([arXiv][33-13])



## 2021 — Cross-Shaped Window Attention

* **CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows** (2021)
  **Improves On:** window attention with limited long-range mixing.
  **Adaptation:** cross-shaped attention windows to expand receptive field efficiently on 2D grids. ([arXiv][33-14])



## 2021 — Scaling Windowed ViTs

* **Swin Transformer V2: Scaling Up Capacity and Resolution** (2021)
  **Improves On:** Swin-style windowed Transformers at larger scales and resolutions.
  **Adaptation:** architectural/training refinements to stabilize and scale window-based attention for high-res vision. ([arXiv][33-15])



## 2022 — Multi-Axis Attention Mixing

* **MaxViT: Multi-Axis Vision Transformer** (2022)
  **Improves On:** either local-window-only or global-only attention patterns.
  **Adaptation:** combine local window attention with grid/axis-style global mixing for scalable vision backbones. ([arXiv][33-16])



## 2022 — BEiT-Style Pretraining Generalized to Vision-Language Transfer

* **Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks** (2022)
  **Contribution Type:** unified pretraining recipe.
  **Reason:** extend BEiT-style masked modeling to support transfer across vision and vision-language tasks. ([arXiv][33-17])



## 2023 — Segment Anything as a Resource + Evaluation Lens

* **Segment Anything** (2023)
  **Target Domain:** promptable, general-purpose image segmentation.
  **Resource:** large-scale segmentation data + a prompt-conditioned model enabling broad, zero-shot segmentation behavior. ([arXiv][33-18])

---

[33-1]: https://arxiv.org/pdf/2010.04159.pdf "Deformable DETR (2020) — arXiv"
[33-2]: https://arxiv.org/pdf/2012.12877.pdf "DeiT: Data-efficient Image Transformers (2020) — arXiv"
[33-3]: https://arxiv.org/pdf/2012.09841.pdf "Taming Transformers (2020) — arXiv"
[33-4]: https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf "Generative Pretraining from Pixels (2020) — OpenAI"
[33-5]: https://arxiv.org/pdf/2102.12092.pdf "Zero-Shot Text-to-Image Generation (2021) — arXiv"
[33-6]: https://arxiv.org/pdf/2102.12122.pdf "Pyramid Vision Transformer (2021) — arXiv"
[33-7]: https://arxiv.org/pdf/2103.14899.pdf "CrossViT (2021) — arXiv"
[33-8]: https://arxiv.org/pdf/2103.15808.pdf "CvT (2021) — arXiv"
[33-9]: https://arxiv.org/pdf/2103.00112.pdf "Transformer in Transformer (2021) — arXiv"
[33-10]: https://arxiv.org/pdf/2104.13840.pdf "Twins (2021) — arXiv"
[33-11]: https://arxiv.org/pdf/2105.15203.pdf "SegFormer (2021) — arXiv"
[33-12]: https://arxiv.org/pdf/2106.08254.pdf "BEiT (2021) — arXiv"
[33-13]: https://arxiv.org/pdf/2106.04803.pdf "CoAtNet (2021) — arXiv"
[33-14]: https://arxiv.org/pdf/2107.00652.pdf "CSWin Transformer (2021) — arXiv"
[33-15]: https://arxiv.org/pdf/2111.09883.pdf "Swin Transformer V2 (2021) — arXiv"
[33-16]: https://arxiv.org/pdf/2204.01697.pdf "MaxViT (2022) — arXiv"
[33-17]: https://arxiv.org/pdf/2208.10442.pdf "Image as a Foreign Language / BEiT pretraining (2022) — arXiv"
[33-18]: https://arxiv.org/pdf/2304.02643.pdf "Segment Anything (2023) — arXiv"
