- **Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs** (2024)
  **Critique:** weak position extrapolation beyond trained context windows.
  **Improvement:** “weave” positional encoding to improve long-context extrapolation behavior. ([NeurIPS Proceedings][34-1])
- **Adaptive Patch Selection for ViTs via Reinforcement Learning** (2025)
  **Missing capability:** adaptive compute—processing all patches is wasteful for many images.
  **Mechanism:** RL-driven patch selection to dynamically choose informative tokens for ViTs. ([Springer][34-2])
- **Olympiad-level formal mathematical reasoning with large language models (AlphaProof)** (2025)
  **Missing capability:** reliable multi-step formal proof search and verification.
  **Mechanism:** LLM-guided formal reasoning integrated with proof-system checking to reach olympiad-level results. ([Nature][34-3])
- **The Rotary Position Embedding May Cause Dimension Inefficiency** (2025)
  **Critique:** RoPE can waste representational capacity across embedding dimensions.
  **Improvement:** analysis + guidance/variants aimed at more dimension-efficient rotary position usage. ([arXiv][34-4])
- **VRoPE: Rotary Position Embedding for Video Large Language Models** (2025)
  **Improves On:** text-only RoPE that does not natively encode space-time structure.
  **Adaptation:** extend rotary positional encoding to video (spatiotemporal) token streams for Video LLMs. ([arXiv][34-5])
- **Maximizing the Position Embedding for Vision Transformers (MPVG)** (2025)
  **Critique:** standard ViT positional embeddings underutilize/limit positional capacity for vision.
  **Improvement:** redesigned vision position embedding scheme to better exploit positional signal in ViTs. ([Local Copy][34-6])
- **SmolVLM: Redefining small and efficient multimodal models** (2025)
  **Contribution Type:** efficiency-focused multimodal model design.
  **Reason:** achieve strong vision-language performance under tight size/compute constraints. ([arXiv][34-7])
- **LOOPE: Learnable Optimal Patch Order in Vision Transformers** (2025)
  **Critique:** fixed raster scan patch order is an arbitrary inductive bias for images.
  **Improvement:** learn an optimal patch ordering to improve positional treatment in ViTs. ([arXiv][34-8])
- **Gated Attention for LLMs: Non-linearity, Sparsity, Sink-Free** (2025)
  **Contribution Type:** attention-layer redesign.
  **Reason:** introduce gating/nonlinearity to encourage sparsity and avoid attention pathologies (e.g., sink behavior). ([arXiv][34-9])
- **Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models** (2025)
  **Critique:** standard RoPE can entangle content and geometry for vision-language alignment.
  **Improvement:** decoupled “cone-like” rotary scheme to better separate and encode spatial structure. ([arXiv][34-10])
- **Rotary Masked Autoencoders Are Versatile Learners** (2025)
  **Contribution Type:** vision self-supervised learning framework refinement.
  **Reason:** combine MAE-style pretraining with rotary-based position handling for broader transfer. ([arXiv][34-11])
- **LLaVA-4D: Embedding Spatiotemporal Prompt into LMMs** (2025)
  **Improves On:** image-centric VLMs that lack explicit spatiotemporal (4D) prompting.
  **Adaptation:** embed spatiotemporal prompts/tokens so multimodal LMs can operate over space-time inputs. ([arXiv][34-12])
- **ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices** (2025)
  **Critique:** fixed-form rotary angles can be brittle across scales/domains.
  **Improvement:** trainable commuting angle matrices to parameterize RoPE more flexibly and robustly. ([arXiv][34-13])
- **Hierarchical Reasoning Model (HRM)** (2025)
  **Missing capability:** deep, stable latent reasoning with variable compute.
  **Mechanism:** hierarchical recurrent computation with fast–slow modules and adaptive halting. ([arXiv][34-14])
- **EVA02-AT: Egocentric Video-Language with Spatial-Temporal RoPE** (2025)
  **Improves On:** VLM encodings that do not cleanly represent space-time for egocentric video.
  **Adaptation:** spatial-temporal RoPE for video-language modeling in egocentric settings. ([arXiv][34-15])
- **TransXSSM: Hybrid Transformer–SSM with Unified RoPE** (2025)
  **Missing capability:** long-context sequence modeling with better scaling than pure attention.
  **Mechanism:** hybrid Transformer–state-space model architecture with a unified RoPE scheme. ([arXiv][34-16])
- **Context-aware Rotary Position Embedding (CARoPE)** (2025)
  **Critique:** context-agnostic rotary encodings may be suboptimal across varying contexts/lengths.
  **Improvement:** make rotary position encoding adapt based on context to improve robustness/extrapolation. ([arXiv][34-17])
- **Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings (PoPE)** (2025)
  **Critique:** entangled content and position signals in common encodings.
  **Improvement:** polar-coordinate positional embedding to separate “what” from “where.” ([arXiv][34-18])
- **Less is More: Recursive Reasoning with Tiny Networks** (2025)
  **Missing capability:** strong multi-step reasoning in small models without simply scaling parameters.
  **Mechanism:** recursive/iterative reasoning procedure enabling tiny networks to refine answers over steps. ([arXiv][34-19])
- **Head-Wise Adaptive Rotary Positional Encoding (HARoPE)** (2025)
  **Critique:** a single shared RoPE schedule can be too rigid across attention heads and modalities.
  **Improvement:** head-wise adaptive rotary encoding so different heads learn different positional behaviors. ([arXiv][34-20])
- **Nested Learning: The Illusion of Deep Learning Architecture** (2025)
  **Contribution Type:** theory/analysis of learning dynamics and depth.
  **Reason:** argues observed “depth benefits” can arise from nested learning effects rather than architecture alone. ([Local Copy][34-21])
- **DoPE: Denoising Rotary Position Embedding** (2025)
  **Critique:** rotary position signals can accumulate noise or become unstable under long-context use.
  **Improvement:** denoising strategy for RoPE to improve stability and downstream performance. ([arXiv][34-22])
- **WALRUS: A Cross-Domain Foundation Model for Continuum Dynamics** (2025)
  **Improves On:** narrow, domain-specific models for PDE/continuum dynamics.
  **Adaptation:** foundation-model approach that tokenizes/represents continuum dynamics to transfer across domains. ([arXiv][34-23])
- **Selective Rotary Position Embedding** (2025)
  **Critique:** applying rotary position uniformly can be inefficient or harmful in some layers/heads/tokens.
  **Improvement:** selectively apply rotary position to the most beneficial components for better efficiency/quality. ([arXiv][34-24])

[34-1]: https://proceedings.neurips.cc/paper_files/paper/2024/file/9446c291a8744a125a0bda5b18f4d5a1-Paper-Conference.pdf "Mesa-Extrapolation (2024) — NeurIPS Proceedings"
[34-2]: https://doi.org/10.1007/s10489-025-06516-z "Adaptive Patch Selection for ViTs via RL (2025) — Springer"
[34-3]: https://www.nature.com/articles/s41586-025-09833-y.pdf "AlphaProof (2025) — Nature"
[34-4]: https://arxiv.org/pdf/2502.11276.pdf "The Rotary Position Embedding May Cause Dimension Inefficiency (2025) — arXiv"
[34-5]: https://arxiv.org/pdf/2502.11664.pdf "VRoPE (2025) — arXiv"
[34-6]: # "MPVG (2025) — Local Copy (no public PDF link provided)"
[34-7]: https://arxiv.org/pdf/2504.05299.pdf "SmolVLM (2025) — arXiv"
[34-8]: https://arxiv.org/pdf/2504.14386.pdf "LOOPE (2025) — arXiv"
[34-9]: https://arxiv.org/pdf/2505.06708.pdf "Gated Attention for LLMs (2025) — arXiv"
[34-10]: https://arxiv.org/pdf/2505.16416.pdf "Circle-RoPE (2025) — arXiv"
[34-11]: https://arxiv.org/pdf/2505.20535.pdf "Rotary Masked Autoencoders (2025) — arXiv"
[34-12]: https://arxiv.org/pdf/2505.12253.pdf "LLaVA-4D (2025) — arXiv"
[34-13]: https://arxiv.org/pdf/2506.03737.pdf "ComRoPE (2025) — arXiv"
[34-14]: https://arxiv.org/pdf/2506.21734.pdf "Hierarchical Reasoning Model (2025) — arXiv"
[34-15]: https://arxiv.org/pdf/2506.14356.pdf "EVA02-AT (2025) — arXiv"
[34-16]: https://arxiv.org/pdf/2506.09507.pdf "TransXSSM (2025) — arXiv"
[34-17]: https://arxiv.org/pdf/2507.23083.pdf "CARoPE (2025) — arXiv"
[34-18]: https://arxiv.org/pdf/2509.10534.pdf "PoPE (2025) — arXiv"
[34-19]: https://arxiv.org/pdf/2510.04871.pdf "Less is More: Recursive Reasoning with Tiny Networks (2025) — arXiv"
[34-20]: https://arxiv.org/pdf/2510.10489.pdf "HARoPE (2025) — arXiv"
[34-21]: # "Nested Learning (2025) — Local Copy (no public PDF link provided)"
[34-22]: https://arxiv.org/pdf/2511.09146.pdf "DoPE (2025) — arXiv"
[34-23]: https://arxiv.org/pdf/2511.15684.pdf "WALRUS (2025) — arXiv"
[34-24]: https://arxiv.org/pdf/2511.17388.pdf "Selective Rotary Position Embedding (2025) — arXiv"
