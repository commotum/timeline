Below is an **aggressive ARC-AGI–anchored pass**: papers that **use ARC / ARC-AGI as a benchmark**, or **explicitly cite it as motivation/inspiration**, *and* whose **core contribution is a computation / reasoning mechanism** (memory, iteration, variable compute, search, planning, program induction, test-time adaptation/compute, equilibrium-ish dynamics). I’ll mark **borderline** items where the contribution drifts toward dataset/tooling or “domain lifting.”

---

## 2019 — The benchmark that makes “missing capability” explicit

* **The Abstraction and Reasoning Corpus (ARC)** (Chollet, 2019)
  **Missing capability:** fluid abstraction + fast skill acquisition from few examples
  **Mechanism:** *(benchmark framing, not a solver)* establishes the “why” behind many later compute-mechanism proposals. ([arXiv][1])

---

## 2021 — Early neurosymbolic + program-synthesis framing (ARC as target)

* **A Neurosymbolic Approach to Abstraction and Reasoning** (Alford, MIT thesis, 2021)
  **Missing capability:** compositional abstraction + search over symbolic hypotheses
  **Mechanism:** neurosymbolic ARC solver framing ARC as **program synthesis** with learned components. ([DSpace][2])

---

## 2022 — Treat ARC as “vision + attention” with explicit structure priors

* **Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the ViTARC Architecture** (Li et al., OpenReview)
  **Missing capability:** visual abstraction with limited examples (ARC’s regime)
  **Mechanism:** a **ViT-style solver** + ARC-specific training/inference components (attention-centric visual reasoning). ([OpenReview][3])
  *(This is closer to “domain solver design” than a single clean compute primitive, but it’s clearly ARC-motivated.)*

---

## 2024 — Planning, induction-vs-transduction, and test-time adaptation as the compute algorithm

* **Generalized Planning for the Abstraction and Reasoning Corpus (GPAR)** (Lei, Lipovetzky, Ehinger, AAAI 2024)
  **Missing capability:** explicit **planning/program-like** generalization instead of pattern fitting
  **Mechanism:** cast ARC as **generalized planning** using **PDDL + planning programs** (with ARC-specific constraints to make planning scalable). ([arXiv][4])

* **Combining Induction and Transduction for Abstract Reasoning** (Li et al., arXiv 2024 → ICLR 2025)
  **Missing capability:** one-shot transduction (direct output prediction) misses precise compositional operations; pure induction misses fuzzy perceptual concepts
  **Mechanism:** a paired framework: **inductive program synthesis model** + **transductive predictor**, combined/ensembled to cover complementary failure modes on ARC. ([arXiv][5])

* **The Surprising Effectiveness of Test-Time Training for Few-Shot Learning** (Akyürek et al., 2024)
  **Missing capability:** static inference underuses per-task compute; fails to adapt to a *new puzzle distribution* at inference
  **Mechanism:** **test-time training (TTT)** on the in-context examples / augmentations as the *inference procedure* (gradient-based refinement loop), reported with strong ARC gains. ([arXiv][6])

* **Towards Efficient Neurally-Guided Program Induction for ARC-AGI** (Ouellette, 2024)
  **Missing capability:** brute-force synthesis/search is inefficient; standard neural mapping struggles with ARC’s OOD generalization
  **Mechanism:** a compute framing of **neurally-guided program enumeration/search** across multiple “spaces” (grid / program / transform), explicitly evaluated on ARC-AGI. ([arXiv][7])

* **Mini-ARC: Solving Abstraction and Reasoning Puzzles with Small Transformer Models** (Fletcher-Hill, 2024)
  **Missing capability:** bigger models aren’t the answer; need *iteration/refinement* and per-task adaptation in small nets
  **Mechanism:** small Transformer family + **test-time training / refinement strategies** as the core compute lever for ARC-like generalization. ([Paul Fletcher-Hill][8])

* **Omni-ARC** (Barbadillo, ARC Prize 2024)
  **Missing capability:** a single “predict output” objective is too narrow for ARC; systems need richer internal task representations
  **Mechanism:** **multi-task** training over ARC-related subtasks + adaptation/refinement pipeline for solving ARC tasks. ([ARC Prize][9])

* **ARC Prize 2024: Technical Report** (Chollet et al., 2024)
  **Missing capability:** “static” models vs “refinement-loop” systems; need test-time adaptation + algorithmic search hybrids
  **Mechanism:** *(report, not a single solver)* but it codifies the compute-mechanism trend (TTT, refinement loops, hybrid search) that subsequent solver papers directly operationalize. ([arXiv][1])

*(Borderline / likely exclude by your criteria)*

* **A 2D nGPT Model for ARC Prize** (Puget, 2024)
  Often centers on 2D-aware modeling and spatial handling (can drift into “domain lifting/2D architecture”). It’s ARC Prize–relevant, but not always cleanly a “computation mechanism” paper. ([arXiv][10])

---

## 2025 — “Refinement loops,” evolutionary test-time compute, memory, and developmental dynamics (ARC-AGI explicitly)

* **Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective** (Franzen, Disselhoff, Hartmann; arXiv 2025 → ICML 2025)
  **Missing capability:** single-view prompting/decoding is brittle; lacks robust search over invariances / perspectives
  **Mechanism:** inference as **multi-augmentation product-of-experts**, typically paired with **test-time fine-tuning per task**—a concrete “compute at test time” algorithm for ARC. ([arXiv][11])

* **ARC-AGI Without Pretraining (CompressARC)** (Liao & Gu, 2025)
  **Missing capability:** reliance on large pretraining; need per-task *learning-as-inference*
  **Mechanism:** per-puzzle **test-time training** of a tiny model from scratch guided by an **MDL/description-length** objective (a “neural code-golf” refinement loop). ([Isaac Liao][12])

* **Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI (SOAR)** (Pourcel, Colas, Oudeyer, 2025)
  **Missing capability:** hand-designed DSLs + static solvers don’t scale; pure LLM prompting lacks reliable search/verification loops
  **Mechanism:** **evolutionary program synthesis** + a loop that **fine-tunes the LLM on its own search traces** (self-improving refinement). ([OpenReview][13])

* **ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus** (Guichard et al., 2025)
  **Missing capability:** one-shot inference struggles with emergence of structured transformations
  **Mechanism:** solve ARC via **Neural Cellular Automata**—iterative, local update dynamics producing emergent structure (“developmental” computation). ([arXiv][14])

* **Less is More: Recursive Reasoning with Tiny Networks (TRM)** (Jolicoeur-Martineau, 2025)
  **Missing capability:** autoregressive one-pass answers are fragile; need repeated self-correction with tiny compute units
  **Mechanism:** a compact **recursive refinement** loop (latent + answer updated over multiple steps), evaluated on ARC-AGI-1/2. ([ar5iv][15])

* **ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory** (Ho et al., 2025)
  **Missing capability:** no persistent cross-task learning of reusable abstractions; “starts from scratch” each puzzle
  **Mechanism:** **lifelong memory** that stores/retrieves reusable reasoning components for ARC-like tasks (memory as a compute primitive). ([ar5iv][16])

* **Productive “refinement loop” systems from ARC Prize writeups (2025 winners list)**
  **Missing capability:** need explore→verify→refine loops instead of single-shot inference
  **Mechanism:** ARC Prize 2025 explicitly highlights **evolutionary test-time compute** and **evolutionary program synthesis** as dominant mechanisms. ([ARC Prize][17])
  *(Many of these are released as competition papers; the ARC Prize post is the cleanest index.)*

* **Boosting Performance on ARC via Perspective / Augmentations** (same PoE line; frequently referenced in ARC Prize context)
  **Missing capability:** invariance handling + robust selection among candidates
  **Mechanism:** multi-view inference aggregation as an explicit compute algorithm. ([Proceedings of Machine Learning Research][18])

---

## Late 2025 — “ARC-AGI is the benchmark; here’s a new compute paradigm”

* **ARC Is a Vision Problem! (Vision ARC / VARC)** (Hu et al., 2025)
  **Missing capability:** language-oriented solver framing misses strong visual priors; needs per-task adaptation
  **Mechanism:** ARC as **image-to-image translation** with ViT trained on ARC + **test-time training** for adaptation; explicitly reports ARC-1 performance. ([arXiv][19])

* **Vector Symbolic Algebras for the Abstraction and Reasoning Corpus** (Joffe, Eliasmith, 2025)
  **Missing capability:** pure neural pattern learning lacks explicit structured manipulation and sample-efficient abstraction
  **Mechanism:** a System-1/System-2 style neurosymbolic solver using **Vector Symbolic Algebras** to represent objects and **guide program synthesis/search** on ARC-AGI. ([arXiv][20])

* **Reflection System for the Abstraction and Reasoning Corpus** (Bikov, 2025)
  **Missing capability:** LLMs alone are unreliable; need iterative critique/verification loops
  **Mechanism:** **LLM-as-judge/referee** + **program synthesis solver** in a reflection loop (search + verification). ([OpenReview][21])

* **ARC Is a Vision Problem! + test-time training theme (indexed by ARC Prize 2025)**
  **Missing capability:** static inference; lack of per-task adaptation
  **Mechanism:** multiple 2025 ARC Prize papers emphasize **test-time refinement loops** as the central algorithm. ([ARC Prize][17])

---

### If you want me to push it even harder

I can do a follow-up sweep that is *only* papers which:

* explicitly cite **ARC-AGI-1** vs **ARC-AGI-2** (or ARC Prize constraints), and
* introduce a named compute mechanism (“refinement loop,” “evolutionary TTC,” “transduction vs induction,” “developmental dynamics,” “lifelong memory,” etc.),

…and I’ll include the **ARC Prize 2024 paper-award cluster** plus the **ARC Prize 2025 paper-award cluster** as separate sub-timelines (since those are basically “compute mechanism proposals with ARC as the proving ground”).

[1]: https://arxiv.org/abs/2412.04604?utm_source=chatgpt.com "[2412.04604] ARC Prize 2024: Technical Report"
[2]: https://dspace.mit.edu/bitstream/handle/1721.1/139305/Alford-salford-meng-eecs-2021-thesis.pdf?utm_source=chatgpt.com "A Neurosymbolic Approach to Abstraction and Reasoning"
[3]: https://openreview.net/forum?id=0gOQeSHNX1&utm_source=chatgpt.com "Tackling the Abstraction and Reasoning Corpus with Vision..."
[4]: https://arxiv.org/abs/2401.07426?utm_source=chatgpt.com "Generalized Planning for the Abstraction and Reasoning Corpus"
[5]: https://arxiv.org/abs/2411.02272?utm_source=chatgpt.com "Combining Induction and Transduction for Abstract Reasoning"
[6]: https://arxiv.org/abs/2411.07279?utm_source=chatgpt.com "The Surprising Effectiveness of Test-Time Training for Few ..."
[7]: https://arxiv.org/abs/2411.17708?utm_source=chatgpt.com "Towards Efficient Neurally-Guided Program Induction for ARC-AGI"
[8]: https://www.paulfletcherhill.com/mini-arc.pdf?utm_source=chatgpt.com "Mini-ARC: Solving Abstraction and Reasoning Puzzles ..."
[9]: https://arcprize.org/blog/arc-prize-2024-winners-technical-report?utm_source=chatgpt.com "ARC Prize 2024 Winners & Technical Report Published"
[10]: https://arxiv.org/html/2412.04604v2?utm_source=chatgpt.com "ARC Prize 2024: Technical Report"
[11]: https://arxiv.org/abs/2505.07859?utm_source=chatgpt.com "Boosting Performance on ARC Is a Matter of Perspective"
[12]: https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/ARC_AGI_Without_Pretraining.pdf "ARC-AGI Without Pretraining"
[13]: https://openreview.net/pdf?id=z4IG090qt2 "Self-Improving Language Models for Evolutionary Program Synthesis:A Case Study on ARC-AGI"
[14]: https://arxiv.org/html/2505.08778v1?utm_source=chatgpt.com "ARC-NCA: Towards Developmental Solutions to the ..."
[15]: https://ar5iv.org/abs/2510.04871 "[2510.04871] Less is More: Recursive Reasoning with Tiny Networks"
[16]: https://ar5iv.org/abs/2509.04439 "[2509.04439] ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory"
[17]: https://arcprize.org/blog/arc-prize-2025-results-analysis "ARC Prize 2025 Results and Analysis"
[18]: https://proceedings.mlr.press/v267/franzen25a.html?utm_source=chatgpt.com "Product of Experts with LLMs: Boosting Performance on ARC ..."
[19]: https://arxiv.org/abs/2511.14761?utm_source=chatgpt.com "ARC Is a Vision Problem!"
[20]: https://arxiv.org/abs/2511.08747?utm_source=chatgpt.com "Vector Symbolic Algebras for the Abstraction and Reasoning Corpus"
[21]: https://openreview.net/forum?id=kRFwzuv0ze&utm_source=chatgpt.com "Reflection System for the Abstraction and Reasoning Corpus"
