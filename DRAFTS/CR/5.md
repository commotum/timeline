## 2019 — ARC-AGI defines the “missing capability” target

* **On the Measure of Intelligence / ARC (Abstraction and Reasoning Corpus)** (Chollet, 2019)
  **Missing capability:** rapid abstraction + recombination on *novel* tasks (few demonstrations, no training-set prep)
  **Mechanism:** *(benchmark framing, not a solver)*—establishes ARC-AGI as a forcing function for “computation beyond feedforward pattern matching.” 

---

## 2024 — ARC Prize 2024 Paper Award cluster (ARC-AGI-1; explicit “mechanism = inference algorithm”)

*(All listed as official Paper Award winners / runners-up in ARC Prize 2024.)* ([ARC Prize][1])

* **Combining Induction and Transduction for Abstract Reasoning** (Li et al., 2024)
  **Missing capability:** pure transduction is brittle; pure synthesis misses perceptual fuzziness
  **Mechanism:** a **hybrid inference system** combining **program induction (synthesis)** with **direct transduction**, covering complementary ARC failure modes. ([ARC Prize][1])

* **The Surprising Effectiveness of Test-Time Training for Abstract Reasoning** (Akyürek et al., 2024)
  **Missing capability:** static inference cannot adapt to a brand-new puzzle at test time
  **Mechanism:** **test-time training (TTT)**—treat gradient-based adaptation as the *inference procedure* per puzzle/task. ([ARC Prize][1])

* **Searching Latent Program Spaces** (Bonnet & Macfarlane, 2024)
  **Missing capability:** discrete search alone is inefficient; direct prediction isn’t reliable
  **Mechanism:** **latent-space search / optimization** inside a model to discover task-solving programs—explicitly positioned as neither pure finetuning nor pure discrete search. ([ARC Prize][1])

* **The LLM ARChitect: Solving ARC-AGI Is a Matter of Perspective** (Franzen et al., 2024)
  **Missing capability:** single-shot decoding is fragile under ARC’s invariances
  **Mechanism:** **multi-view / perspective ensembling + candidate selection** as the core inference algorithm. ([ARC Prize][1])

* **Omni-ARC** (Barbadillo, 2024)
  **Missing capability:** single mechanism doesn’t cover ARC’s diversity
  **Mechanism:** a **system-of-systems** approach (explicit mixture of methods; search/learn hybrids) aimed at ARC-AGI-1 generalization. ([ARC Prize][1])

* **Mini-ARC: Solving Abstraction and Reasoning Puzzles with Small Transformer Models** (Fletcher-Hill, 2024)
  **Missing capability:** “just scale params” isn’t the right axis; need compute procedures that work in small models
  **Mechanism:** small-model ARC solvers emphasizing **refinement / adaptation loops** (mechanism-centric, not size-centric). ([ARC Prize][1])

* **Towards Efficient Neurally-Guided Program Induction for ARC-AGI** (Ouellette, 2024)
  **Missing capability:** naive enumeration/synthesis is too expensive
  **Mechanism:** **neurally-guided program search / induction** (search shaped by learned priors). ([ARC Prize][1])

* **A 2D nGPT Model For ARC Prize** (Puget, 2024) — **borderline (often drifts toward “2D domain lifting”)**
  **Missing capability:** robust reasoning over grid structure
  **Mechanism:** ARC solver built around a 2D-aware modeling stack; can be more “representation/architecture” than a clean compute primitive. ([ARC Prize][1])

---

## 2024 — ARC Prize 2024 Technical Report (explicitly mechanistic synthesis of the space)

* **ARC Prize 2024: Technical Report** (Chollet et al., Dec 5, 2024)
  **Missing capability:** standard deep learning “no adaptation at test time” fails on novelty
  **Mechanism:** highlights three dominant **compute-mechanism families** for ARC-AGI-1: **DL-guided program synthesis**, **TTT**, and **hybrids that combine synthesis + transduction**; also explicitly calls out latent-space search as a distinct adaptation mechanism. 

---

## 2025 — ARC Prize 2025 Paper Award cluster (ARC-AGI-2; explicit “reasoning systems”)

*(All listed as official Paper Award winners / runners-up / honorable mentions for ARC Prize 2025, which is explicitly ARC-AGI-2-focused.)* ([ARC Prize][2])

### Paper Awards (top 3 + runners-up)

* **Less is More: Recursive Reasoning with Tiny Networks (TRM)** (Jolicoeur-Martineau, 2025)
  **Missing capability:** stable multi-step reasoning with small models; one-pass inference is brittle
  **Mechanism:** a **recursive refinement network** with separate latent + answer states trained for multi-step improvement; explicitly reported on ARC-AGI-1 and ARC-AGI-2. ([ARC Prize][2])

* **Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI (SOAR)** (Pourcel, Colas, Oudeyer, 2025)
  **Missing capability:** reliable program induction requires search + learning, not just prompting
  **Mechanism:** **evolutionary program synthesis** plus a loop that **fine-tunes an LLM on its own search traces** (self-improving refinement). ([ARC Prize][2])

* **ARC-AGI Without Pretraining (CompressARC)** (Liao & Gu, 2025)
  **Missing capability:** dependence on large pretraining; need per-puzzle learning-as-inference
  **Mechanism:** per-task **MDL / “neural code golf”** with **single-puzzle training** (test-time optimization as the solver). ([ARC Prize][2])

* **Vector Symbolic Algebras for the Abstraction and Reasoning Corpus** (Joffe & Eliasmith, 2025)
  **Missing capability:** robust compositional manipulation / structured variable binding
  **Mechanism:** **vector-symbolic algebra** machinery used as a compute substrate for ARC-style composition. ([ARC Prize][2])

* **From Parrots to Von Neumanns: How Evolutionary Test-Time Compute Achieved SOTA on ARC-AGI** (Berman, 2025)
  **Missing capability:** one-shot inference doesn’t do the explore→verify→refine loop needed for ARC-AGI-2
  **Mechanism:** **Evolutionary Test-Time Compute**—explicit **program evolution** driven by verification feedback during inference. ([ARC Prize][2])

* **Efficient Evolutionary Program Synthesis** (Pang, 2025)
  **Missing capability:** brittle single attempts; need scalable synthesis under tight budgets
  **Mechanism:** per-task **evolutionary synthesis + verification**, with library/abstraction building to steer search. ([ARC Prize][2])

* **ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus** (Guichard et al., 2025)
  **Missing capability:** one-pass inference struggles to produce emergent structured transformations
  **Mechanism:** **neural cellular automata**—iterative local update dynamics as the computation. ([ARC Prize][2])

* **ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory** (Ho et al., 2025)
  **Missing capability:** no persistent accumulation of reusable abstractions across puzzles
  **Mechanism:** **lifelong external memory** for storing/retrieving compositional reasoning components. ([ARC Prize][2])

### Honorable Mentions (still explicitly ARC Prize 2025 / ARC-AGI-2)

* **ARC-AGI is a Vision Problem!** (Hu et al., 2025)
  **Missing capability:** mismatch between modality/inductive bias and ARC grid structure; needs per-task adaptation
  **Mechanism:** solver framing emphasizing **vision-first modeling + adaptation/refinement** rather than pure text/program routes. ([ARC Prize][2])

* **Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective** (Franzen, Disselhoff, Hartmann, 2025)
  **Missing capability:** single-view decoding is unstable; lacks robust invariance handling
  **Mechanism:** **product-of-experts / multi-augmentation aggregation** as the inference algorithm. ([ARC Prize][2])

* **Exploring the combination of search and learn for the ARC25 challenge** (Barbadillo, 2025)
  **Missing capability:** neither pure search nor pure learning generalizes reliably
  **Mechanism:** explicit **search–learn hybrid loops** for ARC-AGI-2. ([ARC Prize][2])

* **Beyond Brute Force: A Neuro-Symbolic Architecture for Compositional Reasoning in ARC-AGI-2** (Das, Ghugarkar, Bhat, McAuley, 2025)
  **Missing capability:** brute force and direct mapping both fail on compositional novelty
  **Mechanism:** an explicit **neuro-symbolic compute pipeline** (structured program-like reasoning with learned components). ([ARC Prize][2])

* **Test-time Adaptation of Tiny Recursive Models** (McGovern, 2025)
  **Missing capability:** fixed recursive models still need task-specific compute under ARC-AGI-2 constraints
  **Mechanism:** **test-time adaptation** loop specialized to TRM-style recursive solvers. ([ARC Prize][2])

* **Rethinking Visual Intelligence: Insights from Video Pretraining** (Acuaviva et al., 2025) — **borderline (can drift toward representation/pretraining)**
  **Missing capability:** better perceptual priors to support reasoning on ARC-like tasks
  **Mechanism:** uses **pretraining regime** as the lever; sometimes less “compute mechanism” than the others, but included as an ARC Prize 2025 honorable mention. ([ARC Prize][2])

* **Don’t throw the baby out with the bathwater: How and why deep learning for ARC** (Cole & Osman, 2025)
  **Missing capability:** dismissing deep learning misses the key: *how it’s used at inference*
  **Mechanism:** argues for deep learning paired with **adaptation/refinement procedures** (TTT / TTC style) as the core algorithmic shift. ([ARC Prize][2])

* **NVARC solution to ARC-AGI-2 2025** (Sorokin & Puget, 2025)
  **Missing capability:** need solver pipelines that explicitly optimize per-task under constraints
  **Mechanism:** documented as the ARC Prize 2025 top-score solution paper (system-level refinement + components). ([ARC Prize][2])

---

## 2025 — ARC Prize 2025 High-Score systems (ARC-AGI-2; “mechanism = system-level inference loop”)

*(These are the “top score winners” with explicit ARC-AGI-2 private-eval results and linked papers.)* ([ARC Prize][2])

* **NVARC** (ARC Prize 2025 #1 top score)
  **Missing capability:** single-pass inference; insufficient per-task adaptation under ARC-AGI-2 novelty
  **Mechanism:** a contest-constrained **ensemble** combining an Architects-style **test-time-trained model** with **TRM-based components** (explicit refinement pipeline). ([ARC Prize][3])

* **the ARChitects (2025 system)** (ARC Prize 2025 #2 top score)
  **Missing capability:** robust reasoning requires iterative improvement + perspective scoring
  **Mechanism:** a “2D-aware” masked/diffusion LLM with **recursive self-refinement** and **perspective-based scoring** (system-level refinement loop). ([ARC Prize][3])

* **MindsAI (2025 system)** (ARC Prize 2025 #3 top score)
  **Missing capability:** needs strong test-time compute orchestration under hard constraints
  **Mechanism:** engineered **TTT / TTFT pipeline** with augmentation ensembles and other test-time controls (compute allocation as the solver). ([ARC Prize][3])

---

If you want the **next** tightening: I can reprint this as a **minimal “only the papers that explicitly mention ARC-AGI-2 (2025) in the title/abstract”** subset (TRM, SOAR, CompressARC, Beyond Brute Force, NVARC paper, etc.) versus “ARC Prize 2025 listed but not necessarily ARC-AGI-2 in the title.”

[1]: https://arcprize.org/competitions/2024/ "ARC Prize 2024"
[2]: https://arcprize.org/blog/arc-prize-2025-results-analysis "ARC Prize 2025 Results and Analysis"
[3]: https://arcprize.org/competitions/2025/ "ARC Prize - 2025 Competition Details"
