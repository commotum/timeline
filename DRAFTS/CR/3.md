## 2014 — External memory as a missing primitive

* **Neural Turing Machines (NTM)** (Graves et al., 2014)
  **Missing capability:** persistent, addressable memory + algorithmic state beyond fixed activations
  **Mechanism:** differentiable **read/write** heads over external memory ([arXiv][1])

---

## 2015 — Multi-hop memory + learned programs + structured memory

* **End-to-End Memory Networks (MemN2N)** (Sukhbaatar et al., 2015)
  **Missing capability:** multi-hop retrieval / iterative evidence aggregation
  **Mechanism:** repeated **attention hops** over an external memory before answering

* **Neural Programmer-Interpreters (NPI)** (Reed & de Freitas, 2015)
  **Missing capability:** program induction with reusable subroutines
  **Mechanism:** a controller that **calls** learned/latent **subprograms**

* **Stack/Queue/Deque-augmented RNNs** (e.g., Grefenstette et al., 2015; Joulin & Mikolov, 2015)
  **Missing capability:** algorithmic counting / hierarchical control
  **Mechanism:** differentiable **data structures** (stack/queue/deque) as part of computation

---

## 2016 — Variable compute (learned halting) + more powerful memory computers

* **Adaptive Computation Time (ACT)** (Graves, 2016)
  **Missing capability:** adaptive depth (“think longer on hard inputs”)
  **Mechanism:** differentiable **halting** to choose computation steps per input ([arXiv][2])

* **Differentiable Neural Computer (DNC)** (Graves et al., 2016)
  **Missing capability:** scalable algorithmic memory manipulation
  **Mechanism:** external memory with differentiable **allocation + temporal linkage**

---

## 2017 — Differentiable logical inference / theorem proving

* **Neural Theorem Provers / Differentiable Proving** (Rocktäschel & Riedel, 2017)
  **Missing capability:** symbolic-style multi-hop logical inference
  **Mechanism:** differentiable **backward chaining / unification**

---

## 2018 — Iterative relational constraint satisfaction as the point

* **Recurrent Relational Networks (RRN)** (Palm et al., 2018)
  **Missing capability:** iterative constraint satisfaction / multi-step relational inference
  **Mechanism:** recurrent **message passing** over relational graphs for many steps ([arXiv][3])

* **Universal Transformer (UT)** (Dehghani et al., 2018)
  **Missing capability:** iterative refinement beyond fixed-depth feedforward stacks
  **Mechanism:** recurrently apply a transformer block (often paired with adaptive halting)

---

# Aggressive sweep: 2019–2025 (test-time compute / search / planning + implicit layers / solver-in-loop)

## 2019 — Fixed-point / solver layers become mainstream “compute mechanisms”

* **Deep Equilibrium Models (DEQ)** (Bai, Kolter, Koltun, 2019)
  **Missing capability:** infinite-depth-style computation with constant memory
  **Mechanism:** solve for a **fixed point** via root-finding; train via **implicit differentiation** ([arXiv][4])

* **Differentiable Convex Optimization Layers** (Agrawal et al., 2019)
  **Missing capability:** exact constraint/optimization inside neural pipelines
  **Mechanism:** embed a **convex solver** as a differentiable layer (solve forward; differentiate backward) ([OpenReview][5])

---

## 2020 — Retrieval / external memory becomes part of the forward pass (compute ≠ just weights)

* **REALM: Retrieval-Augmented Language Model Pre-Training** (Guu et al., 2020)
  **Missing capability:** updatable factual knowledge without re-training weights
  **Mechanism:** learned **retriever + reader**; retrieval is in-loop computation

* **Depth-adaptive / early-exit Transformers (dynamic depth)** (2020-era variants)
  **Missing capability:** variable compute per input
  **Mechanism:** learn policies to **skip layers / exit early** (ACT lineage)

---

## 2021 — Stabilizing “think longer” (learned halting done right)

* **PonderNet: Learning to Ponder** (Banino et al., 2021)
  **Missing capability:** stable adaptive compute / variable number of reasoning steps
  **Mechanism:** probabilistic **halting distribution** over repeated computation steps ([arXiv][6])

* **Jacobian regularization for equilibrium models** (Bai et al., 2021)
  **Missing capability:** stable DEQ convergence/training
  **Mechanism:** regularize Jacobians to stabilize fixed-point dynamics (DEQ engineering line)

---

## 2022 — Test-time “more compute” via sampling, self-consensus, and tool interaction

* **Self-Consistency for Chain-of-Thought** (Wang et al., 2022)
  **Missing capability:** single greedy reasoning path is brittle
  **Mechanism:** sample multiple reasoning traces and **aggregate/select** the consistent answer

* **ReAct: Synergizing Reasoning and Acting in Language Models** (Yao et al., 2022)
  **Missing capability:** pure feedforward reasoning can’t gather missing info / ground itself
  **Mechanism:** interleave **reasoning + actions** (tool/env calls) as the inference loop ([arXiv][7])

* **Memorizing Transformers / kNN-augmented attention (inference-time memory)** (2022)
  **Missing capability:** store and use new facts at inference without weight updates
  **Mechanism:** fast **kNN retrieval** over cached activations inside the model

---

## 2023 — Explicit search and planning over thoughts (tree/graph), plus self-improving loops

* **Tree of Thoughts (ToT)** (Yao et al., 2023)
  **Missing capability:** exploration, lookahead, backtracking beyond left-to-right decoding
  **Mechanism:** explicit **tree search** over intermediate “thought” states with self-evaluation ([arXiv][8])

* **Toolformer: Language Models Can Teach Themselves to Use Tools** (Schick et al., 2023)
  **Missing capability:** reliable tool use (calc/search/KB) as part of computation, not just prompting
  **Mechanism:** self-supervised training to insert/execute **API calls** during inference ([arXiv][9])

* **Reflexion / Self-Refine family (test-time improvement loops)** (2023)
  **Missing capability:** learning from failures without gradient updates
  **Mechanism:** store structured self-feedback (“reflections”) and iteratively refine outputs at test time

* **Process Reward Models (PRM) introduced/standardized for reasoning traces** (Lightman et al., 2023)
  **Missing capability:** outcome-only rewards provide weak credit assignment for multi-step reasoning
  **Mechanism:** step-level **process supervision/rewarding** used for search/RL at inference

---

## 2024 — Verifiers + process rewards become the engine of test-time scaling

* **Generative Verifiers / GenRM: Reward Modeling as Next-Token Prediction** (NeurIPS 2024)
  **Missing capability:** better verification signals to guide best-of-N / search at test time
  **Mechanism:** train verifiers as **generative** models; use them to steer **Best-of-N** and related TTS ([NeurIPS][10])

* **Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning (PAV / “progress rewards”)** (Setlur et al., 2024)
  **Missing capability:** scalable, informative step-level rewards without dense human labeling
  **Mechanism:** define process reward as **progress** (step-level advantage) under a prover policy; improves test-time search + online RL ([arXiv][11])

* **Advancing Process Verification for LLM Reasoning** (EMNLP 2024)
  **Missing capability:** reliable step verification beyond naive self-checking
  **Mechanism:** improved **process verification** schemes that directly target better test-time selection/search ([ACL Anthology][12])

---

## 2025 — “Reasoning models” via RL + hierarchical latent compute + principled TTS

* **DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning** (DeepSeek-AI, 2025)
  **Missing capability:** strong multi-step reasoning *emerging from training* (not just prompting)
  **Mechanism:** large-scale **RL fine-tuning** to elicit reasoning behaviors; includes an **R1-Zero** pure-RL variant ([arXiv][13])

* **Hierarchical Reasoning Model (HRM)** (Wang et al., 2025)
  **Missing capability:** deep, stable latent reasoning with variable compute without emitting long CoT token traces
  **Mechanism:** hierarchical recurrent computation with **fast/slow modules**, **adaptive halting (ACT)**, and **one-step equilibrium-style training** ([arXiv][14])

* **Scaling up Test-Time Compute with Latent Reasoning** (NeurIPS/OpenReview 2025)
  **Missing capability:** test-time scaling that doesn’t require generating ever-more tokens
  **Mechanism:** iterate a recurrent block to “reason in latent space,” unrolling to arbitrary depth at inference ([NeurIPS][15])

* **The Lessons of Developing Process Reward Models…** (Zhang et al., 2025)
  **Missing capability:** practical, effective PRMs that generalize (data/labeling is hard)
  **Mechanism:** engineering + methodological guidance for building PRMs that actually work (enabling better search/RL at test time) ([arXiv][16])

* **R-PRM: Reasoning-Driven Process Reward Modeling** (She et al., 2025)
  **Missing capability:** PRMs that are both data-efficient and accurate at step evaluation
  **Mechanism:** redesign PRM training/objectives to improve step-level supervision quality ([ACL Anthology][17])

* **The Art of Scaling Test-Time Compute for LLMs** (2025)
  **Missing capability:** principled guidance on which test-time scaling strategies win under fixed budgets
  **Mechanism:** systematic framework/analysis of TTS strategy choices ([arXiv][18])

---

If you want the *most exhaustive* version (still in this exact timeline style), the next additions I’d fold in are:

* **MCTS/planning-style LLM agent papers** (RAP/LATS-style families) and
* **differentiable “solver modules”** beyond convex (SAT/ILP/DP relaxations),
  because they’re the two biggest remaining clusters that match your criteria but can double the 2019–2025 list.

[1]: https://arxiv.org/abs/1410.5401?utm_source=chatgpt.com "Neural Turing Machines"
[2]: https://arxiv.org/abs/1603.08983?utm_source=chatgpt.com "Adaptive Computation Time for Recurrent Neural Networks"
[3]: https://arxiv.org/abs/1711.08028?utm_source=chatgpt.com "[1711.08028] Recurrent Relational Networks"
[4]: https://arxiv.org/abs/1909.01377?utm_source=chatgpt.com "[1909.01377] Deep Equilibrium Models"
[5]: https://openreview.net/forum?id=1EuxRTe0WN&utm_source=chatgpt.com "PonderNet: Learning to Ponder"
[6]: https://arxiv.org/abs/2107.05407?utm_source=chatgpt.com "[2107.05407] PonderNet: Learning to Ponder"
[7]: https://arxiv.org/abs/2210.03629?utm_source=chatgpt.com "ReAct: Synergizing Reasoning and Acting in Language Models"
[8]: https://arxiv.org/abs/2305.10601?utm_source=chatgpt.com "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
[9]: https://arxiv.org/abs/2302.04761?utm_source=chatgpt.com "Toolformer: Language Models Can Teach Themselves to Use Tools"
[10]: https://neurips.cc/virtual/2024/104300?utm_source=chatgpt.com "Generative Verifiers: Reward Modeling as Next-Token ..."
[11]: https://arxiv.org/abs/2410.08146?utm_source=chatgpt.com "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning"
[12]: https://aclanthology.org/2024.emnlp-main.125.pdf?utm_source=chatgpt.com "Advancing Process Verification for Large Language ..."
[13]: https://arxiv.org/abs/2501.12948?utm_source=chatgpt.com "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning"
[14]: https://arxiv.org/html/2506.21734v1?utm_source=chatgpt.com "Hierarchical Reasoning Model"
[15]: https://neurips.cc/virtual/2025/poster/117966?utm_source=chatgpt.com "Scaling up Test-Time Compute with Latent Reasoning"
[16]: https://arxiv.org/pdf/2501.07301?utm_source=chatgpt.com "The Lessons of Developing Process Reward Models in ..."
[17]: https://aclanthology.org/2025.emnlp-main.679.pdf?utm_source=chatgpt.com "R-PRM: Reasoning-Driven Process Reward Modeling"
[18]: https://arxiv.org/html/2512.02008v1?utm_source=chatgpt.com "The Art of Scaling Test-Time Compute for Large Language ..."
