                                             Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI-10)
                                                             Relative Entropy Policy Search
                                                                                            ¨                           ¨
                                                       JanPeters, Katharina Mulling, Yasemin Altun
                                                                                                                         ¨
                                   MaxPlanckInstitute for Biological Cybernetics, Spemannstr. 38, 72076 Tubingen, Germany
                                                     {jrpeters,muelling,altun}@tuebingen.mpg.de
                                             Abstract                                           Policy updates may often result in a loss of essential in-
                   Policysearchisasuccessfulapproachtoreinforcementlearn-                    formation due to the policy improvement step. For exam-
                   ing. However, policy improvements often result in the loss of             ple, a policy update that eliminates most exploration by tak-
                   information. Hence, it has been marred by premature con-                  ing the best observed action often yields fast but premature
                   vergence and implausible solutions. As ﬁrst suggested in the              convergence to a suboptimal policy. This problem was ob-
                   context of covariant policy gradients (Bagnell and Schneider              served by Kakade (2002) in the context of policy gradients.
                   2003), many of these problems may be addressed by con-                    There, it can be attributed to the fact that the policy param-
                   straining the information loss. In this paper, we continue this           eter update δθ was maximizing it collinearity δθT∇ J to
                   path of reasoning and suggest the Relative Entropy Policy                                                                               θ
                                                                                             the policy gradient while only regularized by ﬁxing the Eu-
                   Search (REPS) method. The resulting method differs signif-                clidian length of the parameter update δθTδθ = ε to a step-
                   icantly from previous policy gradient approaches and yields               size ε. Kakade (2002) concluded that the identity metric
                   an exact update step. It works well on typical reinforcement              of the distance measure was the problem, and that the us-
                   learning benchmark problems.                                              age of the Fisher information metric F(θ) in a constraint
                                                                                             δθTF(θ)δθ = ε leads to a better, more natural gradient.
                                         Introduction                                        Bagnell and Schneider (2003) clariﬁed that the constraint
                Policy search is a reinforcement learning approach that at-                  introduced in (Kakade 2002) can be seen as a Taylor expan-
                tempts to learn improved policies based on information ob-                   sion of the loss of information or relative entropy between
                served in past trials or from observations of another agent’s                the path distributions generated by the original and the up-
                actions (Bagnell and Schneider 2003).            However, policy             dated policy. Bagnell and Schneider’s (2003) clariﬁcation
                search, as most reinforcement learning approaches, is usu-                   serves as a key insight to this paper.
                ally phrased in an optimal control framework where it di-                       In this paper, we propose a new method based on this in-
                rectly optimizes the expected return. As there is no notion                  sight, that allows us to estimate new policies given a data
                of the sampled data or a sampling policy in this problem                     distribution both for off-policy or on-policy reinforcement
                statement, there is a disconnect between ﬁnding an optimal                   learning. We start from the optimal control problem state-
                policy and staying close to the observed data. In an online                  ment subject to the constraint that the loss in information
                setting, many methodscandealwiththisproblembystaying                         is bounded by a maximal step size. Note that the meth-
                close to the previous policy (e.g., policy gradient methods                  odsproposedin(BagnellandSchneider2003;Kakade2002;
                allow only small incremental policy updates). Hence, ap-                     Peters and Schaal 2008) used a small ﬁxed step size instead.
                proaches that allow stepping further away from the data are                  Aswedonotworkinaparametrizedpolicygradientframe-
                problematic, particularly, off-policy approaches Directly op-                work, we can directly compute a policy update based on all
                timizing a policy will automatically result in a loss of data as             information observed from previous policies or exploratory
                an improved policy needs to forget experience to avoid the                   sampling distributions. All sufﬁcient statistics can be deter-
                mistakes of the past and to aim on the observed successes.                   minedbyoptimizingthedualfunction that yields the equiv-
                However, choosing an improved policy purely based on its                     alent of a value function of a policy for a data set. We show
                return favors biased solutions that eliminate states in which                that the method outperforms the previous policy gradient al-
                only bad actions have been tried out. This problem is known                  gorithms (Peters and Schaal 2008) as well as SARSA (Sut-
                as optimization bias (Mannor et al. 2007). Optimization bi-                  ton and Barto 1998).
                ases may appear in most on- and off-policy reinforcement
                learning methods due to undersampling (e.g., if we cannot                    Background&Notation
                sample all state-actions pairs prescribed by a policy, we will               Weconsidertheregularreinforcememtlearningsetting(Sut-
                overﬁt the taken actions), model errors or even the policy                   ton and Barto 1998; Sutton et al.          2000) of a stationary
                update step itself.                                                          Markov decision process (MDP) with n states s and m ac-
                            c                                                                tions a. When an agent is in state s, he draws an action
                Copyright  2010, Association for the Advancement of Artiﬁcial
                Intelligence (www.aaai.org). All rights reserved.                            a ∼ π(a|s) from a stochastic policy π. Subsequently, the
                                                                                      1607
