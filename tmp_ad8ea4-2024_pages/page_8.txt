                            nˆ  TruthfulQA Hellaswag MMLU ARC-c                                            b    TruthfulQA Hellaswag MMLU ARC-c
                           56         38.95             80.27           60.29       64.25                 90         37.44              80.12          60.74        64.68
                           64         38.68             80.23           60.55       64.76                180         38.18              80.23          60.48        64.76
                           72         38.51             80.27           60.22       64.16                360         38.10              80.09          60.16        64.68
                           80         38.10             80.09           60.16       64.68                720         38.78              80.29          60.35        64.33
                           88         37.74             80.17           60.61       64.76              Table 7: Influence of the interval numbers b on the
                           96         38.60             80.14           60.09       64.76              Hugging Face Open LLM benchmark.
                        Table 5: Influence of interpolation dimension numbers
                        nˆ on the Hugging Face Open LLM benchmark.
                                    b       0-4k        4-8k         8k+        Avg.                   but only generalizes to limited lengths on down-
                                  90       31.47       31.26       30.44        31.06                  stream tasks (Kazemnejad et al., 2023). RoPE (Su
                                  180      31.68       31.09       30.51        31.09                  et al., 2021) cannot generalize to lengths beyond
                                  360      31.64       31.40       30.43        31.15                  its pre-training length.
                                  720      31.32       31.03       30.18        30.84                      Someworkshavebeendonetoovercomesuch
                        Table 6: Influence of the interval numbers b on the long                       limitation. Ruoss et al. (2023) randomize token’s
                        context benchmark.                                                             position embedding during pre-training, enabling
                                                                                                       the model based on RoPE to generalize to prede-
                        turbance score Di(PE ,P ) is significantly larger                              termined sequence lengths. This effectively guar-
                                                        ′    L
                                                       L                                               antees consistency in the distribution of rotation
                        than Di(PI ,P ). We illustrate the influence of
                                          ′    L
                                        L                                                              angles when generalizing to predetermined lengths,
                        interpolation dimension numbers on downstream                                  demonstrating that rotation angle distribution con-
                        tasks in Table 5, where the value of nˆ has little                             sistency is crucial for the model’s ability to gen-
                        effect and different datasets prefer different nˆ.                             eralize. Chen et al. (2023); bloc97 (2023b,a); Liu
                        5.3     Influence of Interval                                                  et al. (2023); Peng et al. (2023) extend the context
                        During the analysis of the rotary angle distribution                           window of existing LLMs (i.e., LLaMA2 (Tou-
                        in eq. (5), we divide [0,2π) into b intervals and                              vron et al., 2023b)) by slightly modifying RoPE’s
                        statistically estimate their distribution. In this part,                       θ (as show in eq. (1)). Chen et al. (2023) achieves
                        weexploretheimpactofb,rangingfrom90to720,                                      proposed to extend the context window by interpo-
                        on the extension of the model’s context window.                                lating positions, using a scaling factor s = L′/L
                        As shown in Table 6, when b = 90, 180 and 360,                                 to uniformly scale θi, and fine-tuning on a small
                        the model’s performance after extension exhibits                               amountofdata to extend the model’s context win-
                        no significant fluctuations. This suggests that the                            dow. bloc97 (2023b,a) base on the Neural Tangent
                        model is capable of tolerating subtle differences                              Kernel (NTK) theory, they scale lower dimensions
                        in rotation angles. The performance drops when                                 less and higher dimensions more, this is also re-
                        b = 720. This is because excessive intervals can                               ferred to as Adjusted Base Frequency (ABF). Liu
                        actually increase the error in the distribution esti-                          et al. (2023) achieves an effect similar to NTK by
                        mation, since the number of rotary angle samples L                             modifying the base of RoPE. YaRN (Peng et al.,
                        is not very large. Table 7 illustrates that the choice                         2023) improved NTK by dividing RoPE dimen-
                        of b does not influence the downstream tasks.                                  sions into three frequency-based groups and ap-
                                                                                                       plying different strategies to each group. Low fre-
                                                                                                       quency (r < α) dimensions use interpolation like
                        6 RelatedWorks                                                                               i
                                                                                                       PI and high frequency (ri > β) dimensions use
                        Long-sequence modeling is a crucial issue in the                               extrapolation, dimensions that fall in-between em-
                        application of LLMs. Recent efforts focus on im-                               ploys the NTK. YaRN achieved good performance,
                        proving position embedding to enable LLMs have                                 but lacked interpretability, the hyperparameters α
                        larger context window. Currently, the most popular                             andβ werealsoempirically chosen, making it hard
                        relative position embedding are ALiBi (Press et al.,                           to obtain the optimal results. Different from these
                        2022) and RoPE (Su et al., 2021). ALiBi (Press                                 empirical method, our work initially highlights the
                        et al., 2022) adds bias to attention, enabling models                          consistency of rotary angle distribution as a theo-
                        to maintain lower perplexity on long sequences,                                retical guidance for extending the context window.
                                                                                                 7295
