                    Tafjord. 2018. Think you have solved question an-    Amirkeivan Mohtashami and Martin Jaggi. 2023. Land-
                    swering? try arc, the AI2 reasoning challenge. CoRR,   mark attention:   Random-access infinite context
                    abs/1803.05457.                                        length for transformers. CoRR, abs/2305.16300.
                 Tri Dao. 2023. Flashattention-2: Faster attention with  OpenAI. 2023.     GPT-4 technical report.    CoRR,
                    better parallelism and work partitioning.  CoRR,       abs/2303.08774.
                    abs/2307.08691.
                 Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,       BowenPeng, Jeffrey Quesnelle, Honglu Fan, and En-
                    and Christopher Ré. 2022. Flashattention: Fast and     rico Shippole. 2023. Yarn: Efficient context win-
                    memory-efficient exact attention with io-awareness.    dow extension of large language models. CoRR,
                    In Advances in Neural Information Processing Sys-      abs/2309.00071.
                    tems 35: Annual Conference on Neural Information     Ofir Press, NoahA.Smith,andMikeLewis.2022. Train
                    Processing Systems 2022, NeurIPS 2022, New Or-         short, test long: Attention with linear biases enables
                    leans, LA, USA, November 28 - December 9, 2022.        inputlengthextrapolation. InTheTenthInternational
                 Hugging Face. 2023. Open llm leaderboard.                 ConferenceonLearningRepresentations, ICLR2022,
                                                                           Virtual Event, April 25-29, 2022. OpenReview.net.
                 DanHendrycks,CollinBurns,StevenBasart,AndyZou,          Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,
                    Mantas Mazeika, Dawn Song, and Jacob Steinhardt.       Chloe Hillier, and Timothy P. Lillicrap. 2020. Com-
                    2020. Measuring massive multitask language under-      pressive transformers for long-range sequence mod-
                    standing. arXiv preprint arXiv:2009.03300.             elling. In 8th International Conference on Learning
                 Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shan-        Representations, ICLR 2020, Addis Ababa, Ethiopia,
                    tanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang,        April 26-30, 2020. OpenReview.net.
                    and Boris Ginsburg. 2024. RULER: what’s the real     Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
                    context size of your long-context language models?     and Yuxiong He. 2020. Zero: memory optimizations
                    CoRR,abs/2404.06654.                                   toward training trillion parameter models. In Pro-
                 Albert Q. Jiang, Alexandre Sablayrolles, Antoine          ceedings of the International Conference for High
                    Roux, Arthur Mensch, Blanche Savary, Chris Bam-        Performance Computing, Networking, Storage and
                    ford, Devendra Singh Chaplot, Diego de Las Casas,      Analysis, SC 2020, Virtual Event / Atlanta, Georgia,
                    Emma Bou Hanna, Florian Bressand, Gianna               USA,November9-19,2020,page20.IEEE/ACM.
                    Lengyel, Guillaume Bour, Guillaume Lample,           Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi
                    Lélio Renard Lavaud, Lucile Saulnier, Marie-           Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane
                    AnneLachaux,Pierre Stock, Sandeep Subramanian,         Legg, and Joel Veness. 2023. Randomized positional
                    Sophia Yang, Szymon Antoniak, Teven Le Scao,           encodings boost length generalization of transform-
                    Théophile Gervet, Thibaut Lavril, Thomas Wang,         ers. In Proceedings of the 61st Annual Meeting of
                    TimothéeLacroix, and William El Sayed. 2024. Mix-      the Association for Computational Linguistics (Vol-
                    tral of experts. CoRR, abs/2401.04088.                 ume2: Short Papers), ACL 2023, Toronto, Canada,
                 Amirhossein      Kazemnejad,        Inkit     Padhi,      July 9-14, 2023, pages 1889–1903. Association for
                    Karthikeyan Natesan Ramamurthy, Payel Das,             Computational Linguistics.
                    and Siva Reddy. 2023. The impact of positional       Jianlin Su, Yu Lu, ShengfengPan, BoWen,andYunfeng
                    encodingonlengthgeneralizationintransformers. In       Liu. 2021. Roformer: Enhanced transformer with
                    Advances in Neural Information Processing Systems      rotary position embedding. CoRR, abs/2104.09864.
                    36:  Annual Conference on Neural Information
                    Processing Systems 2023, NeurIPS 2023, New           QwenTeam.2024. Qwen2technicalreport.
                    Orleans, LA, USA, December 10 - 16, 2023.
                 Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.     HugoTouvron,Thibaut Lavril, Gautier Izacard, Xavier
                    Truthfulqa: Measuring how models mimic human           Martinet, Marie-Anne Lachaux, Timothée Lacroix,
                    falsehoods. In Proceedings of the 60th Annual Meet-    Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
                    ing of the Association for Computational Linguistics   Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
                    (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,    Grave, and Guillaume Lample. 2023a. Llama: Open
                    May22-27,2022,pages3214–3252. Association for          and efficient foundation language models. CoRR,
                    Computational Linguistics.                             abs/2302.13971.
                 Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An,          Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
                    Xipeng Qiu, and Dahua Lin. 2023. Scaling laws          bert, Amjad Almahairi, Yasmine Babaei, Nikolay
                    of rope-based extrapolation. CoRR, abs/2310.05209.     Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
                                                                           Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
                 Ilya Loshchilov and Frank Hutter. 2019. Decoupled         Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
                    weight decay regularization. In 7th International      Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
                    ConferenceonLearningRepresentations, ICLR2019,         Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
                    New Orleans, LA, USA, May 6-9, 2019. OpenRe-           thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
                    view.net.                                              Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
                                                                    7297
