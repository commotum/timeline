                 7 Conclusion                                          windowofevenlargermodelstoachievestronger
                 In this work, we proposed to study the context win-    long contextual abilities.
                 dowextensionfromadistributionalperspectiveand          9 EthicsStatement
                 demonstrated that the consistency of rotary angle
                 distributions has a significant impact on extending   Wearetotallyawarethattextgenerationtechnology
                 the context window of LLMs based on the rotary         has a potential to be used maliciously to generate
                 position embedding. We designed a framework to         fake, toxic, or offensive content. We are aware that
                 select scaling strategies with the guidance of mini-   if LLMsgenerateharmfulortoxicinformation, our
                 mizingthedisturbance of rotary angle distributions.    approach cannot explicitly prevent it. However,
                 Experimental results demonstrated the effective-       since the models and datasets used in our study are
                 ness and superiority of our approach. Although our     publicly available and examined, we are confident
                 approach is limited by the rotary position embed-      that our approach will not introduce toxic content
                 ding, we believe that our distributional perspective   during the length extension phase.
                 has the potential to inspire future work.              10 Acknowledgments
                 8 Limitations                                         Xiaocheng Feng is the corresponding author of
                 Our method is limited by the rotary position em-       this work.   We thank the anonymous review-
                 bedding, which is not currently available for LLMs     ers for their insightful comments.     This work
                 with other embedding methods. However, this is        was supported by the National Natural Science
                 not a serious problem because (1) the most power-      Foundation of China (NSFC) (U22B2059, grant
                 ful opensourceLLMs,suchasLLaMA2,utilizethe             62276078),theKeyR&DProgramofHeilongjiang
                 rotarypositionembedding,and(2)ourapproachad-          via grant 2022ZX01A32, the International Cooper-
                 dresses the problem from a theoretical perspective,    ation Project of PCL, PCL2022D01 and the Funda-
                 whichcanbebettergeneralizedtootherembedding            mental Research Funds for the Central Universities
                 frameworks in future research than empirical work.    (Grant No.HIT.OCEF.2023018).
                   When applying the model to long contextual
                 tasks, the quadratic computational complexityprob-     References
                 lem of transformers still exists. Fortunately, our
                 method does not introduce more computational           Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
                 overhead in the inference phase. Besides, we are         Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
                 compatible with other computationally efficient          Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
                 Transformer methods.                                     and Juanzi Li. 2023. Longbench: A bilingual, mul-
                                                                          titask benchmark for long context understanding.
                   Our method does not make any structural im-            CoRR,abs/2308.14508.
                 provements to the rotation position embedding or       bloc97. 2023a. Dynamically scaled rope further in-
                 interpolation methods, so it still does not fully        creases performance of long context llama with zero
                 achieve the optimal situation with the distribution      fine-tuning.
                 perturbation D(P â€²,P ) = 0. This provides inspi-
                                   L   L                                bloc97. 2023b. Ntk-aware scaled rope allows llama
                 ration for future exploration.                           models to have extended (8k+) context size without
                   Theaccuracy of our estimated rotary angle dis-         any fine-tuning and minimal perplexity degradation.
                 tribution is affected by the pre-training sequence
                 length L, since the rotary angles are regarded as      Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong
                 sampledLtimesfromtherealrotaryangledistribu-             Liang, and Lidong Bing. 2024. CLEX: continuous
                 tion. Currently, our method can achieve satisfying       length extrapolation for large language models. In
                                                                          The Twelfth International Conference on Learning
                 improvementformodelswithL = 4k,andwillper-               Representations, ICLR 2024, Vienna, Austria, May
                 form better when applied for models with longer          7-11, 2024. OpenReview.net.
                 pre-training length.                                   ShouyuanChen,ShermanWong,LiangjianChen,and
                   Duetothe constraints of computing resources,           YuandongTian. 2023. Extending context window of
                 our experiments are limited to LLaMA2-7B and             large language models via positional interpolation.
                 LLaMA2-13B, and the long contextual ability is           CoRR,abs/2306.15595.
                 also constrained by the model size. In the future,     Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
                 wehopetoapplyourmethodtoextendthecontext                 Ashish Sabharwal, Carissa Schoenick, and Oyvind
                                                                   7296
