                         Model        Model    Context     TruthfulQA     Hellaswag    MMLU ARC-c Avg.
                         Name          Size    Window
                     LLaMA2-7B          7B        4k          38.74          77.38       46.96     52.22    53.82
                        PI(s=2)         7B        8k          38.03          76.61       44.02     50.68    50.35
                        PI(s=4)         7B        16k         35.99          76.08       45.26     49.74    51.77
                      YaRN(s=2)         7B        8k          39.10          76.83       46.05     51.45    53.36
                      YaRN(s=4)         7B        16k         38.90          77.10       45.98     51.19    53.29
                       Ours(s=2)        7B        8k          39.92          76.80       46.18     51.88    53.70
                       Ours(s=4)        7B        16k         39.83          76.91       46.96     51.45    53.79
                     LLaMA2-13B        13B        4k          37.37          80.83       59.70     64.25    60.54
                        PI(s=2)        13B        8k          37.68          80.25       59.44     63.99    60.34
                        PI(s=4)        13B        16k         35.35          79.94       58.76     61.77    58.96
                      YaRN(s=2)        13B        8k          37.71          80.31       59.99     64.59    60.65
                      YaRN(s=4)        13B        16k         38.53          80.35       59.05     64.16    60.52
                       Ours(s=2)       13B        8k          38.10          80.09       60.16     64.68    60.76
                       Ours(s=4)       13B        16k         39.26          80.03       59.57     64.08    60.74
                Table 2: Comparative performance of various context window extension methods relative to the original LLaMA2
                onthe Hugging Face Open LLM benchmark.
                categorizes the test samples into groups based on    ging Face Open LLM Leaderboard (Face, 2023)
                length intervals of 0-4k, 4-8k, and 8k+ to provide   to observe how its ability in the original length
                an analysis of the model’s performance variations    range changes after extending the context window.
                at different input lengths.                          Specifically, we use 0-shot TruthfulQA (Lin et al.,
                    Table 1 shows a side-by-side comparison of the   2022) and Hellaswag (Zellers et al., 2019), 5-shot
                LLaMA2modelextendedfrom4ktothecontext                MMLU(Hendrycksetal.,2020)and25-shotARC-
                length of 8k and 16k via PI (Chen et al., 2023),     c (Clark et al., 2018). The results demonstrate that
                YaRN (Peng et al., 2023) and our method. We          the performance using our method to extend the
                observe that models of different parameter sizes,    context window is not significantly affected.
                employing our method as the extension method,           As illustrated in Table 1, when extending the
                achieve optimal average results when extended        LLaMA2-7Bmodelto8kwithourapproach, we
                to various context lengths. Compared to PI, our      observe only a 0.12 average score decrease com-
                method achieves an average score improvement of      pared to the original model. Meanwhile, extending
                up to 4.33% when extending the context window        the context window of the LLaMA2-7B model to
                of LLaMA2-7B to 16k. To further demonstrate          16k using YaRN results in a maximum average
                the model’s performance when surpassing the pre-     performance drop of 0.53, which is further exacer-
                training length, we also report the average scores   bated in the case of PI. When applying our method
                for evaluations with lengths greater than 4k. When   to extend the context window of the LLaMA2-13B
                extended to 16k, we can observe that models us-      model, we can even achieve a slightly average per-
                ing our method maintain their performance in the     formance improvement, suggesting that extending
                extended context length range, whereas the model     the model’s context window with our method does
                employing PI exhibits performance degradation        not substantially harm the model’s capability.
                at the 7B model and YaRN exhibits performance
                degradation at the 13B model. We also evaluated      4.4   Passkey Retrieval
                the perplexity of the models as well as their per-   Tostudytheeffective context window size of our
                formance on the RULER benchmark (Hsieh et al.,       modelafter extension, i.e. the maximum distance
                2024), as shown in Appendix B.2.                     of a token that can be effectively attended to during
                4.3   Short Context Validation                       inference. We further evaluate the model’s ability
                                                                     to retrieve a simple passkey from a massive amount
                Wefurther evaluate the LLaMA2 models on the          of text via passkey retrieval task (Mohtashami and
                standard short context benchmark from the Hug-       Jaggi, 2023). Following the experimental setup of
                                                                 7293
