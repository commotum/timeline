                     +
                [HNA 17] JoelHestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md.
                         Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically.
                         arXiv preprint arXiv:1712.00409, 2017.
                   [HR18] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. arXiv
                         preprint arXiv:1801.06146, 2018.
                 [HVD15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
                         preprint arXiv:1503.02531, 2015.
                 [HYC01] SeppHochreiter, A Steven Younger, and Peter R Conwell. Learning to Learn Using Gradient Descent.
                         In International Conference on Artiﬁcial Neural Networks, pages 87–94. Springer, 2001.
                     +
                 [HZJ 19] Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini,
                         Dani Yogatama, and Pushmeet Kohli. Reducing sentiment bias in language models via counterfactual
                         evaluation. arXiv preprint arXiv:1911.03064, 2019.
                     +                                                                       ´
               [IBGC 14] MohitIyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daume III. A neural
                         network for factoid question answering over paragraphs. In Empirical Methods in Natural Language
                         Processing, 2014.
                [IDCBE19] DaphneIppolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of
                         generated text is easiest when humans are fooled. arXiv preprint arXiv:1911.00650, 2019.
                [JCWZ17] MandarJoshi,EunsolChoi,DanielS.Weld,andLukeZettlemoyer. TriviaQA: A large scale distantly
                         supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.
                   [JN20] Zheng Junyuan and Gamma Lab NYC. Numeric transformer - albert, March 2020.
                 [JVS+16] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits
                         of language modeling. arXiv preprint arXiv:1602.02410, 2016.
                 [JYS+19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
                         TinyBERT:Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351, 2019.
                 [JZC+19] YingJu, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu. Technical report on
                         conversational question answering. arXiv preprint arXiv:1909.10772, 2019.
                [KCR+18] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond
                         the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of North
                         American Chapter of the Association for Computational Linguistics (NAACL), 2018.
                     +
                [KKS 20] DanielKhashabi,TusharKhot,AshishSabharwal,OyvindTafjord,PeterClark,andHannanehHajishirzi.
                         Uniﬁedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700, 2020.
                 [KMB20] SarahE.Kreps,MilesMcCain,andMilesBrundage. Allthenewsthat’sﬁttofabricate: Ai-generated
                         text as a tool of media misinformation, 2020.
                [KMH+20] JaredKaplan,SamMcCandlish,TomHenighan,TomB.Brown,BenjaminChess,RewonChild,Scott
                         Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.
                [KPR+19] TomKwiatkowski,Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
                         Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
                         Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural ques-
                         tions: a benchmark for question answering research. Transactions of the Association of Computational
                         Linguistics, 2019.
                   [KR16] YoonKimandAlexanderM.Rush. Sequence-levelknowledgedistillation. Arxiv, 2016.
                   [LB02] EdwardLoperandStevenBird. Nltk: The natural language toolkit, 2002.
                   [LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. arXiv preprint
                         arXiv:1901.07291, 2019.
                                                          70
