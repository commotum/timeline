               Theneural recognition model is trained on replayed experiences as well as “fantasies”, or programs
               sampled randomly from the learned library as a generative model. These random programs deﬁne
               tasks which the system solves during the dream phase, and the neural network is trained to predict the
               solutions found given the observable data for each imagined task.
                   Viewed as a probabilistic inference problem, DreamCoder observes a training set of tasks, written
               X,andinfers both a program ρx solving each task x ∈ X, as well as a prior distribution over programs
               likely to solve tasks in the domain (Fig. 2 middle). This prior is encoded by a library, written L, which
               deﬁnes a generative model over programs, written P[ρ|L] (see S4.3). The neural network helps to
               ﬁnd programs solving a task by predicting, conditioned on the observed examples for that task, an
               approximate posterior distribution over programs likely to solve it. The network thus functions as
               a recognition model that is trained jointly with the generative model, in the spirit of the Helmholtz
               machine (12). We write Q(ρ|x) for the approximate posterior predicted by the recognition model. At a
               high level wake/sleep cycles correspond to iterating the following updates, illustrated in Fig. 2; these
               updates serve to maximize a lower bound on the posterior over L given X (S4.1).
                             ρx = argmax P[ρ|x,L] ∝ P[x|ρ]P[ρ|L], for each task x ∈ X            Wake
                                       ρ:
                                   Q(ρ|x)islarge  Y
                             L=argmaxP[L]                  max        P[x|ρ]P[ρ|L]               Sleep: Abstraction
                                                      ρarefactoring of ρ
                                      L          x∈X                x
                 Train Q(ρ|x) ≈ P[ρ|x,L], where x ∼ X (‘replay’) or x ∼ L (‘fantasy’)            Sleep: Dreaming     (1)
               where P[L] is a description-length prior over libraries (S4.5) and P[x|ρ] is the likelihood of a task
               x∈Xgivenprogramρ. Forexample,thislikelihood is 0 or 1 when x is speciﬁed by inputs/outputs,
               and when learning a probabilistic program, the likelihood is the probability of the program generating
               the observed task.
                   This 3-phase inference procedure works through two distinct kinds of bootstrapping. During
               each sleep cycle the next library bootstraps off the concepts learned during earlier cycles, growing an
               increasingly deep learned library. Simultaneously the generative and recognition models bootstrap each
               other: A more specialized library yields richer dreams for the recognition model to learn from, while
               a more accurate recognition model solves more tasks during waking which then feed into the next
               library. Both sleep phases also serve to mitigate the combinatorial explosion accompanying program
               synthesis. Higher-level library routines allow tasks to be solved with fewer function calls, effectively
               reducing the depth of search. The neural recognition model down-weights unlikely trajectories through
               the search space of all programs, effectively reducing the breadth of search.1
                   Wakephase. Wakingconsists of searching for task-speciﬁc programs with high posterior proba-
               bility, or programs that combine high likelihood (because they solve a task) and high prior probability
               (because they have short description length in the current language). During each Wake cycle we
               sample tasks from a random minibatch of the training set (or, depending on domain size and complex-
               ity, the entire training set). We then search for programs solving each of these tasks by enumerating
               programs in decreasing order of their probability under the recognition model Q(ρ|x), and checking
               if a program ρ assigns positive probability to solving that task (P[x|ρ] > 0). Because the model may
                  1WethankSamTenkaforthisobservation. Inparticular, the difﬁculty of search during waking is roughly proportional to
                     depth
               breadth   , where depth is the total size of a program and breadth is the number of library functions with high probability
               at each decision point in the search tree spanning the space of all programs. Library learning decreases depth at the expense
               of breadth, while training a neural recognition model effectively decreases breadth by decreasing the number of bits of
               entropy consumed by each decision (function call) made when constructing a program solving a task.
                                                                   5
