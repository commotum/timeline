                                                    Fixed Point Diffusion Models
                                                                                                          †
                                              Xingjian Bai *                    LukeMelas-Kyriazi *
                                          University of Oxford                   University of Oxford
                                        xingjianbai@gmail.com                 lukemk@robots.ox.ac.uk
                     Diffusion Transformer (DiT): 674M Parameters             Fixed Point Diffusion Model (FPDM): 85M Parameters
             Figure 1. Fixed Point Diffusion Model (FPDM) is a novel and highly efficient approach to image generation with diffusion models. FPDM integrates an
             implicit fixed point layer into a denoising diffusion model, converting the sampling process into a sequence of fixed point equations. Our model significantly
             decreases model size and memory usage while improving performance in settings with limited sampling time or computation. We compare our model, trained
             at a 256 × 256 resolution against the state-of-the-art DiT [37] on four datasets (FFHQ, CelebA-HQ, LSUN-Church, ImageNet) using compute equivalent to
             20DiTsamplingsteps. FPDM(right) demonstrates enhanced image quality with 87% fewer parameters and 60% less memory during training.
                                     Abstract                                ciency. Compared to the state-of-the-art DiT model [37],
                                                                             FPDMcontains87%fewerparameters,consumes60%less
                Weintroduce the Fixed Point Diffusion Model (FPDM),          memory during training, and improves image generation
             a novel approach to image generation that integrates the        quality in situations where sampling computation or time is
             conceptoffixedpointsolvingintotheframeworkofdiffusion-          limited. Our code and pretrained models are available at
             based generative modeling. Our approach embeds an im-           https://lukemelas.github.io/fixed-point-diffusion-models/.
             plicit fixed point solving layer into the denoising network
        arXiv:2401.08741v1  [cs.CV]  16 Jan 2024of a diffusion model, transforming the diffusion process
             into a sequence of closely-related fixed point problems.        1. Introduction
             Combined with a new stochastic training method, this ap-
             proach significantly reduces model size, reduces memory         The field of image generation has experienced significant
             usage, and accelerates training. Moreover, it enables the       recent advancements driven by the development of large-
             development of two new techniques to improve sampling           scale diffusion models [22, 36, 37, 40, 46, 47]. Key to these
             efficiency: reallocating computation across timesteps and       advancements have been increased model size, computa-
             reusing fixed point solutions between timesteps. We con-        tional power, and the collection of extensive datasets [4, 11,
             duct extensive experiments with state-of-the-art models on      15, 24, 44, 45, 53], collectively contributing to a marked
             ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demon-              improvement in generation performance.
             strating substantial improvements in performance and effi-         Despite these strides, the core principles of diffusion net-
                                                                             works have remained largely unchanged since their develop-
                *Equal Contribution.                                         ment [22]. They typically consist of a fixed series of neural
                †Corresponding author.                                       networklayers, either with a UNet architecture [41] or, more
                                                                         1
                                                                                                                                                                                       Fixed Point Diffusion Model (FPDM)
                     recently, a vision transformer architecture [13, 50]. However,                                                       Diffusion Transformer (DiT)
                     as diffusion models are increasingly deployed in production,                                                        .
                                                                                                                                                         …
                     especially on mobile and edge devices, their large size and                                                                                                                      f
                                                                                                                                                                               Noise
                                                                                                                                                                                                       ﬁxed- 
                                                                                                                                 Noise
                                                                                                                                                                                           f                      f
                                                                                                                                                                                                       point
                                                                                                                                                                                            pre                    post
                                                                                                                                            f      f            f
                                                                                                                                                                                 x
                                                                                                                                             1      2            L
                                                                                                                                                                                                      ..
                                                                                                                                   x
                                                                                                                                                                                  T
                     computational costs pose significant challenges.                                                               T
                                                                                                                                                         …
                                                                                                                                                                                                       × N
                          This paper introduces the Fixed Point Diffusion Model                                                                                 t = T .                                           t = T .
                     (FPDM), which integrates an implicit fixed point solving
                     layer into the denoising network of a diffusion model. In                                                                           …
                                                                                                                                                                                                                          ...
                                                                                                                                                                                                      f
                                                                                                                                                                                                       ﬁxed- 
                                                                                                                                                                                           f                      f
                                                                                                                                                                        ...
                                                                                                                                                                                                       point
                                                                                                                                                                                            pre                    post
                                                                                                                                            f      f            f
                                                                                                                                             1      2            L
                     contrast to traditional networks with a fixed number of layers,                                                                                                                  ..
                                                                                                                                                         …
                                                                                                                                                                                                                             Sample x
                                                                                                                                                                          Sample x
                                                                                                                                                                                                                                   0
                     FPDMisabletoutilize a variable amount of computation                                                                                                        0
                                                                                                                                                                                                       × N
                                                                                                                                                              t = T-1 .
                     at each timestep, with the amount of computation directly                                                                                                                                   t = T-1.
                     influencing the accuracy of the resulting solutions. This fixed                                           Figure 2. The architecture of FPDM compared with DiT. FPDM keeps
                     point network is then applied sequentially, as in standard                                                the first and last transformer block as pre and post processing layers and
                     diffusion models, to progressively denoise a data sample                                                  replaces the explicit layers in-between with an implicit fixed point layer.
                     from pure Gaussian noise.                                                                                 Sampling from the full reverse diffusion process involves solving many
                                                                                                                               of these fixed point layers in sequence, which enables the development of
                          FPDMoffersefficiency gains at two levels of granularity:                                             newtechniques such as timestep smoothing (Sec. 3.3) and solution reuse
                     that of individual timesteps and that of the entire diffusion                                             (Sec. 3.3).
                     process. First, at the timestep level, it provides:                                                       2. Related Work
                     1. A substantial reduction in parameter count compared to
                          previous networks (87% compared to DiT [37]).                                                        Diffusion Models (DMs).                          Diffusion models [2, 22], or
                     2. Reduced memory usage during both training and sam-                                                     score-based generative models [47, 48], are the source of
                          pling (60% compared to DiT [37]).                                                                    tremendous recent progress in image generation. They learn
                     Second, at the diffusion process level, it provides:                                                      to reverse a Markovian noising process using a denoiser
                     1. The ability to smoothly distribute or reallocate computa-                                              parametrized by a neural network, traditionally a U-Net [41].
                          tion among timesteps. This contrasts with all previous                                               Thedenoisingparadigmcanbeseenasthediscretizationofa
                          diffusion models, which must perform a full forward pass                                             stochastic differential equation in a continuous domain [49].
                          at every sampling timestep.                                                                          Later work equipped DMs with different sampling meth-
                     2. Thecapacity to reuse solutions from one fixed-point layer                                              ods [33, 46, 52] and applied conditional control from multi-
                          asaninitializationforthelayerinthesubsequenttimestep,                                                ple modalities [12, 35, 48]. Recently, DMs with transformer-
                          further improving efficiency.                                                                        based architectures (DiTs) were shown to be highly effec-
                                                                                                                               tive [37]; FPDM builds upon the DiT architecture.
                     Ourfixed-point network thereby delivers immediate benefits,                                                    The heavy memory and computation requirements of
                     in the form of reduced size and memory (Sec. 3.2), and                                                    DMsscale up quadratically with the image resolution and
                     further benefits when integrated into the diffusion process, in                                           linearly with the number of sampling timesteps. To reduce
                     the form of increased flexibility during sampling (Sec. 3.3).                                             training cost, LDM [40] proposes to downsample images
                          To realize these benefits, it is imperative to train our                                             with a pre-trained Variational Autoencoder [28] and perform
                     models using an efficient and effective differentiable fixed-                                             denoising in latent space. However, the inference cost of
                     point solver. Although several implicit training methods                                                  DMsisstill considered their primary drawback.
                     exist in the literature [5, 14, 17], we find these existing ap-
                     proaches to be unstable or underperformant in our setting.                                                Implicit Networks and Deep Equilibrium Models.
                     Hence, we develop a new training procedure named Stochas-                                                 Whereas traditional neural networks calculate outputs by
                     tic Jacobian-Free Backpropagation (S-JFB) (Sec. 3.4), in-                                                 performing a pass through a stack of layers, implicit neural
                     spired by Jacobian-Free Backpropagation (JFB) [14]. This                                                  networksdefinetheiroutputsbythesolutionsofdynamicsys-
                     procedure is stable, highly memory-efficient, and surpasses                                               tems. Specifically, Deep Equilibrium Models (DEQs) [5, 16]
                     standard JFB in performance.                                                                              define their output by the fixed point of an equilibrium layer
                          Wedemonstratetheefficacyofourmethodthroughexten-                                                     f . The equilibrium state of DEQs, z∗, is equivalent to the
                                                                                                                                 θ
                     sive experiments (Sec. 4) on four popular image generation                                                output of an infinite-depth, weight-sharing explicit neural
                     datasets: LSUN-Church [53], CelebA-HQ [24], FFHQ [4],                                                     network: lim                    f (zk) = f (z∗) = z∗. In its forward
                                                                                                                                                      k→∞ θ                    ∗ θ
                     and ImageNet [11]. FPDM excels over standard diffu-                                                       pass, the equilibrium state z                      can be computed by apply-
                     sion models when computational resources during sampling                                                  ing solvers like Broyden’s method [9] or Anderson’s ac-
                     are limited. Finally, detailed analysis and ablation studies                                              celeration [3]. In the backward pass, one can implicitly
                     (Sec. 5) demonstrate the efficacy of our proposed network,                                                differentiate through the equilibrium state z∗, or use one of
                     sampling techniques, and training methods.                                                                the recently-proposed accelerated training methods [14, 17].
                                                                                                                          2
              Applications of DEQs include optical flow [7], image seg-             found throughout the literature include Newton’s method,
              mentation [6] and inverse imaging problems [19].                      quasi-Newton methods such as Broyden’s method, and An-
                                                                                    derson’s acceleration. In these cases, one can analytically
              Recent Work Combining Diffusion and DEQs.                 In the      backpropagate through x∗ via implicit differentiation [14].
              past year, two works have merged DMs and DEQs. Differ-                However, these methods can come with significant memory
              ently from our proposal, these approaches tried to convert            and computational costs. Recently, a new iterative solving
              the entire diffusion process into a single fixed point equa-          methoddenoted Jacobian-Free Backpropagation (JFB) was
              tion. [38] considers the entire diffusion trajectory as a single      introduced to circumvent the need for complex and costly
              sample and solves for the fixed point of the trajectory, con-         implicit differentiation; we discuss and extend upon this
              verting the sequential diffusion process into a parallel one.         method in Sec. 3.4.
              [18] distills a pretrained diffusion model into a single-step         3.2. Fixed Point Denoising Networks
              DEQ. These works are exciting but come with their own
              drawbacks: the former is an inference-time technique that             Our proposed fixed-point denoising network (Fig. 2) inte-
              consumessignificantly more memory than standard ances-                grates an implicit fixed-point layer into a diffusion trans-
              tral sampling, while the latter requires a pretrained diffusion       former. The network consists of three stages: 1) explicit
              model and has not scaled to datasets larger than CIFAR-10.            timestep-independent preprocessing layers f(1:l) : X → X,
                                                                                                                                    pre
                                                                                    2) a implicit timestep-conditioned fixed-point layer f          :
              3. Methods                                                                                                                         fp
                                                                                    X×X×T →X,and3)explicittimestep-independent
                                                                                    postprocessing layers f(1:l) : X → X. The function f
              3.1. Preliminaries                                                                              post                                 fp
              Implicit Neural Networks.         The neural network layer is         takes as input both the current fixed-point solution x and
              the foundational building block of deep learning. While               a value x˜ called the input injection, which is the projected
              early neural networks used only a few layers [29, 31], mod-           output of the preceding explicit layers. One can think of ffp
                                                                                    as a map f(x,t˜ ) : X → X conditional on the input injection
              ern networks such as large-scale transformers [13, 50] often                      fp
              consist of dozens of layers connected by residual blocks.             and timestep, for which we aim to find a fixed point. The
                                                                                                                    (t)
                                                                                    network processes an input x         as follows:
              Typically, these layers share a similar internal structure and                                        input
              dimensionality, with each having distinct set of parameters.
                                                                                           (t)     (1:l) (t)
              In essence, most networks are defined explicitly: their opera-             x    =f       (x     )                                  (1)
                                                                                           pre    pre    input
              tions are precisely defined by their layer weights. Running a                (t)                 (t)
                                                                                         x˜   =projection(x      )         input injection       (2)
              forward pass always entails processing inputs with the entire                                    pre
                                                                                          ∗(t)         ∗(t)  (t)
                                                                                        x     =f (x       , x˜  , t)  via fixed point solving    (3)
              set of layers.                                                                      fp
                 Onthe other hand, implicit models define the function                    (t)      (1:l) ∗(t)
                                                                                         x    =f       (x    )                                   (4)
              or procedure to be computed by the network, rather than                     post    post
              the exact sequence of operations. This category includes              Theoutput x(t) is used to compute the loss (during training)
              models that integrate differential equation solvers (Neural                         post
                                                                                    or the input x(t−1) to the next timestep (during sampling).
              ODE/CDE/SDEs;[10,26,27]),aswellasmodelsincorporat-                                   input
              ing fixed point solvers (fixed point networks or DEQs; [5]).              Whereas explicit networks consume a fixed amount of
              Ourproposed FPDMbelongstothislatter group.                            computation, this implicit network can adapt based on the
                                                                                    desired level of accuracy or even on the difficulty of the input.
              Differentiable Fixed Point Solving.        Given a function f         In this way, it unlocks a new tradeoff between computation
              on X, a fixed point solver aims to compute x∗ ∈ X such                and accuracy. Moreover, since the implicit layer replaces a
              that f(x∗) = x∗. The computation of fixed points has been             large number of explicit layers, it significantly decreases its
              the subject of centuries of mathematical study [51], with the         size and memory consumption.
              existence and uniqueness of a system’s fixed points often                 Finally, note that our denoising network operates in latent
              proved with the Banach fixed-point theorem and its vari-              space rather than pixel space. That is, we apply a Variational
              ants [1, 20, 23].                                                     Autoencoder [28, 40] to encode the input image into latent
                 In our case, f       = f is a differentiable function              space and perform all processing in latent space.
                                            θ
              parametrized by θ, and we are interested in both solving for          3.3. Fixed Point Diffusion Models (FPDM)
              the fixed point and backpropagating through it. The simplest
              solving method is fixed point iteration, which iteratively ap-        FPDMincorporatesthefixed point denoising network pro-
              plies f until convergence to x∗. Under suitable assumptions,          posed above into a denoising diffusion process.
                     θ
              iteration converges linearly to the unique fixed point of an              Weassumethereaderisalready familiar with the basics
              equilibrium system (Thrm 2.1 in [14]). Alternative methods            of diffusion models and provide only a brief summary; if
                                                                                 3
                                                                                                    timestep in order to use more timesteps over the sampling
                                                                                                    process (see Fig. 3). In other words, our implicit model
                                                                                                    can effectively “smooth out” the computation over more
                                                                                                    timesteps. By contrast, with explicit models such as DiT,
                                                                                                    the amount of computation directly determines the number
                                                                                                    of timesteps, since a full forward pass is needed at each
                                                                                                    timestep. Indeed, we find that when the amount of compute
                                                                                                    is relatively limited, it is highly beneficial to smooth out the
                                                                                                    compute among more timesteps than would be done with a
                 Figure 3. Illustration of Transformer Block Forward Pass Allocation                traditional model. The effectiveness of smoothing is shown
                 in FPDMandDiT.SinceDiThastoperformfullforwardpassesateach                          empirically in section Sec. 5.1.
                 timestep, under limited compute, it can only denoise at a few timesteps
                 with large gaps. FPDM achieves a more balanced distribution through
                 smoothing, thereby reducing the discretization error. Additionally, FPDM           Reallocating Computation Across Timesteps                         Beyond
                 offers the flexibility to adjust forward pass allocation per timestep with
                 heuristics like Increasing and Decreasing. Refer to Section 3.3 for details.       smoothing out computation over timesteps, FPDM enables
                                                                                                    one to vary the number of forward passes at each timestep,
                 not, we provide an overview in the Supplementary Mate-                             thereby dynamically controlling the solving accuracy at dif-
                 rial. Diffusion models learn to reverse a Markovian nois-                          ferent stages of the denoising process. This capability en-
                 ing process in which a sample X ∼ q(X ) from a target                              ables the implementation of diverse heuristics. For example,
                                                            0           0                           wecanallocateagreaternumberofiterationsatthestart(“de-
                 data distribution q(X ) is noised over a series of timesteps
                                            0                                                       creasing” heuristic) or the end (“increasing” heuristic) of the
                 t ∈ [0,T]. The size of each step of this process is gov-
                                                               T                                    diffusion process (see Fig. 3). Additionally, FPDM supports
                 erned by a variance schedule {β }                   as q(X |X         ) =
                          √                                  t t=0            t    t−1              adaptive allocations of forward passes; further discussions
                 N(X; 1−βX ,βI). where each q(X |X                                   ) is a
                        t           t   t−1     t                           t    t−1                about adaptive algorithms can be found in the supplementary
                 Gaussian distribution. We learn the distribution q(X                  |X )
                                                                                   t−1     t        material.
                 using a network s (X            |X ) ≈ q(X          |X ), which in our
                                       θ     t−1     t           t−1     t
                 case is a fixed point denoising network. The generative
                 process then begins with a sample from the noise distribu-                         Reusing Solutions.          Theconvergence speed of fixed point
                 tion q(X ) and denoises it over a series of steps to obtain a
                            T                                                                       solving meaningfully depends on the initial solution pro-
                 sample from the target distribution q(X ).
                                                                   0                                vided as input. A poor guess of the initial solution would
                    Theprimarydrawbackofdiffusion models as a class of                              make the convergence slower. Considering each timestep
                 generative models is that they are relatively slow to sample.                      independently, the usual approach would be to initialize the
                 Asaresult, during sampling, it is very common to only use                          fixed-point iteration of each timestep using the output of
                 a small subset of all diffusion timesteps and take correspond-                     the (explicit) layers. However, considering sequential fixed
                 ingly larger sampling steps; for example, one might train                          point problems in the diffusion process, we can reuse the
                 with 1000 timesteps and then sample images with as few as                          solution from the fixed point layer at the previous timestep
                 N=5,10,or20timesteps.                                                              as the initial solution for the next timestep. Formally, we
                    Naturally, one could replace the explicit denoising net-                                                                              ∗(t−1)
                                                                                                    can initialize the iteration in Eq. (3) with x                rather than
                 work inside a standard diffusion model with a fixed point                          with x(t) .
                 denoising network, and make no other changes; this would                                   pre
                 immediately reduce model size and memory usage, as dis-                                The intuition for this idea is that adjacent timesteps of
                 cussed previously. However, we can further improve effi-                           the diffusion process only differ by a small amount of noise,
                 ciency during sampling by exploiting the fact that we are                          so their fixed point problems should be similar. Hence, the
                 solving a sequence of related fixed point problems across all                      solutions of these problems should be close, and the solution
                 timesteps, instead of a single one. We present two opportu-                        of the previous timestep would be a good initialization for
                 nities for improvement: smoothing/reallocating computation                         the current timestep. A similar idea was explored in [8],
                 across timesteps and reusing solutions.                                            which used fixed point networks for optical flow estimation.
                                                                                                    3.4. Stochastic Jacobian-Free Backpropagation
                 Smoothing Computation Across Timesteps.                        The flexi-          The final unresolved aspect of our method is how to train
                 bility afforded by fixed-point solving enables us to allocate                      the network, i.e. how to effectively backpropagate through
                 computation between timesteps in a way that is not possible                        the fixed point solving layer. While early work on DEQs
                 with traditional diffusion models. For a given computational                       usedexpensive(Jacobian-based)implicitdifferentiationtech-
                 budget for sampling, we can reduce the the number of for-                          niques [5], recent work has found more success using ap-
                 wardpasses (i.e. number of fixed point iterations) used per                        proximate and inexpensive (Jacobian-free) gradients [14].
                                                                                               4
                         Concretely, this consists of first computing an approxi-                                           pared to the 1-step gradient, our method consumes more
                    mate fixed point (either via iteration or Broyden’s method)                                             memory and compute because it backpropagates through
                    without storing any intermediate results for backpropaga-                                               multiple unrolled iterations rather than a single iteration.
                    tion, and then taking a single additional fixed point step                                              However, it is still drastically more efficient than either im-
                    while storing intermediate results for backpropagation in the                                           plicit differentiation or using traditional explicit networks,
                    standard way. During the backward pass, the gradient is                                                 and it significantly outperforms the 1-step gradient in our
                    only computed for the final step, and so it is referred to as a                                         experiments (Tab. 4).
                    “1-step gradient” or Jacobian-Free Backpropagation (JFB). 1
                         Formally, this approximates the gradient of the loss L                                             4. Experiments
                    with respect to the parameters θ of the implicit layer ffp with                                         4.1. Experimental Setup
                    fixed point x∗(t) by
                                                               ∗(t)      (t)                    ∗(t)     (t)              Model The architecture of FPDM is based on the cur-
                     ∂L            ∂L                 ∂f (x           , x˜    , t)     ∂f (x           , x˜    , t)
                            =                 I −         fp                               fp                               rent state-of-the-art in generative image modeling, DiT-
                     ∂θ              ∗(t)                           ∗(t)                           ∂θ
                                ∂x                             ∂x                                                          XL/2[37], which serves as a strong baseline in our experi-
                                                      ∗(t)     (t)                                                          ments. Adhering to the architecture in [37], we operate in
                                   ∂L ∂f (x                , x˜    , t)
                            ≈                   fp                                                                          latent space using the Variational Autoencoder from [28, 40].
                                     ∗(t)               ∂θ
                                ∂x                                                                                          In addition, we have equipped both the baseline DiT and our
                    The equality above is a consequence of the Implicit Func-                                               FPDMwithtwoadvancesfromtherecentdiffusionliterature:
                    tion Theorem [42] and the approximation simply drops the                                                (1) training to predict velocity rather than noise [43], and
                    inverse Jacobian term. This simplification is rigorously justi-                                         (2) modifying the denoising schedule to have zero terminal
                    fied by Theorem 3.1 in [14] under appropriate assumptions.                                              signal-to-noise ratio [32]. We include these changes to show
                         Despite the simplicity of the 1-step gradient, we found                                            that our improvements are orthogonal to other improvements
                    that it performed poorly in large-scale experiments. To im-                                             madeinthediffusion literature.
                    prove performance without sacrificing efficiency, we use a                                                  Ournetwork consists of three sets of layers: pre-layers,
                    newstochastic approach to compute approximate gradients.                                                an implicit fixed point layer, and post-layers. All layers
                         Our approach (Algorithm 1) samples random variables                                                have the same structure and 24M parameters, except the
                    n ∼ U[0,N] and m ∼ U[1,M] at each training step. Dur-                                                   implicit layer has an additional projection for input injection.
                    ing the forward pass, we perform n fixed point iterations                                               Through empirical analysis, we find that a single pre/post
                    without gradient to obtain an approximate solution,2, and                                               layer can achieve strong results (see Sec. 5.4). Consequently,
                    then we perform m additional iterations with gradient. Dur-                                             the number of parameters in our full network is only 86M,
                    ing the backward pass, we backpropagate by unrolling only                                               markedly lower than 674M parameters in the standard DiT
                    through the last m iterations. The constants N and M are                                               XL/2model,whichhas28explicit layers.
                    hyperparameters that refer to the maximum number of train-
                    ing iterations without and with gradient, respectively. When                                            Training           We perform experiments on four diverse and
                    sampling, the number of iterations used at each timestep is                                             populardatasets: Imagenet, CelebA-HQ,LSUNChurch,and
                    flexible and can be chosen independently of N or M. Com-                                                FFHQ.Allexperimentsareperformedatresolution256. The
                    Algorithm 1 Stochastic Jacobian-Free Backpropagation                                                    ImageNet experiments are class-conditional, whereas those
                           Input hidden states x, timestep t                                                                on other datasets are unconditional. For a fair comparison,
                                                                                                                           wetrain our models and baseline DiT models for the same
                       1: function FORWARD(x)                                                                               amount of time using the same computational resources. All
                       2:        x˜ ← PROJ(x)               # input injection                                               models are trained on 8 NVIDIA V100 GPUs; the models
                       3:        for n iterations drawn uniformly from 0 to N do                                            for the primary experiments on ImageNet are trained for four
                       4:              x←FORWARDPASSWITHOUTGRAD(x,x,˜ t)                                                    days (equivalent to 400,000 DiT training steps), while those
                       5:        for m iterations drawn uniformly from 1 to M do                                            for the other datasets and for the ablation experiments are
                       6:              x←FORWARDPASSWITHGRAD(x,x,˜ t)                                                       trained for one day (equivalent to 100,000 DiT steps). We
                       7:        BACKPROP(loss,x)                                                                           train using Stochastic JFB with M = N = 12 and provide
                       8:        return x                                                                                   an analysis of this setup in Sec. 5.5.
                                                                                                                                TheImageNetexperimentsareclass-conditional,whereas
                         1Wenotethat this process has been referred to in the literature by many                            those on other datasets are unconditional. For ImageNet,
                    names, including the 1-step gradient, phantom gradient, inexact gradient,                               following DiT, we train using class dropout of 0.1, but we
                    and Jacobian-Free backpropagation.                                                                      compute quantitative results without classifier-free guidance.
                         2By “without gradient”, we mean that these iterations do not store
                    any intermediate results for backpropagation, and they are not included in                             We train with a total batch size of 512 and learning rate
                    gradient computation during the backward pass.                                                         1e−4. Weusealinear diffusion noise schedule with β                                          =
                                                                                                                                                                                                                 start
                                                                                                                      5
                    Blocks    Model         FID          FID       Params.      Training              Dataset            Model       FID      Dataset       Model      FID
                                          (DDPM)       (DDIM)                   Memory                CelebA-HQ          DiT         65.2     FFHQ          DiT        58.1
                    140       DiT          148.0        110.0       674M       25.2 GB                                   FPDM 11.1                          FPDM 18.2
                              FPDM          85.8        33.9        85M        10.2 GB                LSUN-Church        DiT         65.6     ImageNet      DiT        80.9
                    280       DiT           80.9        35.2        674M       25.2 GB                                   FPDM 22.7                          FPDM 43.3
                              FPDM          43.3        22.4        85M        10.2 GB
                              DiT           37.9        16.5        674M       25.2 GB             Table 2. Quantitative Results Across Four Datasets. FPDM consistently
                    560       FPDM          26.1        19.6        85M        10.2 GB             outperforms DiT [37] on CelebA-HQ, FFHQ, LSUN-Church, and Imagenet
                                                                                                   with 280 transformer block forward passes. All models are trained and
                                                                                                   evaluated at resolution 256px using the same amount of compute and
                Table 1. Quantitative Results on ImageNet. Despite having 87% fewer                identical hyperparameters.
                 parameters and using 60% less memory during training, FPDM outperforms
                 DiT [37] at 140 and 280 transformer block forward passes and achieves
                 comparable performance at 560 passes.
                 0.0001 and β          = 0.02, modified to have zero terminal
                                  end
                 SNR[32]. We use v-prediction as also recommended by
                 [32]. Following DiT [37], we learn the variance σ along
                with the velocity v.
                    Finally, with regard to the evaluations, all evaluations
                wereperformedusing50000images(FID-50K)exceptthose
                 in Tab. 5 and Fig. 6, which were computed using 1000 im-
                 ages due to computational constraints.
                                                                                                   Figure 4. Timestep smoothing significantly improves performance.
                 4.2. Sampling Quality and Cost Evaluation                                         Given the same amount of sampling compute (280 transformer blocks),
                                                                                                   FPDMenablesustoallocatecomputation among more or fewer diffusion
                To measure image quality, we employ the widely-used                                timesteps, creating a tradeoff between the number of fixed-point solving
                 Frechet Inception Distance (FID) 50K metric [21]. To mea-                         iterations per timestep and the number of timesteps in the diffusion process
                 sure the computational cost of sampling, previous studies                         (See Sec. 3.3). Here we explore the performance of our model on ImageNet
                 on diffusion model sampling have counted the number of                            with fixed point iterations ranging from 1 iteration (across 93 timesteps) to
                                                                                                   68iterations (across 4 timesteps). Each timestep also has 1 pre- and post-
                 function evaluations (NFE) [25, 34, 54]. However, given the                       layer, so sampling with k iterations utilizes k + 2 blocks of compute per
                 implicit nature of our model, a more granular approach is                         timestep. The circle and dashed lines show the performance of the baseline
                 required. In our experiments, both implicit and explicit net-                     DiT-XL/2modelwith28layers,whichinourformulationcorresponds to
                works consist of transformer blocks with identical size and                        smoothing over 26 iterations. Although our model is slightly worse than
                                                                                                   DiTat26iterations, it significantly outperforms DiT when smoothed across
                 structure, so the computational cost of each sampling step                        moretimesteps, demonstrating the effectiveness of timestep smoothing.
                 is directly proportional to the number of transformer block
                 forward passes executed; the total cost of sampling is the                        given the most limited compute. Our method’s improve-
                 product of this amount and the total number of timesteps.3
                Asaresult, we quantify the sampling cost in terms of total                         ments are orthogonal to those gained from using better sam-
                 transformer block forward passes. 4                                               plers; our model effectively lowers the FID score with both
                                                                                                   DDIMandDDPM.At560forwardpasses,ourmethodout-
                 4.3. Results                                                                      performs DiT with DDPM but not DDIM, and for more
                 In Tab. 1, we first present a quantitative comparison of our                      than 560 it is outperformed by DiT. Note that the number of
                 proposed FPDM against the baseline DiT, under different                           parameters in FPDMis only 12.9% of that in DiT, and it con-
                 amounts of sampling compute. Notably, given 140 and 280                           sumes60%lessmemoryduringtraining(reducing memory
                 transformer block forward passes, our best model signifi-                         from 25.2 GB to only 10.2 GB at a batch size of 64).
                 cantly outperforms DiT, with the widest performance gap                               Tab. 2 extends the comparison between FPDM and DiT
                                                                                                   to three additional image datasets: FFHQ, CelebA-HQ, and
                    3To be predise the implicit layer includes an extra projection for input       LSUN-Church. Our findings are consistent across these
                 injection, but this difference is negligible.                                     datasets, with FPDM markedly improving the FID score
                    4For example, sampling from a FPDM with one pre/post-layer and
                 26 fixed point iterations across S timesteps requires the same amount of          despite being nearly one-tenth the size of DiT.
                 compute/time as a FPDM with two pre/post layers and 10 iterations using               Fig. 1 shows qualitative results of our model compared to
                2S timesteps; this computation cost is also the same as that of a traditional      DiT. All images are computed using the same random seeds
                 DiT with 28 layers across S timesteps. Formally, the sampling cost of             andclasses using a classifier-free guidance scale 4.0 and 560
                 FPDMis calculated by (n        +n +n )·Swheren andn
                                             pre    iters    post              pre       post      transformer block forward passes (20 timesteps for DiT).
                 are the number of pre- and post-layers, n       the number of fixed point
                                                             iters
                 iterations, and S the number of sampling steps.                                   FPDMuses8fixedpointiterations per block with timestep
                                                                                               6
                       Baseline (DiT)
                                        1 Iter.        2 Iters.
                                                                     3 Iters.       5 Iters.       6 Iters.       8 Iters.
                                                                                                                                              18 Iters.
                                                                                                                                12 Iters.
                                                                                                                                                              26 Iters.     33 Iters.      38 Iters.      54 Iters.      68 Iters.
                      Figure 5. Qualitative Results for Smoothing Computation Across Timesteps. We show visual results of FPDM using different numbers of fixed point
                      solving iterations, while keeping the total amount of sampling compute fixed (560 transformer blocks). Our method demonstrates similar performance
                      compared to the baseline with 20 to 30 iterations per timestep and superior generation quality with 4 to 8 iterations, as reflected quantitatively in Fig. 4.
                                Train Iters. (M, N)                     3          6         12            24                     5. Analysis and Ablation Studies
                                FID                                43.0        43.2       61.5        567.6                       5.1. Smoothing Computation Across Timesteps
                     Table 3. Performance For Varying Numbers of Fixed Point Iterations                                           InFig.4and5,weexaminetheeffectofsmoothingtimesteps
                      in Training. This table compares various choices of M and N values in                                       as described in Sec. 3.3. We sample across a range of fixed
                     Algorithm 1. They represent a tradeoff between training speed and fixed                                      point iterations and timesteps, keeping the total sampling
                      point convergence accuracy. Results indicate that the optimal values for M                                  cost (i.e. the total number of transformer block forward
                      and N range from 3 to 6.                                                                                    passes) constant. This explores the trade-off between the
                             Train iters without grad (N)                               Method             FID                    convergence of the fixed point iteration at each timestep and
                                                                        JFB(1-Step Grad)                 567.6                    the discretization error of the larger diffusion process.
                             6                                               Multi-Step JFB                48.2                        Balancingthenumberofiterationsandtimestepsiskeyto
                                                                             Stochastic JFB                43.2                   obtaining optimal performance. Intuitively, when using very
                                                                        JFB(1-Step Grad)                 567.6                    few iterations per timestep, the process fails to converge ade-
                             12                                              Multi-Step JFB                79.9                   quately at each step, and the resulting error compounds. Con-
                                                                             Stochastic JFB                61.5                   versely, allocating too many iterations to too few timesteps
                                                                                                                                  results in unnecessary computation on already converged
                     Table 4. Performance of Stochastic Jacobian-Free Backpropagation (S-                                         solving iterations, resulting in discretization errors arising
                     JFB)comparedtoJFB(1-stepgradient). Wefindthat1-stepgradient,the                                              from larger gaps between timesteps. An ideal strategy in-
                      most commonmethodfortraining DEQ[5] models, struggles to optimize                                           volves using just enough fixed-point iterations to achieve
                      models on the large-scale ImageNet dataset, whereas a multi-step version                                    a satisfactory solution, thereby maximizing the number of
                      of it performs well and our stochastic multi-step version performs (S-JFB)
                      even better. The 1-step gradient always unrolls with gradient through a                                     possible timesteps. For instance, with 280 transformer block
                      single iteration (M = 1) of fixed point solving, whereas the stochastic                                     forward passes, we see that the optimal range lies between 4
                     version unrolls though m ∼ U(1,M) iterations for M = 12.                                                     and 8 iterations per timestep.
                           Iters. per Step         3          5           6           8          12          26                   5.2. Reallocating Computation Across Timesteps
                           Constant                48.0       45.8        46.6        47.3       48.5        62.5                 In Tab. 5, the increasing heuristic outperforms the contant
                           Decreasing              48.0       46.3        47.3        48.3       49.1        63.2                 and decreasing ones; allocating resources more toward the
                           Increasing              46.7       44.8        45.9        45.6       48.0        61.7                 later stages of the denoising process improves generation
                     Table 5. Performance of Iteration Allocation Heuristics. Constant                                            quality and detail. Note that such flexibility in resource allo-
                      uses a fixed iteration count per diffusion timestep, while Increasing and                                   cation is a novel feature of FPDM, not possible in previous
                     Decreasing vary their iteration counts linearly with respect to the timestep.                                explicit diffusion models.
                      smoothing. Our model produces sharper images with more                                                      5.3. Reusing Solutions
                      detail, likely due to its ability to spread the computation                                                 Asdescribed in Sec. 3.3, we explore reusing the fixed point
                      amongtimesteps, as discussed in Sec. 5.1.                                                                   solution from each timestep to initialize the subsequent step.
                                                                                                                            7
                                                                                 With Reuse
                                                                                 Without Reuse
                  150
                FID100
                   50
                         1    2     3    5    6    8    12   18    26   33   38   54    68
                                      Number of Fixed Point Iterations per Timestep
                 (a) Performance improvement from reusing solutions across timesteps.                  Figure 7. Performance of Different Number of Pre/Post Layers. We
                                                                                                       find that using at least one pre/post layer is always better than none; fewer
                                                                                                       explicit layers perform better on small compute budgets, whereas more
                                                                                                       explicit layers can better leverage large budgets.
                                                                                                       servation aligns with our intuition: Adjacent timesteps with
                                                                                                       less noise tend to have highly similar corresponding fixed
                                                                                                       point systems, where reusing is more effective.
                                                                                                       5.4. Pre/Post Layers
                                                                                                       One of the many ways FPDM differs from prior work on
              (b) Convergence of f.p. iteration for nine timesteps, with and without reuse.            DEQsisthatweincludeexplicit layers before and after the
                                                                                                       implicit fixed point layer. In Fig. 7, we conduct an ablation
                                                                                                       analysis, training networks with 0, 1, 2, and 4 pre/post layers.
                                                                                                       Weseethatusingatleast 1 explicit pre/post layer is always
                                                                                                       better than 0. For small compute budgets it is optimal to use 1
                                                                                                       pre/post layer, and for larger budgets it is optimal to use 2 or
                                                                                                       4. Broadly, we observe that using more explicit layers limits
                                                                                                       flexibility thereby reducing performance at lower compute
                                                                                                       budgets, but improves performance at higher budgets.
                                                                                                       5.5. Training Method
                 (c) Convergence at iteration across timesteps, with and without reuse.                In Tab. 3, we compare versions of Stochastic Jacobian-Free
                                                                                                       Backpropagation with different values of M and N, the
                 Figure 6. The Impact of Reuse on Fixed Point Accuracy. In (a), we                     upper bounds on the number of training iterations with and
                 examine the performance of sampling with and without reusing solutions                without gradient. M and N reflect a training-time tradeoff
                 for different numbers of iterations per timestep; it considerably helps when          between speed and fixed point convergence. As M and
                 using a small number of iterations per timestep. In (b) and (c), we examine
                 the convergence of individual timesteps. We see that reuse delivers particu-          N increase, each training step contains more transformer
                 larly large benefits for smaller (less-noisy) timesteps. Note that these plots        block forward and backward passes on average; the fixed
                 contain 10 lines as they are plotted for 10 random batches of 32 images               point approximations become more accurate, but each step
                 from ImageNet.                                                                        consumes more time and memory. We find the optimal
                 In Fig. 6a, we see that reusing makes a big difference when                           values of M and N are quite low: 3 or 6. Using too many
                 performing a small number of iterations per timestep and a                            iterations (e.g. 24) is detrimental as it slow down training.
                 negligible difference when performing many iterations per                                 In Tab. 4, we compare to JFB (also called 1-step gradient),
                 timestep. Intuitively, reusing solutions reduces the number                           which has been used in prior work on DEQs [14], and a
                 of iterations needed at each timestep, so it improves the per-                        multi-step variant of it. We find that training with multiple
                 formance when the number of iterations is severely limited.                           steps is essential to obtaining good results, and that using a
                 Fig. 6b and Fig. 6c illustrate the functionality of reusing by                        stochastic number of steps delivers further improvements.
                 examiningattheindividualtimesteplevel. For each timestep                              5.6. Limitations
                 t, we use the difference between the last two fixed point
                 iterations, δ , as an indicator for convergence. Reusing de-                          Theprimarylimitationofourmodelisthatitperformsworse
                                 t
                 creases δt for all timesteps except a few noisiest steps, and                         than the fully-explicit DiT model when sampling computa-
                 reusing is most effective at less noisy timesteps. This ob-                           tion and time are not constrained. The performance gains
                                                                                                  8
               fromourmodelinresource-constrained settings stem largely                   [9] C. G. Broyden. A class of methods for solving nonlinear
               from smoothing and reusing, but in scenarios with saturated                    simultaneous equations. Mathematics of Computation, 19
               timesteps and iterations, the efficacy of these techniques                     (92):577–593, 1965. 2
               is reduced. In such cases, our network resembles a trans-                [10] Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel.
               former with weight sharing [30, 39], which underperform                        Learning neural event functions for ordinary differential equa-
               vanilla transformers. Hence, we do not expect to match the                     tions. In 9th International Conference on Learning Represen-
               performance of DiT, which has 8× more parameters, when                         tations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.
               sampling with an unlimited amount of resources.                                OpenReview.net, 2021. 3
                                                                                        [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li
               6. Conclusions                                                                 Fei-Fei. Imagenet: A large-scale hierarchical image database.
                                                                                              In IEEE Conference on Computer Vision and Pattern Recog-
               We introduce FPDM, a pioneering diffusion model                                nition, pages 248–255, 2009. 1, 2
               characterized by fixed point implicit layers.          Compared          [12] Prafulla Dhariwal and Alexander Nichol. Diffusion mod-
               to  traditional   Diffusion Transformers (DiT), FPDM                           els beat gans on image synthesis. In Advances in Neural
               significantly reduces model size and memory usage.                             Information Processing Systems, pages 8780–8794, 2021. 2
               In the context of diffusion sampling, FPDM enables                       [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
               us to develop new techniques such as solution reusing                          Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
               and timestep smoothing, which give FPDM enhanced                               Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
               flexibility in computational allocation during inference.                      vain Gelly, et al. An image is worth 16x16 words: Transform-
               This flexibility makes it particularly effective in scenarios                  ers for imagerecognitionatscale. InInternationalConference
               where computational resources are constrained. Future work                     onLearning Representations, 2020. 2, 3
               could explore new ways of leveraging this flexibility as                 [14] Samy WuFung,HowardHeaton,QiuweiLi,DanielMcKen-
               well as scaling to larger datasets such as LAION-5B [45].                      zie, Stanley J. Osher, and Wotao Yin. JFB: jacobian-free back-
                                                                                              propagation for implicit networks. In Thirty-Sixth AAAI Con-
                                                                                              ference on Artificial Intelligence, AAAI 2022, Thirty-Fourth
               References                                                                     Conference on Innovative Applications of Artificial Intelli-
                                                                                              gence, IAAI 2022, The Twelveth Symposium on Educational
                [1] Ravi P Agarwal, Maria Meehan, and Donal O’regan. Fixed                    Advances in Artificial Intelligence, EAAI 2022 Virtual Event,
                    point theory and applications. Cambridge university press,                February 22 - March 1, 2022, pages 6648–6656. AAAI Press,
                    2001. 3                                                                   2022. 2, 3, 4, 5, 8
                [2] Namrata Anand and Tudor Achim. Protein structure and se-            [15] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan
                    quence generation with equivariant denoising diffusion prob-              Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten,
                    abilistic models. arXiv preprint arXiv:2205.15019, 2022.                  Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Or-
                    2                                                                         gad, Rahim Entezari, Giannis Daras, Sarah M. Pratt, Vivek
                [3] Donald G. Anderson. Iterative procedures for nonlinear inte-              Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen
                    gral equations. J. ACM, 12(4):547–560, 1965. 2                            Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna,
                                                                                              Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran
                [4] Haoran Bai, Di Kang, Haoxian Zhang, Jin-shan Pan, and                     Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont,
                    Linchao Bao. Ffhq-uv: Normalized facial uv-texture dataset                Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon,
                    for 3d face reconstruction. ArXiv, abs/2211.13874, 2022. 1, 2             Vaishaal Shankar, and Ludwig Schmidt.         Datacomp: In
                [5] Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilib-            search of the next generation of multimodal datasets. CoRR,
                    rium models. In Advances in Neural Information Processing                 abs/2304.14108, 2023. 1
                    Systems 32: Annual Conference on Neural Information Pro-            [16] Zhengyang Geng and J. Zico Kolter. Torchdeq: A library
                    cessing Systems 2019, NeurIPS 2019, December 8-14, 2019,                  for deep equilibrium models. https://github.com/
                    Vancouver, BC, Canada, pages 688–699, 2019. 2, 3, 4, 7                    locuslab/torchdeq,2023. 2
                [6] Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale          [17] Zhengyang Geng, Xin-Yu Zhang, Shaojie Bai, Yisen Wang,
                    deep equilibrium models. Advances in Neural Information                   and Zhouchen Lin. On training implicit models. In Ad-
                    Processing Systems, 33:5238–5250, 2020. 3                                 vances in Neural Information Processing Systems 34: Annual
                [7] Shaojie Bai, Zhengyang Geng, Yash Savani, and J Zico Kolter.              Conference on Neural Information Processing Systems 2021,
                    Deepequilibrium optical flow estimation. In Proceedings of                NeurIPS 2021, December 6-14, 2021, virtual, pages 24247–
                    the IEEE/CVF Conference on Computer Vision and Pattern                    24260, 2021. 2
                    Recognition, pages 620–630, 2022. 3                                 [18] Zhengyang Geng, Ashwini Pokle, and J Zico Kolter. One-
                [8] Shaojie Bai, Zhengyang Geng, Yash Savani, and J. Zico                     step diffusion distillation via deep equilibrium models. In
                    Kolter.    Deep equilibrium optical flow estimation.        In            Thirty-seventh Conference on Neural Information Processing
                    IEEE/CVF Conference on Computer Vision and Pattern                        Systems, 2023. 3
                    Recognition, CVPR2022,NewOrleans,LA,USA,June18-24,                  [19] Davis Gilton, Gregory Ongie, and Rebecca Willett. Deep
                    2022, pages 610–620. IEEE, 2022. 4                                        equilibrium architectures for inverse problems in imaging.
                                                                                    9
                    IEEETransactions on Computational Imaging, 7:1123–1133,                International Conference on Learning Representations, 2021.
                    2021. 3                                                                2
              [20] Andrzej Granas and James Dugundji. Fixed point theory.             [34] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
                    Springer, 2003. 3                                                      Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffu-
              [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-               sion probabilistic model sampling in around 10 steps. arXiv
                    hard Nessler, and Sepp Hochreiter. Gans trained by a two               preprint arXiv:2206.00927, 2022. 6
                    time-scale update rule converge to a local nash equilibrium.      [35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
                    In Advances in Neural Information Processing Systems 30:               Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
                    Annual Conference on Neural Information Processing Sys-                MarkChen. Glide: Towards photorealistic image generation
                    tems2017,December4-9,2017,LongBeach,CA,USA,pages                       and editing with text-guided diffusion models. arXiv preprint
                    6626–6637, 2017. 6                                                     arXiv:2112.10741, 2021. 2
              [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-        [36] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
                    sion probabilistic models. In Advances in Neural Information           Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever,
                    Processing Systems, pages 6840–6851, 2020. 1, 2                        and Mark Chen. GLIDE: Towards photorealistic image gen-
              [23] V.I. Istratescu. Fixed Point Theory: An Introduction. Springer          eration and editing with text-guided diffusion models. In
                    Dordrecht, Dordrecht, 1 edition, 1981. eBook Packages                  International ConferenceonMachineLearning,pages16784–
                    Springer Book Archive. 3                                               16804, 2022. 1
              [24] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.         [37] William Peebles and Saining Xie. Scalable diffusion models
                    Progressive growing of gans for improved quality, stability,           with transformers. CoRR, abs/2212.09748, 2022. 1, 2, 5, 6
                    and variation. ArXiv, abs/1710.10196, 2017. 1, 2
              [25] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.           [38] Ashwini Pokle, Zhengyang Geng, and J Zico Kolter. Deep
                    Elucidating the design space of diffusion-based generative             equilibrium approaches to diffusion models. Advances in
                    models. arXiv preprint arXiv:2206.00364, 2022. 6                       Neural Information Processing Systems, 35:37975–37990,
              [26] Patrick Kidger, James Morrill, James Foster, and Terry J.               2022. 3
                    Lyons. Neural controlled differential equations for irregular     [39] Machel Reid, Edison Marrese-Taylor, and Yutaka Matsuo.
                    time series. In Advances in Neural Information Processing              Subformer: Exploringweightsharingforparameterefficiency
                    Systems 33: Annual Conference on Neural Information Pro-               in generative transformers. In Findings of the Association for
                    cessing Systems 2020, NeurIPS 2020, December 6-12, 2020,               Computational Linguistics: EMNLP 2021, pages 4081–4090,
                    virtual, 2020. 3                                                       2021. 9
              [27] Patrick Kidger, James Foster, Xuechen Li, and Terry J. Lyons.      [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
                    Neural sdes as infinite-dimensional gans. In Proceedings                                      ¨
                                                                                           Patrick Esser, and Bjorn Ommer. High-resolution image
                    of the 38th International Conference on Machine Learning,              synthesis with latent diffusion models. In IEEE Conference
                    ICML2021,18-24July2021,VirtualEvent,pages5453–5463.                    onComputerVisionandPattern Recognition, pages 10684–
                    PMLR,2021. 3                                                           10695, 2022. 1, 2, 3, 5
              [28] Diederik P. Kingma and Max Welling. Auto-encoding varia-           [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
                    tional bayes. In 2nd International Conference on Learning              Convolutional networks for biomedical image segmentation.
                    Representations, ICLR 2014, Banff, AB, Canada, April 14-16,            In Medical Image Computing and Computer-Assisted Inter-
                    2014, Conference Track Proceedings, 2014. 2, 3, 5                      vention - MICCAI 2015 - 18th International Conference Mu-
              [29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Im-            nich, Germany, October 5 - 9, 2015, Proceedings, Part III,
                    agenet classification with deep convolutional neural networks.         2015. 1, 2
                    In Advances in Neural Information Processing Systems 25:          [42] W. Rudin. Principles of Mathematical Analysis. McGraw-
                    26th Annual Conference on Neural Information Processing                Hill, 3 edition, 1976. 5
                    Systems 2012. Proceedings of a meeting held December 3-6,         [43] Tim Salimans and Jonathan Ho. Progressive distillation for
                    2012, Lake Tahoe, Nevada, United States, pages 1106–1114,              fast sampling of diffusion models. In International Confer-
                    2012. 3                                                                ence on Learning Representations, 2021. 5
              [30] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
                    Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert      [44] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
                    for self-supervised learning of language representations. In           Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
                    International Conference on Learning Representations, 2019.            Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-
                    9                                                                      400M: open dataset of clip-filtered 400 million image-text
                                     ´                                                     pairs. CoRR, abs/2111.02114, 2021. 1
              [31] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick
                    Haffner. Gradient-based learning applied to document recog-       [45] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
                    nition. Proc. IEEE, 86(11):2278–2324, 1998. 3                          CadeGordon,RossWightman,MehdiCherti,TheoCoombes,
              [32] ShanchuanLin,BingchenLiu,JiashiLi,andXiaoYang. Com-                     Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick
                    mondiffusion noise schedules and sample steps are flawed.              Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud-
                    CoRR,abs/2305.08891, 2023. 5, 6                                        wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-
              [33] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo                   5B: an open large-scale dataset for training next generation
                    numerical methods for diffusion models on manifolds. In                image-text models. In NeurIPS, 2022. 1, 9
                                                                                 10
             [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
                  diffusion implicit models. In International Conference on
                  Learning Representations, 2020. 1, 2
             [47] Yang Song and Stefano Ermon. Generative modeling by
                  estimating gradients of the data distribution. Proc. NeurIPS,
                  32, 2019. 1, 2
             [48] Yang Song and Stefano Ermon. Improved techniques for
                  training score-based generative models. Proc. NeurIPS, 33:
                  12438–12448, 2020. 2
             [49] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
                  hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
                  generative modeling through stochastic differential equations.
                  In International Conference on Learning Representations,
                  2021. 2
             [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-
                  eit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
                  Polosukhin. Attention is all you need. In Advances in Neural
                  Information Processing Systems 30: Annual Conference on
                  Neural Information Processing Systems 2017, December 4-9,
                  2017, Long Beach, CA, USA, pages 5998–6008, 2017. 2, 3
             [51] J. Wallis. A Treatise of Algebra, Both Historical and Practical.
                  John Playford, 1685. 3
             [52] Daniel Watson, William Chan, Jonathan Ho, and Moham-
                  madNorouzi. Learning fast samplers for diffusion models
                  by differentiating through sample quality. In International
                  Conference on Learning Representations, 2021. 2
             [53] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianx-
                  iong Xiao.  Lsun: Construction of a large-scale image
                  dataset using deep learning with humans in the loop. ArXiv,
                  abs/1506.03365, 2015. 1, 2
             [54] Qinsheng Zhang and Yongxin Chen. Fast sampling of dif-
                  fusion models with exponential integrator. arXiv preprint
                  arXiv:2204.13902, 2022. 6
                                                                         11
                                                   Fixed Point Diffusion Models
                                                        Supplementary Material
             A. Diffusion Models                                           ward passes). For this, an online binary probing scheme can
             Asstated in Sec. 3.3, we provide an overview of diffusion     be employed: use binary search to select a Θ′, on which we
             models here in case any readers are not familiar with diffu-  perform inference for one batch of images. If the number of
                                                                                                             ′
             sion models.                                                  forward passes used to meet the Θ threshold exceeds our
                                                                                                 ′
                Diffusion denoising probabilistic models add noise to a    budget, we increase Θ in subsequent iterations; conversely,
             data sample X drawn from a target data distribution q(X ).    if the number is below our budget, we decrease Θ′. Note that
                          0                                         0      only constant time of probing is needed to find a sufficiently
             This noising process is executed in a series of steps, where  goodthresholdatthebeginningoftheinference. Thiscompu-
             each step adds a specific quantity of noise controlled by a   tational cost would be negligible, especially when sampling
                                   T
             variance schedule {βt}   . At each step, the new data sam-
                                   t=0                                     manybatches of images.
             ple X is generated from the previous one X   according to
                  t                                √ t−1
             the distribution q(X |X   ) = N(X ; 1−β X          , β I).
                                t   t−1          t        t  t−1   t
             Thereverse diffusion process, or generative process, starts
             with a noisy sample from q(X ) ∼ N(0,1) and aims to it-
                                          T
             eratively remove the noise to recover a sample from the orig-
             inal distribution q(X ). This reverse process is learned by a
                                0
             neural network, approximating the distribution q(X   |X )
             as s (X    |X ) ≈ q(X     |X ).                  t−1   t
                θ   t−1   t        t−1   t
             B. Additional Qualitative Examples
             WeprovideexamplesonCelebA-HQ,LSUNChurch,FFHQ,
             and ImageNet in Figs. 8 to 11. For each dataset, we sample
             48 images using DDPM with 560 transformer block for-
             ward passes at resolution 256px. Note that the images are
             not cherry-picked. We also provide additional qualitative
             comparisons with DiT in Fig. 12.
             C. Description of an Adaptive Allocation Algo-
                  rithm
             FPDMallowsfortheadjustment of solution accuracy at dif-
             ferent stages of the denoising process. As noted in Sec. 3.3,
             in addition to implementing straightforward heuristics such
             as “increasing” and “decreasing”, it supports using adaptive
             algorithms to allocate the forward passes across timesteps.
             Weleaveanin-depth investigation of adaptive algorithms to
             future work, but we give an example below to demonstrate
             howonesuchalgorithmcouldwork.
                Westartbyconsideringθ ,thedifferencebetweenthelast
                                       t
             two solving solutions, as a metric of solution quality at each
             step. This aligns with our observation in Fig. 6b, where θt
             decreases as more fixed point iterations (i.e. forward passes)
             are applied. Then a simple adaptive algorithm could be to
             simply set an error threshold Θ and iterate the fixed point
             iteration process at each timestep t continue until θ falls
                                                                t
             below Θ. Then the global threshold Θ controls the number
             of forward passes.
                The only question left would be how to choose Θ to
             match a given computational budget (i.e. a number of for-
                                                                        1
                                      2
       Figure 8. Additional qualitative examples on CelebA-HQ. Examples are sampled using the DDPM sampler with 560 transformer block forward passes at
       resolution 256px. These images are not cherry-picked.
                                      3
       Figure 9. Additional qualitative examples on LSUN Church. Examples are sampled using the DDPM sampler with 560 transformer block forward passes
       at resolution 256px. These images are not cherry-picked.
                                      4
       Figure 10. Additional qualitative examples on FFHQ. Examples are sampled using the DDPM sampler with 560 transformer block forward passes at
       resolution 256px. These images are not cherry-picked.
                                      5
       Figure 11. Additional qualitative examples on ImageNet. Examples are sampled using the DDPM sampler with 560 transformer block forward passes at
       resolution 256px. These images are not cherry-picked.
        CelebA-HQ (Ours)
        CelebA-HQ (DiT)
        h (Ours)
        LSUN Churc
        h (DiT)
        LSUN Churc
        FFHQ (Ours)
        FFHQ (DiT)
        eNet  (Ours)
        Imag
        eNet  (DiT)
        Imag
       Figure 12. Additional qualitative comparison with DiT. We show examples on CelebA-HQ, LSUN Church, FFHQ, and ImageNet. All images are sampled
       using the DDPM sampler with 560 transformer block forward passes at resolution 256px. The images are not cherry-picked.
                                      6
