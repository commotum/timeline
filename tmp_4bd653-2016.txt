=== Page 1 ===
Published as a conference paper at ICLR 2017
                     VARIATIONAL LOSSY AUTOENCODER
                             ‚Ä†‚Ä°                ‚Ä°            ‚Ä°         ‚Ä†‚Ä°               ‚Ä°
                      XiChen ,DiederikP.Kingma ,TimSalimans ,YanDuan ,PrafullaDhariwal ,
                                   ‚Ä†‚Ä°            ‚Ä°           ‚Ä†‚Ä°
                      JohnSchulman ,IlyaSutskever ,PieterAbbeel
                      ‚Ä† UCBerkeley, Department of Electrical Engineering and Computer Science
                      ‚Ä° OpenAI
                      {peter,dpkingma,tim,rocky,prafulla,joschu,ilyasu,pieter}@openai.com
                                                     ABSTRACT
                            Representation learning seeks to expose certain aspects of observed data in a
                            learnedrepresentationthat‚Äôs amenabletodownstreamtaskslikeclassiÔ¨Åcation. For
                            instance, a good representation for 2D images might be one that describes only
                            global structure and discards information about detailed texture. In this paper,
                            we present a simple but principled method to learn such global representations
                            by combining Variational Autoencoder (VAE) with neural autoregressive models
                            such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us
                            to have control over what the global latent code can learn and by designing the
                            architecture accordingly, we can force the global latent code to discard irrelevant
                            information such as texture in 2D images, and hence the VAE only ‚Äúautoencodes‚Äù
                            data in a lossy fashion. In addition, by leveraging autoregressive models as both
                            prior distribution p(z) and decoding distribution p(x|z), we can greatly improve
                            generative modeling performance of VAEs, achieving new state-of-the-art results
                            on MNIST, OMNIGLOTandCaltech-101Silhouettes density estimation tasks as
                            well as competitive results on CIFAR10.
                     1   INTRODUCTION
                     Akeygoalofrepresentation learning is to identify and disentangle the underlying causal factors of
                     the data, so that it becomes easier to understand the data, to classify it, or to perform other tasks
                     (Bengio et al., 2013). For image data this often means that we are interested in uncovering the
                     ‚Äúglobal structure‚Äù that captures the content of an image (for example, the identity of objects present
                     in the image) and its ‚Äústyle‚Äù, but that we are typically less interested in the local and high frequency
                     sources of variation such as the speciÔ¨Åc textures or white noise patterns.
                     Apopularapproachfor learning representations is to Ô¨Åt a probabilistic latent variable model, an ap-
                     proach also known as analysis-by-synthesis (Yuille & Kersten, 2006; Nair et al., 2008). By learning
       arXiv:1611.02731v2  [cs.LG]  4 Mar 2017a generative model of the data with the appropriate hierarchical structure of latent variables, it is
                     hoped that the model will somehow uncover and untangle those causal sources of variations that
                     we happen to be interested in. However, without further assumptions, representation learning via
                     generative modeling is ill-posed: there are many different possible generative models with different
                     (or no) kinds of latent variables that all encode the same probability density function on our ob-
                     served data. Thus, the results we empirically get using this approach are highly dependent on the
                     speciÔ¨Åc architectural and modeling choices that are made. Moreover, the objective that we optimize
                     is often completely disconnected from the goal of learning a good representation: An autoregressive
                     modelofthedatamayachievethesamelog-likelihoodasavariationalautoencoder(VAE)(Kingma
                     &Welling, 2013), but the structure learned by the two models is completely different: the latter
                     typically has a clear hierarchy of latent variables, while the autoregressive model has no stochastic
                     latent variables at all (although it is conceivable that the deterministic hidden units of the autore-
                     gressive models will have meaningful and useful representations). For this reason, autoregressive
                     models have thus far not been popular for the purpose of learning representations, even though they
                     are extremely powerful as generative models (see e.g. van den Oord et al., 2016a).
                     A natural question becomes: is it possible to have a model that is a powerful density estimator
                     and at the same time has the right hierarchical structure for representation learning? A potential
                     solution would be to use a hybrid model that has both the latent variable structure of a VAE, as
                                                          1

=== Page 2 ===
Published as a conference paper at ICLR 2017
                         well as the powerful recurrence of an autoregressive model. However, earlier attempts at combining
                         thesetwokindsofmodelshaverunintotheproblemthattheautoregressivepartofthemodelendsup
                         explaining all structure in the data, while the latent variables are not used (Fabius & van Amersfoort,
                         2014; Chung et al., 2015; Bowman et al., 2015; Serban et al., 2016; Fraccaro et al., 2016; Xu &
                         Sun, 2016). Bowman et al. (2015) noted that weakening the autoregressive part of the model by,
                         for example, dropout can encourage the latent variables to be used. We analyze why weakening
                         is necessary, and we propose a principled solution that takes advantage of this property to control
                         what kind of information goes into latent variables. The model we propose performs well as a
                         density estimator, as evidenced by state-of-the-art log-likelihood results on MNIST, OMNIGLOT
                         and Caltech-101, and also has a structure that is uniquely suited for learning interesting global
                         representations of data.
                         2   VAESDONOTAUTOENCODEINGENERAL
                         A VAE is frequently interpreted as a regularized autoencoder (Kingma & Welling, 2013; Zhang
                         et al., 2016), but the conditions under which it is guaranteed to autoencode (reconstruction being
                         close to original datapoint) are not discussed. In this section, we discuss the often-neglected fact
                         that VAEs do not always autoencode and give explicit reasons why previous attempts to apply VAE
                         in sequencemodelingfoundthatthelatentcodeisgenerallynotusedunlessthedecoderisweakened
                         (Bowmanetal., 2015; Serban et al., 2016; Fraccaro et al., 2016). The understanding of when VAE
                         does autoencode will be an essential building piece for VLAE.
                         2.1  TECHNICAL BACKGROUND
                         Let x be observed variables, z latent variables and let p(x,z) be the parametric model of their
                         joint distribution, called the generative model deÔ¨Åned over the variables. Given a dataset X =
                           1     N
                         {x ,...,x } we wish to perform maximum likelihood learning of its parameters:
                                                                    N
                                                         logp(X) = Xlogp(x(i)),                                (1)
                                                                    i=1
                         but in general this marginal likelihood is intractable to compute or differentiate directly for Ô¨Çexible
                         generative models that have high-dimensional latent variables and Ô¨Çexible priors and likelihoods. A
                         solution is to introduce q(z|x), a parametric inference model deÔ¨Åned over the latent variables, and
                         optimize the variational lower bound on the marginal log-likelihood of each observation x:
                                            logp(x) ‚â• Eq(z|x)[logp(x,z)‚àílogq(z|x)] = L(x;Œ∏)                    (2)
                         where Œ∏ indicates the parameters of p and q models.
                         There are various ways to optimize the lower bound L(x;Œ∏); for continuous z it can be done efÔ¨Å-
                         ciently through a re-parameterization of q(z|x) (Kingma & Welling, 2013; Rezende et al., 2014).
                         This way of optimizing the variational lower bound with a parametric inference network and re-
                         parameterization of continuous latent variables is usually called VAE. The ‚Äúautoencoding‚Äù termi-
                         nology comes from the fact that the lower bound L(x;Œ∏) can be re-arranged:
                                             L(x;Œ∏) = Eq(z|x)[logp(x,z)‚àílogq(z|x)]                             (3)
                                                     =Eq(z|x)[logp(x|z)]‚àíDKL(q(z|x)||p(z))                     (4)
                         where the Ô¨Årst term can be seen as the expectation of negative reconstruction error and the KL
                         divergence term can be seen as a regularizer, which as a whole could be seen as a regularized
                         autoencoder loss with q(z|x) being the encoder and p(x|z) being the decoder. In the context of
                         2D images modeling, the decoding distribution p(x|z) is usually chosen to be a simple factorized
                         distribution, i.e. p(x|z) = Q p(xi|z), and this setup often yields a sharp decoding distribution
                                                    i
                         p(x|z) that tends to reconstruct original datapoint x exactly.
                         2.2  BITS-BACK CODING AND INFORMATION PREFERENCE
                         It‚Äôs straightforward to see that having a more powerful p(x|z) will make VAE‚Äôs marginal generative
                         distribution p(x) = Rzp(z)p(x|z)dz more expressive. This idea has been explored extensively
                                                                    2

=== Page 3 ===
Published as a conference paper at ICLR 2017
                          in previous work applying VAE to sequence modeling (Fabius & van Amersfoort, 2014; Chung
                          et al., 2015; Bowman et al., 2015; Serban et al., 2016; Fraccaro et al., 2016; Xu & Sun, 2016),
                          where the decoding distribution is a powerful RNN with autoregressive dependency, i.e., p(x|z) =
                          Q p(x |z,x ). SinceRNNsareuniversalfunctionapproximatorsandanyjointdistributionoverx
                            i    i    <i
                          admits an autoregressive factorization, the RNN autoregressive decoding distribution can in theory
                          represent any probability distribution even without dependence on z.
                          However, previous attempts have found it hard to beneÔ¨Åt from VAE when using an expressive de-
                          coding distribution p(x|z). Indeed it‚Äôs documented in detail by Bowman et al. (2015) that in most
                          cases when an RNN autoregressive decoding distribution is used, the latent code z is completely
                          ignored and the model regresses to be a standard unconditional RNN autoregressive distribution that
                          doesn‚Äôt depend on the latent code. This phenomenon is commonly attributed to ‚Äúoptimization chal-
                          lenges‚Äù of VAE in the literature (Bowman et al., 2015; Serban et al., 2016; Kaae S√∏nderby et al.,
                          2016) because early in the training the approximate posterior q(z|x) carries little information about
                          datapoint x and hence it‚Äôs easy for the model to just set the approximate posterior to be the prior to
                          avoid paying any regularization cost DKL(q(z|x)||p(z)).
                          Here we present a simple but often-neglected observation that this phenomenon arises not just due
                          to optimization challenges and instead even if we can solve the optimization problems exactly, the
                          latent code should still be ignored at optimum for most practical instances of VAE that have in-
                          tractable true posterior distributions and sufÔ¨Åciently powerful decoders. It is easiest to understand
                          this observation from a Bits-Back Coding perspective of VAE.
                          It is well-known that Bits-Back Coding is an information-theoretic view of Variational Inference
                          (Hinton & Van Camp, 1993; Honkela & Valpola, 2004) and speciÔ¨Åc links have been established
                          between Bits-Back Coding and the Helmholtz Machine/VAE (Hinton & Zemel, 1994; Gregor et al.,
                          2013). Here we brieÔ¨Çy relate VAE to Bits-Back Coding for self-containedness:
                          First recall that the goal of designing an efÔ¨Åcient coding protocol is to minimize the expected code
                          length of communicating x. To explain Bits-Back Coding, let‚Äôs Ô¨Årst consider a more naive coding
                          scheme. VAE can be seen as a way to encode data in a two-part code: p(z) and p(x|z), where z
                          can be seen as the essence/structure of a datum and is encoded Ô¨Årst and then the modeling error
                          (deviation from z‚Äôs structure) is encoded next. The expected code length under this naive coding
                          scheme for a given data distribution is hence:
                                               C     (x) = E               [‚àílogp(z)‚àílogp(x|z)]                      (5)
                                                naive        x‚àºdata,z‚àºq(z|x)
                          This coding scheme is, however, inefÔ¨Åcient. Bits-Back Coding improves on it by noticing that
                          the encoder distribution q(z|x) can be used to transmit additional information, up to H(q(z|x))
                          expected nats, as long as the receiver also has access to q(z|x). The decoding scheme works as
                          follows: a receiver Ô¨Årst decodes z from p(z), then decodes x from p(x|z) and, by running the
                          same approximate posterior that the sender is using, decodes a secondary message from q(z|x).
                          Hence, to properly measure the code length of VAE‚Äôs two-part code, we need to subtract the extra
                          information from q(z|x). Using Bit-Back Coding, the expected code length equates to the negative
                          variational lower bound or the so-called Helmholtz variational free energy, which means minimizing
                          code length is equivalent to maximizing the variational lower bound:
                                        CBitsBack(x) = E                [logq(z|x) ‚àílogp(z)‚àílogp(x|z)]               (6)
                                                         x‚àºdata,z‚àºq(z|x)
                                                     =E         [‚àíL(x)]                                              (7)
                                                         x‚àºdata
                          Casting the problem of optimizing VAE into designing an efÔ¨Åcient coding scheme easily allows us
                          to reason when the latent code z will be used: the latent code z will be used when the two-part code
                          is an efÔ¨Åcient code. Recalling that the lower-bound of expected code length for data is given by
                          the Shannon entropy of data generation distribution: H(data) = E       [‚àílogp     (x)], we can
                          analyze VAE‚Äôs coding efÔ¨Åciency:                                 x‚àºdata        data
                                        CBitsBack(x) = E                [logq(z|x) ‚àílogp(z)‚àílogp(x|z)]               (8)
                                                         x‚àºdata,z‚àºq(z|x)
                                                     =E         [‚àílogp(x)+D       (q(z|x)||p(z|x))]                  (9)
                                                         x‚àºdata                KL
                                                     ‚â•E         [‚àílogp     (x)+D (q(z|x)||p(z|x))]                  (10)
                                                         x‚àºdata        data        KL
                                                     =H(data)+E            [D    (q(z|x)||p(z|x))]                  (11)
                                                                    x‚àºdata    KL
                                                                        3

=== Page 4 ===
Published as a conference paper at ICLR 2017
                            Since Kullback Leibler divergence is always non-negative, we know that using the two-part code
                            derived from VAE suffers at least an extra code length of DKL(q(z|x)||p(z|x)) nats for using a
                            posterior that‚Äôs not precise. Many previous works in Variational Inference have designed Ô¨Çexible
                            approximate posteriors to better approximate true posterior (Salimans et al., 2014; Rezende & Mo-
                            hamed, 2015; Tran et al., 2015; Kingma et al., 2016). Improved posterior approximations have
                            showntobeeffectiveinimprovingvariationalinference but none of the existing methods are able to
                            completely close the gap between approximate posterior and true posterior. This leads us to believe
                            that for most practical models, at least in the near future, the extra coding cost DKL(q(z|x)||p(z|x))
                            will exist and will not be negligible.
                            OnceweunderstandtheinefÔ¨ÅciencyoftheBits-BackCodingmechanism,it‚Äôssimpletorealizewhy
                            sometimes the latent code z is not used: if the p(x|z) could model pdata(x) without using informa-
                            tion from z, then it will not use z, in which case the true posterior p(z|x) is simply the prior p(z)
                            and it‚Äôs usually easy to set q(z|x) to be p(z) to avoid incurring an extra cost DKL(q(z|x)||p(z|x)).
                            Andit‚Äôs exactly the case when a powerful decoding distribution is used like an RNN autoregressive
                            distribution, which given enough capacity is able to model arbitrarily complex distributions. Hence
                            there exists a preference of information when a VAE is optimized: information that can be modeled
                            locally by decoding distribution p(x|z) without access to z will be encoded locally and only the
                            remainder will be encoded in z.
                            Wenotethat one common way to encourage putting information into the code is to use a factorized
                            decoder p(x|z) = Q p(x |z) but so long as there is one dimension x that‚Äôs independent of all
                                                  i    i                                             j
                            other dimensions for true data distribution, p    (x) = p      (x )p     (x   ), then the latent code
                                                                          data         data  j  data   6=j
                            doesn‚Äôt contain all the information about x since at least x will be modeled locally by factorized
                                                                                        j
                            p(x|z). This kind of independence structure rarely exists in images so common VAEs that have
                            factorized decoder autoencode almost exactly. Other techniques to encourage the usage of the latent
                            code include annealing the relative weight of of D    (q(z|x)||p(z)) in the variational lower bound
                                                                               KL
                            (Bowman et al., 2015; Kaae S√∏nderby et al., 2016) or the use of free bits (Kingma et al., 2016),
                            whichcanservethedualpurposeofsmoothingtheoptimizationlandscapeandcancelingoutpartof
                            the Bits-Back Code inefÔ¨Åciency DKL(q(z|x)||p(z|x)).
                            3    VARIATIONAL LOSSY AUTOENCODER
                            The discussion in Section 2.2 suggests that autoregressive models cannot be combined with VAE
                            since information will be preferred to be modeled by autoregressive models. Nevertheless, in this
                            section, we present two complementary classes of improvements to VAE that utilize autoregressive
                            models fruitfully to explicitly control representation learning and improve density estimation.
                            3.1   LOSSY CODE VIA EXPLICIT INFORMATION PLACEMENT
                            Even though the information preference property of VAE might suggest that one should always use
                            the full autoregressive models to achieve a better code length/log-likelihood, especially when slow
                            data generation is not a concern, we argue that this information preference property can be exploited
                            to turn the VAE into a powerful representation learning method that gives us Ô¨Åne-grained control
                            over the kind of information that gets included in the learned representation.
                            When we try to learn a lossy compression/representation of data, we can simply construct a de-
                            coding distribution that‚Äôs capable of modeling the part of information that we don‚Äôt want the lossy
                            representation to capture, but, critically, that‚Äôs incapable of modelling the information that we do
                            want the lossy representation to capture.
                            For instance, if we are interested in learning a global representation for 2D images that doesn‚Äôt
                            encode information about detailed texture, we can construct a speciÔ¨Åc factorization of the autore-
                            gressive distribution such that it has a small local receptive Ô¨Åeld as decoding distribution, e.g.,
                            p    (x|z) = Q p(x |z,x                     ). Notice that, as long as x                  is smaller
                             local           i    i     WindowAround(i)                              WindowAround(i)
                            than x   , p    (x|z) won‚Äôt be able to represent arbitrarily complex distribution over x without de-
                                  <i   local
                            pendence on z since the receptive Ô¨Åeld is limited such that not all distributions over x admit such
                            factorizations. In particular, the receptive Ô¨Åeld window can be a small rectangle adjacent to a pixel
                            xi and in this case long-range dependency will be encoded in the latent code z. On the other hand,
                            if the true data distribution admits such factorization for a given datum x and dimension i, i.e.
                                                                             4

=== Page 5 ===
Published as a conference paper at ICLR 2017
                             p     (x |x                  ) = p      (x |x   ), then the information preference property discussed
                               data  i   WindowAround(i)        data   i   <i
                             in Section 2.2 will apply here, which means that all the information will be encoded in local au-
                             toregressive distribution for xi. Local statistics of 2D images like texture will likely be modeled
                             completely by a small local window, whereas global structural information of an images like shapes
                             of objects is long-range dependency that can only be communicated through latent code z. There-
                             fore we have given an example VAE that will produce a lossy compression of 2D images carrying
                             exclusively global information that can‚Äôt be modeled locally.
                             Notice that a global representation is only one of many possible lossy representations that we can
                             construct using this information preference property. For instance, the conditional of an autoregres-
                             sive distribution might depend on a heavily down-sampled receptive Ô¨Åeld so that it can only model
                             long-range pattern whereas local high-frequency statistics need to be encoded into the latent code.
                             Hencewehavedemonstratedthatwecanachieveexplicitplacementofinformationbyconstraining
                             the receptive Ô¨Åeld/factorization of an autoregressive distribution that‚Äôs used as decoding distribution.
                             We want to additionally emphasize the information preference property is an asymptotic view in
                             a sense that it only holds when the variational lowerbound can be optimized well. Thus, we are
                             not proposing an alternative to techniques like free bits Kingma et al. (2016) or KL annealing, and
                             indeed they are still useful methods to smooth the optimization problem and used in this paper‚Äôs
                             experiments.
                             3.2    LEARNED PRIOR WITH AUTOREGRESSIVE FLOW
                             InefÔ¨Åciency in Bits-Back Coding, i.e., the mismatch between approximate posterior and true poste-
                             rior, can be exploited to construct a lossy code but it‚Äôs still important to minimize such inefÔ¨Åciency
                             to improve overall modeling performance/coding efÔ¨Åciency. We propose to parametrize the prior
                             distribution p(z;Œ∏) with an autoregressive model and show that a type of autoregressive latent code
                             can in theory reduce inefÔ¨Åciency in Bits-Back coding.
                             It is well-known that limited approximate posteriors impede learning and therefore various expres-
                             sive posterior approximationshavebeenproposedtoimproveVAE‚Äôsdensityestimationperformance
                             (Turner et al., 2008; Mnih & Gregor, 2014; Salimans et al., 2014; Rezende & Mohamed, 2015;
                             Kingma et al., 2016). One such class of approximate posteriors that has been shown to attain good
                             empirical performance is based on the idea of Normalizing Flow, which is to apply an invertible
                             mapping to a simple random variable, for example a factorized Gaussian as commonly used for
                             q(z|x), in order to obtain a complicated random variable. For an invertible transformation between
                             a simple distribution y and a more Ô¨Çexible z, we know from the change-of-variable technique that
                             logq(z|x) = logq(y|x) ‚àí logdet dz and using q(z|x) as approximate posterior will decrease the
                                                                   dy
                             coding efÔ¨Åciency gap DKL(q(z|x)||p(z|x)) provided the transformation is sufÔ¨Åciently expressive.
                             Kingma et al. (2016) introduced Inverse Autoregressive Flow, which is a powerful class of such
                                                                                        y ‚àí¬µ (y      )
                                                                                         i   i  1:i‚àí1                                +
                             invertible mappingsthathavesimpledeterminant: z =                        , where¬µ (.) ‚àà R,œÉ (.) ‚àà R
                                                                                    i     œÉi(y1:i‚àí1)            i           i
                             are general functions that can be parametrized by expressive neural networks, such as MADE and
                             PixelCNN variants (Germain et al., 2015; van den Oord et al., 2016a). Inverse autoregressive Ô¨Çow
                             is the inverse/whitening of autoregressive Ô¨Çow: y = z œÉ (y           ) +¬µ (y       ). We refer interested
                                                                                 i     i i   1:i‚àí1      i  1:i‚àí1
                             readers to (Rezende & Mohamed, 2015; Kingma et al., 2016) for in-depth discussions on related
                             topics.
                             In this paper, we propose to parametrize our learnable prior as an autoregressive Ô¨Çow from some
                             simple noise source like spherical Gaussian. Next, we show that using latent code transformed
                             by autoregressive Ô¨Çow (AF) is equivalent to using inverse autoregressive Ô¨Çow (IAF) approximate
                             posterior, which explains why it can similarly improve Bits-Back Coding efÔ¨Åciency. Moreover,
                             comparedwithanIAFposterior,anAFpriorhasamoreexpressivegenerativemodelthatessentially
                             ‚Äúcomes for free‚Äù.
                             For an autoregressive Ô¨Çow f, some continuous noise source  is transformed into latent code z:
                             z = f(). Assumingthedensityfunctionfornoisesourceisu(),wesimilarlyknowthatlogp(z) =
                             logu()+logdet d.
                                                 dz
                                                                                 5

=== Page 6 ===
Published as a conference paper at ICLR 2017
                             Simply re-arranging the variational lowerbound for using AF prior reveals that having an AF latent
                             code z is equivalent to using an IAF posterior for  that we can interpret as the new latent code:
                                 L(x;Œ∏) = E            [logp(x|z) +logp(z)‚àílogq(z|x)]                                           (12)
                                              z‚àºq(z|x)                                                                 
                                                                                                       d
                                          =E               ‚àí1     logp(x|f())+logu()+logdet             ‚àílogq(z|x)            (13)
                                              z‚àºq(z|x),=f   (z)                                       dz
                                                                Ô£Æ                                                     d Ô£π
                                                           ‚àí1   Ô£Ø                                                         Ô£∫
                                          =E                      logp(x|f())+logu()‚àí(logq(z|x)‚àílogdet                 )      (14)
                                              z‚àºq(z|x),=f   (z) Ô£∞                             |           {z         dz}Ô£ª
                                                                                                       IAFPosterior
                             AF prior is the same as IAF posterior along the encoder path, f‚àí1(q(z|x)), but differs along the
                             decoder/generator path: IAF posterior has a shorter decoder path p(x|z) whereas AF prior has a
                             deeper decoder path p(x|f()). The crucial observation is that AF prior and IAF posterior have the
                             same computation cost under the expectation of z ‚àº q(z|x), so using AF prior makes the model
                             moreexpressive at no training time cost.
                             4    EXPERIMENTS
                             In this paper, we evaluate VLAE on 2D images and leave extensions to other forms of data to
                             future work. For the rest of the section, we deÔ¨Åne a VLAE model as a VAE that uses AF prior
                             and autoregressive decoder. We choose to implement conditional distribution p(x|z) with a small-
                             receptive-Ô¨Åeld PixelCNN (van den Oord et al., 2016a), which has been proved to be a scalable
                             autoregressive model.
                             For evaluation, we use binary image datasets that are commonly used for density estimation tasks:
                             MNIST(LeCunetal., 1998) (both statically binarized 1 and dynamically binarized version (Burda
                             et al., 2015a)), OMNIGLOT (Lake et al., 2013; Burda et al., 2015a) and Caltech-101 Silhouettes
                             (Marlin et al., 2010). All datasets uniformly consist of 28x28 binary images, which allow us to use
                             a uniÔ¨Åed architecture. VAE networks used in binary image datasets are simple variants of ResNet
                             VAEs described in (Salimans et al., 2014; Kingma et al., 2016). For the decoder, we use a variant
                             of PixelCNN that has 6 layers of masked convolution with Ô¨Ålter size 3, which means the window of
                             dependency,xWindowAround(i),islimitedtoasmalllocalpatch. Duringtraining,‚Äùfreebits‚Äù(Kingma
                             et al., 2016) is used improve optimization stability. Experimental setup and hyperparameters are
                             detailed in the appendix. Reported marginal NLL is estimated using Importance Sampling with
                             4096samples.
                             Wedesignedexperiments to answer the following questions:
                                    ‚Ä¢ CanVLAElearnlossycodesthatencodeglobalstatistics?
                                    ‚Ä¢ Doesusing AFpriors improves upon using IAF posteriors as predicted by theory?
                                    ‚Ä¢ Doesusingautoregressivedecodingdistributionsimprovedensityestimationperformance?
                             4.1   LOSSY COMPRESSION
                             First we are interested in whether VLAE can learn a lossy representation/compression of data by
                             using the PixelCNN decoder to model local statistics. We trained VLAE model on Statically Bina-
                             rized MNIST and the converged model has E[D           (q(z|x)||p(z))] = 13.3 nats = 19.2 bits, which
                                                                               KL
                             is the number of bits it uses on average to encode/compress one MNIST image. By comparison, an
                             identical VAE model with factorized decoding distribution will uses on average 37.3 bits in latent
                             code, and this thus indicates that VLAE can learn a lossier compression than a VAE with regular
                             factorized conditional distribution.
                             The next question is whether VLAE‚Äôs lossy compression encodes global statistics and discards
                             local statistics. In Fig 1a, we visualize original images xdata and one random ‚Äúdecompression‚Äù
                             x              from VLAE: z ‚àº q(z|x         ),x               ‚àºp(x|z). We observe that none of the
                              decompressed                           data    decompressed
                                1Weusetheversion provided by Hugo Larochelle.
                                                                                6

=== Page 7 ===
Published as a conference paper at ICLR 2017
                                                  (a)  Original    test-set  images (left)            (b) Samples from VLAE
                                                  and ‚Äúdecompressioned‚Äù versions from
                                                  VLAE‚Äôslossycode(right)
                                                                      Figure 1: Statically Binarized MNIST
                                 decompressions is an exact reconstruction of the original image but instead the global structure of
                                 the image was encoded in the lossy code z and regenerated. Also worth noting is that local statistics
                                 are not preserved but a new set of likely local statistics are generated in the decompressed images:
                                 the binary masks are usually different and local styles like stroke width are sometimes slightly dif-
                                 ferent.
                                 However,weremarkthatthelossycodezdoesn‚Äôtalwayscapturethekindofglobalinformationthat
                                 we care about and it‚Äôs dependent on the type of constraint we put on the decoder. For instance, in
                                 Fig 4b, we show decompressions for OMNIGLOT dataset, which has more meaningful variations
                                 in small patches than MNIST, and we can observe that semantics are not preserved in some cases.
                                 This highlights the need to specify the type of statistics we care about in a representation, which will
                                 be different across tasks and datasets, and design decoding distribution accordingly.
                                                  (a)  Original    test-set  images (left)            (b) Samples from VLAE
                                                  and ‚Äúdecompressioned‚Äù versions from
                                                  VLAE‚Äôslossycode(right)
                                                                               Figure 2: OMNIGLOT
                                 4.2    DENSITY ESTIMATION
                                 Next we investigate whether leveraging autoregressive models as latent distribution p(z) and as
                                 decoding distribution p(x|z) would improve density estimation performance.
                                 To verify whether AF prior is able to improve upon IAF posterior alone, it‚Äôs desirable to test
                                 this model without using autoregressive decoder but instead using the conventional independent
                                 Bernoulli distribution for p(x|z). Hence we use the best performing model from Kingma et al.
                                                                                            7

=== Page 8 ===
Published as a conference paper at ICLR 2017
                                                        Table 1: Statically Binarized MNIST
                                           Model                                             NLLTest
                                           Normalizing Ô¨Çows (Rezende & Mohamed, 2015)          85.10
                                           DRAW(Gregoretal.,2015)                            <80.97
                                           Discrete VAE (Rolfe, 2016)                          81.01
                                           PixelRNN(vandenOordetal.,2016a)                     79.20
                                           IAFVAE(Kingmaetal.,2016)                            79.88
                                           AFVAE                                               79.30
                                           VLAE                                                79.03
                          (2016) on statically binarized MNIST and make the single modiÔ¨Åcation of replacing the original
                          IAFposterior with an equivalent AF prior, removing the context. As seen in Table 1, VAE with AF
                          prior is outperforming VAE with an equivalent IAF posterior, indicating that the deeper generative
                          model from AF prior is beneÔ¨Åcial. A similar gain carries over when an autoregressive decoder is
                          used: on statically binarized MNIST, using AF prior instead of IAF posterior reduces train NLL by
                          0.8 nat and test NLL by 0.6 nat.
                          Next we evaluate whether using autoregressive decoding distribution can improve performance and
                          we show in Table 1 that a VLAE model, with AF prior and PixelCNN conditional, is able to out-
                          perform a VAE with just AF prior and achieves new state-of-the-art results on statically binarized
                          MNIST.
                          In addition, we hypothesize that the separation of different types of information, the modeling global
                          structure in latent code and local statistics in PixelCNN, likely has some form of good inductive bi-
                          ases for 2D images. In order to evaluate if VLAE is an expressive density estimator with good
                          inductive biases, we will test a single VLAE model, with the same network architecture, on all
                          binary datasets. We choose hyperparameters manually on statically binarized MNIST and use the
                          samehyperparameters to evaluate on dynamically binarized MNIST, OMNIGLOT and Caltech-101
                          Silhouettes. We also note that better performance can be obtained if we individually tune hyperpa-
                          rameters for each dataset. As a concrete demonstration, we report the performance of a Ô¨Åne-tuned
                          VLAEonOMNIGLOTdatasetinTable3.
                                                      Table 2: Dynamically binarized MNIST
                                          Model                                             NLLTest
                                          Convolutional VAE + HVI (Salimans et al., 2014)     81.94
                                          DLGM2hl+IWAE(Burdaetal.,2015a)                      82.90
                                          Discrete VAE (Rolfe, 2016)                          80.04
                                          LVAE(KaaeS√∏nderbyetal.,2016)                        81.74
                                          DRAW+VGP(Tranetal.,2015)                          <79.88
                                          IAFVAE(Kingmaetal.,2016)                            79.10
                                          Unconditional Decoder                               87.55
                                          VLAE                                                78.53
                          Table 3: OMNIGLOT.[1](Burdaetal.,2015a),        Table 4: Caltech-101 Silhouettes. [1] (Born-
                          [2] (Burda et al., 2015b), [3] (Gregor et al.,  schein & Bengio, 2014), [2] (Cho et al., 2011),
                          2015), [4] (Gregor et al., 2016),               [3] (Du et al., 2015), [4] (Rolfe, 2016), [5]
                                                                          (Goessling & Amit, 2015),
                               Model                    NLLTest
                               VAE[1]                     106.31               Model                   NLLTest
                               IWAE[1]                    103.38               RWSSBN[1]                  113.3
                               RBM(500hidden)[2]          100.46               RBM[2]                     107.8
                               DRAW[3]                   <96.50                NAISNADE[3]                100.0
                               ConvDRAW[4]               <91.00                Discrete VAE [4]           97.6
                               Unconditional Decoder      95.02                SpARN[5]                   88.48
                               VLAE                       90.98                Unconditional Decoder      89.26
                               VLAE(Ô¨Åne-tuned)            89.83                VLAE                       77.36
                                                                        8

=== Page 9 ===
Published as a conference paper at ICLR 2017
                          Asseen in Table 2,3,4, with the same set of hyperparameters tuned on statically binarized MNIST,
                          VLAE is able to perform well on the rest of datasets, signiÔ¨Åcantly exceeding previous state-of-
                          the-art results on dynamically binarized MNIST and Caltech-101 Silhouettes and tying statistically
                          with best previous result on OMNIGLOT. In order to isolate the effect of expressive PixelCNN as
                          decoder, we also report performance of the same PixelCNN trained without VAE part under the
                          name‚ÄúUnconditional Decoder‚Äù.
                          4.3   NATURAL IMAGES: CIFAR10
                          In addition to binary image datasets, we have applied VLAE to the CIFAR10 dataset of natural
                          images. Density estimation of CIFAR10 images has been a challenging benchmark problem used by
                          manyrecent generative models and hence is great task to position VLAE among existing methods.
                          We investigated using ResNet (He et al., 2016) and DenseNet (Huang et al., 2016) as building
                          blocks for VAE networks and observed that DenseNet reduces overÔ¨Åtting. We also propose a new
                          optimization technique that blends the advantages of KL annealing (Serban et al., 2016) and ‚Äùfree
                          bits‚Äù (Kingma et al., 2016) to stabilize learning on this challenging dataset. Detailed experimental
                          setup is described in Appendix.
                          VLAEiscomparedtoothermethodsonCIFAR10inTable5. WeshowthatVLAEmodelsattainnew
                          state-of-the-art performance among other variationally trained latent-variable models. DenseNet
                          VLAEmodel also outperforms most other tractable likelihood models including Gated PixelCNN
                          and PixelRNN and has results only slightly worse than currently unarchived state-of-the-art Pixel-
                          CNN++.
                          Table 5: CIFAR10. Likelihood for VLAE is approximated with 512 importance samples. [1]
                          (van den Oord et al., 2016a), [2] (Dinh et al., 2014), [3] (van den Oord & Schrauwen, 2014), [4]
                          (Dinh et al., 2016), [5] (van den Oord et al., 2016b), [6] (Salimans et al., 2017), [7] (Sohl-Dickstein
                          et al., 2015), [8] (Gregor et al., 2016), [9] (Kingma et al., 2016)
                                      Method                                                     bits/dim ‚â§
                                      Results with tractable likelihood models:
                                      Uniform distribution [1]                                   8.00
                                      Multivariate Gaussian [1]                                  4.70
                                      NICE[2]                                                    4.48
                                      DeepGMMs[3]                                                4.00
                                      Real NVP[4]                                                3.49
                                      PixelCNN[1]                                                3.14
                                      Gated PixelCNN [5]                                         3.03
                                      PixelRNN[1]                                                3.00
                                      PixelCNN++[6]                                              2.92
                                      Results with variationally trained latent-variable models:
                                      DeepDiffusion [7]                                          5.40
                                      Convolutional DRAW [8]                                     3.58
                                      ResNet VAEwithIAF[9]                                       3.11
                                      ResNet VLAE                                                3.04
                                      DenseNet VLAE                                              2.95
                          Wealso investigate learning lossy codes on CIFAR10 images. To illustrate how does the receptive
                          Ô¨Åeld size of PixelCNN decoder inÔ¨Çuence properties of learned latent codes, we show visualizations
                          of similar VLAEmodelswithreceptiveÔ¨Åeldsofdifferentsizes. SpeciÔ¨ÅcallywesayareceptiveÔ¨Åeld,
                          x               , has size AxB when a pixel x can depend on the rectangle block of size AxB
                           WindowAround(i)                            i
                          immediately on top of x as well as the  A‚àí1 pixels immediately to the left of x . We use this
                                                 i                  2                                    i
                          notation to refer to different types of PixelCNN decoders in Figure 3.
                          From (a)-(c) in Figure 3, we can see that larger receptive Ô¨Åelds progressively make autoregressive
                          decoders capture more structural information. In (a), a smaller receptive Ô¨Åeld tends to preserve
                          rather detailed shape information in the lossy code whereas the latent code only retains rough shape
                          in (c) with a larger receptive Ô¨Åeld.
                                                                        9

=== Page 10 ===
Published as a conference paper at ICLR 2017
                                                 (a) 4x2                                      (b) 5x3
                                                 (c) 7x4                                 (d) 7x4 Grayscale
                           Figure 3: CIFAR10: Original test-set images (left) and ‚Äúdecompressioned‚Äù versions from VLAE‚Äôs
                           lossy code (right) with different types of receptive Ô¨Åelds
                           It‚Äôs interesting to also note that in (a)-(c), oftentimes color information is partially omitted from
                           latent codes and one explanation can be that color is very predictable locally. However, color
                           information can be important to preserve if our task is, for example, object classiÔ¨Åcation.  To
                           demonstrate how we can encode color information in the lossy code, we can choose to make
                           PixelCNN decoder depend only on images‚Äô grayscale versions. In other words, instead of choos-
                           ing the decoder to be plocal(x|z) = Q p(xi|z,xWindowAround(i)), we use a decoder of the form
                           p    (x|z) = Q p(x |z,Grayscale(xi                    )). In (d) of Figure 3, we visualize lossy
                            local           i    i               WindowAround(i)
                           codes for a VLAE that has the same receptive Ô¨Åeld size as (c) but uses a ‚Äúgrayscale receptive Ô¨Åeld‚Äù.
                           Wenotethatthelossycodesin(d)encoderoughlythesamestructuralinformationasthosein(c)but
                           generally generate objects that are more recognizable due to the preservation of color information.
                           This serves as one example of how we can design the lossy latent code carefully to encode what‚Äôs
                           important and what‚Äôs not.
                           5   RELATED WORK
                           Weinvestigate a fusion between variational autoencoders with continuous latent variables (Kingma
                           &Welling, 2013; Rezende et al., 2014) and neural autoregressive models. For autoregression, we
                           speciÔ¨Åcally apply a novel type of architecture where autoregression is realised through a carefully
                                                                         10

=== Page 11 ===
Published as a conference paper at ICLR 2017
                   constructed deep convolutional network, introduced in the PixelCNN model for images (van den
                   Oordetal.,2016a,b). Thesefamilyofconvolutionalautoregressivemodelswasfurtherexplored,and
                   extended, for audio in WaveNet (Oord et al., 2016), video in Video Pixel Networks (Kalchbrenner
                   et al., 2016b) and language in ByteNet (Kalchbrenner et al., 2016a).
                   Thecombinationoflatentvariableswithexpressivedecoderwaspreviouslyexploredusingrecurrent
                   networks mainly in the context of language modeling (Chung et al., 2015; Bowman et al., 2015;
                   Serbanetal., 2016; Fraccaro et al., 2016; Xu & Sun, 2016). Bowman et al. (2015) has also proposed
                   to weaken an otherwise too expressive decoder by dropout to force some information into latent
                   codes.
                   Concurrent with our work, PixelVAE (Gulrajani et al., 2016) also explored using conditional Pixel-
                   CNNasaVAE‚Äôsdecoder and has obtained impressive density modeling results through the use of
                   multiple levels of stochastic units.
                   Usingautoregressive model on latent code was explored in the context of discrete latent variables in
                   DARN(Gregoretal.,2013). Kingmaetal.(2016),KaaeS√∏nderbyetal.(2016),Gregoretal.(2016)
                   and Salimans (2016) explored VAE architecture with an explicitly deep autoregressive prior for
                   continuouslatentvariables, but the autoregressive data likelihood is intractable in those architectures
                   and needs to inferred variationally. In contrast, we use multiple steps of autoregressive Ô¨Çows that
                   has exact likelihood and analyze the effect of using expressive latent code.
                   Optimization challenges for using (all levels of) continuous latent code were discussed before and
                   practical solutions were proposed (Bowman et al., 2015; Kaae S√∏nderby et al., 2016; Kingma et al.,
                   2016). In this paper, we present a complementary perspective on when/how should the latent code
                   be used by appealing to a Bits-Back interpretation of VAE.
                   Learning a lossy compressor with latent variable model has been investigated with Con-
                   vDRAW (Gregor et al., 2016). It learns a hierarchy of latent variables and just using high-level
                   latent variables will result in a lossy compression that performs similarly to JPEG. Our model simi-
                   larly learns a lossy compressor but it uses an autoregressive model to explicitly control what kind of
                   information should be lost in compression.
                   6  CONCLUSION
                   In this paper, we analyze the condition under which the latent code in VAE should be used, i.e. when
                   does VAEautoencode, and use this observation to design a VAE model that‚Äôs a lossy compressor of
                   observed data. At modeling level, we propose two complementary improvements to VAE that are
                   showntohavegoodempirical performance.
                   VLAE has the appealing properties of controllable representation learning and improved density
                   estimation performance but these properties come at a cost: compared with VAE models that have
                   simple prior and decoder, VLAE is slower at generation due to the sequential nature of autoregres-
                   sive model.
                   Moving forward, we believe it‚Äôs exciting to extend this principle of learning lossy codes to other
                   forms of data, in particular those that have a temporal aspect like audio and video. Another promis-
                   ing direction is to design representations that contain only information for downstream tasks and
                   utilize those representations to improve semi-supervised learning.
                   REFERENCES
                   Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
                    perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798‚Äì1828,
                    2013.
                   ¬®
                   Jorg Bornschein and Yoshua Bengio. Reweighted wake-sleep. arXiv preprint arXiv:1406.2751,
                    2014.
                   Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Ben-
                    gio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
                                                   11

=== Page 12 ===
Published as a conference paper at ICLR 2017
                           Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv
                              preprint arXiv:1509.00519, 2015a.
                           Yuri Burda, Roger B Grosse, and Ruslan Salakhutdinov. Accurate and conservative estimates of mrf
                              log-likelihood using reverse annealing. In AISTATS, 2015b.
                           KyungHyun Cho, Tapani Raiko, and Alexander T Ihler. Enhanced gradient and adaptive learning
                              rate for training restricted boltzmann machines. In Proceedings of the 28th International Confer-
                              ence on Machine Learning (ICML-11), pp. 105‚Äì112, 2011.
                           Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
                              gio. A recurrent latent variable model for sequential data. In Advances in neural information
                              processing systems, pp. 2980‚Äì2988, 2015.
                           Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: non-linear independent components esti-
                              mation. arXiv preprint arXiv:1410.8516, 2014.
                           Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv
                              preprint arXiv:1605.08803, 2016.
                           ChaoDu,JunZhu,andBoZhang. Learningdeepgenerative models with doubly stochastic mcmc.
                              arXiv preprint arXiv:1506.04557, 2015.
                           Otto Fabius and Joost R van Amersfoort.      Variational recurrent auto-encoders.  arXiv preprint
                              arXiv:1412.6581, 2014.
                           Marco Fraccaro, S√∏ren Kaae S√∏nderby, Ulrich Paquet, and Ole Winther. Sequential neural models
                              with stochastic layers. arXiv preprint arXiv:1605.07571, 2016.
                           Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
                              for distribution estimation. arXiv preprint arXiv:1502.03509, 2015.
                           Marc Goessling and Yali Amit. Sparse autoregressive networks. arXiv preprint arXiv:1511.04776,
                              2015.
                           Karol Gregor, Andriy Mnih, and Daan Wierstra. Deep AutoRegressive Networks. arXiv preprint
                              arXiv:1310.8499, 2013.
                           Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural network
                              for image generation. arXiv preprint arXiv:1502.04623, 2015.
                           KarolGregor,FredericBesse,DaniloJimenezRezende,IvoDanihelka,andDaanWierstra. Towards
                              conceptual compression. arXiv preprint arXiv:1604.08772, 2016.
                           IshaanGulrajani, KundanKumar,FarukAhmed,AdrienAliTaiga,FrancescoVisin,DavidVazquez,
                              and Aaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint
                              arXiv:1611.05013, 2016.
                           Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
                              networks. arXiv preprint arXiv:1603.05027, 2016.
                           Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the
                              descriptionlengthoftheweights. InProceedingsofthesixthannualconferenceonComputational
                              learning theory, pp. 5‚Äì13. ACM, 1993.
                           Geoffrey E Hinton and Richard S Zemel.         Autoencoders, minimum description length, and
                              Helmholtz free energy. Advances in neural information processing systems, pp. 3‚Äì3, 1994.
                           Antti Honkela and Harri Valpola.     Variational learning and bits-back coding: an information-
                              theoretic view to bayesian learning. IEEE Transactions on Neural Networks, 15(4):800‚Äì810,
                              2004.
                           Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected
                              convolutional networks. arXiv preprint arXiv:1608.06993, 2016.
                                                                           12

=== Page 13 ===
Published as a conference paper at ICLR 2017
                           Casper Kaae S√∏nderby, Tapani Raiko, Lars Maal√∏e, S√∏ren Kaae S√∏nderby, and Ole Winther.
                             How to train deep variational autoencoders and probabilistic ladder networks. arXiv preprint
                             arXiv:1602.02282, 2016.
                           NalKalchbrenner,LasseEspheholt,KarenSimonyan,AaronvandenOord,AlexGraves,andKoray
                             Kavukcuoglu. eural machine translation in linear time. arXiv preprint arXiv:1610.00527, 2016a.
                           Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex
                             Graves,andKorayKavukcuoglu. Videopixelnetworks. arXivpreprintarXiv:1610.00527,2016b.
                           Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. Proceedings of the 2nd
                             International Conference on Learning Representations, 2013.
                           Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse
                             autoregressive Ô¨Çow. arXiv preprint arXiv:1606.04934, 2016.
                           Brenden M Lake, Ruslan R Salakhutdinov, and Josh Tenenbaum. One-shot learning by inverting a
                             compositional causal process. In Advances in neural information processing systems, pp. 2526‚Äì
                             2534, 2013.
                                          ¬¥
                           YannLeCun,LeonBottou,YoshuaBengio,andPatrickHaffner. Gradient-basedlearningapplied to
                             document recognition. Proceedings of the IEEE, 86(11):2278‚Äì2324, 1998.
                           Benjamin M Marlin, Kevin Swersky, Bo Chen, and Nando de Freitas. Inductive principles for
                             restricted boltzmann machine learning. In AISTATS, pp. 509‚Äì516, 2010.
                           Andriy Mnih and Karol Gregor. Neural variational inference and learning in belief networks. arXiv
                             preprint arXiv:1402.0030, 2014.
                           Vinod Nair, Josh Susskind, and Geoffrey E Hinton. Analysis-by-synthesis by learning to invert
                             generative black boxes. In International Conference on ArtiÔ¨Åcial Neural Networks, pp. 971‚Äì981.
                             Springer, 2008.
                           Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,
                             Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for
                             raw audio. arXiv preprint arXiv:1609.03499, 2016.
                           DaniloRezendeandShakirMohamed.VariationalinferencewithnormalizingÔ¨Çows. InProceedings
                             of The 32nd International Conference on Machine Learning, pp. 1530‚Äì1538, 2015.
                           Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-
                             imate inference in deep generative models. In Proceedings of the 31st International Conference
                             onMachineLearning(ICML-14),pp.1278‚Äì1286,2014.
                           Jason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200, 2016.
                           TimSalimans. Astructuredvariationalauto-encoderforlearningdeephierarchiesofsparsefeatures.
                             arXiv preprint arXiv:1602.08734, 2016.
                           Tim Salimans, Diederip P. Kingma, and Max Welling. Markov chain Monte Carlo and variational
                             inference: Bridging the gap. arXiv preprint arXiv:1410.6460, 2014.
                           Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
                             pixelcnn with discretized logistic mixture likelihood and other modiÔ¨Åcations.  arXiv preprint
                             arXiv:1701.05517, 2017.
                           Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron
                             Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-
                             erating dialogues. arXiv preprint arXiv:1605.06069, 2016.
                           Jascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
                             vised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, 2015.
                           Dustin Tran, Rajesh Ranganath, and David M Blei. Variational gaussian process. arXiv preprint
                             arXiv:1511.06499, 2015.
                                                                         13

=== Page 14 ===
Published as a conference paper at ICLR 2017
                          Richard E Turner, Pietro Berkes, and Maneesh Sahani. Two problems with variational expectation
                             maximisation for time-series models. In Proceedings of the Workshop on Inference and Estima-
                             tion in Probabilistic Time-Series Models, pp. 107‚Äì115, 2008.
                          Aaron van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep
                             gaussian mixture models. In Advances in Neural Information Processing Systems, pp. 3518‚Äì
                             3526, 2014.
                          Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
                             arXiv preprint arXiv:1601.06759, 2016a.
                          Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-
                             ray Kavukcuoglu.    Conditional image generation with pixelcnn decoders.      arXiv preprint
                             arXiv:1606.05328, 2016b.
                          Weidi Xu and Haoze Sun. Semi-supervised variational autoencoders for sequence classiÔ¨Åcation.
                             arXiv preprint arXiv:1603.02514, 2016.
                          Alan Yuille and Daniel Kersten. Vision as bayesian inference: analysis by synthesis?   Trends in
                             cognitive sciences, 10(7):301‚Äì308, 2006.
                          Biao Zhang, Deyi Xiong, and Jinsong Su. Variational neural machine translation. arXiv preprint
                             arXiv:1605.07869, 2016.
                                                                         14

=== Page 15 ===
Published as a conference paper at ICLR 2017
           APPENDIX
           A DETAILEDEXPERIMENT SETUP FOR BINARY IMAGES
           For VAE‚Äôs encoder and decoder, we use the same ResNet (He et al., 2015) VAE architecture as the
           one used in IAF MNIST experiment (Kingma et al., 2016). The only difference is that the decoder
           networknow,insteadofoutputinga28x28x1spatialfeaturemaptospecifythemeanofafactorized
           bernoulli distribution, outputs a 28x28x4 spatial feature map that‚Äôs concatenated with the original
           binary image channel-wise, forming a 28x28x5 feature map that‚Äôs then fed through a typical masked
           PixelCNN(vandenOordetal.,2016a). AssucheventhoughthePixelCNNconditionsonthelatent
           code, we don‚Äôt call it a Conditional PixelCNN because it doesn‚Äôt use the speciÔ¨Åc architecture that
           wasproposedinvandenOordetal.(2016b). ForthePixelCNN,ithas6maskedconvolutionlayers
           with123x3Ô¨ÅltersorganizedinResNetblocks,andithas4additional1x1convolutionResNetblock
           betweeneveryothermaskedconvolutionlayertoincreaseprocessingcapacitysinceitemploysfewer
           maskedconvolutions than usual. All the masked convolution layer have their weights tied to reduce
           overÔ¨Åtting on statically binarized MNIST, and untying the weights will increase performance for
           other datasets. Experiments are tuned on the validation set and then Ô¨Ånal experiment was run with
           train and validation set, with performance evaluated with test set. Exponential Linear Units (Clevert
           et al., 2015) are used as activation functions in both VAE network and PixelCNN network. Weight
           normalization is everywhere with data-dependent initialization (Salimans & Kingma, 2016).
           Alatent code of dimension 64 was used. For AF prior, it‚Äôs implemented with MADE (Germain
           et al., 2015) as detailed in Kingma et al. (2016). We used 4 steps of autoregressive Ô¨Çow and each
           Ô¨Çowis implemented by a 3-layer MADE that has 640 hidden units and uses Relu (Nair & Hinton,
           2010)asactivationfunctions. DifferingfromthepracticeofKingmaetal.(2016),weusemean-only
           autoregressive Ô¨Çow, which we found to be more numerically stable.
           In terms of training, Adamax (Kingma & Ba, 2014) was used with a learning rate of 0.002. 0.01
           nats/data-dim free bits (Kingma et al., 2016) was found to be effective in dealing with the problem
           of all the latent code being ignored early in training. Polyak averaging (Polyak & Juditsky, 1992)
           wasusedtocomputetheÔ¨Ånalparameters, with Œ± = 0.998.
           All experiments are implemented using TensorFlow (Abadi et al., 2016).
           B ADDITIONALEXPERIMENT SETUP FOR CIFAR10
           Latent codes are represented by 16 feature maps of size 8x8, and this choice of spatial stochas-
           tic units are inspired by ResNet IAF VAE (Kingma et al., 2016). Prior distribution is factorized
           Gaussian noise transformed by 6 autoregressive Ô¨Çows, each of which is implemented by a Pixel-
           CNN(vandenOordetal.,2016a)with2hiddenlayers and 128 feature maps. Between every other
           autoregressive Ô¨Çow, the ordering of stochastic units is reversed.
           ResNetVLAEhasthefollowingstructureforencoder: 2ResNetblocks,Convw/stride=2,2ResNet
           blocks, Conv w/ stride=2, 3 ResNet blocks, 1x1 convolution and has a symmetric decoder. Channel
           size = 48 for 32x32 feature maps and 96 for other feature maps. DenseNet VLAE follows a similar
           structure: replacing 2 ResNet blocks with one DenseNet block of 3 steps and each step produces
           a certain number of feature maps such that at the end of a block, the concatenated feature maps is
           slightly more than the ResNet VLAE at the same stage.
           Conditional PixelCNN++ (Salimans et al., 2017) is used as the decoder. SpeciÔ¨Åcally the channel-
           autoregressive variant is used to ensure there is sufÔ¨Åcient capacity even when the receptive Ô¨Åeld is
           small. SpeciÔ¨Åcally, the decoder PixelCNN has 4 blocks of 64 feature maps where each block is
           conditioned on previous blocks with Gated ResNet connections and hence the PixelCNN decoders
           we use are shallow but very wide. For 4x2 receptive Ô¨Åeld experiment, we use 1 layer of vertical
           stack convolutions and 2 layers of horizontal stack convolutions; for 5x3 receptive Ô¨Åeld experiment,
           weuse2layersofvertical stack convolutions and 2 layers of horizontal stack convolutions; For 5x3
           receptive Ô¨Åeld experiment, we use 2 layers of vertical stack convolutions and 2 layers of horizontal
           stack convolutions; For 7x4 receptive Ô¨Åeld experiment, we use 3 layers of vertical stack convolutions
           and 3 layers of horizontal stack convolutions; for 7x4 Grayscale experiment, we transform RGB
                               15

=== Page 16 ===
Published as a conference paper at ICLR 2017
                        imagesintogray-scale images via this speciÔ¨Åc transformation: (0.299‚àóR)+(0.587G)+(0.114B).
                        Best density estimation result is obtained with 7x4 receptive Ô¨Åeld experiments.
                        C SOFTFREEBITS
                        ‚ÄùFree bits‚Äù was a technique proposed in (Kingma et al., 2016) where K groups of stochastic units
                        are encouraged to be used through the following surrogate objective:
                                                              K
                             e                             X
                             L =E        E      [logp(x|z)] ‚àí    maximum(Œª,Ex‚àºM[DKL(q(zj|x)||p(zj))])
                              Œª     x‚àºM q(z|x)
                                                             j=1
                        This technique is easy to use since it‚Äôs usually easy to determine the minimum number of bits/nats,
                        Œª, stochastic units need to encode. Choosing Œª is hence easier than setting a Ô¨Åxed KL annealing
                        schedule (Serban et al., 2016).
                        Ontheotherhand,KlannealinghasthebeneÔ¨Åtofthesurrogateobjective will smoothly become the
                        true objective, the variational lower bound where as ‚Äùfree bits‚Äù has a sharp transition at the boundary.
                        Therefore, we propose to still use Œª as hyperparameter to specify at least Œª nats should be used but
                        try to change the optimization objective as slowly as possible:
                                      L          (x;Œ∏) =E      [logp(x|z)] ‚àí Œ≥D  (q(z|x)||p(z))
                                        SoftFreeBits      q(z|x)              KL
                        where 0 < Œ≥ ‚â§ 1.
                        And we make the optimization smoother by changing Œ≥ slowly online to make sure at least Œª nats
                        are used: when Kl is too much higher than Œª (we experimented wide range of thresholds from 3%
                        to 30%, all of which yield improved results, and we tend to use 5% us a threshold), Œ≥ is increased,
                        and when Kl lower than Œª, Œ≥ is decreased to encourage information Ô¨Çow.
                        WefounditsufÔ¨Åcient to increase/decrease at 10% increment and didn‚Äôt further tune this parameter.
                        D AUTOREGRESSIVE DECODER WITHOUT AUTOREGRESSIVE PRIOR
                        In this section, we investigate the scenario of just using an autoregressive decoder without using
                        an autoregressive prior. We compare the exact same model in three conÔ¨Ågurations: 1) using small-
                        receptive-Ô¨Åeld PixelCNN as an unconditional density estimator; 2) using small-receptive-Ô¨Åeld as
                        a decoder in a VAE with Gaussian latent variables; 3) replacing Gaussian latent variables with
                        autoregressive Ô¨Çow latent variables in 2).
                                            Table 1: Ablation on Dynamically binarized MNIST
                                         Model                             NLLTest     KL
                                         Unconditional PixelCNN              87.55      0
                                         PixelCNNDecoder+GaussianPrior       79.48    10.60
                                         PixelCNNDecoder+AFPrior             78.94    11.73
                        In Table 1, we can observe that each step of modiÔ¨Åcation improves density estimation performance.
                        In addition, using an autoregressive latent code makes the latent code transmit more information as
                        showninthedifference of E[DKL(q(z|x)||p(z))].
                        E CIFAR10GENERATEDSAMPLES
                        REFERENCES
                        Martƒ±n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S
                          Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. TensorÔ¨Çow: Large-scale machine
                          learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.
                                                                 16

=== Page 17 ===
Published as a conference paper at ICLR 2017
                                              (a) 4x2 @ 3.12 bits/dim                           (b) 7x4 @ 2.95 bits/dim
                                                  Figure 4: CIFAR10: Generated samples for different models
                                         ¬¥
                             Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
                                learning by Exponential Linear Units (ELUs). arXiv preprint arXiv:1511.07289, 2015.
                             Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder
                                for distribution estimation. arXiv preprint arXiv:1502.03509, 2015.
                             KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
                                nition. arXiv preprint arXiv:1512.03385, 2015.
                             Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
                                arXiv:1412.6980, 2014.
                             Diederik P Kingma, Tim Salimans, and Max Welling. Improving variational inference with inverse
                                autoregressive Ô¨Çow. arXiv preprint arXiv:1606.04934, 2016.
                             VinodNairandGeoffreyEHinton. RectiÔ¨Åedlinearunitsimproverestrictedboltzmannmachines. In
                                Proceedingsofthe27thInternationalConferenceonMachineLearning(ICML-10),pp.807‚Äì814,
                                2010.
                             Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging.
                                SIAMJournalonControlandOptimization, 30(4):838‚Äì855, 1992.
                             Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to ac-
                                celerate training of deep neural networks. arXiv preprint arXiv:1602.07868, 2016.
                             Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
                                pixelcnn with discretized logistic mixture likelihood and other modiÔ¨Åcations.           arXiv preprint
                                arXiv:1701.05517, 2017.
                             Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron
                                Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for gen-
                                erating dialogues. arXiv preprint arXiv:1605.06069, 2016.
                             Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
                                arXiv preprint arXiv:1601.06759, 2016a.
                             Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Ko-
                                ray Kavukcuoglu.       Conditional image generation with pixelcnn decoders.             arXiv preprint
                                arXiv:1606.05328, 2016b.
                                                                                 17
