        Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
        [31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and
          Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh
          Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems,
          2022.
        [32] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-
          lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
          TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,Jeffrey Wu, Clemens Winter, Chris Hesse, Mark
          Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
          Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,
          M.Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
          volume 33, pages 1877–1901. Curran Associates, Inc., 2020.
        [33] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
          Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Represen-
          tations, 2022.
        [34] Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the question? In Bonnie
          Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical
          Methods in Natural Language Processing (EMNLP), pages 2173–2185, Online, November 2020. Association for
          Computational Linguistics.
        [35] Matthew Renze. The effect of sampling temperature on problem solving in large language models. In Yaser
          Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguis-
          tics: EMNLP 2024, pages 7346–7356, Miami, Florida, USA, November 2024. Association for Computational
          Linguistics.
        [36] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James
          Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor
          Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy,
          Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and
          Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, 2022.
        [37] Francois Chollet, Mike Knoop, Bryan Landers, Greg Kamradt, Hansueli Jud, Walter Reade, and Addison Howard.
          Arc prize 2024. https://kaggle.com/competitions/arc-prize-2024, 2024. Kaggle.
        [38] Mike Knoop. Introducing the arc-agi public leaderboard, Jun 2024.
        [39] Francois Chollet, Mike Knoop, Bryan Landers, Greg Kamradt, Hansueli Jud, Walter Reade, and Addison Howard.
          Arc prize website 2025. https://arcprize.org, 2025. ARC-AGI.
        [40] Arcathon winners 2023. https://lab42.global/winners/, 2023. Accessed: 2025-03-05.
        [41] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
          Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv, abs/2001.08361, 2020.
        [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and
          Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the
          36th International Conference on Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA, 2024.
          Curran Associates Inc.
        [43] Katherine L Hermann, Hossein Mobahi, Thomas Fel, and Michael C Mozer. On the foundations of shortcut
          learning. arXiv preprint arXiv:2310.16228, 2023.
        [44] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of
          simplicity bias in neural networks. Advances in Neural Information Processing Systems, 33:9573–9585, 2020.
        [45] Thomas Fel, Louis Bethune, Andrew Kyle Lampinen, Thomas Serre, and Katherine Hermann. Understanding
          visual feature reliance through the lens of complexity, 2024.
        [46] Stephanie C.Y. Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K Singh, Pierre Harvey
          Richemond, James McClelland, and Felix Hill. Data distributional properties drive emergent in-context learning
          in transformers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in
          Neural Information Processing Systems, 2022.
        [47] B.M. Lake and M. Baroni. Human-like systematic generalization through a meta-learning neural network. Nature,
          623:115–121, 2023.
        [48] Ekin Akyürek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. The surprising effectiveness
          of test-time training for abstract reasoning, 2024.
                               15
