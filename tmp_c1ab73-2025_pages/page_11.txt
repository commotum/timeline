                  Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                  properties [46], enabling impressive performance on various tasks. ICL appears to support a more data-efficient form of
                  meta-learning, one capable of storing and leveraging priors in a flexible way—especially appealing for applications like
                  ARC.
                  Anotable illustration of forward-pass ICL exhibiting strong generalization is the MLC architecture [47]. MLC mines
                  for examples similar to the new task on their dataset, and, similar to our methodology, combines all relevant examples
                  into the model’s forward pass at once, allowing it to function as a meta-learner within the forward pass. This design
                  substantially improves performance and generalization, highlighting the potential of ICL-based approaches for tasks
                  that require complex reasoning.
                  5.1  Concurrent WorkBuildingonTTFT
                  Aninteresting replication that is based on our proposed Test-Time Fine Tuning (TTFT) approach can be found in the
                  work of [48], who explore the idea of adapting models on-the-fly for ARC tasks. Their method explicitly incorporates a
                  similar short fine-tuning phase during inference on ARC, closely mirroring our TTFT paradigm. This aligns with our
                  findings that dynamic adjustments at evaluation can significantly enhance performance, especially when encountering
                  tasks requiring newly discovered transformations or abstractions. Also based on [49, 50] their methodology and results
                  strongly corroborate and align with ours.
                  Building further upon our proposed TTFT approach, [51] recently explored the interplay between inductive and
                  transductive reasoning specifically within the ARC domain. Their study trains neural networks on synthetic datasets
                  generated from Python-based implementations of ARC transformations, based on [49], they also use TTFT for their
                  transductive domain. Their experiments highlight that inductive program synthesis excels in precise symbolic tasks,
                  whereas transduction demonstrates strength in more perceptually oriented problems. By effectively ensembling these
                  complementary models, their approach achieves strong results, strongly validating the effectiveness and flexibility of
                  TTFT-based adaptation to achieve high performance.
                  5.2  CodedatainLLMtraining
                  Emphasizing coding and code training in LLMs is not new. Coding datasets form a significant part of LLM pre-training
                  corpora, even in non-coding based models [52, 53]. It is correlated with improved performance on reasoning tasks, [54]
                  find that code based models consistently outperform text based models in reasoning, even on synthetic reasoning tasks
                  formulated as natural text. Further, [24] show that pre-training LLMs with a mix of text and code increases the general
                  reasoning capability of the model. They also show that code at the instruction tuning stage enhances task specific
                  reasoning. [55] also show that code models, even when outputting text, outperform text models on few shot structured
                  reasoning evaluations. More recently, [56] carefully ablate the effects of code-data in pre-training and find positive
                  effects on compositional tasks like semantic parsing and mathematics.
                  5.3  ARCdataset’srelated work
                  The Abstraction and Reasoning Corpus (ARC) dataset [1] presents a significant challenge for artificial intelligence
                  systems due to its emphasis on reasoning from minimal examples. Numerous approaches have been proposed to tackle
                  ARCtasks, ranging from leveraging LLMs to developing specialized neural architectures and neuro-symbolic methods
                  to brute force search based methods.
                  5.3.1  Evaluating LLMsonARCTasks
                  Several studies have explored the capabilities of LLMs on ARC tasks without additional training. Mirchandani et al.[57]
                  investigated whether LLMs can act as general pattern machines by providing the entire ARC task as context to GPT-4
                  and GPT-3.5. They achieved an accuracy of 10.6% on the combined ARC dataset and 6.75% on the public test set,
                  indicating limited performance. Similarly, Mitchell et al.[58] compared GPT-4 and GPT-4V to human performance on a
                  simplified version of ARC called ConceptARC [59], finding that GPT-4V did not significantly improve performance
                  over GPT-4 and that both models underperformed compared to humans.
                  Other works have attempted to improve LLM performance by altering input representations. [60] translated visual
                  ARCtasksintotextual descriptions to leverage LLMs’ reasoning capabilities, achieving 20% accuracy with GPT-3 on
                  the ARC training set. [61] emphasize the importance of object-based representations, introducing an object-centric
                  encoding to feed into LLMs. They tested GPT-4 on the easiest 50 ARC tasks and solved 23 out of 50.
                                                                         11
