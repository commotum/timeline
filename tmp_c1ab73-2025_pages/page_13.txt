             Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                 • Lack of Contextual Reasoning: Approaches that process grid-pairs in isolation, such as CodeIt [63] and [14]
                   and GPAR[64]struggle with tasks that require understanding relationships across multiple examples.
                 • Lack of focus on perceptual reasoning: Methods that do not focus on perceptual reasoning seem to face
                   difficulties due to the complexity of searching through an almost infinite space of possible transformations.
                 • Evaluation on Public Data: Some studies evaluate their models on the public ARC dataset, which may have
                   been exposed in pre-training data, potentially inflating performance estimates.
             6  Conclusion
             In contrast to previous work, our approach leverages LLMs with a focus on tackling arc with broader perceptual
             reasoning as the core component. We achieve this by both training a performant perceptual reasoner using existing
             state-of-the-art contextualizing models (LLMs). And secondly, by fine-tuning this perceptual reasoner on ARC tasks
             for each task at test time. Unlike previous methods we prove out our approach by evaluating on the full private test
             set, ensuring that our methodology generalizes well to unseen tasks. We achieve state-of-the-art performance on the
             ARC-AGIprivatetest set while being much more computationally efficient than existing methods.
             Acknowledgments
             This research was conducted with the help of the Google TPU Research Cloud (TRC) program. We would like to thank
             the TRC program for providing the TPU resources and for their continued support.
             References
             [1] François Chollet. On the measure of intelligence. ArXiv, abs/1911.01547, 2019.
             [2] Lucas Beyer, Olivier J. H’enaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. Are we done
                with imagenet? ArXiv, abs/2006.07159, 2020.
             [3] Arsenii Kirillovich Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The conceptARC benchmark:
                Evaluating understanding and generalization in the ARC domain. Transactions on Machine Learning Research,
                2023.
             [4] SuvirMirchandani,FeiXia,PeteFlorence,brianichter,DannyDriess,MontserratGonzalezArenas,KanishkaRao,
                Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. In 7th Annual Conference on
                Robot Learning, 2023.
             [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
                transformers for language understanding. In North American Chapter of the Association for Computational
                Linguistics, 2019.
             [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
                Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An
                image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on
                Learning Representations, 2021.
             [7] François Chollet, Katherine Tong, Walter Reade, and Julia Elliott. Abstraction and reasoning challenge. https:
                //kaggle.com/competitions/abstraction-and-reasoning-challenge,2020. Kaggle.
             [8] Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse? towards
                understanding the effectiveness of maml. In International Conference on Learning Representations, 2020.
             [9] Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning without
                memorization. In International Conference on Learning Representations, 2020.
             [10] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching networks
                for one shot learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural
                Information Processing Systems, volume 29. Curran Associates, Inc., 2016.
             [11] Shell Xu Hu, Da Li, Jan Stühmer, Minyoung Kim, and Timothy M. Hospedales. Pushing the limits of simple
                pipelines for few-shot learning: External data and fine-tuning make a difference. In 2022 IEEE/CVF Conference
                onComputerVision and Pattern Recognition (CVPR), pages 9058–9067, 2022.
                                                   13
