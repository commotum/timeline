        Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
        [49] Jack Cole and Mohamed Osman. Dataset-induced meta-learning (and other tricks): Improving model efficiency
          onarc. https://lab42.global/community-model-efficiency/,2023. Accessed: March 3, 2025.
        [50] Jack Cole, Mohamed Osman, Michael Hodel, Keith Duggar, and Tim Scarfe. Machine learning street talk.
          https://www.youtube.com/watch?v=jSAT_RuJ_Cg,2024. Accessed: June 2024.
        [51] Wen-Ding Li, Keya Hu, Carter Larsen, Yuqing Wu, Simon Alford, Caleb Woo, Spencer M. Dunn, Hao Tang,
          Michelangelo Naim, Dat Nguyen, Wei-Long Zheng, Zenna Tavares, Yewen Pu, and Kevin Ellis. Combining
          induction and transduction for abstract reasoning, 2024.
        [52] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
          Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha
          Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,
          Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,
          Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier
          Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
          Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
          Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Olek-
          sandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele
          Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling
          language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023.
        [53] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
          Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave,
          and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.
        [54] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,
          Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang,
          Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana Acosta-Navas, Drew Arad Hudson,
          Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav
          Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S.
          Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar,
          Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen
          Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. Transactions on Machine
          Learning Research, 2023. Featured Certification, Expert Certification.
        [55] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are
          few-shot commonsense learners. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings
          of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384–1403, Abu Dhabi,
          United Arab Emirates, December 2022. Association for Computational Linguistics.
        [56] Anonymous. Howdoescodepretraining affect language model task performance? Submitted to Transactions on
          Machine Learning Research, 2024. Under review.
        [57] SuvirMirchandani,FeiXia,PeteFlorence,brianichter,DannyDriess,MontserratGonzalezArenas,KanishkaRao,
          Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. In 7th Annual Conference on
          Robot Learning, 2023.
        [58] Melanie Mitchell, Alessandro B. Palmarini, and Arsenii Kirillovich Moskvichev. Comparing humans, GPT-4, and
          GPT-4vonabstraction and reasoning tasks. In AAAI 2024 Workshop on ”Are Large Language Models Simply
          Causal Parrots?”, 2023.
        [59] Arsenii Kirillovich Moskvichev, Victor Vikram Odouard, and Melanie Mitchell. The conceptARC benchmark:
          Evaluating understanding and generalization in the ARC domain. Transactions on Machine Learning Research,
          2023.
        [60] G. Camposampiero, L. Houmard, B. Estermann, J. Mathys, and R. Wattenhofer. Abstract visual reasoning enabled
          by language. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),
          pages 2643–2647, Los Alamitos, CA, USA, jun 2023. IEEE Computer Society.
        [61] Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, and Elias Boutros Khalil. LLMs and the abstraction
          and reasoning corpus: Successes, failures, and the importance of object-based representations. Transactions on
          Machine Learning Research, 2024.
        [62] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. Hypothesis search:
          Inductive reasoning with language models. In The Twelfth International Conference on Learning Representations,
          2024.
                               16
