                   Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                   demandforgeneratingdiverseandgenuinelynewabstractionsattest-time, we opted for this straightforward, guaranteed
                   methodofupdate, even though other adaptation techniques may also offer promising results and sufficient updating
                   power.
                   3.2.2  Attention and masking
                  Wespecifically choose encoder-decoder architectures because they incorporate non-causal (unmasked) attention within
                   the encoder, allowing each token to simultaneously attend to the entire input sequence. This capability is critical for
                   enabling the model to fully interpret and contextualize ARC riddles from the outset.
                   Bycontrast, if the riddle were presented using causal (masked) attention, tokens appearing earlier in the sequence
                   wouldnothaveaccess to the complete context, preventing them from forming accurate early-stage representations or
                   hypotheses simply due to lack of available information. Tokens representing input grids would be unable to attend
                   forward to their corresponding output grids, significantly limiting the model’s reasoning about intended transformations.
                   Tovalidate the practical importance of this non-causal attention mechanism, we experimentally compared our encoder-
                   decoder approach against similarly sized causal decoder-only models and found that the encoder-decoder structure
                   yielded substantially better performance. We were not able to run experiments to disambiguate whether this is due to
                   the non-causal attention masking or the encoder-decoder architecture itself, but its likely that the non-causal attention is
                   the main factor here.
                   3.2.3  Specialization to the riddle
                   Test-time fine-tuning can also enhance the precision required to produce completely accurate outputs. Even when
                   the model correctly identifies the transformation function, minor execution errors—such as inaccuracies of a pixel
                   or two—mayoccur. These errors are likely due to limited model depth or capacity, restricting the model’s ability to
                   execute transformations perfectly on the first attempt. TTFT can mitigate these issues by adapting the model specifically
                   to the current riddle, refining its “execution” capabilities, and enabling it to achieve precise, pixel-perfect outputs.
                   3.2.4  BeamSearchforSolutionSpacedecoding
                   Because our model generates output grids autoregressively, a purely greedy decoding strategy is brittle: even a single
                   incorrect token leads to an unrecoverable trajectory. Beam search [34] addresses this by maintaining multiple candidate
                   solutions simultaneously, pruning all but the most promising branches at each decoding step according to cumulative
                   probabilities.
                   This strategy allows the solver to effectively handle cases where the correct next token may initially have a lower
                   probability but becomes clearer in subsequent steps. Conversely, incorrect trajectories naturally lose confidence as they
                   proceed; a well-calibrated model will assign increasingly uniform probabilities across candidate tokens when uncertain,
                   causing these erroneous paths to be rapidly discarded. Although models can occasionally become confidently wrong,
                   beamsearch generally remains beneficial [34], and the ARC dataset in particular reaps substantial advantages from this
                   capability: each riddle has precisely one correct solution, amplifying the divergence between correct and incorrect paths
                   under beam search.
                   3.3  Augment,Inference, Reverse augmentation and Vote (AIRV)
                  Wepropose a test-time augmentation strategy called Augment, Inference, Reverse-Augmentation, and Vote (AIRV).
                   Theprocedure begins by applying a spatial transformation to the input riddle (e.g., rotation or flipping). We then get
                   predictions on the transformed riddle to obtain a predicted output grid, which is subsequently reversed back to the
                   original orientation. Finally, we gather multiple such predictions from different spatial augmentations and use a voting
                   schemetoselect the most frequent (or most confident) output grid.
                   Unlike beam search [34] or temperature sampling [35], AIRV can generate duplicate predictions (after reversing). This
                   enables a voting mechanism that amplifies strong, consistent solutions and filters out noisy variants. This is particularly
                   useful in the ARC setting, where each riddle has only one correct answer. Assuming a somewhat competent model, a
                   voting mechanism then provides a very effective way to make salient the more dominant and consistent grid predictions
                   (the more dominant solution ideas) from other more noisy predictions. In our opinion, this works because given a
                   reasonably performant model, there are many ways that solutions can be incorrect, while there is only exactly one
                   correct solution. This can be seen as analogous to the clustering step in the AlphaCode methodology [36] where the
                   generated programs are clustered and the most common program cluster is selected as a proxy for most likely correct
                   program.
                                                                             7
