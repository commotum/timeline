                  Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                  reliance on memorization. To mitigate this, we deliberately err on the side of overspecification in our generated riddles,
                  ensuring sufficient information for the model to unambiguously determine the intended solution.
                  Wegiveamoredetaileddescription of the synthetic riddle generators in Appendix A.
                  3.2  Solution Part 2: Optimizing in the Evaluation Loop (Test-Time Fine Tuning)
                  During evaluation, we leverage each test riddle’s demonstration examples to create synthetic training data. Specifically,
                  weselect a grid pair from the riddle’s provided examples and repurpose it as a new “test example” forming a new,
                  smaller riddle. We have the answer to this new riddle, as it was taken from the demonstration examples, and so we can
                  use that to train the model at test time. To get a lot more data and ensure this riddle differs from the original, we apply
                  several augmentations:
                         • Color permutation: Randomly shuffle the color labels throughout the riddle.
                         • Spatial transformations: Rotate, flip, or transpose the input and output grids, sampling from the dihedral
                           group D .
                                   4
                         • Shuffling: Randomly reorder the input demonstration examples.
                  Wethenperformabrief round of fine-tuning on these augmented riddles before generating predictions for the test grids.
                  This procedure can be seen as a form of test-time training [27] and is referred to here as Test-Time Fine Tuning (TTFT).
                  3.2.1  Motivation for TTFT Through Iterative Reframing
                  The central idea behind Test-
                  TimeFineTuning(TTFT)isthat
                  the solver may initially make mis-
                  takes on the private test set, and
                  we can exploit that feedback to
                  refine its approach. Much like
                  a human solver, the model can
                  re-evaluate and iterate on poten-
                  tial solutions. For instance, con-
                  sider the scenario illustrated in
                  Figure 3, where the correct trans-
                  formation is to select the color                   Figure 3: An example of a simpler ARC riddle.
                  of the line that does not intersect
                  others. A solver or human might instead begin by hypothesizing that the intended solution is to choose the color of the
                  thinnest line, perhaps because the first few examples happen to support that interpretation.
                  This kind of oversight often arises from limited attention or inadequate depth of processing: the model (or a human)
                  fails to fully observe all relevant information and thereby locks into a flawed “framing.” Once the solver is committed
                  to an incorrect framing (for example, consistently searching for the thinnest line), it will have to continue to discard
                  potentially crucial data and remain blind to the correct pattern. The input grids have already been processed with
                  the incorrect framing/bias. In such a case, only a complete reprocessing of the grids—where the correct framing is
                  established from the outset—can guide the solver to correctly infer the solution.
                  Proper framing is consistently shown to be critical for perceptual models, greatly influencing performance. One
                  illustration of this appears in [28], which observes that language models often ignore information in the middle of a
                  prompt, yet perform significantly better when the framing (in the form of the question or key instruction) appears at
                  both the beginning and end of the prompt. Another relevant example is instruction tuning in LLMs [29, 30], wherein
                  models are trained to adopt a “helpful” framing instead of a purely next-word prediction mode.
                  Hence, mechanisms for “reframing” are crucial in solving perceptually rich tasks, especially in the ARC setting, where
                  each riddle’s solution demands a tailored framing. TTFT provides a means to adapt these frames based on feedback
                  derived from newly generated training data—mirroring the human process of iteratively revisiting and adjusting
                  hypotheses until the training examples are correctly solved, before finally addressing the test grids.
                  Whytakefull parameter update steps (Full fine tuning)        Although several lighter-weight alternatives exist for
                  adapting models to downstream tasks—including chain-of-thought prompting [31], few-shot prompting [32], and
                  low-rank adaptation methods [33]—we chose full parameter updates primarily due to simplicity and reliability. Training
                  the model using full parameter updates is naturally powerful enough to generate the needed abstractions. Given ARC’s
                                                                          6
