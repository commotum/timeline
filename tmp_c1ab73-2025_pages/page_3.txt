                    Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                    2   Problemanddesiredproperties of a solver
                    2.1  Dataset description
                   TheAbstraction and Reasoning Corpus (ARC) dataset D consists of a collection of tasks (also called riddles in this
                    paper) T N , where each task T is defined as follows:
                            ii=1                     i
                                                                    n
                                                           (i)   (i) i                 (i)      (i)
                           • Aset of training examples (x     , y  )    , where each x    and y    are input and output grids, for each task Ti
                                                           j    j   j=1                j        j
                             respectively. The input and output grids together are referred to as a grid-pair or example. Both terms are used
                             interchangeably.
                           • Aset of test inputs x(i)mi , with corresponding outputs y(i)mi to be predicted.
                                                   k k=1                                  k k=1
                    Each grid is a 2D array x ∈ Ch×w of variable height h and width w, where C is a set of 10 colors. The number of
                    training examples ni and test examples mi are variable across tasks and usually range from 2-6.
                                                                                       (i)     (i)
                   Theobjective is to infer a task-specific function f such that f (x     ) = y    for all training examples, and then apply f
                                                                       i            i  j       j                                               i
                    to the test inputs to obtain the test outputs.
                    2.2  WhatdoesasolverneedtoexcelonARC?
                    Eachriddle is akin to a small dataset of input-to-output examples. A solver must use associative learning as a part of the
                    process of solving the riddle. Solvers must develop associations between individual input and output grids and across
                    the different input and output grid pairs, then apply the relevant learned associations to the given test input grid.
                    In contrast to this, both vision-based meta-learning and natural language based reasoning datasets have a memorization
                    problem. High performing methods on those datasets were found to learn generic features that allow very high levels of
                    accuracy without significant meta-learning. These methods were found to rely more on pretraining knowledge rather
                    than meta-learning to gain new skills on the new tasks provided by these datasets. [8, 9]
                   This is possible in those datasets because the tasks share a lot of common structure, for example the Mini-Imagenet
                    dataset [10], where the subtasks are all image-classification-based and good general object representations can be
                    learned and reused. This zero-shot feature reuse was shown to take place with the MAML algorithm on Mini-ImageNet
                    [8]. These zero-shot models can outperform meta-learning models on these tasks, holding state-of-the-art results [11, 9].
                    For a good ARC solver, the opposite of this is desirable. Encoding supposedly good features can lead to incorrect
                    assumptions and missing relevant details. Instead, it is desirable to encode a more sophisticated learning process that
                    can reason about the new examples and the possible transformations. In-context-learning (ICL) is an initial candidate
                    here [12, 13]. The ARC dataset is a challenging test of whether a system can perform this more true form of learning
                    from a few examples.
                   Acertain amount of flexibility and precision will be necessary within the inner workings of a solver, to satisfy the
                    above requirements and perform with high accuracy. The solver needs to identify how relevant points in the input
                    get transformed, including all the rules involved. The solver then needs to be dynamic enough to not only develop
                    representations, but also access those dynamically created representations to be able to correctly apply them in the new
                    context (test input pair).
                    2.3  Theassociative learning ability in the forward pass of a solver
                    Not all few-shot learning or meta-learning algorithms are well-suited for ARC. Some architectures, such as zero-shot
                    learners with shallow mixing, primarily rely on generating independent deep embeddings for individual grids. These
                    embeddings are then combined in a relatively simple manner to produce an output grid or a classification result. One
                    example of this is [14].
                    One example of such an ill-suited architecture is Proto-Net [15]. In a straightforward application of Proto-Net to
                   ARC, each input-output grid-pair is embedded separately into a vector, and these vectors are averaged to create a
                   “prototypical” representation. While this approach allows for a simple projection-based inference, it lacks the ability to
                    capture essential interactions across grid-pairs
                   Toeffectively reason about transformations across multiple examples, a model needs to process all grid-pairs in unison
                    rather than simply averaging individual representations. Without this capability, the model may overlook crucial shared
                                                                                 3
