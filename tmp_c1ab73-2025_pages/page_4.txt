                  Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                  patterns and transformations necessary for solving the task. The limitations of such shallow interaction architectures
                  becomeparticularly evident in tasks where multiple examples must be considered together to infer a general rule.
                  In the proto-net example model, the only cross grid-pair interaction is the averaging of the embeddings. This averaging
                  is not complex enough and even may destroy information inadvertently, due to the diversity of riddle objectives possible.
                  Consider a riddle that mirrors red objects horizontally and blue objects vertically. If the test input contains both red
                  and blue objects, a solver must see and recognize these transformations jointly. Shallow architectures like CodeIt [16]
                  struggle with such tasks because they lack mechanisms for simultaneous reasoning across all grid-pairs.
                  However, incorporating a structured form of associative learning in the forward pass enables a solver to process
                  grid-pairs holistically. This is further discussed in Section 3.1, where we explore how structured cross-grid interactions
                  can significantly enhance generalization and performance. Another example of this, applied step-by-step to an ARC
                  riddle, is worked through in more detail in 3.2
                  3   Solution
                  3.1  Solution Part 1: emphasizing In-Context-Learning (ICL)
                  3.1.1  Associative learning in LLMs’ forward pass
                  Recent work has uncovered growing evidence that large language models (LLMs) engage in a form of associative
                  learning [13, 17]. For instance, there is now substantial support for the idea that LLMs can identify and utilize
                  Probabilistic Context-Free Grammars (PCFGs), effectively operating as versatile pattern-recognition tools [4]. In
                  natural language, contextual nuances play a crucial role—each word’s meaning can shift dramatically based on its
                  placement within a sentence—so extensive in-context learning appears necessary for these models to generate tokens
                  that are both coherent and context-appropriate. Collectively, these observations indicate that LLMs can establish and
                  exploit relationships among input tokens, an essential ingredient for achieving strong performance on ARC tasks.
                  [18] find that models like BART and T5 can represent and track changing entity states over the course of a narrative
                  (e.g., whether an object is empty or who possesses it), even without explicit supervision. Their analysis also shows
                  that these representations tend to be localized in specific tokens and can be directly manipulated, causing the model
                  to update its subsequent text generation accordingly. Crucially, most of this dynamic-tracking ability comes from
                  extensive open-domain pre-training rather than fine-tuning, indicating that LLMs may possess the requisite capacity
                  motivated in subsection 2.3 for solving analogous perceptual reasoning tasks.
                  Modelchoice WebaseourapproachontheLongT5encoder-decodermodel[19],leveragingitsextendedcontext
                  length to accommodate larger riddles. The T5 family was selected for its sequence-to-sequence capabilities, having
                  been trained on a transformation task from non-causal to causal text [20]. This pre-training instills non-causal attention
                  mechanisms within the encoder, making it well-suited for associative learning.
                  Tofine-tune the model, we encode each riddle as a single text sequence, where grids are unrolled row-wise, with pixel
                  colors represented numerically and rows separated by spaces. By presenting the complete riddle as a unified input, the
                  model processes all grid-pairs simultaneously, allowing tokens to influence each other, shown in Figure 2. This aligns
                  with the recommendations in Section 2.3.
                     solve: train input1 2999 4299 4442 2922 output1 19 4 4 294 2999 4429 4492 2922.
                     input2 27757 27525 22277 57757 52257 output2 29 5 5 275 27757 27275 27257
                     55727 52257.
                     test tinput1 4884448 8844844 4844488 4848848 8888484 8448844 8848884
                     toutput1 55 7 7 48 4884448 8488884 4484448 4888448 8848484 8484844 8848884.
                  Figure 2: An example of the text prompt fed to our model. Here, each input and output grid is unrolled into a flat
                  sequence of pixel-color values, which are then concatenated with keywords such as train, test, input, and output.
                  Thephrase solve: indicates that the model should produce the correct transformed grid (toutput1) corresponding to
                  the given test grid (tinput1).
                  Direct output   This direct output methodology stands in contrast to some prior work that instead attempts to produce
                  code as an intermediate output that can then be run to produce the output grid from the input [16]. While this has some
                  benefits, such as being able to produce a solution that can be run and tested, it also has some drawbacks. Producing code
                  that can solve the riddle is typically a much harder task than simply producing the output grid directly [21]. A code
                                                                          4
