                   DON’T THROW THE BABY OUT WITH THE BATHWATER: HOW
                                      ANDWHYDEEPLEARNINGFORARC
                                                             1 2                 3
                                                    Jack Cole   , MohamedOsman
                                                                          1        2
                                                  MindwareConsulting, Inc. MindsAI
                                                                       3
                                                              Tufa Labs
                                           jackcole@mindware.mobi, mohamed@tufalabs.ai
                                                             ABSTRACT
                        TheAbstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems.
                        Despitethetypically low performance on ARC,thedeeplearningparadigmremainsthemosteffective
                        knownstrategyforgeneratingskillful (state-of-the-art) neural networks (NN) across varied modalities
                        and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these
                        skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles
                        downonthatandcontinues to leverage this paradigm by incorporating on-the-fly NN training at test
                        time.
                        Wedemonstrate that fully committing to deep learning’s capacity to acquire novel abstractions yields
                        state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer
                        (rather than just a pre-trained network) as integral components of the inference process, fostering
                        generalization to unseen tasks.
                        Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and
                        enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment
                        Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first
                        to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in
                        accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured
                        first place in the 2023 ARCathon competition, while the final version achieved the current best score
                        onthe ARCprivate test-set (58%).
                        Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains,
                        underscoring the central importance mechanisms that improve broad perceptual reasoning (deep
                        learning)—over focusing on logical inference methodologies—in achieving high accuracy on ARC
                        and similar domains.
       arXiv:2506.14276v2  [cs.AI]  31 Oct 20251Introduction
                Somecriticisms of the current deep learning (DL) paradigm rightly note that current best models and methods are
                overfit to the popular datasets [1]. The limitations of testing on large datasets have become apparent. For example,
                performance on ImageNet has exceeded saturation, where models are progressing by learning patterns in the biases that
                labelers of ImageNet tended to make [2]. This is an indication of a gap in the current testing paradigm. Trained models
                are tested on data that is similar to the data they were trained on (in-distribution data). Models are evaluated on existing
                skills and we neglect benchmarking the efficiency of the learning process itself [1].
                Alternatively, consider the example of this single riddle from the Abstraction and Reasoning Corpus (ARC) [1], shown
                in Figure 1. Any system attempting to solve the riddle must work from the three provided examples and should infer
                that it is necessary to transform each colored square into its corresponding pattern. Due to the simplicity of this dataset’s
                setup, little pre-training knowledge can be leveraged to solve these riddles. Instead, the solver must “figure out” the
                   1The MindsAI team in the ARC-AGI 2024 competition also consisted of Michael Hodel who has had a significant impact on the
                team’s success in 2024.
