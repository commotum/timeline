                 Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                 Experiments on model size   TheLargemodelgetsalowerscoreonTTFTbutahigherscoreonZeroShotandAIRV
                 only, even with the lighter training. We see this trend holding across small, base and large models, where zero shot and
                 AIRVonlyperformancetrendswithsizeofthemodel. This scaling behavior is typical in deep learning [41, 42], but
                 in this testing regime (ARC), can be explained by the fact that the larger the model the bigger and more expressive
                 forward pass will be (more, and wider layers means more associations that can be made in the forward pass). This
                 possibly accounts for the increased performance in the AIRV and zero shot setting, aligning well with the motivation in
                 subsection 3.1.
                 Benefits of increased pre-training on the ARC dataset did not beat the benefits of simply increasing the model size. This
                 perhaps indicates that the forward pass flexibility of the models is impacted much more by the model size compared
                 to pre-training. This aligns with general theory regarding model scaling laws and how they impact typical reasoning
                 benchmarks [42, 12].
                 Theeffect of pre-training on TTFT    Interestingly, while increased pre-training does not seem to beat the effect of
                 simply increasing the model size, when it comes to zero shot and AIRV only performance of these models, it does
                 significantly improve score in the TTFT + AIRV setting compared to the base model. This effect is not likely only due
                 to that larger models take more time to train (and TTFT) as the base model still sees a much larger boost in performance
                 (300%)fromTTFTthanbothlargeandsmallmodels(140%and240%respectively). Wediscusswhythismaybein
                 section 4.2.
                 Contextualization at Test Time vs. Pre-Training on ARC Riddles   Wehavemotivated why high quality contextual-
                 ization is a crucial element for tackling the ARC dataset. Yet, our experimentation shows that substantial pre-training
                 on ARCriddles remains indispensable for achieving state-of-the-art performance with TTFT, culminating in our 2024
                 highest score on ARC-AGI. While the space of possible ARC transformations is vast, pre-training does not merely
                 “leak” memorized solutions. Instead, it imparts both the foundational “core knowledge priors” described by [1] and a
                 range of more subtle but highly important priors. By this, we refer to heuristics such as a preference for simpler, human
                 preferred transformations (the “simple/simpler transformation” bias), a drive to validate transformation hypotheses
                 across examples (the “looking for confirmation” bias), and even the basic notion that each example is formed by a
                 paired input and output grid. Without these biases deeply embedded in the model weights, the solver would be far less
                 efficient in forming or testing hypotheses during inference, and a lot of the forward pass would be spent at just merely
                 realizing these basic things about the problem setup.
                 This is further supported by the following recent findings. [43, 44] have demonstrated that predictive features emerge
                 at different layers during pre-training, with simpler but equally predictive features appearing earlier in the network.
                 Recently, [45] also show that longer pre-training allows for complex but predictive representations to “sediment” (move
                 into the earlier layers). We hypothesize that the extensive ARC pre-training allows for more “room” for test time
                 features to emerge (during test-time fine-tuning), because the base arc priors have sufficiently sedimented into the
                 very early layers. This sedimentation process, may be the crucial key for enabling the model to handle more complex,
                 task-specific reasoning when test-time fine-tuned on unseen riddles.
                 Contrasting pre-training with program synthesis   These priors are particularly relevant when considering a solver
                 that is based on program synthesis. Program-synthesis-based approaches explicitly encode pair association and
                 transformation-confirmation heuristics by searching for a program that correctly transforms input grids into their
                 corresponding output. While these methods avoids the need for extensive domain-specific pre-training, it still requires
                 significant human intervention to guide the program synthesis algorithm. Specifically, humans must direct the algorithm
                 to search for transformations that align input grids with output grids and ensure that the transformations are correct
                 using the other grid-pairs. Moreover, program synthesis solvers, with their manually encoded heuristics, are generally
                 less effective when faced with perceptual problems that involve an almost limitless range of possible transformations
                 and framings.
                 These are important considerations to keep in mind when considering the trade-offs between extensive pre-training
                 compute and why its necessary, and the explicit program synthesis approach.
                 5   Related work
                 Classically, meta-learning can be regarded as a strategy to automate model design or parameter selection across
                 numerous tasks, often formulated as a two-level optimization problem. In such setups, an “outer” model accumulates
                 meta-knowledge, while an “inner” model adapts rapidly to each new task. More recent advances in in-context learning
                 (ICL) have sidestepped explicit inner–outer distinctions, instead relying on the model’s forward pass to perform meta-
                 learning. This behavior is commonly observed in transformer architectures trained on data with specific distributional
                                                                      10
