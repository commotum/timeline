                Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                            Figure 5: Results for the fully trained base model in the 3 different test-time configurations.
                            (a) Zero-Shot                     (b) AIRV only                    (c) TTFT + AIRV
                Figure 6: Performance on the ARC’s private test set. Each subfigure analyzes the impact of the different configurations
                of the test time techniques introduced. The small and large models trained on a subset of the training data are compared
                to the base model trained on the full training data. The effect of increasing model size is contrasted with the effect of
                increasing the number of training examples across the different test time techniques. We see that, model size has a
                significant impact despite a significantly reduced training set, except for when TTFT implemented at test time.
                                   Model              ZeroShot(NoTTFT/AIRV)        AIRVOnly TTFT+AIRV
                                                                                                        1
                           Fully trained Base LongT5               5                   13             39
                         Partially trained small LongT5            0                    5             12
                         Partially trained large LongT5           10                   18             26
                AIRV: Augmentationandvotingalsoenablehigherperformance. Absolute AIRV gains show positive scaling with
                model size. AIRV alone can gain up to 260% (in the base model, fully trained scenario).
                TTFT: Test-time fine-tuning significantly increases the model’s score, results are consistent across model sizes
                and training regimes. Performing TTFT before running inference with AIRV leads to an additional 300% gain in
                performance.
                Anearly version of this approach achieved first place in the 2023 ARCathon [40]. A version of this approach with more
                optimizations and extended ARC training, achieved the highest score on the ARC-AGI private test set in 2024 (58%)
                [39].
                   1Aversion of this approach with more optimizations and extended ARC training, achieved the highest score on the ARC-AGI
                private test set in 2024 (58%) [39].
                                                                   9
