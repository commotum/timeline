                                                                                                                                                Neuron
                                                                                                                   Perspective
                 recently, George and Hawkins have suggested that the canon-            dancy (Attneave, 1954; Barlow, 1961; Dan et al., 1996), the Info-
                 ical microcircuit implements a form of Bayesian processing             max principle (Linsker, 1990; Atick, 2011; Kay and Phillips,
                 (GeorgeandHawkins,2009).Inthefollowingsection,wepursue                 2011),andperceptionashypothesistesting(Gregory,1968,1980).
                 similar ideas but ground them in the framework of predictive             The most popular scheme—for Bayesian ﬁltering in neuronal
                 coding and propose a cortical circuit that could implement             circuits—is predictive coding (Srinivasan et al., 1982; Buchs-
                 predictive coding through canonical interconnections. In partic-       baum and Gottschalk, 1983; Rao and Ballard, 1999). In this
                 ular, we ﬁnd that the proposed circuitry agrees remarkably well        context, surprise corresponds (roughly) to prediction error. In
                 with quantitative characterizations of the canonical microcircuit      predictive coding, top-down predictions are compared with
                 (Haeusler and Maass, 2007).                                            bottom-up sensory information to form a prediction error.
                                                                                        This prediction error is used to update higher-level representa-
                 ACanonicalMicrocircuit for Predictive Coding                           tions, upon which top-down predictions are based. These opti-
                 This section considers the computational role of cortical micro-       mized predictions then reduce prediction error at lower levels.
                 circuitry in more detail. We try to show that the computations           To predict sensations, the brain must be equipped with a
                 performed by canonical microcircuits can be speciﬁed more              generative model of how its sensations are caused (Helmholtz,
                 precisely than one might imagine and that these computations           1860). Indeed, this led Geoffrey Hinton and colleagues to
                 can be understood within the framework of predictive coding.           proposethat the brain is an inference (Helmholtz) machine (Hin-
                 In brief, we will show that (hierarchical Bayesian) inference about    ton and Zemel, 1994; Dayan et al., 1995). A generative model
                 the causes of sensory input can be cast as predictive coding.          describes how variables or causes in the environment conspire
                 This is important because it provides formal constraints on the        to producesensoryinput.Generativemodelsmapfrom(hidden)
                 dynamics one would expect to ﬁnd in neuronal circuits. Having          causes to (sensory) consequences. Perception then corre-
                 established these constraints, we then attempt to match them           spondstotheinversemappingfromsensationstotheir causes,
                 with the neurobiological constraints afforded by the canonical         while action can be thought of as the selective sampling of
                 microcircuit. The endpoint of this exercise is a canonical micro-      sensations. Crucially, the form of the generative model dictates
                 circuit for predictive coding.                                         the form of the inversion—for example, predictive coding. Fig-
                                                                                        ure 3 depicts a general model as a probabilistic graphical
                 Predictive Coding and the Free Energy Principle                        model.Aspecialcaseofthesemodelsarehierarchicaldynamic
                 It might be thought impossible to specify the computations per-        models (see Figure 4), which grandfather most parametric
                 formedbythebrain.However,therearesomefairlyfundamental                 models in statistics and machine learning (see Friston, 2008).
                 constraints on the basic form of neuronal dynamics. The argu-          These models explain sensory data in terms of hidden causes
                 mentgoesasfollows—andcanberegardedasabriefsummary                      andstates. Hidden causes and states are both hidden variables
                 of the free energy principle (see Friston, 2010 for details).          that cause sensations but they play slightly different roles:
                     d Biological systems are homeostatic (or allostatic), which        hidden causes link different levels of the model and mediate
                       meansthat they minimize the dispersion (entropy) of their        conditional dependencies among hidden states at each level.
                       interoceptive and exteroceptive states.                          Conversely, hidden states model conditional dependencies
                     d Entropy is the average of surprise over time, which means        over time (i.e., memory) by modeling dynamics in the world. In
                       that biological systems minimize the surprise associated         short, hidden causesandstatesmediatestructuralanddynamic
                       with their sensory states at each point in time.                 dependencies, respectively.
                     d In statistics, surprise is the negative logarithm of Bayesian      The details of the graph in Figure 3 are not important; it just
                       model evidence, which means that biological systems—             provides a way of describing conditional dependencies among
                       like the brain—must continually maximize the Bayesian            hidden states and causes responsible for generating sensory
                       evidence for their (generative) model of sensory inputs.         input. These dependencies meanthatwecaninterpretneuronal
                     d Maximizing Bayesian model evidence corresponds to                activity as message passing among the nodes of a generative
                       Bayesian ﬁltering of sensory inputs. This is also known as       model, in which each canonical microcircuit contains represen-
                       predictive coding.                                               tations or expectations abouthiddenstatesandcauses.Inother
                                                                                        words, the form of the underlying generative model deﬁnes
                    These arguments mean that by minimizing surprise, through           the form of the predictive coding architecture used to invert the
                 selectingappropriatesensations,thebrainisimplicitlymaximizing          model.ThisisillustratedinFigure4,whereeachnodehasasingle
                 the evidence for its own existence—this is known as active infer-      parent. We will deal with this simple sort of model because it
                 ence. In other words, to maintain a homeostasis, the brain must        lends itself to an unambiguous description in terms of bottom-
                 predict its sensory states on the basis of a model. Fulﬁlling those    up (feedforward) and top-down (feedback) message passing.
                 predictions corresponds to accumulating evidence for that              Wenowlookathowperceptionormodelinversion—recovering
                 model—andthebrainthatembodiesit.Theimplicitmaximization                thehiddenstatesandcausesofthismodelgivensensorydata—
                 of Bayesian model evidence provides an important link to the           might be implemented at the level of a microcircuit.
                 Bayesian brain hypothesis (Hinton and van Camp, 1993; Dayan
                 et al., 1995; Knill and Pouget, 2004) and many other compelling        Predictive Coding and Message Passing
                 proposals about perceptual synthesis, including analysis by            In predictive coding, representations (or conditional expecta-
                 synthesis (Neisser, 1967; Yuille and Kersten, 2006), epistemolog-      tions) generate top-down predictions to produce prediction
                 ical automata (MacKay, 1956), the principle of minimum redun-          errors. These prediction errors are then passed up the hierarchy
                 702 Neuron76,November21,2012ª2012ElsevierInc.
