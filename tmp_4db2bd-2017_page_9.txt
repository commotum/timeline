                     Published as a conference paper at ICLR 2017
                     7   DISCUSSION AND FUTURE WORK
                     WehavepresentedaframeworkforimprovingIPSsystemsbyusingneuralnetworkstotranslatecues
                     in input-output examples to guidance over where to search in program space. Our empirical results
                     showthat for many programs, this technique improves the runtime of a wide range of IPS baselines
                     by 1-3 orders. We have found several problems in real online programming challenges that can be
                     solved with a program in our language, which validates the relevance of the class of problems that
                     wehavestudied in this work. In sum, this suggests that we have made signiﬁcant progress towards
                     being able to solve programming competition problems, and the machine learning component plays
                     an important role in making it tractable.
                     There remain some limitations, however. First, the programs we can synthesize are only the simplest
                     problems on programming competition websites and are simpler than most competition problems.
                     Manyproblemsrequire more complex algorithmic solutions like dynamic programming and search,
                     which are currently beyond our reach. Our chosen DSL currently cannot express solutions to many
                     problems. To do so, it would need to be extended by adding more primitives and allow for more
                     ﬂexibility in program constructs (such as allowing loops). Second, we currently use ﬁve input-output
                     examples with relatively large integer values (up to 256 in magnitude), which are probably more
                     informativethantypical(smaller)examples.WhileweremainoptimisticaboutLIPS’sapplicabilityas
                     the DSL becomes more complex and the input-output examples become less informative, it remains
                     to be seen what the magnitude of these effects are as we move towards solving large subsets of
                     programming competition problems.
                     Weforesee many extensions of DeepCoder. We are most interested in better data generation pro-
                     cedures by using generative models of source code, and to incorporate natural language problem
                     descriptions to lessen the information burden required from input-output examples. In sum, Deep-
                     Coder represents a promising direction forward, and we are optimistic about the future prospects of
                     using machine learning to synthesize programs.
                     ACKNOWLEDGMENTS
                     Theauthors would like to express their gratitude to Rishabh Singh and Jack Feser for their valuable
                                                       2
                     guidance and help on using the Sketch and λ program synthesis systems.
                     REFERENCES
                     Alex A. Alemi, Franc¸ois Chollet, Geoffrey Irving, Christian Szegedy, and Josef Urban. DeepMath -
                       deep sequence models for premise selection. In Proocedings of the 29th Conference on Advances
                       in Neural Information Processing Systems (NIPS), 2016.
                     RudyRBunel,AlbanDesmaison,PawanKMudigonda,PushmeetKohli,andPhilipTorr. Adaptive
                       neural compilation. In Proceedings of the 29th Conference on Advances in Neural Information
                       Processing Systems (NIPS), 2016.
                     Peter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The Helmholtz machine.
                       Neural computation, 7(5):889–904, 1995.
                                     ´                                        ¨
                     Krzysztof Dembczynski, Willem Waegeman, Weiwei Cheng, and Eyke Hullermeier. On label de-
                       pendence and loss minimization in multi-label classiﬁcation. Machine Learning, 88(1):5–45,
                       2012.
                     Krzysztof J. Dembczynski, Weiwei Cheng, and Eyke Hllermeier. Bayes optimal multilabel classiﬁ-
                       cation via probabilistic classiﬁer chains. In Proceedings of the 27th International Conference on
                       Machine Learning (ICML), 2010.
                     John K. Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations from
                       input-output examples. In Proceedings of the 36th ACM SIGPLAN Conference on Programming
                       Language Design and Implementation (PLDI), 2015.
                     Alexander L. Gaunt, Marc Brockschmidt, Rishabh Singh, Nate Kushman, Pushmeet Kohli, Jonathan
                       Taylor, and Daniel Tarlow. Terpret: A probabilistic programming language for program induction.
                       CoRR,abs/1608.04428, 2016. URL http://arxiv.org/abs/1608.04428.
                                                          9
