           [39] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
              Transformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint
              arXiv:1901.02860, 2019.
           [40] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan
              Salakhutdinov. Transformer dissection: A uniﬁed understanding of transformer’s attention via
              the lens of kernel. arXiv preprint arXiv:1908.11775, 2019.
           [41] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv
              preprint arXiv:2102.08602, 2021.
           [42] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
              conference on computer vision and pattern recognition, pages 7132–7141, 2018.
           [43] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating
              convolution designs into visual transformers. arXiv preprint arXiv:2103.11816, 2021.
           [44] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping
              Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction
              without convolutions. arXiv preprint arXiv:2102.12122, 2021.
           [45] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical
              automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF
              Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703, 2020.
           [46] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond
              empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.
           [47] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
              stochastic depth. In European conference on computer vision, pages 646–661. Springer, 2016.
           [48] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-
              thinking the inception architecture for computer vision. In Proceedings of the IEEE conference
              oncomputervision and pattern recognition, pages 2818–2826, 2016.
           [49] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
              arXiv:1711.05101, 2017.
           [50] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
              networks. In European conference on computer vision, pages 630–645. Springer, 2016.
           [51] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
              arXiv:1606.08415, 2016.
           [52] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V Le. Funnel-transformer: Filtering out
              sequential redundancy for efﬁcient language processing. arXiv preprint arXiv:2006.03236,
              2020.
                               13
