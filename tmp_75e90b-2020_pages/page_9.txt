Published as a conference paper at ICLR 2021

3.2
2.4
1.6
0.8
-0.0
--0.8

Figure 6: The visualization of learned positional correlations by TUPE-A.

5 RELATED WORK

As introduced in Sec.2, Shaw et al. (2018) was the first work to leverage relative positional encoding to
Transformer. Most of the other works are based on Shaw et al. (2018). For example, Transformer-XL
(Dai et al., 2019) re-parameterize the self-attention to integrate relative positional encoding directly.
T5 (Raffel et al., 2019) simplified the vector representation of relative positions in Shaw et al. (2018)
to a scalar. He et al. (2020) extended Shaw et al. (2018) by adding the position-to-word correlation
for relative position. In (Kitaev & Klein, 2018), the authors show that separating positional and
content information in the Transformer encoder can lead to an improved constituency parser. We
mathematically show that such disentanglement also improves Transformer in general language
pre-training. There are some other parallel works to enhance the absolute positional encoding in
Transformer, but not directly related to our work. For example, Shiv & Quirk (2019) extended the
sequence positional encoding to tree-based positional encoding in Transformer; Wang et al. (2019)
extended positional encoding to complex-valued domain; Liu et al. (2020) modeled the positional
encoding by dynamical systems.

6 CONCLUSION

We propose TUPE (Transformer with Untied Positional Encoding), which improves existing methods
by two folds: untying the correlations between words and positions, and untying [CLS] from
sequence positions. Specifically, we first remove the absolute positional encoding from the input
of the Transformer and compute the positional correlation and word correlation separately with
different projection matrices in the self-attention module. Then, we untie [CLS] by resetting the
positional correlations related to [CLS]. Extensive experiments demonstrate that TUPE achieves
much better performance on GLUE benchmark. Furthermore, with a better inductive bias over the
positional information, TUPE can even outperform the baselines while only using 30% pre-training
computational costs.

REFERENCES

Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.

Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao,
Jianfeng Gao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for unified language
model pre-training. arXiv preprint arXiv:2002. 12804, 2020.

Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look at?
an analysis of bertâ€™s attention. ACL 20/9, pp. 276, 2019a.

Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training
text encoders as discriminators rather than generators. In International Conference on Learning
Representations, 2019b.

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-
nov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint
arXiv:1901.02860, 2019.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv: 1810.04805, 2018.
