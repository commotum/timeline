Published as a conference paper at ICLR 2021

4 4 Relative positional correlation
Add & Layer Norm
Add & Layer Norm
fF
Dy it
Untie [CLS]
nN 4
Add & Layer Norm
Add & Layer Norm
f Sc SG ue uk
woh “fe >So ao
Word embedding as input directly Absolute positional correlation

Figure 1: The architecture of TUPE. The positional correlation and word correlation are computed
separately, and added together in the self-attention module. The positional attention related to the
[CLS] token is treated more positionless, to encourage it captures the global information.

receives and summarizes useful information from all the positions, and the contextual representation
of [CLS] will be used as the representation of the sentence in the downstream tasks. As the role of
the [CLS] symbol is different from regular words that naturally contain semantics, we argue that it
will be ineffective if we treat its position the same as word positions in the sentence. For example, if
we apply the relative positional encoding to this symbol, the attention distribution of some heads will
likely be biased to the first several words, which hurts the understanding of the whole sentence.

Based on the investigation above, we propose several simple, yet effective modifications to the
current methods, which lead to a new positional encoding called Transformer with Untied Positional
Encoding (TUPE) for language pre-training, see Figure 1. In TUPE, the Transformer only uses the
word embedding as input. In the self-attention module, different types of correlations are separately
computed to reflect different aspects of information, including word contextual correlation and
absolute (and relative) positional correlation. Each kind of correlation has its own parameters and will
be added together to generate the attention distribution. A specialized positional correlation is further
set to the [CLS] symbol, aiming to capture the global representation of the sentence correctly. First,
we can see that in TUPE, the positional correlation and word contextual correlation are de-coupled
and computed using different parameters. This design successfully removes the randomness in
word-to-position (or position-to-word) correlations and gives more expressiveness to characterize the
relationship between a pair of words or positions. Second, TUPE uses a different function to compute
the correlations between the [CLS] symbol and other positions. This flexibility can help the model
learn an accurate representation of the whole sentence.

We provide an efficient implementation of TUPE. To validate the method, we conduct extensive
experiments and ablation studies on the GLUE benchmark dataset. Empirical results confirm that our
proposed TUPE consistently improves the model performance on almost all tasks. In particular, we
observe that by imposing this inductive bias to encode the positional information, the model can be
trained more effectively, and the training time of the pre-training stage can be largely improved.

2 PRELIMINARY

2.1 ATTENTION MODULE

The attention module (Vaswani et al., 2017) is formulated as querying a dictionary with key-value
pairs, e.g., Attention(Q, K,V) = softmax(25—)V, where d is the dimensionality of the hidden
