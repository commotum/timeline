Published as a conference paper at ICLR 2021

A PROOF OF PROPOSITION 1

Proof. We first construct a circulant matrix Bof shape 2n x 2n using B. Note that B is a Toeplitz

matrix consisting of 2n — 1 values {b_(n—1),b-(n—2),°** 50," ++ + 0(n—2);6(n—1)}- For ease of
reference, we further define b_,, = by) = bo. Then B is constructed as
bp—j —n<k-j<n
Bye = § OK-j-an N<k- J <2n : (12)

be-jtan —2n<k-—j<-—n
To avoid any confusion, we use k and j in the subscription for positions and use 7 as the imaginary
unit. It is easy to check that Bisa circulant matrix by showing that Bye = Byes and Byan-1 =
Byy1,0 for any 0 < j,k < 2n — 1. Using Theorem 7 in Gray (2006), the matrix B can be factorized
into B = QDQ*. Q is an 2n x 2n Vandermonde matrix, where Qhj = pelt G+ hin and D

is a 2n x 2n diagonal matrix. Since the top-left n x nm submatrix of B is B, we can rewrite
B= BG . We also rewrite Q = G , where G is defined in the theorem. Then we have
C2 C3 C4

QD = [é.| ™ | [G* C4]. Considering that D is a diagonal matrix, we can obtain

B= GDG* using block matrix multiplication. oO

B EXPERIMENTAL DETAILS

Pre-training. Following BERT (Devlin et al., 2018), we use the English Wikipedia corpus and
BookCorpus (Zhu et al., 2015) for pre-training. By concatenating these two datasets, we obtain
a corpus with roughly 16GB in size. We follow a couple of consecutive pre-processing steps:
segmenting documents into sentences by Spacy>, normalizing, lower-casing, and tokenizing the texts
by Moses decoder (Koehn et al., 2007), and finally, applying byte pair encoding (BPE) (Sennrich
et al., 2015) with setting the vocabulary size as 32,768.

We found the data cleaning is quite important for language pre-training. Specially, we de-duplicate
the documents, normalize the punctuations, concatenate the short sequences, replace the URL and
other hyperlinks to special tokens, and filter the low-frequency tokens. Therefore, our re-implemented
baselines, like BERT, and achieve higher GLUE scores than the original papers.

We use masked language modeling as the objective of pre-training. We remove the next sentence
prediction task and use FULL-SENTENCES mode to pack sentences as suggested in RoBERTa (Liu
et al., 2019). We train the models for 1000k steps where the batch size is 256 and the maximum
sequence length is 512. The masked probability is set to 0.15, with replacing 80% of the masked
positions by [MASK], 10% by randomly sampled words, and keep the remaining 10% unchanged.
We use Adam (Kingma & Ba, 2014) as the optimizer, and set the its hyperparameter € to le-6 and
(G1, 82) to (0.9, 0.999). The peak learning rate is set to le-4 with a 10k-step warm-up stage. After
the warm-up stage, the learning rate decays linearly to zero. We set the dropout probability to 0.1,
gradient clip norm to 1.0, and weight decay to 0.01. Besides the final checkpoint, we also save
intermediate checkpoints ({ 100k, 300k, 600k: } steps) and fine-tune them on downstream tasks, to
check the efficiency of different methods.

Fine-tuning. We use the GLUE (General Language Understanding Evaluation) dataset (Wang et al.,
2018) as the downstream tasks to evaluate the performance of the pre-trained models. Particularly, we
use nine tasks in GLUE, including CoLA, RTE, MRPC, STS, SST, QNLI, QQP, and MNLI-m/mm.
For the evaluation metrics, we report Matthews correlation for CoLA, Pearson correlation for STS-B,
and accuracy for other tasks. We use the same optimizer (Adam) with the same hyperparameters as in
pre-training. Following previous works, we search the learning rates during the fine-tuning for each
downstream task. The setting details are listed in Table 2. For a fair comparison, we do not apply
any tricks for fine-tuning. Each configuration will be run five times with different random seeds,
and the median of these five results on the development set will be used as the performance of one
configuration. We will ultimately report the best number over all configurations.

Shttps://spacy.io

12
