Published as a conference paper at ICLR 2021

representations, and Q (Query), A (Key), V (Value) are specified as the hidden representations of the
previous layer. The multi-head variant of the attention module is popularly used which allows the
model to jointly attend to the information from different representation sub-spaces, and is defined as

Multi-head(Q, K,V) = Concat(heady, --- sheadin)W®
head), = Attention(QW2, KW, VW), (1)

where we e Rk WE 6 ROK, WY © ROX, and W° © R¥4v 4 are learnable project
matrices, H is the number of heads. dx and dy are the dimensionalities of Key and Value.

The self-attention module is one of the key components in Transformer and BERT encoder (Devlin
et al., 2018). For simplicity, we use the single-head self-attention module and set dx = dy = d fora

demonstration. We denote x! = (a4, x4--- , 2!,) as the input to the self-attention module in the /-th
layer, where 7 is the length of the sequence and each vector a} € R® is the contextual representation
of the token at position i. 2! = (z{,z5---, 2!) is the output of the attention module. Then the
self-attention module can be written as
exp(aij) LyyVil Ly tye@dty (ol yr K\D
a= Fela)” W*), where aj; = —=(x;W&")(a,W*")*. (2)
“Ls —1 exp(aiyr) 7 , vd ' q

As we can see, the self-attention module does not make use of the order of the sequence, i.e., is
permutation-invariant. However, natural language is well-structured and word order is important for
language understanding (Sutskever et al., 2014). In the next section, we show several previous works
that proposed different ways of incorporating positional information into the Transformer model.

2.2 POSITIONAL ENCODING

Generally, there are two categories of methods that encode positional information in the Transformer
model, absolute positional encoding and relative positional encoding.

Absolute Positional Encoding. The original Transformer (Vaswani et al., 2017) proposes to use
absolute positional encoding to represent positions. In particular, a (learnable) real-valued vector
pi € R¢ is assigned to each position i. Given a sentence, p; will be added to the word embedding w;
at position i, and w; + p; will be used as the input to the model, e.g, x} = w; + p;. In such a way,
the Transformer can differentiate the words coming from different positions and assign each token
position-dependent attention. For example, in the self-attention module in the first layer, we have

1
ays? = yaw + p)W?)((w; + p)WE)?. (3)
Relative Positional Encoding. In absolute positional encoding, using different p; for different
position 7 helps Transformer distinguish words at different positions. However, as pointed out in
Shaw et al. (2018), the absolute positional encoding is not effective for the model to capture the
relative word orders. Therefore, besides using absolute positional encoding, Shaw et al. proposes a
relative positional encoding as an inductive bias to help the learning of attention modules,

ny 7 yn 4)

adel — ale We!) (axt Wet tal i

where al, _

; € R¢ is learnable parameter and can be viewed as the embedding of the relative position
j — vin layer 1. In this way, embedding a, explicitly models the relative word orders. TS (Raffel

et al., 2019) further simplifies it by eliminating ay in Query-Key product.

ts 1 ad ty Ky?
Qj; = qo Maj; W Yo + bj-i- (5)

For each j — i, bj_; is a learnable scalar? and shared in all layers.

* Specifically, in Shaw et al. (2018); Raffel et al. (2019), the relative position j — i will be first clipped to a
pre-defined range, e.g., clip(j — i, —t, t), t = 128. The embedding is defined over the possible values of the
clipped range, i.e., [—t, t]. Besides, Shaw et al. also tried to add vector ay. ; to the value V in the output of
self-attention, but the experiment results indicate that it did not improve much,
