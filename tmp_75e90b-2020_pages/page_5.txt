Published as a conference paper at ICLR 2021

we wk

we wk ue uk
Wi w;
+ +

(a) Absolute positional encoding. (b) Untied absolute positional encoding.

Figure 3: Instead of adding the absolute positional embedding to the word embedding in the input
(left), we compute the positional correlation and word correlation separately with different projection
matrices, and add them together in the self-attention module (right).

Our modification. To overcome these problems, we propose to directly model the relationships
between a pair of words or positions by using different projection matrices and remove the two terms
in the middle. That is, we use

oy = Fale WO) (eh WE)P 4 (pi U%)(pU®)?, a)

where U?,U*® € R®*4 are the projection matrice for the positional embedding, and scaling term
Tai is used to retain the magnitude of a;; (Vaswani et al., 2017) . A visualization is put in Figure 3.
Our proposed method can be well combined with the relative positional encoding in Raffel et al.
(2019) by simply changing Eq. (5) to

aij = (ele WHT 4 2 (pV) (pT)? +d) (8)
3.2 UNTIE THE [CLS] SYMBOL FROM POSITIONS

Note that in language representation learning, the input sequence to the Transformer model is not
always a natural sentence. In BERT, a special symbol [CLS] is attached to the beginning of the
input sentence. This symbol is designed to capture the global information of the whole sentence. Its
contextual representation will be used to make predictions in the sentence-level downstream tasks
after pre-training (Devlin et al., 2018; Liu et al., 2019).
We argue that there will be some disadvantages if we treat this token the same as other natural words
in the attention module. For example, the regular words usually have strong local dependencies in
the sentence. Many visualizations (Clark et al., 2019a; Gong et al., 2019) show that the attention
distributions of some heads concentrate locally. If we process the position of [CLS] the same as the
position of natural language words, according to the aforementioned local concentration phenomenon,
[CLS] will be likely biased to focus on the first several words instead of the whole sentence. It will
potentially hurt the performance of the downstream tasks.

Our modification. We give a specific design in the attention module to untie the [CLS] symbol
from other positions. In particular, we reset the positional correlations related to [CLS]. For better
demonstration, we denote v;; as the content-free (position-only) correlation between position 7 and j.

For example, when using the absolute positional encoding in Eq. (7), vij = eq (PU )(p; U*)?;
when using relative positional encoding in Eq. (8), vj; = eq (pi U®)(p;U*)* + b;«. We reset the
values of vj; by the following equation:
vy 141,79 A 1,(not related to [CLS])
resetg(v,i,7) = <0; i=1,(from [CLS] to others) ; (9)
62 i#1,j =1,(from others to [CLS])

where 6 = {6}, 02} is a learnable parameter. A visualization is put in Figure 4.
