                                       100                             83.1
                                      Accuracy80 71.8
                                        60                                                   55.6
                                        40
                                      Exact Match 20   7.1   10.5            3.7   6.8             6.4   4.7
                                          0          LT                    ST                 ST w/ II
                                                                   Architecture Type
                                               Abacus, OOD               FIRE, OOD                NoPE, OOD
                          Figure 8:   Accuracy of models on the bitwise OR task when trained on data with size up to 20,
                          varying over different positional embeddings and architectures. Abacus Embeddings heavily improve
                          performance on this task.
                          Wetrain standard transformer, standard transformer with input injection and looped transformer
                          models on the position wise or task, on a dataset where the maximum length of either input vector is
                          twenty. This result is shown in Figure 8. Here we see that the Abacus Embeddings allow all models
                          to generalize further on this task than the other embeddings which prior work for addition focuses on.
                          Aswithaddition, we see that looped transformers perform better than the standard architectures with
                          FIREorNoPEembeddings. Wedonotethattheseaccuraciesarenotashighwereportforaddition.
                          Wehypothesize this is because the model is having to repeatedly predict the same token multiple
                          times, this has been thought to be the cause of errors in prior addition work[Qian et al., 2022]. When
                          weanalyzedtheerrors in this task we found they were predominantly caused by the model outputting
                          one too few or too many zeros.
                          A.3.1   ExampleData
                                                   000010⊕00000000000000 = 00001000000000
                                                           000100⊕0000000 = 0001000
                                                               001⊕00000=00100
                          A.4   Addition Models Trained on Varying Data Sizes
                          Across Figure 9, we see that increasing the size of the operands in the training set allows for better
                          generalization above one hundred digits for all models. This is partially due to the sampling method
                          for training Abacus Embeddings. As the offset randomization hyperparameter k = 100 is fixed across
                          experiments, there are more embeddings trained if the operands seen during training are longer. The
                          size of the OOD set below 100 is reduced as the size of the operands seen during training increases,
                          as the ID category now includes this data. However, this does still show that the size of the operands
                          seen during training directly impacts the generalization, with larger training sizes allowing for better
                          generalization.
                          A.5   ExtremeLengthGeneralizationforAddition
                          Absolute positional embeddings must be learned during training otherwise they are unusable at
                          test time. This limits our Abacus Embeddings which are trained with the offset randomization
                          hyperparameter k = 100. One possible way to resolve this generalization problem is to increase
                          the value of k during testing. In Figure 10 (left), we show the exact match accuracy of five looped
                          transformer models, with eight layers in the recurrent block and two recurrences trained on size 20
                                                                         15
