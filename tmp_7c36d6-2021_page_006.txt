                                  Table 2: The comparison between LaSynth and baseline neural program synthesis models in our evaluation.
                                                               LaSynth      Exec     Shin et al.   Bunel et al.    RobustFill     Property Signatures
                                                                             [12]       [44]            [9]            [17]                [39]
                                    +Programexecution              3          3           3              −              −                   −
                                    Nointerpreter needed           3          −           −              3              3                   3
                                                                                                  I       O
                                 Theoperation predictor takes two attention vectors s and s                  as the representations of input-output
                                                                                                  t       t
                                 examples, and yields an operator embedding opˆ . To compute the aggregated embedding vector for
                                                                                            t
                                 all input-output examples, we modify Eqn. 2 to also take opˆ t as an input of the max pooling layer:
                                                                                                           I(j)   O(j)     (j)
                                                          mt =MaxPool                       (tanh(W[s          ; s     ; opˆ  ]))                      (7)
                                                                              j∈{1,2,...,K}                t      t        t
                                 Totrain the operation predictor, we add an additional loss LOp:
                                                                             L=LProg+LExec+LOp                                                         (8)
                                 LOp is designed to ensure that the operation predictor predicts operations related to IO pairs, and we
                                 defer the details to Appendix B.2.
                                 Limitations. In our current implementation of the operation predictor, the operation table is only
                                 able to enumerate the arithmetic operations over a pre-deﬁned constant set, thus it requires that the
                                 set of possible numerical values in input-output pairs is ﬁnite. One way of extending our operation
                                 predictor to support potentially unbounded numerical calculation is to combine it with the subword
                                 tokenizer, which has been commonly used in recent language models [16, 11, 4]. We consider
                                 designing general-purpose number representation for better mathematical reasoning as future work.
                                 5     Experiments
                                 In this section, we discuss our results on synthesizing programs in Karel and C languages. We ﬁrst
                                 show that LaSynth achieves competitive performance on Karel benchmark. Then we present the
                                 results on our restricted C benchmark, and demonstrate that our approach signiﬁcantly outperforms
                                 existing neural program synthesis models. Finally, we discuss the effect of iterative retraining.
                                 5.1    Karel Program Synthesis
                                 5.1.1     Evaluation Setup
                                 Karel domain. Karel is an educational programming language [41], and has been studied in recent
                                 works on neural program synthesis from input-output examples [15, 9, 12, 44]. A Karel program
                                 controls a robot in a 2D grid world. There are instructions that control the robot, e.g., move, turnLeft
                                 and PutMarker, as well as conditionals and loops, i.e., if, repeat and while. See Appendix A for
                                 grammarspeciﬁcation and the state representation.
                                 WetrainandevaluateallmodelsontheKareldatasetintroducedin[9]. Thedatasetcontainsrandomly
                                 sampled programs from the Karel DSL (1.1M training samples, 2.5K samples in the validation set
                                 and2.5K samplesinthetestset). Eachprogramincludes5input-outputpairsasthespeciﬁcation, and
                                 the sixth pair as the held-out test case. Following the prior work, we evaluate two metrics: (1) Exact
                                 Match: the predicted program is the same as the ground truth; (2) Generalization: the predicted
                                 program satisﬁes both the input-output pairs and the held-out input-output test case.
                                 Baselines. Bunel et al. [9] designed the ﬁrst program synthesis model for the Karel benchmark with a
                                 similar high-level design as RobustFill, but they use convolutional neural networks (CNN) to encode
                                 the Karel grid maps. Compared to LaSynth, this model does not utilize any program execution
                                 information, and does not include our latent executor. Instead of directly synthesizing the program
                                 frominput-output examples, the model in Shin et al. [44] ﬁrst predicts the execution traces containing
                                 the robot actions from the input-output pairs, then decodes the program based on the execution traces.
                                 This model improves the prediction performance over Bunel et al., but it requires the full execution
                                 traces for model training and an interpreter for execution. Exec [12] leverages the execution states of
                                 partial generated programs to guide the subsequent synthesis process, but the execution states are
                                 obtained from the Karel interpreter rather than learned by the model, thus this approach represents
                                 the ideal scenario where the partial programs could be executable.
                                                                                             6
