                                 Published as a conference paper at ICLR 2017
                                   Table 1: Search speedups on programs of length T = 3 due to using neural network predictions.
                                 Timeout needed            DFS                   Enumeration                   λ2                  Sketch         Beam
                                 to solve            20%    40%    60%        20%     40%    60%        20%     40% 60%           20%     40%        20%
                                                                                                                                    3       3          3
                                 Baseline          41ms 126ms 314ms          80ms 335ms 861ms          18.9s   49.6s 84.2s      >10 s >10 s        >10 s
                                 DeepCoder         2.7ms 33ms 110ms          1.3ms 6.1ms 27ms          0.23s   0.52s 13.5s       2.13s   455s        292s
                                 Speedup          15.2×    3.9×    2.9×     62.2× 54.6× 31.5×         80.4× 94.6× 6.2×         >467× >2.2×        >3.4×
                                 andruntimesof3s,2s,1s,4s,thetimeoutrequiredtosolve50%oftaskswouldbe2s.Moredetailed
                                 experimental results are discussed in Appendix B.
                                 In the main experiment, we tackled a large-scale problem of searching for programs consistent with
                                 input-output examples generated from programs of length T = 5 (search space size on the order of
                                    10
                                 10 ), supported by a neural network trained with programs of shorter length T = 4. Here, we only
                                 consider P = 100 programs for reasons of computational efﬁciency, after having veriﬁed that this
                                 does not signiﬁcantly affect the results in Table 1. The table in Fig. 3a shows signiﬁcant speedups
                                 for DFS, Sort and add enumeration, and λ2 with Sort and add enumeration, the search techniques
                                 capable of solving the search problem in reasonable time frames. Note that Sort and add enumeration
                                                                                                                                     4
                                 without the neural network (using prior probabilities of functions) exceeded the 10 second timeout
                                 in two cases, so the relative speedups shown are crude lower bounds.
                                                                                                                                                  T    :
                                                                                                                     103                           train
                                                                                                                                                  4
                                 Timeout needed              DFS                  Enumeration             λ2
                                                                                                                     102                          3
                                 to solve            20% 40% 60%                20%     40% 60%           20%
                                                                                          4       4                  101                          2
                                 Baseline            163s 2887s 6832s         8181s >10 s >10 s           463s     Speedup                        1
                                 DeepCoder            24s   514s 2654s            9s   264s 4640s          48s         0
                                 Speedup            6.8× 5.6× 2.6×            907× >37× >2×              9.6×        10                           none
                                                                                                                          1    2     3     4    5
                                                                                                                       Length of test programs T
                                                                       (a)                                                                        test
                                                                                                                                   (b)
                                 Figure 3: Search speedups on programs of length T = 5 and inﬂuence of length of training programs.
                                 WehypothesizethatthesubstantiallylargerperformancegainsonSortandadd schemesascompared
                                 to gains on DFS can be explained by the fact that the choice of attribute function (predicting presence
                                 offunctionsanywhereintheprogram)andlearningobjectiveoftheneuralnetworkarebettermatched
                                 to the Sort and add schemes. Indeed, a more appropriate attribute function for DFS would be one
                                 that is more informative of the functions appearing early in the program, since exploring an incorrect
                                 ﬁrst function is costly with DFS. On the other hand, the discussion in Sect. 4.5 provides theoretical
                                 indication that ignoring the correlations between functions is not cataclysmic for Sort and add
                                 enumeration, since a Rank loss that upper bounds the Sort and add runtime can still be minimized.
                                 In Appendix G we analyse the performance of the neural networks used in these experiments, by
                                 investigating which attributes (program instructions) tend to be difﬁcult to distinguish from each
                                 other.
                                 5.2     GENERALIZATION ACROSS PROGRAM LENGTHS
                                 To investigate the encoder’s generalization ability across programs of different lengths, we trained
                                 a network to predict used functions from input-output examples that were generated from programs
                                 of length Ttrain ∈ {1,...,4}. We then used each of these networks to predict functions on 5 test sets
                                 containinginput-outputexamplesgeneratedfromprogramsoflengthsT                            ∈{1,...,5},respectively.
                                                                                                                      test
                                 Thetest programs of a given length T were semantically disjoint from all training programs of the
                                 samelength T and also from all training and test programs of shorter lengths T0 < T.
                                 For each of the combinations of T             and T     , Sort and add enumerative search was run both with
                                                                           train      test
                                 and without using the neural network’s predictions (in the latter case using prior probabilities) until
                                 it solved 20% of the test set tasks. Fig. 3b shows the relative speedup of the solver having access to
                                 predictions from the trained neural networks. These results indicate that the neural networks are able
                                 to generalize beyond programs of the same length that they were trained on. This is partly due to the
                                                                                             7
