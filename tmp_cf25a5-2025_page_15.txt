                        Test-Time Learning for Large Language Models
       However, decoder-only models, while excelling in generative tasks, often lack the nuanced comprehension abilities required
       for deep understanding of long and complex input contexts.
       Encoder-decoder models. Encoder-decoder models incorporate both the encoder and decoder components of the Trans-
       former, combining the strengths of the two structures to handle tasks that require input understanding and sequence
       generation. Prominent examples of encoder-decoder models include GLM from Tsinghua University (Du et al., 2022), T5,
       FLAN-T5,andUL2fromGoogle(Raffeletal.,2020;Chungetal.,2024;Tayetal.,2023), as well as BART from Meta
       (Lewis et al., 2020a). For instance, GLM adopts an autoregressive blank-infilling objective to effectively address three core
       challenges in NLP: natural language understanding (NLU), unconditional text generation, and conditional generation. With
       a maximum capacity of 130 billion parameters, it is pre-trained on datasets such as BookCorpus (Tay et al., 2023) and
       Wikipedia. GLMsurpasses BERT on the SuperGLUE benchmark by 4.6%-5.0% and demonstrates superior performance
       compared to FLAN-T5 on both NLU and generation tasks using fewer parameters and training data.
       A.2. Retrieval-Augmented Generation
       Asoneofthemostrepresentative techniques in the field of generative AI, Retrieval-Augmented Generation (RAG) aims
       to enhance the quality of the generated text content with retrieved information. It achieves this by integrating two critical
       components: (i) a retrieval mechanism that accesses relevant documents or information from external knowledge sources,
       and (ii) a generative module that synthesizes this information to produce coherent and contextually accurate text (Lewis
       et al., 2020b). By combining these capabilities, RAG models are able to generate not only fluent and human-like text but
       also outputs that are grounded in up-to-date and factual data, significantly enhancing their reliability and applicability in
       real-world scenarios. We categorized existing RAG methods into the following two main classes according to whether
       training is needed for further discussion: training-free approaches and training-based approaches.
       Training-free RAG. Training-free RAG methods address the challenges of frequent fine-tuning and updating model
       parameters, which require substantial computational resources and time (Lewis et al., 2020b). These approaches leverage
       retrieved knowledge directly at inference time by incorporating the retrieved text into the prompt, eliminating the need
       for additional training. As the performance of large language models (LLMs) is highly sensitive to input queries, many
       training-free RAG methods refine prompts by integrating external knowledge (Jiang et al., 2023; Li et al., 2023; Kim et al.,
       2023; Rametal., 2023; Trivedi et al., 2023; Wang et al., 2023). For example, In-Context RALM (Ram et al., 2023) augments
       the generation process by prepending retrieved documents to the original prompt without altering LLM parameters. IRCoT
       (Trivedi et al., 2023) enhances reasoning by interleaving chain-of-thought (CoT) generation and retrieval, ensuring access
       to more relevant information across iterative reasoning steps. SKR (Wang et al., 2023) enables flexible utilization of both
       internal and external knowledge by guiding LLMs to decide whether a question can be answered based on internal knowledge
       before invoking the retriever. Despite their efficiency, training-free RAG methods often face limitations in optimizing the
       retriever and generator for specific downstream tasks, leading to suboptimal utilization of retrieved knowledge. To address
       this, training-based RAG approaches fine-tune both components, enabling large language models to effectively adapt and
       integrate external information.
       Training-based RAG. This kind of methods aim to optimize both the retriever and generator to enhance their alignment
       and effectiveness. One typical success is DPR (Karpukhin et al., 2020), which employs two independent BERT (Devlin
       et al., 2019) encoders for queries and passages and trains them via contrastive learning. Ren et al. (2023) employs a
       two-stage approach, starting with the pretrain S-BERT (Reimers & Gurevych, 2019) as a retrieval backbone, enhanced
       by an adaptive hybrid strategy to effectively gather relevant demonstration. Next, a T5 model is used as the generator,
       which is further fine-tuned to align with the target labels and inputs. In contrast, RA-DIT (Lin et al., 2024) first fine-tuning
       LLMstoeffectively use retrieved knowledge and then refining the retriever to align with the modelâ€™s requirements. To
       address indiscriminate retrieval and the incorporation of irrelevant passages, Self-RAG (Asai et al., 2024) introduces special
       tokens to dynamically assess the necessity of retrieval and control its behavior. More recently, MemoRAG (Qian et al.,
       2024) incorporates a memory module that generates context-specific cues to link the knowledge base to precise information,
       improving retrieval accuracy and response quality.
       Despite their advantages, RAG models rely heavily on the quality and relevance of the retrieved knowledge, as inaccuracies
       or irrelevant information can directly compromise the quality of the generated output. Furthermore, the dual-step process of
       retrieval and generation for each query introduces significant computational overhead, posing challenges for real-time and
       resource-constrained applications.
                                  15
