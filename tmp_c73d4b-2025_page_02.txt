           Figure 1: The uniform and reusable structure as well as multi time scale update in the brain are the
           key components to unlock the continual learning in humans. Nested Learning (NL) allows for multi
           time-scale update for each component of the brain, while showing that well-known architectures such
           as Transformers are in fact linear layers with different frequency updates.
           For decades, AI research has focused on designing machine learning algorithms that learn from
           data [2–5] or experience [6–8]; often by optimizing an objective L(θ) over parameters θ ∈ Θ with
           gradient-based methods. While traditional machine learning techniques required careful engineering
           and domain expertise to design feature extractors, limiting their ability to directly process and learn
           from natural data [9], deep representation learning offered a fully automated alternative to discover
           the representations needed for the task. Thereafter, deep learning has been an inseparable part of the
           large-scale computational models with seminal success in chemistry and biology [10], games [11, 12],
           computer vision [13, 14], and multimodal and natural language understanding [15–17].
           Stacking of multiple layers, as it is done in deep learning models, provides the models with larger
           capacity, better expressive power in representing complex features, and more internal computations
           (e.g., #FLOPS) [18–20], all of which are critical and desirable characteristics for static tasks that
           require in-distribution predictions over a previously fixed set. This deep design, however, is not
           a universal solution to all the challenges and cannot help the expressive power of the models in
           multiple aspects, for example: (i) The computational depth of deep models might not change with
           more layers [21, 22], leaving their ability to implement complex algorithms untouched compared
           to traditional shallow approaches [23]; (ii) The capacity of some class of parameters might show
           marginal improvement with increasing the depth/width of the model [24]; (iii) The training process
           might converge to a suboptimal solution, mainly due to the suboptimal choice of the optimizer or its
           hyperparameters; and (iv) The model’s ability to fast adapt to a new task, continually learn, and/or
           generalize to out-of-distribution data might not changed with stacking more layers and requires more
           careful designs.
           The core part of the efforts to overcome the above challenges and to enhance the capability of
           deep learning models concentrate on: (1) developing more expressive class of parameters (i.e.,
           neural architectures) [13, 25–28]; (2) introducing objectives that can better model the tasks [29–
           32]; (3) designing more efficient/effective optimization algorithms to find better solutions or with
           more resilience to forgetting [33–36]; and (4) scaling the model size to enhance its expressivity,
           whenthe“right” choice of architecture, objective, and optimization algorithms are made [24, 37, 38].
           Collectively, these advancementsandnewfindingsonscalingpatternsofdeepmodelshaveestablished
           the foundations upon which Large Language Models (LLMs) have been built.
           ThedevelopmentofLLMsmarksapivotalmilestoneindeeplearningresearch: aparadigmshiftfrom
           task-specific models to more general-purpose systems with various emergent capabilities as a result
           of scaling the “right” architectures [38, 39]. Despite all their success and remarkable capabilities in
           diverse sets of tasks [15, 40, 41], LLMs are largely static after their initial deployment phase, meaning
           that they successfully perform tasks learned during pre- or post-training, but are unable to continually
           acquire new capabilities beyond their immediate context. The only adaptable component of LLMs
           is their in-context learning ability–a (known to be emergent) characteristic of LLMs that enables
           fast adaption to the context and so perform zero- or few-shot tasks [38]. Beyond in-context learning,
           recent efforts to overcome the static nature of LLMs either are computationally expensive, require
           external components, lack generalization, and/or might suffer from catastrophic forgetting [42–44],
           which has led researchers to question if there is a need to revisit how to design machine learning
                               2
