                           Figure 3: A comparison of Hope architectural backbone with Transformers (Normalization and
                           potential data-dependent components are removed for the sake of clarity).
                           Table 1: Performance of HOPE and baselines on language modeling and common-sense reasoning
                           tasks. Hybrid models are marked with ∗.
                            Model         Wiki.  LMB. LMB. PIQA        Hella.  Wino.  ARC-e   ARC-c   SIQA BoolQ     Avg.
                                           ppl ↓  ppl ↓  acc ↑  acc ↑  acc_n ↑ acc ↑   acc ↑  acc_n ↑ acc ↑   acc ↑   ↑
                            HOPE(ours)    26.05   29.38  35.40  64.62  40.11   51.19   56.92   28.49  38.33  60.12  46.90
                                                                760Mparams/30Btokens
                            Transformer++ 25.21   27.64  35.78  66.92  42.19   51.95   60.38   32.46  39.51  60.37  48.69
                            RetNet        26.08   24.45  34.51  67.19  41.63   52.09   63.17   32.78  38.36  57.92  48.46
                            DeltaNet      24.37   24.60  37.06  66.93  41.98   50.65   64.87   31.39  39.88  59.02  48.97
                            TTT           24.17   23.51  34.74  67.25  43.92   50.99   64.53   33.81  40.16  59.58  47.32
                                 ∗
                            Samba         20.63   22.71  39.72  69.19  47.35   52.01   66.92   33.20  38.98  61.24  51.08
                            Titans (LMM)  20.04   21.96  37.40  69.28  48.46   52.27   66.31   35.84  40.13  62.76  51.56
                            HOPE(ours)    20.53   20.47  39.02  70.13  49.21   52.70   66.89   36.05  40.71  63.29  52.26
                                                                1.3B params / 100B tokens
                            Transformer++ 18.53   18.32  42.60  70.02  50.23   53.51   68.83   35.10  40.66  57.09  52.25
                            RetNet        19.08   17.27  40.52  70.07  49.16   54.14   67.34   33.78  40.78  60.39  52.02
                            DeltaNet      17.71   16.88  42.46  70.72  50.93   53.35   68.47   35.66  40.22  55.29  52.14
                                 ∗
                            Samba         16.13   13.29  44.94  70.94  53.42   55.56   68.81   36.17  39.96  62.11  54.00
                            Titans (LMM)  15.60   11.41  49.14  73.09  56.31   59.81   72.43   40.82  42.05  60.97  56.82
                            HOPE(ours)     15.11  11.63  50.01  73.29  56.84   60.19   72.30   41.24  42.52  61.46  57.23
                           4   Experiments
                           For the sake of space, in the main paper, we report the results of the HOPE’s evaluation on language
                           modeling, and common-sense reasoning, tasks. However, we report an extensive set of results,
                           including on experiments on optimizers, emergence of in-context learning, continual learning abilities
                           of HOPE, ablation studies, long-context tasks, etc. in the appendix. Details about the experimental
                           setups and other used datasets are in Appendix G
                           LanguageModelingandCommon-senseReasoning. Wefollowrecentsequencemodelingstud-
                           ies [28, 67, 68] and report the results of HOPE and baselines with size of 340M, 760M, and 1.3B on
                           language modeling and also commonsense reasoning downstream tasks. These results are reported
                           in Table 1. HOPE demonstrate a very good perfomance across all the scales and benchmark tasks,
                           outperforming both Transformers and recent modern recurrent neural networks, including Gated
                           DeltaNet and Titans. Comparing HOPE to Titans and Gated DeltaNet, we can see that dynamically
                           changing the key, value, and query projections based on the context as well a deep memory module
                           can result in a model with lower perplexity and higher accuracy in benchmark results.
                                                                         10
