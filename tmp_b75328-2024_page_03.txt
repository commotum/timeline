                        digit first is popular [Lee et al., 2023, Shen et al., 2023, Zhou et al., 2023, 2024]. Furthermore,
                        changing the data format by adding explicit index characters improves model capability for addition
                        [Zhou et al., 2023, 2024, Olsson et al., 2022]. Other work approaches arithmetic by embedding real
                        numbers by scaling a single fixed token-embedding for numbers [Golkar et al., 2023]. Moreover,
                        Dziri et al. [2023] show multiplication is a hard problem for GPT-3 [Brown et al., 2020] even when
                        finetuned on this task. Dziri et al. [2023] further show that GPT-4 [OpenAI, 2023] struggles to obtain
                        high in-distribution accuracy on multiplication, even with a scratchpad. However, Lee et al. [2023]
                        find that with a detailed scratchpad, small transformers can perform multiplication in-distribution.
                        Arithmetic is a subset of the larger class of algorithmic reasoning problems that focus on the ability
                        to learn and execute algorithms and generalize to longer problems [Anil et al., 2022b, Jelassi et al.,
                                                    ˇ    ´
                        2023, Yang et al., 2023b, Velickovic et al., 2022, Rodionov and Prokhorenkova, 2024, Testolin,
                        2024]. The more general algorithmic reasoning field includes work on various architectures and
                                                                              ˇ    ´
                        data modalities aimed at learning algorithms from data. Velickovic et al. [2022] and Rodionov and
                        Prokhorenkova [2024], for example, train neural networks to execute specific algorithmic tasks by
                        training on input-output pairs as well as intermediate steps and hints. In a similar vein and although
                        initially appreciated for efficiency, weight sharing and recurrence can be used to make models
                        adaptive and help generalize to harder problems [Dehghani et al., 2018, Sukhbaatar et al., 2019,
                        Lanetal., 2020, Ibarz et al., 2022]. Schwarzschild et al. [2021] and Bansal et al. [2022] explore an
                        end-to-end learning approach using recurrent convolutional neural networks to learn algorithms from
                        input-output pairs, tackling algorithmic tasks like prefix sums, mazes, and chess. Weight sharing for
                        algorithmic reasoning is also helpful with transformers and we use the looped transformer in some
                        of our experiments below. A looped transformer has a transformer block called recurrently on its
                        ownoutput lending itself to executing iterative algorithms [Giannou et al., 2023, Yang et al., 2023a,
                        de Luca and Fountoulakis, 2024]. Additionally, recent work aims to improve reasoning in LLMs
                        [Zhou et al., 2023], but McLeish et al. [2024] demonstrate that LLMs, even with code interpreters,
                        are less than perfect at algorithmic reasoning tasks, indicating a crucial need for advancements in
                        our methodologies. This paper takes a step towards improving LLM arithmetic and algorithmic
                        capabilities without tool use.
                        Positional Embeddings.   Indicating the position of tokens in a sequence to transformer models is
                        critical for language modeling [Vaswani et al., 2017]. Absolute positional embeddings (APE) are
                        learned embeddings that are added to token embeddings before the first layer of the transformer
                        [Vaswani et al., 2017]. However, these absolute embeddings inhibit length generalization [Press et al.,
                        2022]. To address this issue, Shaw et al. [2018] propose relative embeddings (RPE) which are embed-
                        ded during the attention computation, a mechanism further simplified by Raffel et al. [2020]. Others
                        build on these works to improve length generalization including Sandwich [Chi et al., 2023], Kerple
                        [Chi et al., 2022], and Alibi [Press et al., 2022] positional embeddings. Additionally, Kazemnejad
                        et al. [2023] show that decoder layers can still learn positional information with no explicit positional
                        embeddings. No positional embeddings (NoPE) can achieve good length generalization performance
                        for small algorithmic tasks and even outperform some specialized embeddings. Rotary Positional
                        Embeddings(RoPE)[Suetal., 2024] are commonly used in state-of-the-art open source transformers
                        [e.g. Touvron et al., 2023]. However, RoPE does limit the length generalization as models are trained
                        only using rotations based on training data length [Kazemnejad et al., 2023, Press et al., 2022]. For
                        improved length generalization, one can add post-training extensions [Peng et al., 2024]. The latest
                        and most useful for arithmetic is Functional Interpolation for Relative Position Embeddings (FIRE)
                        [Li et al., 2023]. FIRE shows the strongest length generalization to date, which leads to length
                        generalization by 2.5× on addition [Zhou et al., 2024] when combined with randomized embeddings
                        [Ruoss et al., 2023]. We go into more detail on some of these positional embeddings in Appendix
                        A.1.1. In this work, we focus on NoPE and FIRE embeddings since these are the best performers for
                        addition in reversed format among existing embeddings [Zhou et al., 2024].
                        3   Achieving Length Generalization for Addition
                        Westudyarangeofmethodsforimprovingthearithmeticcapabilities of language models trained
                        from scratch centering on two main hypotheses: (1) the positional information for individual digits
                        within numbers is being lost and (2) recurrence can improve the reasoning abilities of transformer
                        architectures on multi-step arithmetic reasoning problems. We briefly discuss the training and
                        evaluation setup before describing each of our improvements in detail.
                                                                   3
