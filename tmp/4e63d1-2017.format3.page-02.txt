                  interpretability and straightforward ways of incorporating domain-speciﬁc knowledge in form of
                  rules.
                  Ourapproachtothis problem is inspired by recent neural network architectures like Neural Turing
                  Machines [19], Memory Networks [20], Neural Stacks/Queues [21, 22], Neural Programmer [23],
                  Neural Programmer-Interpreters [24], Hierarchical Attentive Memory [25] and the Differentiable
                  Forth Interpreter [26]. These architectures replace discrete algorithms and data structures by end-to-
                  end differentiable counterparts that operate on real-valued vectors. At the heart of our approach is the
                  idea to translate this concept to basic symbolic theorem provers, and hence combine their advantages
                  (multi-hop reasoning, interpretability, easy integration of domain knowledge) with the ability to
                  reason with vector representations of predicates and constants. Speciﬁcally, we keep variable binding
                  symbolic but compare symbols using their subsymbolic vector representations.
                  Concretely, we introduce Neural Theorem Provers (NTPs): End-to-end differentiable provers for
                  basic theorems formulated as queries to a KB. We use Prolog’s backward chaining algorithm as
                  a recipe for recursively constructing neural networks that are capable of proving queries to a KB
                  using subsymbolic representations. The success score of such proofs is differentiable with respect to
                  vector representations of symbols, which enables us to learn such representations for predicates and
                  constants in ground atoms, as well as parameters of function-free ﬁrst-order logic rules of predeﬁned
                  structure. By doing so, NTPs learn to place representations of similar symbols in close proximity in a
                  vector space and to induce rules given prior assumptions about the structure of logical relationships
                  in a KB such as transitivity. Furthermore, NTPs can seamlessly reason with provided domain-speciﬁc
                  rules. As NTPs operate on distributed representations of symbols, a single hand-crafted rule can
                  be leveraged for many proofs of queries with symbols that have a similar representation. Finally,
                  NTPsdemonstrate a high degree of interpretability as they induce latent rules that we can decode to
                  human-readable symbolic rules.
                  Ourcontributions are threefold: (i) We present the construction of NTPs inspired by Prolog’s back-
                  ward chaining algorithm and a differentiable uniﬁcation operation using subsymbolic representations,
                  (ii) we propose optimizations to this architecture by joint training with a neural link prediction model,
                  batch proving, and approximate gradient calculation, and (iii) we experimentally show that NTPs can
                  learn representations of symbols and function-free ﬁrst-order rules of predeﬁned structure, enabling
                  them to learn to perform multi-hop reasoning on benchmark KBs and to outperform ComplEx [7], a
                  state-of-the-art neural link prediction model, on three out of four KBs.
                  2  Background
                  In this section, we brieﬂy introduce the syntax of KBs that we use in the remainder of the paper.
                  Werefer the reader to [27, 28] for a more in-depth introduction. An atom consists of a predicate
                  symbol and a list of terms. We will use lowercase names to refer to predicate and constant symbols
                  (e.g. fatherOf and BART), and uppercase names for variables (e.g. X,Y,Z). As we only consider
                  function-free ﬁrst-order logic rules, a term can only be a constant or a variable. For instance,
                  [grandfatherOf,Q,BART] is an atom with the predicate grandfatherOf, and two terms, the
                  variable Q and the constant BART. We consider rules of the form H :– B, where the body B is a
                  possibly empty conjunction of atoms represented as a list, and the head H is an atom. We call a rule
                  with no free variables a ground rule. All variables are universally quantiﬁed. We call a ground rule
                  with an empty body a fact. A substitution set ψ = {X /t ,...,X /t } is an assignment of variable
                                                   1 1     N N
                  symbols Xi to terms ti, and applying substitutions to an atom replaces all occurrences of variables
                  Xi by their respective term ti.
                  Given a query (also called goal) such as [grandfatherOf,Q,BART], we can use Prolog’s backward
                  chaining algorithm to ﬁnd substitutions for Q [8] (see appendix A for pseudocode). On a high level,
                  backward chaining is based on two functions called OR and AND. OR iterates through all rules
                  (including rules with an empty body, i.e., facts) in a KB and uniﬁes the goal with the respective
                  rule head, thereby updating a substitution set. It is called OR since any successful proof sufﬁces
                  (disjunction). If uniﬁcation succeeds, OR calls AND to prove all atoms (subgoals) in the body of
                  the rule. To prove subgoals of a rule body, AND ﬁrst applies substitutions to the ﬁrst atom that is
                  then proven by again calling OR, before proving the remaining subgoals by recursively calling AND.
                  This function is called AND as all atoms in the body need to be proven together (conjunction). As
                  an example, a rule such as [grandfatherOf,X,Y] :– [[fatherOf,X,Z],[parentOf,Z,Y]] is used
                                                 2
