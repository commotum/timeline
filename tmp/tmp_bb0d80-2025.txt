                                      ARC-NCA:TowardsDevelopmentalSolutionstothe
                                                    Abstraction and Reasoning Corpus
                                    1                   1                           2                              3                             1,4
              Etienne Guichard , Felix Reimers , and Mia Kvalsund , and Mikkel Lepperød , and Stefano Nichele
                                                         1Østfold University College, Halden, Norway
                                                              2University of Oslo, Oslo, Norway
                                                         3Simula Research Laboratory, Oslo, Norway
                                                         4Oslo Metropolitan University, Oslo, Norway
                                                                    stefano.nichele@hiof.no
                                         Abstract
                 TheAbstractionandReasoningCorpus(ARC),laterrenamed
                 ARC-AGI, poses a fundamental challenge in artificial gen-
                 eral intelligence (AGI), requiring solutions that exhibit robust
                 abstraction and reasoning capabilities across diverse tasks,
                 while only few (with median count of three) correct exam-
                 ples are presented. While ARC-AGI remains very challeng-
                 ing for artificial intelligence systems, it is rather easy for hu-
                 mans. ThispaperintroducesARC-NCA,adevelopmentalap-
                 proach leveraging standard Neural Cellular Automata (NCA)          Figure 1: Example ARCtask, adapted from (Chollet, 2019).
                 and NCAenhanced with hidden memories (EngramNCA) to
                 tackle the ARC-AGI benchmark. NCAs are employed for
                 their inherent ability to simulate complexdynamicsandemer-         ”input grid,” which is a rectangular array of cells with vary-
                 gent patterns, mimicking developmental processes observed          ing dimensions (up to 30 rows by 30 columns), where each
                 in biological systems. Developmental solutions may offer a
                 promising avenue for enhancing AI’s problem-solving capa-          cell holds one of ten distinct ”values,” and an ”output grid,”
                 bilites beyond mere training data extrapolation. ARC-NCA           which can be entirely derived from the attributes and struc-
                 demonstrates how integrating developmental principles into         ture of the input grid. One of such tasks is depicted in Figure
                 computational models can foster adaptive reasoning and ab-         1. The purpose is to examine the example pairs to grasp the
                 straction. We show that our ARC-NCA proof-of-concept re-           nature of the problem and utilize this understanding to pro-
                 sults may be comparable to, and sometimes surpass, that of
                 ChatGPT4.5,atafraction of the cost.                                ducethecorrespondingoutputgridforeachgiventestinput.
                                                                                    Two attempts can be performed for each input grid. Each
                 Data/Code:        https://github.com/etimush/                      task is handcrafted by human designers with a unique logi-
               ARC_NCA                                                              cal structure, making it very difficult to prepare for each task
                                                                                    in advance. Such emphasis on few-shot learning and the ne-
                 Videos:            https://etimush.github.io/                      cessity for broad generalization make ARC-AGIparticularly
               ARC-NCA-Videos/                                                      demanding for current AI systems.
                                                                                       In contrast, humans excel at these tasks, leveraging innate
                 Submission type: Full Paper                                        cognitive abilities to discern patterns and apply abstract rea-
                                                                                    soning with minimal examples. This disparity underscores
                                                                                    a fundamental gap in current AI methods and highlights the
         arXiv:2505.08778v1  [cs.AI]  13 May 2025
                                     Introduction                                   need for novel approaches.
                                                                                       Onepromising avenue lies in the realm of developmental
               Progress towards artificial general intelligence (AGI) neces-        computation, inspired by the processes observed in living
               sitates benchmarksthatrigorouslyassessanagent’scapacity              systems. Neural Cellular Automata (NCA) (Gilpin, 2019;
               for abstraction, generalization, and reasoning. The Abstrac-         Mordvintsev et al., 2020; Nichele et al., 2017), exemplify
               tion and Reasoning Corpus (ARC), introduced by (Chollet,             this approach. NCAs are computational models where each
               2019), is one of such benchmarks. It comprises a collec-             cell on a lattice updates its state based on local interac-
               tion of visual pattern transformation tasks, each defined by         tions governed by neural networks, leading to the emer-
               a few input-output examples, challenging AI models to in-            gence of complex global patterns. As such, NCAs have
               fer the underlying transformation rules and apply them to            been used as models of biological morphogenesis (Ran-
               novel instances. Test pairs consist of two components: an            dazzo and Mordvintsev, 2023; Stovold, 2023; Pontes-Filho
              Figure 2: Diagram depicting one pass of the Growing NCA update step and its neural network model. Adapted from (Mordv-
              intsev et al., 2020).
              et al., 2022; Sudhakaran et al., 2021), where local cellular       mark. To the best of our knowledge, this is the first time
              interactions give rise to organized structures during devel-       NCAsare used for the 2D ARC-AGI benchmark. Engram-
              opment (such as bodies and brains). Furthermore, biolog-           NCAischosen,inadditiontostandardNCAs,becauseitre-
              ical brains employ cognitive mechanisms that may mirror            lies on mechanismsforlearninglow-levelmorphologiesand
              developmental processes to facilitate reasoning, abstraction,      manipulations first, and then a regulation mechanism for de-
              and problem-solving through dynamic, iterative, and self-          ciding when and where such primitives should be activated
              organizing processes. Examples include iterative refinement        and propagated, which is considered a suitable mechanism
              of mental schemas through interactions with the environ-           for abstraction and reasoning tasks. By emulating the princi-
              ment (McVee et al., 2005; Neumann and Kopcha, 2018),               ples of biological development and cognitive development,
              hierarchical structuring to break down tasks in sub-tasks          our models aim to capture essential aspects of human-like
              (Botvinick et al., 2009; Meunier et al., 2009), and predic-        abstraction and reasoning. Our ARC-NCA approach may
              tive modeling to anticipate outcomes and proactively adjust        be considered a program synthesis approach, where a cus-
              solutions (Friston, 2003; Seth, 2014; Millidge et al., 2021).      tom NCA (a ”program”) is generated for the task at hand
              The hypothesis tested in this work is whether the develop-         with a fine-tuning process akin to test-time training. Our
              mental nature of NCAs makes them particularly suited for           proof-of-concept demonstrates that ARC-NCA may reach
              tasks like those in the ARC-AGI benchmark.                         performances comparable, and sometimes superior, to ex-
                 In the last years, most approaches for ARC-AGI relied           isting models (including ChatGPT 4.5, see results and dis-
              on discrete program search, a brute force methodology. Re-         cussion section for details), but with significantly reduced
              cently, Large Language Models (LLMs) have been utilized            computational resources. We hope that our work will spark
              in different ways, including for optimizing domain-specific        a renewed interest within the artificial life community for
              languages (Chollet et al., 2024). Further, LLMs have been          radically new approaches to abstraction and reasoning.
              used for program synthesis with the intention of generating                            Related Works
              programs in general-purpose languages, e.g., Python, that
              attempt to solve the task at hand. Test-time training, also        Theapplication of cellular automata (CA) models, and mor-
              known as inference-time fine-tuning, has been rather popu-         phogenetic models in general (Wolfram, 1997), to the ARC-
              lar in the last few months to allow inference-timeadaptations      AGI benchmark (Chollet, 2019) remains an underexplored
              based on unseen test samples. Often, hybrid approaches,            area. However, several developments in CA research sug-
              including program synthesis and transductions, i.e., direct        gest potential avenues for applying CA methodologies to
              prompting an LLM, have been combined. However, solving             ARCtasks. Inparticular, one architectural choice that opens
              the ARCisstill an open problem and the solution might still        up opportunities for learning CA rules is Neural Cellular
              lie in uncharted areas of model selection.                         Automata (Gilpin, 2019; Mordvintsev et al., 2020; Nichele
                 In this paper, we introduce ARC-NCA, a novel approach           et al., 2017), where a neural network replaces more tradi-
              that leverages the developmental dynamics of standard Neu-         tional CA lookup tables. NCA was proposed as a possible
              ral Cellular Automata (Mordvintsev et al., 2020) and an en-        embodied controller by (Variengien et al., 2021), where an
              hanced variant with hidden memory states, termed Engram-           NCA was connected to a reinforcement learning environ-
              NCA(Guichardetal.,2025),totackletheARC-AGIbench-                   ment in a closed loop, thus demonstrating a self-organising
              ”brain”. Another interesting line of research is aiming at      an unmodified version of EngramNCA, EngramNCA v2,
              critical NCAs (Pontes-Filho et al., 2023; Guichard, 2024),      v3, and v4, modified versions of EngramNCA with ARC-
              i.e., CA models operating at the edge-of-chaos (Langton,        specific augmentations.
              1990), which could be a powerful pre-training strategy. A          WebelievethestandardNCAmodelneedsnodetailedin-
              NCA for image manipulation, named Vision Transformer            troduction. In short, it is implemented as a differentiable
              Cellular Automata (ViTCA), is proposed in (Tesfaldet et al.,    neural network embedded in a cellular automaton frame-
              2022), where attention heads are included in the model, in-     work, where each cell maintains a continuous state vec-
              spired by the transformer architecture (Vaswani et al., 2017).  tor updated through convolutional neural networks (CNNs)
              (Reimers et al., 2023) proposes a variation with local-self     with learned local update rules. The architecture is depicted
              attention, while (Kvalsund et al., 2024) presents an evolved    in Figure 2. However, EngramNCA is a relatively recent
              attention-like mechanism. In general, transformers can learn    model and thus warrants a brief introduction. Its NCA fea-
              elementary CA rules (Burtsev, 2024), opening up interest-       tures dual-state cells with distinct public (interaction-based)
              ing opportunities of potentially combining CA and LLMs          and private (memory-based) states. The model is an en-
              for ARC-AGI in the future. A work using an evolutionary         semble that includes: GeneCA, an NCA which generates
              approach is (Fischer et al., 2020), where grammatical evolu-    morphological patterns from a seed cell encoding genetic
              tion is employed for optimizing expressions in a domain-        primitives (Figure 3); GenePropCA, an NCA which propa-
              specific language for incremental image transformations.        gates and activates these genetic primitives across the cell
              AnacceleratedJAXimplementationofCA,includingNCA,                network (Figure 4), similar to RNA-based communication
              is proposed in (Faldor and Cully, 2024), where they also        (Shomrat and Levin, 2013). EngramNCA is trained in two
              attempt to employ a 1D-NCA for solving the much sim-            stages: first, GeneCA is trained to grow primitive morpholo-
              pler 1D-ARC dataset (Xu et al., 2023), an unofficial sim-       gies containing immutable private memory encodings, using
              plified adaptation of ARC-AGI composed of 1-dimensional         only publicly visible channels for coordination; then, Gene-
              rows of pixels, which significantly reduces task complex-       PropCA is trained to modulate the private memory of cells
              ity. For a recent report on popular attempts at solving         without altering their visible states, enabling the transfer of
              the ARC-AGIchallenge,includingprogramsynthesismeth-             genetic information across the grid. For details on the model
              ods with deep learning techniques, please refer to (Chollet     see (Guichard et al., 2025).
              et al., 2024). Very recently, in April 2025, OpenAI has an-
              nouncedthattheirmostpowerfulmodelsatthattime,named                               CAArchitectureDetails
              o3 and o4 mini (two reasoning models using support to-            CAtype             Augmentations      Channels, Hid-
              kens for planning and summoning internal tokens to run                                                  denSize
              Python code as part of their reasoning, before providing an       NCA                None (standard     50, 64
              answer), achieved promising scores in ARC-AGI (Chollet,                              NCA)
              2025;Kamradt,2025). Specifically, o3-lowscored41%,o3-             EngramNCAv1        None (standard     50, (32,32)
              medium53%,o4-mini-low21%,ando4-mini-medium41%,                                       EngramNCA)
              all on the semi-private evaluation set. Additionally, two o3      EngramNCAv2        Sensing            50, (32,32)
              versions tested with high compute resources (namely using         EngramNCAv3        Sensing        +   50, (32,32)
              6 and 1024 independent inference samples) scored 75.7%                               Toroidal
              and 87.5%, using 33.5 million and 5.7 billion tokens. The         EngramNCAv4        Sensing        +   50, (32,32)
              reported cost for the version with 6 inference samples was                           Toroidal + Lo-
              201USDpersample,whiletheversionwith1024was172x                                       cal vs Global
              more expensive. This staggering cost might be significantly
              reduced by alternative architectures.                           Table 1: Architecture detail for all CA variants. The differ-
                                                                              ent notations for NCA and EngramNCA on Channels, Hid-
                              ModelsandMethods                                den Size are due to the split versus standard architecture be-
              This section details the models used in obtaining develop-      tween the two.
              mental solutions to the Abstraction and Reasoning Corpus.
              Wechiefly explore NCA models and their derivatives in the          Table 1 shows the different CA architectures. The aug-
              form of classic NCA and EngramNCA (and modifications            mentations are detailed in sections Local versus Global So-
              to EngramNCA).                                                  lutions, Toroidal versus Non-Toroidal Problems, and Inap-
              NCAmodels                                                       propriate Sensing.
              WechoosetotesttheGrowingNCAaspresentedby(Mord-                  FromARCtoNCASpace
              vintsev et al., 2020), along with four versions of Engram-      The ARC dataset mainly comprises 2D grids with integer
              NCApresentedin(Guichard et al., 2025): EngramNCA v1,            values. Each grid can range from 1x1 to 30x30 in size, with
              Figure 3: Diagram depicting one pass of the EngramNCA GeneCA update step and its neural network model. Adapted from
              (Guichard et al., 2025).
              Figure 4: Diagram depicting one pass of the EngramNCA GenePropCA update step and its neural network model. Adapted
              from (Guichard et al., 2025).
              values ranging between 0 and 9.                                    in an HSL(hue, saturation, and lightness) color spectrum,
                We address two major issues with transforming ARC                starting with black for 0.
              grids into NCA-compatible ones:                                    We then transform the ARC problems into an RGB −
              From 2D Integer grid to 3D real-valued lattice.      NCA        αlattice using an integer-to-HSL-to-RGB − α conversion
              mainly operate on 3D lattices of dimensions H,W,C,              equation:
              where H is the height, W is the width, and C is the number
              of channels, most commonly four channels for RGB − α                    h= v ×360 (Huecalculation)                      (1)
              values of an image, and an arbitrary amount of hidden chan-                 n
              nels. To transform the ARC grids into NCA lattices, we first            l = 0.5   (Fixed Lightness)                     (2)
              assume two conditions:                                                  s = 0.8   (Fixed Saturation)                    (3)
              • Constant α: all colors represented by the 10 integers have           C=(1−|2l−1|)×s×(v>0) (Chroma)                    (4)
                the same alpha value of 1                                                      C
              • Equalspacing: all10colors(0-9)areequallyspacedapart                 M=(l− 2)×(v>0).                                   (5)
                 Here, v is the integer value in that grid position, and n is        ARCSpecificAugmentations
               the total number of colors.                                           The ARC dataset provides some specific challenges that
                                                                                   NCAcanhavedifficulties dealing with, one such challenge
                                                               
                                           h                                       was discussed in From ARC to NCA Space. However, we
                                                                
                        X=c× 1−                  (mod 2) −1               (6)
                                     60                                           also identified three other challenges:
                                                                                     Toroidal versus Non-Toroidal Problems           - In general,
                                             ◦           ◦
                                     C if0 ≤h<60
                                                                                    NCAoperates on a toroidal lattice. While this is desirable
                                   
                                              ◦             ◦
                                   
                                                                                    for tasks such as growingmorphologies,asitmeansthemor-
                                     X if60 ≤h<120
                             R′ =              ◦             ◦                      phology is positionally invariant, it causes issues in ARC-
                                     X if240 ≤h<300
                                   
                                               ◦             ◦                      AGI problems where absolute positions and grid edges are
                                   
                                     C if300 ≤h<360
                                   
                                   
                                                                                    a necessary part of the reasoning. Disabling this behavior is
                                   
                                     0    otherwise                                  also not a reasonable option, as some ARC-AGI problems
                                             ◦           ◦                          becomeeasier to solve if information propagates toroidally.
                                   X if0 ≤h<60
                                   
                                   
                                              ◦             ◦                         Weremedy this in EngramNCA v3 and EngramNCA v4
                                   
                                   C if60 ≤h<120
                               ′               ◦             ◦                      in two ways: by splitting the functionality of the GeneCA
                             G =X if120 ≤h<180                                      and GenePropCA.Theformeractsonanon-toroidallattice,
                                               ◦             ◦
                                   X if180 ≤h<240
                                                                                    while the latter acts on a toroidal lattice, and by giving each
                                   
                                   
                                   0     otherwise                                  cell channel-wise local self-attention.
                                               ◦             ◦                        The hypothesis is that by splitting the functionality and
                                   X if120 ≤h<180
                                   
                                                                                    imbuing it with attention, the EngramNCA might be able to
                                               ◦             ◦
                                   
                                   C if180 ≤h<240
                             B′ =              ◦             ◦                      choose whether or not it exhibits toroidal functionality.
                                   C if240 ≤h<300
                                               ◦             ◦
                                   X if300 ≤h<360                                   Local versus Global Solutions      - Another problem comes
                                   
                                   
                                   
                                   0     otherwise                                  in the form of whether the NCA should focus on global or
                                                                                     local information when solving ARC problems (or a mix of
                                                                                     the two). This should, in theory, not be a problem. However,
                                 R=(R′+M)×255                             (7)        we qualitatively observe that some problems struggle with
                                          ′                                          fine-grained local information.
                                 G=(G +M)×255                             (8)          We introduce a patch training scheme to force the NCA
                                 B=(B′+M)×255                             (9)        to focus on local information. This scheme works on the
                                 α=255∗(v>0)                             (10)        same principle as the standard way of NCA training, with
                 We extend the channel dimension of the RGB − α lat-                 the key difference being that the NCA is trained on- and loss
               tice with a binary encoding for each pixel based on its color.        is accumulated over- 3x3 patches of the grid, instead of the
               Wefinally pad the channel dimension with ones to reach the            entire grid at once. Since this is an augmentation to NCA
               desired number of hidden channels.                                    training, it becomes more costly to train the NCA, thus, we
                                                                                     choose to only use this augmentation in EngramNCA v4.
               Dealingwithchanginggridsizes.          CertainARCproblems
               containsolutionswhosegridsizediffersfromtheinput. This                Inappropriate Sensing       -  Due to NCA’s initial appli-
               presents a pernicious problem in that NCAs cannot modify              cations being the simulation of the growth of organisms,
               their grid size. To deal with this, we explore two methods:           the sensing mechanisms somewhat mimic biological cells’
                                                                                     chemosensing mechanisms, in the form of gradient sensing
               • Ignore the problematic grids: Remove them from the                  kernels. While a helpful analogy, this might present a funda-
                 training procedure.                                                 mental limitation for the purposes of ARC. To combat this,
               • Maximal size padding: Pad every problem to the max-                 wechoosetoaugmentEngramNCAv3andEngramNCAv4
                 imal 30x30 grid size with a special padding value, one              with fully learnable sensing filters in place of the Sobel and
                 uniquely only found in the padded areas, and allow the              Laplacian filters. The number of filters was chosen to match
                 NCAtomodifytheamountofpadding.                                      that of the standard EngramNCA.
                 Due to computational constraints, we choose to mainly                                        Training
               focus on ignoring the problematic grids.       However, Fur-          Determining the Quality of Solutions
               ther Experiments details the experiments done with Maxi-
               mal size padding. All results will be reported on the 262             Duringtraining,theNCAeffectivelyproducesanimage. We
               problems that do not require resizing.                                ostensibly do not consider the developmental steps the NCA
                                                                                                     The GeneCA model iteratively updates the state.
                                                                                                                                                                         Visible channels of the final state are compared against 
                                                                                                   Followed by the GenePropCA model doing the same.
                                                                                                                                                                    the target ARC grid to compute the mean square error to be used 
                                                                                                  Backprop-through-time updates both sets of parameters.                            as the loss for backpropagation.
                                                                                                                                                                                The new state is added back to the pool.
                                                                                                                  GeneCA Parameters
                                                          The initial state is sampled 
                                                         without replacement from a            Step 1                Step 2                       Step N
                                                                 pool of states
                                                                                                                                                                                                     Target Image
                                                                           .                Stage 1    Stage 2    Stage 1    Stage 2           Stage 1    Stage 2                          L2 Loss                                  Pool
                                                                                                               GenePropCA Parameters
                                                             Figure 5: One backpropagation step of training EngramNCA for solving ARC problems.
                          takes to reach the final solution. Thus, we take the loss to be                                                                                                             Results
                          MSEPixelWiseLossasin(Mordvintsevetal.,2020).                                                                                 General Results
                               Todetermine whether a problem is solved, we look at the                                                                 In this section, we present the results of each CA in the form
                          mean pixel error across the generated NCA. An evaluation                                                                     of Mean log(loss) and the CA solve rate. The same results
                          loss of log(MSEPixelWiseLoss) ≤ −7, where this loss                                                                          were obtained for the union of different CA. As a reminder
                          wasevenlydistributedamongpixels,wasexperimentallyde-                                                                         to the reader, two answers may be submitted when solving
                          termined to produce exact solutions to the ARC problems.                                                                     ARC; thus, by taking the union (each model produces one
                                                                                                                                                       output) we still produce a valid submission.
                          ModelTraining
                          We choose to solve ARC via test-time training. As stated                                                                                                                    CAResults
                          by (Chollet, 2019; Chollet et al., 2024), program generators                                                                    Model                                     Meanlog(loss)                        Solve Rate
                          must be able to learn from new information. We take this                                                                        NCA                                       -4.31                                10.7%
                          to mean that our program generator, the system that trains                                                                      EngramNCAv1                               -3.63                                6.5%
                          NCAs,cantrainanewCAperproblem. Foreveryproblem,                                                                                 EngramNCAv2⋆                              -4.03                                9.2%
                          wetrainanewCAfromscratchonthe2-3trainingexamples                                                                                EngramNCAv3                               -4.35                                12.9%
                          and evaluate its performance on the unseen sample. All our                                                                      EngramNCAv4                               -4.20                                10.3%
                          experiments are run on the ARC-AGI public evaluation set.                                                                       Chat GPT4.5⋆⋆                             N/A                                  10.3%
                               Figure 5 shows one training iteration of the training pro-                                                              Table 2: Mean log(loss) and solve rate for all four CA vari-
                          cedure for EngramNCA versions. The training procedure                                                                        ations. The best results are highlighted in green, and the
                          mirrors that of (Guichard et al., 2025) with one key modifi-                                                                 worst results are highlighted in red. ⋆Due to space con-
                          cation. Due to training both the GeneCA and GenePropCA                                                                       straints in the paper, and the fact that the results are very
                          from scratch for each problem, the GeneCA weights are                                                                        similar to EngramNCA v4, we omit EngramNCA v2 from
                          not frozen, and both sets of weights are co-optimized. The                                                                   manyoftheresultdiscussions, including the unions of mod-
                          standard NCA was instead trained with the same procedure                                                                     els. ⋆⋆TheresultsforChatGPT4.5aretakenfromtheARC-
                          shownin(Mordvintsev et al., 2020).                                                                                           AGIleaderboard (ARC Team, 2025). Note that such results
                                                                                                                                                       were obtained on the ARC-AGI private evaluation set, in-
                                                                                                                                                     stead of the public evaluation set as for our results.
                                                                         P P P                                                      2
                                                                  1          H       W        C                       ˆ
                               PixelWiseMSE = H×W×C                          i=0     j=0      k=0 I(i,j,k)−I(i,j,k)(11)
                                                                                                                                                            Table 2 shows the mean loss(log) and solve rate for each
                               WhereH,W,Carethedimensionsoftheimage,I isthe                                                                            CA.EngramNCAv3performsbestinbothcategorieswitha
                                                                   ˆ
                          reference image, and I is the final state of the NCA.                                                                        near 13% solve rate. In contrast, EngramNCA v1 performs
                               We use AdamW as the optimizer, with a learning rate                                                                     the worst in both metrics, with a solve rate of 6.5%.
                          (LR) of 1e − 3. For each problem, the CA are trained for                                                                          Table 3 shows the cost comparison between the CA mod-
                          3000 iterations, with a 66% reduction in LR at 2000 itera-                                                                   els we experimented with and Chat GPT 4.5. We chose to
                          tions.                                                                                                                       comparetoChatGPT4.5asithassolveratessimilartoours
                                 ModleCost/Task
                Model                      Cost ($/Task)
                NCA                        ≈3e-4
                EngramNCAv1                ≈3e-4
                EngramNCAv2                ≈4e-4                                 (a) Example solution generated by the standard NCA.
                EngramNCAv3                ≈4e-4
                EngramNCAv4                ≈5e-4
                Chat GPT4.5⋆               0.29
             Table 3: Cost per task for each model. We calculate the
             estimated cost of our models on an NVIDIA RTX 4070 Ti                     (b) Two training pairs used for training.
             by taking the average time and power usage (W) to train
             a task for each model and multiplying it by the cost/kWh       Figure 6: An example of solution generated by standard
             in our area ($0.37/kWh). Chat GPT 4.5 cost taken from          NCAandrelativetraining pairs.
             (ARCTeam,2025).
             and is one of the most popular LLMs. At roughly the same
             performance, we see a 1000x decrease in cost.
                                 CAUnionResults                                 (a) Example solution generated by the EngramNCA v1.
                CA                      Mean            Solve Rate
                                        log(loss)
                NCA∪v1                  -3.97           13.7%
                NCA∪v3                  -4.32           14.8%
                NCA∪v4                  -4.25           13.7%                          (b) Two training pairs used for training.
                v1∪v3                   -3.98           15.3%
                v3∪v4                   -4.27           14.8%               Figure7: AnexampleofsolutiongeneratedbyEngramNCA
                v1∪v4                   -3.92           12.5%               v3andrelative training pairs.
                NCA∪v1∪v3∪v4            -4.12           17.6%
                  Table 4: Mean log(loss) and solve rate for unions.        below. Such a solution grows correctly in an incremental
                Table 4 shows the mean log(loss) and solve rate for six     manner by the NCA, which generalizes to unseen y coordi-
             unions of the CA. In this case, the union of EngramNCa v1      nates.
             and EngramNCA v3 performs best for the solve rate, with          Figure 7 shows an example of one of the solutions pro-
             a 15.3% solve rate. Effectively, half of the EngramNCA v1      duced by the EngramNCA v1 model, the standard version
             solutions were not found in EngramNCA v3. All unions           of EngramNCA. This problem presents horizontal and ver-
             perform roughly equal or better than the single best model,    tical lines (of different color in different examples) crossing
             indicating that all models have some non-overlapping prob-     and therefore composing constrained spaces in the middle
             lems they can solve. NCA and EngramNCA v3 performed            andopenspacesontheoutside. Thecorrectsolutionfillsthe
             bestformeanloss(log),whichistobeexpectedastheyboth             closed and open parts with given colors. The CA solution
             had the lowest mean losses. EngramNCA v1 and Engram-           grows cells of green color to fill the entire space; however
             NCAv4performedworstinbothcategories.                           when they are surrounded by boundaries they are able to
                                                                            change to the right color.
             Solved Problems                                                  Figure 8 shows an example of one of the solutions pro-
             Inthissection, wehighlightonesolvedproblemperCAtype            duced by the EngramNCA v3 model. In this test, the input
             to showthedevelopmentalstepstheCAmodelstaketosolve             containssinglepixelsandthecorrectsolutionconnectsthose
             ARCproblems. Morevideoexamplescanbefoundhere.                  on the same horizontal or vertical line. The CA grows lines
                Figure 6a shows an example of one of the solutions pro-     fromthepixels and sometimes overshoots after the connect-
             duced by the NCA model, while Figure 6b shows the two          ing pixel; however it is able to remove the parts not needed
             training examples. In this problem, a line of a given length   that reach the boundaries.
             is presented in a random y coordinate and the correct solu-      Figure 9 shows an example of one of the solutions pro-
             tion corresponds to adding green lines of increasing length    duced by the EngramNCA v4 model. This test contains a
             above the input line and orange lines of decreasing length     single vertical line on the left side of the grid. The correct
                                                                                              CAResultswithLoosenedThreshold
                                                                                     Model                         Solve Rate
                                                                                     NCA                           15.6%
                                                                                     EngramNCAv1                   9.9%
                   (a) Example solution generated by the EngramNCA v3.               EngramNCAv2                   11.8%
                                                                                     EngramNCAv3                   16.4%
                                                                                     EngramNCAv4                   16.8%
                                                                                  Table 5: Solve rate for CA modes when loss threshold is
                                                                                  loosened to -6
                           (b) Two training pairs used for training.                                    CAUnionResults
              Figure8: AnexampleofsolutiongeneratedbyEngramNCA                      CA                   Meanlog(loss)       Solve Rate
              v3andrelative training pairs.                                         NCA∪v1               -3.97               18.3%
                                                                                    NCA∪v3               -4.32               20.2%
                                                                                    NCA∪v4               -4.25               20.2%
                                                                                    v1∪v3                -3.98               18.7%
                                                                                    v3∪v4                -4.27               20.9%
                                                                                    v1∪v4                -3.92               19.8%
                   (a) Example solution generated by the EngramNCA v4.              NCA∪v1∪v3            -4.12               24%
                                                                                    ∪v4
                                                                                  Table 6: Mean log(loss) and solve rate for all six CA union
                                                                                  variations with loosened loss threshold.
                           (b) Two training pairs used for training.              solution produced by EngramNCA v3. We can see that the
              Figure9: AnexampleofsolutiongeneratedbyEngramNCA                    model has the general concepts to solve the problem cor-
              v4andrelative training pairs.                                       rectly. However, three pixels are miscolored in regions with
                                                                                  muchopenspace. Thisindicatesanedgecasethatwasprob-
                                                                                  ably absent in the training set. Figure 11 shows an example
              solution grows a horizontal line on the bottom and a diago-         of a near solution produced by EngramNCa v1. In this ex-
              nal line from the bottom left corner to the top right corner.       ample, the model produces an exact solution at some point.
              The CA grows a solution that crosses the toroidal boundary          However, due to the general asynchronous nature of NCA,
              and grows from both corners which eventually connects in            weletthemodelrununtilitendsinastablestate. Thisstable
              the middle. Solutions generalize to different grid sizes.           state is off by one pixel.
              AlmostSolvedProblems                                                Reasoning Pitfalls
              ARC-NCAhave the ability to produce partial solutions, or            Occasionally, we observe problems where the models (qual-
              ”almost solved” problems. These solutions typically have            itatively) manages some of the reasoning steps necessary to
              a few pixels wrong (or slightly wrong) but could serve as           solve a particular problem, but fall short of a perfect com-
              the basis for further refinement. It is also possible that these    pletion. In this section we showcase some of the model-
              few mistakes can be removed with improvements to the ar-            problem pair and attempt to reason about what reasoning
              chitecture or simply by increasing the size of the NCA. To          pitfalls they might have encountered.
              determine what sort of performance we would obtain by fo-              Figure 12 depicts an example of a partial reasoned solu-
              cusing on the partial solutions, we loosen the loss threshold       tion produced by the EngramNCA v4 model. Here we can
              to −6. Table 5 shows the solve rate when the loss thresh-           see the model learns one of the two reasoning steps, that of
              old is loosened. The models solve anywhere from 2% - 6%             growing a pattern of the correct shape on the orange dots.
              more of the problems, indicating that there is potential for        However, it fails to generalise to any pattern on the left and
              muchbetter performance from relatively small adjustments.           gets the exact pixel colors wrong.
              Table 6 indicate the unions of the results from different CA        Further Experiments
              models.
                 We further analyze some examples from those with mi-             In this section, we detail the results of two further exper-
              nor mistakes next. Figure 10 shows an example of a near             iments conducted: Increasing the dimension of the hidden
                                                                                                    (a) Input         (b) True Solution     (c) EngramNCAv4
                        (a) Input         (b) True Solution    (c) EngramNCAv3
                                                                                                           (d) Two training pairs used for training.
                              (d) Two training pairs used for training.                      Figure 12: An example of partial reasoning success in a solution
                                                                                             of ARCgenerated by EngramNCAv4
                Figure 10: An example of a near solution produced by En-                                                 CAResults
                gramNCAv3                                                                        Model              Solve          Solve          Cost
                                                                                                                    Rate @-7       Rate @-6       ($/Task)
                                                                                                 EngramNCA          16.1%          19.8%          ≈5e-4
                                                                                                 v3
                                                                                                 EngramNCA          16%            27%            ≈7e-4
                                                                                                 v3Padded
                                                                                                 Chat     GPT       10.3%          10.3%          0.29
                                                                                                 4.5⋆
                        (a) Input         (b) True Solution    (c) EngramNCAv1               Table 8: Solve rate for larger EngramNCA v3 and its maxi-
                                                                                             mally padded version compared to Chat GPT 4.5. ⋆ For the
                                                                                             sake of neatness, Chat GPT 4.5 results are displayed on the
                                                                                             same table, even though they are not comparable through
                                                                                             Meanlog(loss).
                              (d) Two training pairs used for training.
                Figure 11: An example of a near solution produced by Engram-                    Table 8 shows the results of EngramNCA v3 and its max-
                NCAv1                                                                        imally padded version as compared to Chat GPT 4.5. By
                                                                                             increasing the hidden size, we can observe an increase in
                                                                                             the number of problems solved. Maximal padding increases
                layer of EngramNCA v3, and solving all ARC-AGI prob-                         the number of problems the CA has to solve, yet we do not
                lems by use of maximal padding as described in Dealing                       see a decrease in the percentage of problems the CA can
                with changing grid sizes.                                                    solve, suggesting that self-size modification is trivial for the
                                                                                             CA or that the extra information provided by the padding
                                    CAArchitectureDetails                                    tokens has helped with some of the problems. The maximal
                   CAtype                Augmentations          Channels, Hid-               padding does incur a cost as NCA memory usage and run
                                                                denSize                      time scale poorly with lattice size. Despite this, they both
                   EngramNCAv3           Sensing          +     50, (132,132)                still outperform Chat GPT 4.5. Leaving room for partial so-
                                         Toroidal                                            lutions, we see that the maximally padded version sees a
                   EngramNCAv3           Sensing          +     50, (132,132)                significant increase in its solve rate (27% versus 16%).
                                         Toroidal + Max-                                                     SummaryandDiscussion
                                         imal Padding
                                                                                             This work introduces ARC-NCA, a developmental frame-
                Table 7: Architecture detail for of EngramNCA v3 and its                     workutilizingNeuralCellularAutomatatoaddressthechal-
                maximally padded version.                                                    lenges posed by the Abstraction and Reasoning Corpus
                                                                                             benchmark, which requires robust abstraction and reason-
                   Table 7 shows the architecture details for larger Engram-                 ing capabilities derived from minimal training data. Our
                NCAv3anditsmaximallypaddedversion.                                           ARC-NCAmodelsexploit the intrinsic properties of NCAs
               to emulate complex, emergent behaviors reminiscent of bi-               the sake of a more rigorous investigation, multiple runs and
               ological developmental processes. We evaluated standard                 their stability should be investigated further. Additionally,
               NCAalongside several modified versions of EngramNCA,                    in order to compete in the official ARC-AGI leaderboard,
               whichwereaugmentedtobetteraccommodatespecificchar-                      solutions would have to be submitted for the semi-private
               acteristics of ARC tasks. These modifications encompassed               and private evaluation sets.
               enhanced sensing mechanisms, adjustments in local versus                  Future directions at the intersection of NCAs and LLMs
               global information processing, and strategies for managing              are considered promising avenues. For example, LLMs may
               toroidal lattice behaviors.                                             beusedtosuggestoptimizedNCAarchitecturalchoicesand
                  The results demonstrated that ARC-NCA models                         hyperparameters. Further, LLMs with reasoning abilities
               achieved solve rates comparable to, and sometimes surpass-              maybeusedaserrorcorrection mechanisms for the (almost
               ing, those of popular LLMs such as ChatGPT 4.5, notably                 correct) developmental solutions provided by NCAs. Other
               at significantly reduced computational costs.       When con-           correction mechanismsmayalsobeconsidered,forexample
               sidering partially correct solutions, success rates increased           relying on NCAs or other computer vision techniques.
               remarkably, indicating potential for further enhancements                 Finally, NCAs operating at an abstract, latent representa-
               such as architectural modifications and parameters scaling.             tion (Menta et al., 2024), may be able to capture basic prim-
               Analysis of solved and partially solved problems provided               itives beneficial for reasoning, by shifting the computation
               insights into the developmental nature of NCAs, revealing               from the input space to the latent space. This may be partic-
               strengths in iterative refinement and emergent reasoning ca-            ularly relevant for architectures as EngramNCA, which try
               pabilities. Conversely, examples of reasoning pitfalls high-            to capture basic primitives first, and then regulation mecha-
               lighted specific limitations in NCAs’ generalization capac-             nisms for their activation and communication.
               ities, particularly in handling fine-grained details or novel
               edge cases not well represented in training examples.                                      Acknowledgments
                  In light of the recent introduction of ARC-AGI-2 (Chol-              This work was partly funded by the priority area ”The Digi-
               let et al., 2025), which presents a more challenging bench-             tal Society” at Østfold University College.
               mark designed to assess AI systems’ adaptability and effi-
               ciency in acquiring new skills, including symbolic interpre-                                    References
               tation, compositional reasoning, and contextual rule appli-             ARC Team,        P.   (2025).        Arc    prize   leaderboard.
               cation, our findings hold particular relevance. ARC-AGI-2                    https://arcprize.org/leaderboard.
               tasks have been solved by humans in under two attempts,                 Botvinick, M. M., Niv, Y., and Barto, A. G. (2009). Hierarchically
               yet current AI systems struggle with single-digit success                    organized behavior and its neural foundations: A reinforce-
               rates. The developmental approach proposed by ARC-NCA                        ment learning perspective. cognition, 113(3):262–280.
               may provide an innovative perspective to tackling abstrac-
               tion and reasoning in AI systems through developmental                  Burtsev, M. (2024). Learning elementary cellular automata with
               processes governed by local interactions, or in combination                  transformers. arXiv preprint arXiv:2412.01417.
               with LLMs. We therefore encourage the artificial life com-              Chollet, F. (2019). On the measure of intelligence. arXiv preprint
               munity to tackle open problems in artificial intelligence.                   arXiv:1911.01547.
                                      Future Works                                     Chollet, F. (2025). Openai o3 breakthrough high score on arc-agi-
                                                                                            pub. https://arcprize.org/blog/oai-o3-pub-breakthrough.
               Besides ARC-AGI-2 as a natural follow up, we outline here               Chollet, F., Knoop, M., Kamradt, G., and Landers, B. (2024).
               several research directions that warrant further investigation.              Arc prize 2024:      Technical report.      arXiv preprint
                  A pre-training mechanism that could facilitate learning                   arXiv:2412.04604.
               each single problem from the few available examples would
               be beneficial. Such pre-training mechanism should provide               Chollet, F., Knopp, M., and Kamradt, G. (2025).       Arc-agi-2.
               knowledge at an abstraction level that is appropriate for the                https://arcprize.org/arc-agi#arc-agi-2.
               type of visual reasoning required for ARC, such as basic                Faldor, M. and Cully, A. (2024). Cax: Cellular automata acceler-
               transformations that can generalize across tasks followed                    ated in jax. arXiv preprint arXiv:2410.02651.
               by task-specific fine-tuning. Alternatively, a criticality pre-                                   ¨
               training could be an interesting direction. Criticality is a be-        Fischer, R., Jakobs, M., Mucke, S., and Morik, K. (2020). Solv-
               havioral regime that is know to be ideal for different kinds                 ing abstract reasoning tasks with grammatical evolution. In
               of computation. One hypothesis is that NCAs at criticality                   LWDA,pages6–10.
               wouldbebettersuitedforlearningARCtasksthanrandomly                      Friston, K. (2003). Learning and inference in the brain. Neural
               initialized NCAs.                                                            Networks, 16(9):1325–1352.
                  Ourresultsaredocumentedonsingletrials,asARCallow                     Gilpin, W. (2019). Cellular automata as convolutional neural net-
               submission of only two candidate solutions. However, for                     works. Physical Review E, 100(3):032402.
               Guichard,E.(2024). Criticallypre-trainedneuralcellularautomata       Shomrat,T.andLevin,M.(2013). Anautomatedtrainingparadigm
                    as robot controllers.                                                reveals long-term memory in planarians and its persistence
                                                                                         through head regeneration. Journal of Experimental Biology,
               Guichard, E., Reimers, F., Kvalsund, M., Lepperød, M., and                216(20):3799–3810.
                    Nichele, S. (2025). Engramnca: a neural cellular automaton
                    modelofmemorytransfer. arXivpreprintarXiv:2504.11855.           Stovold, J. (2023). Neural cellular automata can respond to signals.
                                                                                         In ALIFE 2023: Ghost in the Machine: Proceedings of the
               Kamradt, G. (2025).   Analyzing o3 and o4-mini with arc-agi.              2023 Artificial Life Conference. MIT Press.
                    https://arcprize.org/blog/analyzing-o3-with-arc-agi.            Sudhakaran, S., Grbic, D., Li, S., Katona, A., Najarro, E., Glanois,
               Kvalsund, M.-K., Ellefsen, K. O., Glette, K., Pontes-Filho, S., and       C., and Risi, S. (2021). Growing 3d artefacts and functional
                    Lepperød, M. E. (2024). Sensor movement drives emergent              machines with neural cellular automata.  In Artificial Life
                    attention and scalability in active neural cellular automata.        Conference Proceedings 33, volume 2021, page 108. MIT
                    bioRxiv, pages 2024–12.                                              Press One Rogers Street, Cambridge, MA 02142-1209, USA
                                                                                         journals-info ....
               Langton, C. G. (1990). Computation at the edge of chaos: Phase       Tesfaldet, M., Nowrouzezahrai, D., and Pal, C. (2022). Attention-
                    transitions and emergent computation. Physica D: nonlinear           based neural cellular automata. Advances in Neural Informa-
                    phenomena, 42(1-3):12–37.                                            tion Processing Systems, 35:8174–8186.
               McVee, M. B., Dunsmore, K., and Gavelek, J. R. (2005). Schema        Variengien, A., Nichele, S., Glover, T., and Pontes-Filho, S. (2021).
                    theory revisited. Review of educational research, 75(4):531–         Towards self-organized control: Using neural cellular au-
                    566.                                                                 tomata to robustly control a cart-pole agent. arXiv preprint
               Menta, A., Archetti, A., and Matteucci, M. (2024). Latent neural          arXiv:2106.15240.
                    cellular automata for resource-efficient image restoration. In  Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
                    ALIFE2024: Proceedings of the 2024 Artificial Life Confer-           Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). At-
                    ence. MIT Press.                                                     tention is all you need. Advances in neural information pro-
               Meunier, D., Lambiotte, R., Fornito, A., Ersche, K., and Bullmore,        cessing systems, 30.
                    E. T. (2009). Hierarchical modularity in human brain func-      Wolfram, S. (1997). New kind of science.
                    tional networks. Frontiers in neuroinformatics, 3:571.
                                                                                    Xu, Y., Li, W., Vaezipoor, P., Sanner, S., and Khalil, E. B. (2023).
               Millidge, B., Seth, A., and Buckley, C. L. (2021). Predictive cod-        Llms and the abstraction and reasoning corpus: Successes,
                    ing: a theoretical and experimental review. arXiv preprint           failures, and the importance of object-based representations.
                    arXiv:2107.12979.                                                    arXiv preprint arXiv:2305.18354.
               Mordvintsev, A., Randazzo, E., Niklasson, E., and Levin, M.
                    (2020). Growing neural cellular automata. Distill, 5(2):e23.
               Neumann, K. L. and Kopcha, T. J. (2018). The use of schema the-
                    ory in learning, design, and technology. TechTrends, 62:429–
                    431.
               Nichele, S., Ose, M. B., Risi, S., and Tufte, G. (2017). Ca-neat:
                    evolved compositional pattern producing networks for cellu-
                    lar automata morphogenesis and replication. IEEE Transac-
                    tions on Cognitive and Developmental Systems, 10(3):687–
                    700.
               Pontes-Filho, S., Nichele, S., and Lepperød, M. (2023). Critical
                    neural cellular automata.
               Pontes-Filho, S., Walker, K., Najarro, E., Nichele, S., and Risi,
                    S. (2022). A single neural cellular automaton for body-brain
                    co-evolution. In Proceedings of the Genetic and Evolutionary
                    Computation Conference Companion, pages 148–151.
               Randazzo, E. and Mordvintsev, A. (2023).      Biomaker ca: a
                    biome maker project using cellular automata. arXiv preprint
                    arXiv:2307.09320.
               Reimers,F.,Jain,S.,Shrestha,A.,andNichele,S.(2023). Pathfind-
                    ingneuralcellularautomatawithlocalself-attention. Zenoob.
               Seth, A. K. (2014). The cybernetic bayesian brain. In Open mind.
                    OpenMIND.FrankfurtamMain: MINDGroup.
