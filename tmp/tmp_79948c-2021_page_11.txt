           References
            [1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep
              convolutional neural networks. In Advances in Neural Information Processing Systems, pages
              1097–1105, 2012.
            [2] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale
              image recognition. In ICLR, 2015.
            [3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
              recognition. In CVPR, 2016.
            [4] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov,
              Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
              In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9,
              2015.
            [5] Mingxing Tan and Quoc V. Le. Efﬁcientnet: Rethinking model scaling for convolutional neural
              networks. ICML, 2019.
            [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
              LukaszKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,
              2017.
            [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
              deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
              2018.
            [8] TomBBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,
              Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
              few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
            [9] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.
              In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
              7794–7803, 2018.
           [10] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention
              augmented convolutional networks. In Proceedings of the IEEE/CVF International Conference
              onComputerVision, pages 3286–3295, 2019.
           [11] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish
              Vaswani. Bottleneck transformers for visual recognition. arXiv preprint arXiv:2101.11605,
              2021.
           [12] Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efﬁcient attention:
              Attention with linear complexities. In Proceedings of the IEEE/CVF Winter Conference on
              Applications of Computer Vision, pages 3531–3539, 2021.
           [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
              ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
              Animageisworth16x16words: Transformers for image recognition at scale. arXiv preprint
              arXiv:2010.11929, 2020.
           [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-
              scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
              recognition, pages 248–255. Ieee, 2009.
           [15] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable
              effectiveness of data in deep learning era. In Proceedings of the IEEE international conference
              oncomputervision, pages 843–852, 2017.
           [16] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
              Hervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv
              preprint arXiv:2012.12877, 2020.
           [17] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou.
              Going deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021.
           [18] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Qibin Hou, and Jiashi
              Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.
                               11
