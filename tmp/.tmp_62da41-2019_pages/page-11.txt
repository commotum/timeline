                       Ponder Net: Learning to Ponder
          of λp in the range (0, 1]. For ACT we sampled uniformly 19 values of τ in the range [2e-4,
          2e-2] and we added also 0, which correspond to not penalising the halting unit at all. For
          both ACT and Ponder, N was set to 20. For PonderNet β was ﬁxed to 0.01
          Appendix C. bAbI.
          C.1 Training and evaluation details
          For this experiment we used the English Question Answer dataset Weston et al. (2015). We
          use the training and test datasets that they provide with the following pre-processing:
           • All text is converted to lowercase.
           • Periods and interrogation marks were ignored.
           • Blank spaces are taken as word separation tokens.
           • Commas only appear in answers, and they are not ignored. This means that, e.g.
            for the path ﬁnding task, the answer ’n,s’ has its own independent label from the
            answer ’n,w’. This also implies that every input (consisting of ’query’ and ’stories’)
            corresponds to a single answer throughout the whole dataset.
           • All the questions are stripped out from the text and put separately (given as ”queries”
            to our system).
           Attraining time, we sample a mini-batch of 64 queries from the training dataset, as well
          as its corresponding stories (which consist of the text prior to the question). As a result, the
          queries are a matrix of 128×11 tokens, and sentences are of size 128×320×11, where 128
          is the batch size, 320 is the max number of stories, and 11 is the max sentence size. We pad
          with zeros every query and group of stories that do not reach the max sentence and stories
          size. For PonderNet, stories and query are used as their naturally corresponding inputs in
          their architecture. The details of the network architecture are described in Section C.2.
           After that mini-batch is sampled, we perform one optimization step using Adam Kingma
          and Ba (2014). We also performed a search on hyperparameters to train on bAbI, with
          ranges reported on Table 4. The network was trained for 2e4 epochs, each one formed by
          100 batch updates.
           For evaluation, we sample a batch of 10,000 elements from the dataset and compute
          the forward pass in the same fashion as done in training. With that, we compute the mean
          accuracy over those examples, as well as the accuracy per task for each of the 20 tasks of
          bAbI. We report average values and standard deviation over the best 5 hyper parameters
          we used.
           For MEMO the results were taken from Banino et al. (2020) and for Universal trans-
          former we used the results in Dehghani et al. (2018).
          C.2 Transformer architecture and hyperparameters
          WeusethesamearchitectureasdescribedinDehghanietal.(2018). Moreconcretely, weuse
          the implementation and hyperparameters described as ’universal transformer small’ that is
                               11
