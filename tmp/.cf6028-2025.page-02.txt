                           1   Introduction
                           Test-time scaling (TTS) has emerged as an effective method to enhance the reasoning capa-
                           bilities of large languagemodels(LLMs)byincreasinginference-timecompute. Anysound
                           TTSstrategy, by definition, will exhibit performance improvements as more compute is al-
                           located. The best TTS strategy to choose, however, remains an open question.
                           Early studies explored sequential scaling methods, either by artificially extending reason-
                           ing traces (Muennighoff et al., 2025) or by encouraging deeper exploration within a sin-
                           gle reasoning direction before switching, thereby mitigating “underthinking” (Wang et al.,
                           2025b). More recent analyses have questioned the benefits of such sequential extensions.
                           Notably, Gema et al. (2025) examined synthetic tasks designed to isolate specific reason-
                           ing abilities, such as counting with distractors, and regression with spurious correlations.
                           Their findings indicate that longer reasoning can reinforce incorrect behaviors, amplify
                           errors, and misalign reasoning paths, thereby degrading accuracy and even introducing
                           safety concerns.
                           Similarly, Hassid et al. (2025) proposed short-m@k, a parallel TTS technique where the final
                           prediction is obtained by majority voting among the m shortest reasoning traces, out of k
                           sampledoutputs. Their results support the idea that shorter, more concise reasoning often
                           outperformsextendeddeliberation.
                           Prior studies, while offering valuable insights, do not account for model variations and
                           rely on older reasoning models. In this work, we revisit these findings using more recent
                           models, including the GPT-OSS (OpenAI et al., 2025) and Qwen3 (Yang et al., 2025) series.
                           Ourfindings reveal that the relationship between compute and performance varies across
                           modelfamilies—adivergenceweattributetodifferencesintheirpost-training algorithms.
                           We argue that distinct post-training methods give rise to varying reasoning horizons.
                           Models with large horizons (large-horizon models) are able to sustain deeper reason-
                           ing by means of longer traces, thereby benefiting in performance on harder tasks where
                           greater thought is necessary. Short-horizon models, however, cannot generate long co-
                           herent traces, thereby making it most suitable for them to prioritize concise reasoning,
                           irrespective of problem difficulty.
                           As a consequence of their training dynamics, short-horizon models commonly emerge
                           from post-training with GRPO or GRPO-like algorithms, aligning with the well-
                           documented length bias introduced by GRPO (Yu et al., 2025). In contrast, long-horizon
                           models are typically produced by alternative reinforcement-learning methods that main-
                           tain stability over extended traces. For example, Qwen3—a long-horizon model—is post-
                           trained using GSPO rather than GRPO (Zheng et al., 2025). This observation supports our
                           hypothesis that the choice of post-training strategy plays a key role in determining a rea-
                           soning model’s effective horizon.
                           Overall, our work highlights the need for a model-aware perspective on TTS that accounts
                           for differences in training methodology, problem difficulty, and compute availability to
                           guideprincipled strategy selection.
                           2   Preliminaries
                           2.1  Test-time scaling methods
                           Test-timescalingstrategiesforLLMsvarywidely,typicallyfallingintoparallel,sequential,
                           hybrid/meta, and internal compute mechanisms (Figure 2). While each class of methods
                           showspromiseinspecificsettings, no single strategy is universally optimal.
                           Parallel scaling strategies improve performance by aggregating answers across multiple
                           independently sampled reasoning traces. Self-consistency (Wang et al., 2023) samples di-
                           verse reasoning paths and chooses the most frequent final answer, significantly improving
                           performance on arithmetic and symbolic tasks. Best-of-n sampling is widely used as a
                           simple parallel method (Snell et al., 2024), though more principled voting strategies like
                           majorityvoting(Lightmanetal.,2023),andMulti-AgentVerification(MAV)(Lifshitzetal.,
                           2025) have been recently proposed. Short-m@k (Hassid et al., 2025) exploits early stop-
                                                                          2
