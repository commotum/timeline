                                                         BENGIO,DUCHARME,VINCENTANDJAUVIN
                                                              n     h      m direct       mix     train.   valid.    test.
                                           MLP10              6   60     100      yes      yes                104     109
                                           Del. Int.          3                                               126     132
                                           Back-off KN        3                                               121     127
                                           Back-off KN        4                                               113     119
                                           Back-off KN        5                                               112     117
                         Table 2: Comparative results on the AP News corpus. See the previous table for the column labels.
                         5.1 AnEnergyMinimization Network
                         Avariant of the above neural network can be interpreted as an energy minimization model following
                         Hinton’s recent work on products of experts (Hinton, 2000). In the neural network described in the
                         previous sections the distributed word features are used only for the “input” words and not for the
                         “output” word (next word). Furthermore, a very large number of parameters (the majority) are
                         expanded in the output layer: the semantic or syntactic similarities between output words are not
                         exploited. In the variant described here, the output word is also represented by its feature vector.
                         The network takes in input a sub-sequence of words (mapped to their feature vectors) and outputs
                         an energy function E which is low when the words form a likely sub-sequence, high when it is
                         unlikely. For example, the network outputs an “energy” function
                                                       E(w        ,···,w )=v.tanh(d+Hx)+n−1b
                                                            t−n+1         t                          ∑ w
                                                                                                     i=0    t−i
                         where b is the vector of biases (which correspond to unconditional probabilities), d is the vector
                         of hidden units biases, v is the output weight vector, and H is the hidden layer weight matrix, and
                         unlike in the previous model, input and output words contribute to x:
                                                        x =(C(w ),C(w         ),C(w      ),···,C(w         ).
                                                                   t       t−1       t−2             t−n+1
                         TheenergyfunctionE(w               ,···,w )canbeinterpreted asanunnormalized log-probability forthe
                                                      t−n+1         t                                          ˆ      t−1
                         joint occurrence of (w          ,···,w ). To obtain a conditional probability P(w |w                ) it is enough
                                                   t−n+1         t                                                 t  t−n+1
                         (but costly) to normalize over the possible values of w , as follows:
                                                                                        t
                                                                                          −E(w       ,···,w )
                                                       ˆ                                 e      t−n+1    t
                                                       P(w |w      ,···,w        )=
                                                            t  t−1         t−n+1          −E(w       ,···,w ,i)
                                                                                      ∑e        t−n+1    t−1
                                                                                        i
                         Note that the total amount of computation is comparable to the architecture presented earlier, and
                         the number of parameters can also be matched if the v parameter is indexed by the identity of the
                         target word (w ). Note that only b           remains after the above softmax normalization (any linear
                                          t                        w
                                                                    t
                         function of the w      for i > 0 is canceled by the softmax normalization). As before, the parameters
                                             t−i                                                 ˆ
                         of the model can be tuned by stochastic gradient ascent on logP(w |w                ,···,w        ), using similar
                         computations.                                                               t   t−1        t−n+1
                             In the products-of-experts framework, the hidden units can be seen as the experts: the joint
                         probability of a sub-sequence (w            ,···,w ) is proportional to the exponential of a sum of terms
                                                               t−n+1         t
                         associated with each hidden unit j, vjtanh(dj +Hjx). Note that because we have chosen to de-
                         compose the probability of a whole sequence in terms of conditional probabilities for each element,
                                                                                1150
