                         Published as a conference paper at ICLR 2024
                         tasks are appealing as they pose challenging problems to LMs yet generated solutions can be easily
                         verified by running unit tests. However, existing coding benchmarks, such as HumanEval (Chen
                         et al., 2021), mostly involve self-contained problems that can be solved in a few lines of code.
                         In the real world, software engineering is not as simple. Fixing a bug might involve navigating a
                         large repository, understanding the interplay between functions in different files, or spotting a small
                         error in convoluted code. Inspired by this, we introduce SWE-bench, a benchmark that evaluates
                         LMsinarealistic software engineering setting. As shown in Figure 1, models are tasked to resolve
                         issues (typically a bug report or a feature request) submitted to popular GitHub repositories. Each
                         task requires generating a patch describing changes to apply to the existing codebase. The revised
                         codebase is then evaluated using the repository’s testing framework.
                         SWE-benchoffersseveraladvantagesoverexistingLMprogrammingbenchmarks. Theseinclude,a
                         realistic setting that utilizes user-submitted issues and solutions, diverse inputs featuring unique code
                         problems from 12 repositories, a robust framework for execution-based evaluation, and the ability
                         to continuously update the benchmark with new instances, requiring minimal human intervention.
                         Weevaluate multiple state-of-the-art LMs on SWE-bench and find that they fail to solve all except
                         the simplest issues. Using a BM25 retriever, Claude 2 is only able to resolve 1.96% of the issues.
                         In addition to SWE-bench our contributions include the release of a training dataset, SWE-bench-
                         train, which is essential for advancing open model development in this challenging domain. This
                         dataset comprises a collection of 19,000 non-testing task instances derived from 37 repositories.
                         Utilizing SWE-bench-train, we release two fine-tuned models, SWE-Llama 7b and 13b, based on
                                             `
                         the CodeLlama (Roziere et al., 2023) model. We find that in some settings SWE-Llama 13b is
                         competitive with Claude 2 and is capable of processing contexts exceeding 100,000 tokens.
                         2   SWE-BENCH
                         SWE-bench is a benchmark featuring GitHub issues from popular repositories that report bugs or
                         request new features, and pull requests that make changes to the repository to resolve these issues.
                         Thetaskistogenerateapullrequestthataddressesagivenissueandpassestestsrelatedtotheissue.
                         2.1  BENCHMARKCONSTRUCTION
                         GitHubisarichdatasourceforsoftwaredevelopment,butrepositories,issues, andpullrequestscan
                         be noisy, ad-hoc, or poorly documented or maintained. To find high-quality task instances at scale,
                         weusea3-stagepipeline as follows.
                                 1                        2                         3
                                       Scrape PRs              Attribute Filter          Execution Filter
                                   12 popular repositories   ✓ Resolves an issue
     ✓Installs successfully

                                   >90% Python Code          ✓ Contributes tests      ✓PR passes all tests
                         Figure 2: SWE-bench task instances are created from merged pull requests that resolve an issue,
                         contributes tests, and install successfully.
                         Stage I: Repo selection and data scraping. We start by collecting pull requests (PRs) from 12
                         popular open-source Python repositories on GitHub, producing about ∼ 90,000 PRs in total. We
                         focus on popular repositories as they tend be better maintained, have clear contributor guidelines,
                         and have better test coverage. Each PR has an associated codebase specified by it’s base commit.
                         Stage II: Attribute-based filtering. We create candidate tasks by selecting the merged PRs that (1)
                         resolve a GitHub issue and (2) make changes to the test files of the repository, which indicates that
                         the user likely contributed tests to check whether the issue has been resolved.
                         Stage III: Execution-based filtering. For each candidate task, we apply the PR’s test content, and
                         log the associated test results before and after the PR’s other content is applied. We filter out task
                         instances without at least one test where its status changes from a fail to pass (henceforth referred
                         to as fail-to-pass test). We also filter out instances that result in installation or runtime errors.
                                                                    2
