                                                       Test-Time Learning for Large Language Models
               Table 2. Comparison of experimental results on the DomainBench and InstructionBench of the AdaptEval (see Supp. B). We mark the
               better scores in bold for better visualization and easier interpretation.
                  Method                                      DomainBench                                       InstructionBench
                                           Geography      Agriculture    Medicine      Finance    Alpaca-GPT4       Dolly      InstructionWild
                  Llama3.2-3B-Instruct       0.2395         0.0850         0.1411      0.2229         0.3564        0.3378         0.2562
                    •Tent                    0.1825         0.0150         0.1571      0.1093         0.0336        0.2105         0.0264
                    •EATA                    0.0064         0.0227         0.0259      0.0149         0.1410        0.0090         0.0122
                    •COME                    0.1000         0.1181         0.1542      0.1200         0.0437        0.2186         0.0697
                    •TLM(Ours)               0.2893         0.1687         0.2308      0.2953         0.3883        0.3470         0.2824
                  Llama3-8B-Instruct         0.2450         0.0834         0.1265      0.2329         0.3752        0.3671         0.2608
                    •Tent                    0.0778         0.0067         0.0105      0.0372         0.2001        0.0036         0.0820
                    •EATA                    0.2081         0.0017         0.0127      0.1257         0.1397        0.1725         0.1088
                    •COME                    0.0048         0.0039         0.0301      0.0328         0.1424        0.0700         0.0240
                    •TLM(Ours)               0.3212         0.1319         0.2372      0.3242         0.4274        0.3785         0.2932
                  Llama2-13B-chat            0.2182         0.0840         0.1315      0.2382         0.3741        0.2892         0.2781
                    •Tent                    0.0320         0.0196         0.1131      0.0049         0.0955        0.0076         0.1108
                    •EATA                    0.2800         0.0771         0.1348      0.1155         0.0811        0.0513         0.1006
                    •COME                    0.1981         0.0380         0.1239      0.0172         0.0806        0.0000         0.0189
                    •TLM(Ours)               0.2668         0.1013         0.2179      0.2760         0.3966        0.3007         0.2865
                  Qwen2.5-7B-Instruct        0.2649         0.0981         0.1313      0.2739         0.4439        0.3121         0.2866
                    •Tent                    0.2362         0.1180         0.0524      0.1648         0.2132        0.1946         0.1710
                    •EATA                    0.2109         0.1203         0.1334      0.2846         0.0000        0.2056         0.1710
                    •COME                    0.2306         0.1180         0.0463      0.1780         0.3781        0.2182         0.1710
                    •TLM(Ours)                0.3081        0.1652         0.2394      0.3311         0.4608        0.3177         0.3482
               viding enough information to drive the model’s learning            Learning on DomainBench (see Supp. B) using both Full-
               process, referred to as an informative sample. By setting          ParamandLow-RankAdaptation(LoRA)(Huetal.,2022)
               S(x) = 0 for uninformative samples, we can reduce un-              updates, and evaluate the LLM’s performance on GSM8K
               necessary backpropagation computations during Test-Time            (Cobbe et al., 2021).     From Figure 2, we observe that
               Learning, thereby improving the overall efficiency. Relying        LoRA, compared to Full-Param updates, better preserves
               on the sample score S(x), we use perplexity loss for model         the model’s originally learned general knowledge, thereby
               training. Then, the sample-efficient perplexity minimization       demonstrating a significant regularization effect. This is
               is to minimize the following objective:                            likely due to LoRA’s ability to fine-tune only a small subset
                                     minS(x)P(x;Θ).                        (5)    of model parameters, which effectively reduces the risk of
                                      Θ                                           overfitting and catastrophic forgetting.
               To obtain the active sample selection score S(x), we pro-          BasedonObservation3,weadopttheLoRAforTest-Time
               pose a perplexity-based weighting scheme to accurately             Learning, where the optimization objective is Eqn. 5 is
               identify reliable samples and emphasize their contribution         modified accordingly as follows:
               to Test-Time Learning. Formally, the active sample selec-
                                                                                                         ˜
               tion score S(x) can be calculated as follows:                             minS(x)P(x;Θ) = minS(x)P(x;Θ+∆Θ),                    (7)
                                                                                          ˜                     ∆Θ
                                                                                          Θ
                                [logP(x;Θ)−logP0]
                   S(x) = λ·e                      · I{P(x;Θ)>P }(x),      (6)
                                                                 0                where ∆Θ = BAiszeroatthebeginningoftraining, with
               where I     (·) is an indicator function, λ and P are a pre-       AusingrandomGaussian initialization and B set to zero,
                        {·}                                       0
               defined threshold. The above weighting function excludes           and we update only ∆Θ during the Test-Time Learning.
               low-perplexity samples from Test-Time Learning and as-
               signs higher weights to high-perplexity test samples, en-          5. Experiments
               abling them to contribute more significantly to model up-
               dates. It is important to note that evaluating S(x) does not       5.1. Experimental Settings
               involve any gradient backpropagation.                              Datasets. To evaluate the effectiveness of our TLM, we
               4.3. Modulating Parameters for Test-Time Learning                  construct a comprehensive benchmark named AdaptEval,
                                                                                  designed to cover diverse tasks and domains. AdaptEval
               Observation3: Low-Rank Adaptation prevents catas-                  consists of three categories of datasets. 1) DomainBench
               trophic forgetting more effectively than Full-Param up-            includes four vertical domain knowledge datasets: Geogra-
               dates during test-time learning. We conduct Test-Time              phy, Agriculture, Medicine, and Finance, and is designed to
                                                                               6
