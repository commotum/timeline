                                                   This CVPRpaperisthe Open Access version, provided by the Computer Vision Foundation.
                                                                     Except for this watermark, it is identical to the accepted version;
                                                               the final published version of the proceedings is available on IEEE Xplore.
                      MVBench: AComprehensiveMulti-modalVideoUnderstandingBenchmark
                                                   1,2,3                        1,3~                      3                     4,3                     3               1,2,3
                              KunchangLi                        Yali Wang                  Yinan He              Yizhuo Li                 Yi Wang             Yi Liu
                                                3                  5,3                       6,3                     4,3                          6,3~                    3,1~
                              ZunWang                Jilan Xu                GuoChen                    Ping Luo               Limin Wang                     YuQiao
                    1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences                2University of Chinese Academy of Sciences            3Shanghai AI Laboratory
                                 4TheUniversity of Hong Kong          5FudanUniversity       6State Key Laboratory for Novel Software Technology, Nanjing University
                                                   Abstract                                                        SpatialUnderstanding:RJSFPPKJLSPITGHKJLUFSPGTF
                      With the rapid development of Multi-modal Large Lan-                                    ①Action                                                 ⑤Scene
                  guage Models (MLLMs), a number of diagnostic bench-                                         What’sthemandoing?                                      Where’stheman?
                                                                                                              ②Object                                                 ⑥Pose
                  marks have recently emerged to evaluate the comprehen-                                      What’sonthetable?                                       What’stheman’spose?
                  sion capabilities of these models. However, most bench-                                     ③Position                                               ⑦Attribute
                  marks predominantly assess spatial understanding in the                                     Is the manonthestage?                                   Whatcoloristhedesk?
                                                                                                              ④Count                       [02:16] Never gonna give!  ⑧Character
                  static image tasks, while overlooking temporal understand-                                  Howmanychairs?                                          Whatarethesubtitles?
                  ing in the dynamic video tasks. To alleviate this issue, we                                 ⑨CognitionWhyisthemansinginginthecanteen?
                  introduce a comprehensive Multi-modal Video understand-
                  ing Benchmark, namely MVBench, which covers 20 chal-
                  lenging video tasks that cannot be effectively solved with a                                   Temporal Understanding:EFGHIJKJLMGHFNIJFJOKPFQKNFI
                  single frame. Speciﬁcally, we ﬁrst introduce a novel static-
                  to-dynamic method to deﬁne these temporal-related tasks.                                   ①Action                  ③Position              ⑦Attribute
                  By transforming various static tasks into dynamic ones, we                                   ActionSequence          MovingDirection         State Change
                  enable the systematic generation of video tasks that require                                 ActionAntonym           ActionLocalization      Moving	Attribute
                  a broad spectrum of temporal skills, ranging from percep-                                    ActionPrediction       ④Count                 ⑧Character
                  tion to cognition. Then, guided by the task deﬁnition, we au-                                Unexpected Action       ActionCount             CharacterOrder
                  tomatically convert public video annotations into multiple-                                  Fine-grainedAction      MovingCount
                  choice QA to evaluate each task. On one hand, such a                                       ②Object                  ⑤Scene                 ⑨Cognition
                  distinct paradigm allows us to build MVBench efﬁciently,                                     ObjectShuffle           SceneTransition         EpisodicReasoning
                  without much manual intervention. On the other hand, it                                      ObjectExistence        ⑥Pose                    EgocentricNavigation
                  guarantees evaluation fairness with ground-truth video an-                                   ObjectInteraction       Fine-grainedPose        CounterfactualInference			
                  notations, avoiding the biased scoring of LLMs.                         More-           Figure 1. Tasks of MVBench. Wedeﬁnetemporaltasksbyadapt-
                                                                                                          ing static image tasks with dynamic evolution. This leads to 20
                  over, we further develop a robust video MLLMbaseline, i.e.,                             challenging tasks of video understanding, which cannot be effec-
                  VideoChat2, by progressive multi-modal training with di-                                tively solved within a single frame. For example, “position” in an
                  verse instruction-tuning data. The extensive results on our                             image can be converted into “moving direction” through a video.
                  MVBenchrevealthat, the existing MLLMs are far from sat-
                  isfactory in temporal understanding, while our VideoChat2                               66, 67]. With such a fast development, there is a natural
                  largely surpasses these leading models by over 15% on                                   question: How can we evaluate the comprehension capa-
                  MVBench. All models and data are available at https:                                    bilities of these MLLMs? Such assessment is vital to con-
                  //github.com/OpenGVLab/Ask-Anything.                                                    ﬁrmtheirdesigneffectiveness and further improve them for
                                                                                                          a broader understanding of open-world multi-modalities.
                  1. Introduction                                                                              In response to this need, a number of benchmarks have
                  In the past few years, Multi-modal Large Language Mod-                                  been launched [17, 42, 46, 83, 90], by evaluating MLLMs
                  els (MLLMs) [1, 16, 25, 37, 39, 44, 54, 97] have gradually                              with Question Answering (QA) formulation of various per-
                  driven the advance in vision-language learning, by plugging                             ception tasks.         However, most of these benchmarks pri-
                  visual encoders within various pretrained LLMs [10, 15, 53,                             marily concentrate on image-based understanding, where
                                                                                                          all the questions are designed for spatial perception in the
                       Interns at Shanghai AI Laboratory. ~ Corresponding authors.                       static images, e.g.,“Is the man on the stage?”, as shown
                                                                                                      22195
             in Fig.  1.  Hence, they suffer from difﬁculty in assess-         ful vision foundation model [40]. Subsequently, we intro-
             ing temporal evolution in dynamic videos, which is criti-         duce a progressive training paradigm with a wide spectrum
             cal to understanding the procedural activities in our realis-     of multi-modal instructions, allowing effective alignment
             tic world. Recently, several attempts have tried to evaluate      between video and language. The evaluations show that,
             MLLMsontemporalperception in videos [35, 48, 56, 80].             our VideoChat2 signiﬁcantly surpasses the top-performing
             But they either work on the very basic video tasks (e.g.,         VideoChat [39] by over 15% accuracy on MVBench, and
             action recognition and prediction in SEED-Bench [35]), or         also achieves the new state-of-the-art results on video con-
             focus on the particular domains (e.g., surprising compre-         versation [48] and zero-shot QA benchmarks [81, 91]. All
             hension in FunQA [80]) and restricted scenes (e.g., indoor        the models and data are publicly available, in order to pave
             scenes in Perception Test [56]). As a result, it is limited to    the path to general video understanding.
             leverage these benchmarks to make a comprehensive evalu-
             ation on the temporal understanding skills of MLLMs. Be-          2. Related Works
             sides, they are collected with labor-intensive annotations,       MLLM. Building upon the signiﬁcant achievements of
             leading to expensive manual intervention. To tackle these         Large Language Models (LLMs) [5, 10, 15, 58, 75], schol-
             problems, we propose a Multi-modal Video understanding            arly interest has increasingly shifted towards the exploration
             Benchmark (MVBench), which aims at comprehensively                and development of Multi-modal Large Language Mod-
             evaluating the temporal perception capabilities of MLLMs          els (MLLMs). This shift aims to augment multi-modal
             in the open world. Compared to these existing benchmarks          understanding and generation capabilities. Groundbreak-
             above, there are two distinct designs in our MVBench.             ing MLLMs such as Flamingo [1] and PaLM-E [16] have
                First, we introduce a novel static-to-dynamic method to        seamlessly fused text and vision, setting precedence with
             systematically deﬁne temporal-related tasks, by adapting          their outstanding performances across a range of multi-
             static image tasks with dynamic evolution. This leads to 20       modal tasks [22, 49, 57, 82]. The recent open-sourcing
             challenging tasks of video understanding in the MVBench,          of LLMs [65–68, 93] further accelerates the emergence
             whichcovers a wide range of temporal understanding skills         of public MLLMs [20, 44, 97]. Notable examples such
             fromperceptiontocognition. Speciﬁcally, we use static im-         as LLaVA [44], MiniGPT-4 [97], and InstructBLIP [11]
             age tasks in the previous multi-modal benchmarks [17, 46]         havecontributed by proposing a series of visual instruction-
             as deﬁnition reference. Then, we augment the question of          tuning data. Venturing beyond text and static images, sev-
             these static tasks with temporal context in the video, e.g.,      eral studies have begun harnessing video modality [39,
             the position task in the image can be ﬂexibly converted into      47, 48, 94], tapping into the vast potential of LLMs for
             the moving-direction task in the video (“Is the man on the        video comprehension tasks [7, 81, 91]. Innovations like
             stage?” ! “What direction is the man moving?”) in Fig.            VideoChat [39], VideoChatGPT [48], and Valley [47] uti-
             1. In this case, we can effectively convert all these static      lize ChatGPT to generate video instruction-tuning data,
             tasks into the corresponding dynamic tasks, which cannot          aiming to enhance instruction-following capabilities. In the
             be solved without reasoning on the whole video.                   VideoChat2, we aim to critically examine the fundamental
                Second,guidedbythetaskdeﬁnition,wedesignanauto-                temporal understanding capabilities of MLLMs, providing
             maticannotationparadigmtogeneratemultiple-choiceQAs               valuable design insights for more robust video MLLMs.
             for each task, by converting 11 public video benchmarks           Benchmark. Traditional Vision-Language (VL) bench-
             with LLMs. On one hand, it can largely reduce the cost            marks [21, 29, 79, 81, 82] have primarily honed in on
             of expensive human annotations. On the other hand, these          speciﬁc capabilities like multi-modal retrieval and vision
             11benchmarkscovervariouscomplexdomainsanddiverse                  QA. The rise of MLLMs has catalyzed benchmarks de-
             scenes, ranging from ﬁrst-person to third-person perspec-         signed for assessing integrated VL tasks.     For example,
             tives, and from indoor to outdoor environments. Hence, our        LVLM-eHub [83] provides an interactive model compari-
             MVBenchisapreferable choice to evaluate the general ca-           son platform through image-related queries. Other bench-
             pability of MLLMsforopen-worldtemporalunderstanding.              markssuchasOwlEval[87],MME[17],SEED-Bench[35],
             More importantly, these benchmarks provide the ground             MM-Vet [90], and MMBench [46] underscore comprehen-
             truth for MVBench which guarantees evaluation fairness            sive VL skills, introducing evaluation metrics that tran-
             and accuracy, avoiding biased scoring of LLMs [48, 80].           scend mere model hierarchies. Meanwhile, the video realm
                Finally, we make a thorough evaluation of various well-        showcased benchmarks like Perception Test [56], exam-
             knownMLLMsonourMVBench.Surprisingly,thesestate-                   ining multi-modal video perception and reasoning, and
             of-the-art image and video MLLMs are far from satisfac-           VideoChatGPT [48] quantiﬁes the capability of dialogue
             tory, in terms of temporal perception and cognition. This         generation from video inputs. FunQA [80] pushes video
             further motivates us to develop a strong video MLLM base-         reasoning limits via counter-intuitive and humorous con-
             line, namely VideoChat2, by bridging LLM with a power-            tent. In contrast to the existing benchmarks, MVBench sets
                                                                           22196
                     Spatial          Temporal             Source        Example
                                        Action              STAR         Whathappenedafter the person took the food?
                                      Sequence                           (A) Ate the medicine. (B) Tidied up the blanket. (C) Put down the cup/glass/bottle. (D) Took the box.
                                        Action              STAR         Whatwill the person do next?
                                      Prediction                         (A) Put down the pillow. (B) Open the door. (C) Take the book. (D) Open the closet/cabinet.
                                        Action            PAXION‡        Whichoneofthesedescriptions correctly matches the actions in the video?
                      Action          Antonym                            (A) not sure (B) scattering something down (C) piling something up
                                     Fine-grained         MiTV1‡         Whatistheaction performed by the person in the video?
                                        Action                           (A) watering (B) leaking (C) pouring (D) planting
                                     Unexpected                          Whatunexpected event contributes to the humor in the video?
                                        Action            FunQA‡         (A) The man left without dancing. (B) Two women hugged each other at the end.
                                                                         (C) The man ﬁnally danced with the woman. (D) Two men hugged each other unexpectedly.
                                  Object Existence       CLEVRER         Are there any moving green objects when the video ends? (A) not sure (B) yes (C) no
                                  Object Interaction        STAR         Whichobject was tidied up by the person? (A) broom (B) cabinet (C) blanket (D) table
                      Object            Object           Perception      Whereis the hidden object at the end of the game from the person’s point of view?
                                        Shufﬂe               Test        (A) Under the ﬁrst object from the left. (B) Under the third object from the left.
                                                                         (C) Under the second object from the left.
                                       Moving           CLEVRER‡ Whatdirectionisthecyanspheremovingwithinthevideo?
                                      Direction                          (A) The object is stationary. (B) Up and to the right. (C) Down and to the left. (D) Down and to the right.
                     Position           Action           Charades-       During which part of the video does the action ‘person sitting on a couch’ occur?
                                     Localization           STA ‡        (A) In the middle of the video. (B) At the end of the video.
                                                                         (C) Throughout the entire video. (D) At the beginning of the video.
                                        Scene                            What’s the right option for how the scenes in the video change?
                      Scene           Transition          MoVQA‡         (A) From the reception desk to the conference room. (B) From the kitchen to the dining area.
                                                                         (C) From the server room to the control center. (D) From the classroom to the library.
                      Count         Action Count        Perception Test  Howmanytimesdidthepersonlaunchobjectsonthetable? (A) 3 (B) 2 (C) 4
                                    MovingCount          CLEVRER         Howmanymetalobjectsexitthescene? (A) 2 (B) 3 (C) 1 (D) 0
                    Attribute     MovingAttribute        CLEVRER         Whatshapeisthemovingobject when the video begins? (A) cylinder (B) sphere (C) cube
                                    State Change        Perception Test  Is the lighting device on at any point? (A) yes (B) I don’t know (C) no
                       Pose       Fine-grained Pose     NTURGB+D‡ Whatistheposeperformedbythepersoninthevideo? (A)pickup(B)sitdown(C)drop(D)standup
                    Character      Character Order      Perception Test  Whatletter did the person write ﬁrst on the paper? (A) l (B) v (C) e
                                      Egocentric          VLN-CE‡        For an agent following instruction: “Go left through the door.” What is the next action it should take?
                                      Navigation                         (A) Turn left and move forward (B) Move forward (C) Stop (D) Turn right and move forward.
                                       Episodic                          WhydidCastledress like a fairy when he was speaking to Emily?
                                      Reasoning            TVQA          (A) To get her to trust him. (B) He secretly loved fairies. (C) He lost a bet with Emily.
                    Cognition                                            (D) It was dressed like a fairy day at school. (E) Mrs Ruiz made him dress up.
                                    Counterfactual                       Whichofthefollowing will happen if the cylinder is removed?
                                      Inference          CLEVRER         (A) The cyan rubber object and the blue cube collide. (B) The brown cube collides with the metal cube.
                                                                         (C) The cyan rubber object and the metal cube collide. (D) The cyan rubber cube collides with the sphere.
                  Table 1. Task examples of MVBench. The videos are collected from the public datasets, including STAR [77], PAXION [74], Moments
                  in Time V1[52], FunQA[80],CLEVRER[88],PerceptionTest[56],Charades-STA[19],MoVQA[95],NTURGB+D[45],VLN-CE[30]
                  and TVQA [33]. Tasks requiring QA generation are marked with “‡”. More details can be found in Section 3.1.
                  itself apart by covering a wide range of temporal tasks, em-                             ranging from perception to cognition. As shown in Fig. 1,
                  phasizing temporally-sensitive videos and efﬁcient use of                                we begin by summarizing 9 main tasks of spatial under-
                  public annotations, and conducting comprehensive evalua-                                 standing from previous benchmarks. Then we enrich these
                  tions of MLLMs’ temporal understanding.                                                  imagetaskswithvideocontext,creatingtemporaltasksthat
                  3. MVBench                                                                               cannotbeeffectivelysolvedwithasingleimageandrequire
                                                                                                           comprehensive video understanding. Finally, we deﬁne 20
                  In this section, we present our MVBench in detail. We ﬁrst                               temporal tasks as follows. Examples are listed in Tab. 1.
                  design the temporal tasks in Tab. 1, and then automatically                                   Action. (1) Action Sequence: Retrieve the events occur-
                  generate multiple-choice QAs for evaluation in Fig. 2.                                   ring before or after a speciﬁc action. (2) Action Prediction:
                  3.1. Temporal Task Deﬁnition                                                             Infer the subsequent events based on the current actions. (3)
                                                                                                           ActionAntonym: Distinguishthecorrectactionfromtwoin-
                  To design the temporal tasks of MVBench, we introduce a                                  versely ordered actions. (4) Fine-grained Action: Identify
                  concise static-to-dynamic method by adapting static tasks                                the accurate action from a range of similar options. (5) Un-
                  with dynamic goals. As discussed in the introduction, most                               expected Action: Detect surprising actions in videos char-
                  existing MLLM benchmarks [17, 46] focus on spatial un-                                   acterized by humor, creativity, or magic. Object. (6) Ob-
                  derstanding with systematical deﬁnitions of static image                                 ject Existence: Determine the existence of a speciﬁc object
                  tasks. Motivatedbythis, weproposeusingthesetaskdeﬁni-                                    during a particular event. (7) Object Interaction: Identify
                  tions as references to systematically design temporal tasks,                             the object that participates in a particular event. (8) Object
                                                                                                       22197
                                                                                                                                                                                                                                                   fgOKIJhPIYFHHKJL
                                   VGHWXFUFYOKIJ                                                                  Various                    _GOG`KUOPGOKIJ                                                                                        fgOKIJhPIYFHHKJL
                                                                                                                   scenes             Video               Each QA pair corresponds                                Order                  Options are randomly
                                                                   Public video datasets with                                      Diversity              to a distinct video                                    Shuffle                 selected and shuffled
                                                                    high-quality annotations                                       Temporal × Too short: minimal movement
                                                                                                                                                          Intermediate duration                                  Length ;;< Different options should
                                                                                               Charades-STA,                      Sensitivity ×                                                                                          have similar and
                                                                                                 NTU RGB+D                                                Too long: complicated context                           Check                  reasonable text lengths
                                                                                                    PAXION,                                               Too easy: indistinguishable
                                                                                                   FunQA, …                        Question ×                                                                          klmnompqrs: hPITgO_FHKLJ
                                              Perception Test                 MiT V1                                               Difficulty × Proper question                                                ::<ℎ>?@ABCD?AEFAG?ℎCHB>IDIJAF@CB
                                                                                                                                                          Too hard: inseparable                                   KELAFHMA?ℎAF?ℎCLA@CE?                               #
                                                                                                   STAR                                                                                                         N OP>F@?E?ℎCBAHℎ?.                                   VideoChat !
                                                                                                                                                             cdeFJFPGOKIJ                                       R OP>F@?E?ℎCJCS?.                         (D)              mPLUG-Owl
                                                    TVQA                  CLEVRER                                                                    !"#                                                        T UℎCEVWCD?AGG?>?AEF>BI.                             $"
                                                                                                                                      Have                   Directly adopt QA                                  X YEMF>F@?E?ℎCBAHℎ?.                                  LLaVA    Otter
                                                                                                                                   options? $%                                                                System	Prompt:Consider temporal evolution
                                                                                                                                                             GenerateQAwith                                      ]>BCS^JJIM>?Dℎ?ℎCLA@CE>F@P>I>??CF?AEF?E
                                                                                  Object                                                                     videoannotations                                    ?ℎCD>^GC>F@GC_^CFDCESCLCF?G,
                                                                                                                                       LLM asks question ⇐ task definition                                       ?ℎC@C?>AJ >F@ KELCKCF?ESEVWCD?G,
                                                            Position                                                                                                                                             >F@?ℎC>D?AEF>F@PEGCESPCBGEFG.
                                                                                                                                             &ℎ()*+,"-)+%$+#)ℎ" .,(!-!/+$*",                                     Z>GC@EFIE^BEVGCBL>?AEFG,
                                                                                                                                             0%1+$.2++$)ℎ"1+*"%?                                                 GCJCD? ?ℎC VCG? EP?AEF ?ℎ>?
                                     Appearance                                         MovingDirection                                Template-based option candidates                                          >DD^B>?CJI>@@BCGGCG?ℎC_^CG?AEF.
                                                                                                                                             45($*)%)ℎ"/"6); 45($*)%)ℎ",+.ℎ);                                 Answer	Prompt:Must output option
                                                         ActionLocalization                                                                  7%2$($*)%)ℎ"/"6); 7%2$($*)%)ℎ",+.ℎ);                                ZCG?[P?AEF: (
                                                                                                                                             8ℎ"%9:"-)+##)()+%$(,!.
                           Figure 2. Generation pipeline of MVBench. Within public annotations, data is carefully ﬁltered and relevant multiple-choice QAs are
                           auto-generated. The effective system prompt and efﬁcient answer prompt are employed to guide MLLMs toward precise outputs.
                           Shufﬂe: Locate the ﬁnal position of an object in an occlu-                                                                             trum of domains and scenes, ranging from ﬁrst-person to
                           sion game. Position. (9) Moving Direction: Ascertain the                                                                               third-person perspectives, and from indoor to outdoor en-
                           trajectory of a speciﬁc object’s movement. (10) Action Lo-                                                                             vironments. (2) Temporal Sensitivity: To guarantee that
                           calization: Determine the time period when a certain action                                                                            each task is temporal sensitive, we eliminate short clips
                           occurs. Scene. (11) Scene transition: Determine how the                                                                                whichgenerally contain negligible motions, and also delete
                           scene transitions in the video. Count. (12) Action Count:                                                                              extremely long videos which often present overly compli-
                           Calculate how many times a speciﬁc action has been per-                                                                                cated contexts that are hard for evaluation. Hence, we se-
                           formed. (13) Moving Count: Calculate how many objects                                                                                  lect videos with intermediate duration, primarily ranging
                           have performed a certain action. Attribute. (14) Moving                                                                                from 5s to 35s. (3) Question Difﬁculty: Overly simple
                           Attribute: Determine the appearance of a speciﬁc moving                                                                                or complex questions may lead to indistinguishable evalu-
                           object at a given moment. (15) State Change: Determine                                                                                 ations, due to similar responses. To balance the question
                           whether the state of a certain object changes throughout the                                                                           difﬁculty, we design the selection criteria for STAR [77]
                           video. Pose. (16) Fine-grained Pose: Identify the accurate                                                                             and CLEVRER [28]. For STAR, we enhance the chal-
                           pose category from a range of similar options. Charac-                                                                                 lenge by randomly shifting the start or end points of the
                           ter. (17) Character Order: Determine the order in which                                                                                video clips, increasing the complexity of localizing speciﬁc
                           the letters appear. Cognition. (18) Egocentric Navigation:                                                                             events. For CLEVRER, we exclude questions that necessi-
                           Forecast the subsequent action, based on an agent’s current                                                                            tate more than 10 conditions (e.g., material, and shape) for
                           navigation instructions. (19) Episodic Reasoning: Perform                                                                              describing speciﬁc events, thus decreasing QA difﬁculty.
                           reasoning on the characters, events, and objects within an                                                                                   QAGeneration. Consideringthatnotalltheannotations
                           episodeofaTVseries. (20)CounterfactualInference: Con-                                                                                  of selected datasets follow the multiple-choice QA format,
                           sider what might happen if a certain event occurs.                                                                                     weautomaticallyconvertthevideoannotationsintothisfor-
                           3.2. Automatic QA Generation                                                                                                           mat via LLMs. Speciﬁcally, we ﬁrst use ChatGPT [53]
                                                                                                                                                                  to generate a question for each video, based on the task
                           Withtheguidanceoftemporaltaskdeﬁnitions,wenextcol-                                                                                     deﬁnition. Then, we create the corresponding answer op-
                           lect and annotate videos for each task.                                                  Speciﬁcally, we                               tions as follows. (1) Template-Based Construction: For
                           design an automatic QA generation paradigm in Fig. 2,                                                                                  most questions, we construct the option candidates directly
                           which efﬁciently converts open-sourced video annotations                                                                               from the ground truth annotations. For example, the candi-
                           into multiple-choice QAs for evaluating MLLMs.                                                                                         datesfortheActionAntonymtaskcontainthecorrectaction,
                                 Data Filtration. To reduce the labor-intensive collec-                                                                           its opposite action, and a not-sure choice. In the case of
                           tion, we propose to select videos from existing benchmarks.                                                                            the Moving Direction task, the option candidates consist of
                           (1) Video Diversity: To boost video diversity, we carefully                                                                            four directions (i.e., up, down, left, right) and the stationary
                           select 11 video datasets (see Tab. 1) that cover a broad spec-                                                                         state.         (2) LLM-Based Generation: For the Unexpected
                                                                                                                                                           22198
                               Conversation            #op0                Reasoning             #op0                    VQA               #op0                                   Instruction Generation
                                                                                                                                                           E%F(/"'/%-"##0%$(,0$=0)"%F$)"/#*($)0$1($)0$#*/F7*0%$)"#01$.
                                  LLaVA                 56,681              LLaVA                 76,643              VQAv2                  29,903        G 30,, 10=" !%F *ℎ" )"#7/0'*0%$ %- =0)"% )(*(#"* ($) *(#H,
                                                                                                                                                           ($)%$"0$#*/F7*0%$"J(<',".
                                  VideoChat             13,884              CLEVR                 30,000              GQA                    30,001            DATASETDESCRIPTION:{dataset_descrption}
                                                                                                                                                               TASK DESCRIPTION:{task_description}
                                  VideoChatGPT          13,303              VisualMRC             15,000              OKVQA                   8,990            INSTRUCTIONEXAMPLE:{instruction_example}
                                                                                                                                                           i(#")%$*ℎ"(5%="<"##(1",!%F$"")*%ℎ",'<"
                               Classification          #op0                 NExTQA                34,132              A-OKVQA                17,056        1"$"/(*"100$#*/F7*0%$#-%/ℎ($),0$1*ℎ"=0)"%*(#H#.
                                  ImageNet               30,000                                                       ViQuAE                  1,152             Human       The dataset contains…      Prompt      ChatGPT
                                                                            CLEVRER_QA            40,000                                                                    In this task, you will…                        Instruction
                                  COCO-ITM               29,919             CLEVRER_MC            42,620              OCR-VQA                11,414                         Here is an example…
                                  Kinetics-710           40,000         SimpleCaption            #op0                 TextVQA                27,113
                                  SthSthV2               40,000                                                       ST-VQA                 26,074          # video data path
                                                                                                                                                             'video': '023601_023650/1023815317.mp4',
                                                                            COCO                 566,747              DocVQA                 39,463          # conversion tasks have multiple QA
                            DetailedCaption #op0                                                                                                             'QA': [{
                                                                            TextCaps              97,765                                                          # instruction as task guidance
                                  MiniGPT-4               3,362                                                       TGIF-Frame             39,149       e       'i': "Go through the video, taking into account 
                                                                                                                                                          l  key aspects, and respond to the question.",
                                                                            WebVid              400,000                                                   p       # no question for caption tasks
                                                                                                                      TGIF-Transition        52,696       m
                                  LLaVA                 23,240                                                                                            a       'q': "What color cliff is the hindu temple on?",
                                                                                                                                                          Ex
                                                                                                                                                          	       # short answer may be phrased
                                                                            YouCook2                8,760                                                 a
                               ParagraphCaptioning      14,575                                                        WebVidQA             100,000        t       'a': "The Hindu temple in the video is situated 
                                                                                                                                                          a
                                                                                                                                                          D  on a green cliff."
                                  VideoChat               6,905             TextVR                39,648              EgoQA                   7,813          }]
                      Figure 3. Instruction-tuning data for VideoChat2. Co-training of VideoChat2 employs both image and video data, with instructions
                      generated by ChatGPT [53]. The resultant dataset comprises 2M samples drawn from 34 diverse datasets across 6 categories.
                      Action task in particular, we leverage ChatGPT for convert-                                                    swer prompt “Best Option: (” to guide MLLMs for option
                      ing open-ended QAs into multiple-choice QA with answer                                                         generation. Results in Tab. 9 demonstrate our prompt’s ef-
                      options. Note that, we use the multiple-choice format in-                                                      fectiveness on various MLLMs, allowing us to use accuracy
                      stead of the open-ended one, for evaluation correction and                                                     as a reliable metric for evaluation.
                      fairness. This is mainly because the open-ended answer has
                      to be scored by LLMs or user studies, which may either in-                                                     4. VideoChat2
                      troduce evaluation bias or manual intervention. Ultimately,
                      weproduce 200 multiple-choice QA pairs for each tempo-                                                         After building our MVBench, weevaluateanumberofpop-
                      ral understanding task. More details of QA generation for                                                      ular image and video MLLMs in Tab. 2. Surprisingly, the
                      all the tasks can be found in the appendix.                                                                    existing MLLMs are far from satisfactory in temporal un-
                           Answer Option Processing. For all questions, we ran-                                                      derstanding.            To ﬁll the gap, we develop a robust video
                      domlysample3to5answeroptionsfromtheavailablecan-                                                               MLLMbaseline,whichisdubbedasVideoChat2.
                      didates, and shufﬂe the option order, to strengthen the eval-                                                  4.1. Instruction-Tuning Data
                      uation’s robustness. Additionally, to prevent the common
                      issue of answer leakage where longer options tend to be                                                        Primarily, the suboptimal performance of MLLMs can be
                      correct, we further use LLM to guarantee that all the answer                                                   attributed to the limited diversity in instruction-tuning data.
                      options of a question are of similar and reasonable lengths.                                                   To address this issue, we introduce the enriched data as
                      3.3. Prompt Design for Evaluation                                                                              shown in Fig. 3, which comprises 2M samples from 34
                                                                                                                                     distinct sources. Following [39, 94], we include both image
                      To emphasize the temporal sensitivity of MLLMs, we craft                                                       and video data in the instruction set to improve training.
                                                                                                                                                                     3
                      a detailed system prompt for evaluation (see the bottom                                                             Motivated by M IT [41], we reorganize all data samples
                      right of Fig. 2). This prompt encourages MLLMs to care-                                                        in a uniform format, as shown on the bottom right of Fig.
                      fully scrutinize video content to answer questions, by pay-                                                    3. There are two keys involved: {‘image’ or ‘video’},
                      ing attention to factors such as the actions and poses of per-                                                 and {‘QA’}.              The ﬁrst key indicates the path to the vi-
                      sons, and the details and movements of object movements.                                                       sion data. The second key represents a list that contains
                           Moreover, another signiﬁcant challenge lies in extract-                                                   task instruction (‘i’) and question-answer(‘q’-‘a’). More-
                                                                                                                                                                             3
                      ing options from MLLMs’ responses. MMBench [46] at-                                                            over, different from M IT, which requires researchers to
                      temptstomatchpredictionswithmultipleoptionformats. If                                                          write 10 instructions per dataset, we use ChatGPT to cre-
                      failed, it resorts to ChatGPT [53] to extract options through                                                  ate them, according to {dataset description}, {task descrip-
                      an intricate design. However, this way is relatively inefﬁ-                                                    tion}, and {instruction example} at the top right of Fig. 3.
                      cient, yielding an alignment rate of only 87% with humans.                                                     Consequently, our whole instruction-tuning data set can be
                      In contrast, our MVBench employs a simple approach that                                                        roughly divided into 6 categories as follows:
                      guarantees 100% rate in option extraction. We enclose the                                                           (1) Conversation aims at enhancing multi-turn conver-
                      options within parentheses in the questions, and use the an-                                                   sational capabilities.                 We collect conversation data from
                                                                                                                               22199
                    Stage1	       VTC               Stage2	                                                Stage3	
                                  VTM                                                         VTG                                                    VTG
                                  VTG                                                                text                                                   answer
                      "           K/V   !            !            K/V   !         Li           "            !            K/V   !         Li           "        !
                        Visual     ! Q QFormer         Visual     ! Q QFormer near                LLM         Visual     ! Q QFormer near              LLM LoRA
                       Encoder                        Encoder                          Vision                Encoder                          Vision
                                                                                     Embedding                                             Embedding
                      image video          text      image video                                  text      image video         instruction       instruction + question
                      Vision-Language	Alignment                  Vision-Language	Connection                                  Instruction Tuning
                Figure 4. Progressive multi-modal training of VideoChat2. Stage1 aligns UMT-L [40], the visual encoder, with QFormer [37] to efﬁ-
                ciently compress extensive visual inputs. Stage2 extends this connection to incorporate LLM, while Stage3 focuses on effective instruction
                tuning to enhance model performance. The terms ‘instruction’, ‘question’ and ‘answer’ means ‘i’, ‘q’ and ‘a’ of ‘QA’ in Fig. 3.
                LLaVA [44] and VideoChat [39]. To expand our data, we                          from [37], we choose the pretrained UMT-L [40] as our
                integrate the caption data from VideoChatGPT [48] into                         visual encoder, due to its powerful capability of spatial-
                conversation format based on the video IDs. (2) Simple                         temporal representation learning.             Moreover, we train
                Caption aims to improve basic visual description capa-                         QFormer with only 15M image captions from CC3M [60]
                bilities.  We choose the widely used COCO Caption [43]                         and CC12M [6] but 10M video captions from WebVid-
                and WebVid [3], together with ﬁrst-order video captions                        10M[3],inorder to enhance video-language modeling.
                from YouCook2 [13]. (3) Detailed Caption aims at en-                               Stage2: Vision-Language Connection. After initial
                richingthecomprehensivecapabilitiesforunderstandingvi-                         alignment, we then connect the visual encoder with the pre-
                sual details.    We leverage the detailed caption data from                    trained LLMs, for building vision-language understanding
                MiniGPT-4 [97], LLaVA [44] and VideoChat [39]. We                              capabilities. Following [37], we apply a linear projection to
                also integrate Paragraph Captioning [31], TextCaps [61],                       further transform the query tokens, and concatenate the pro-
                and TextVR [78], which require uniquely comprehend-                            jectedtokenswiththetexttokensintoLLMforvision-based
                ing text within images and videos.               (4) VQA aims to               caption generation (i.e., VTG). But different from [37], we
                improve visual question-answering capabilities.                We in-          unfreeze the visual encoder for better alignment with LLM.
                clude the basic VQA (VQAv2 [22], GQA [26], TGIF-                               In addition to the aforementioned training data in Stage1,
                QA [27] and WebVidQA [84]), knowledge-based VQA                                we further introduce 2M image captions (COCO [43], Vi-
                (OK-VQA[49],AOK-VQA[59]andViQuAE[34]),OCR-                                     sual Genome [32], and SBU [55]) and 10M video captions
                based VQA (OCR-VQA[51], TextVQA [62], ST-VQA [4]                               (InternVid [73]), to enrich the caption diversity.
                andDocVQA[50]),andegocentricVQAfromEgo4D[23].                                      Stage3: Instruction Tuning. In the ﬁnal stage, we em-
                (5) Reasoning focuses on enhancing diverse reasoning ca-                       ploy the proposed data in Section 4.1 for instruction tuning.
                pacities. We use LLaVA-reasoning [44] and CLEVR [28]                           Tobetteralignresponseswithinstructions, weuselow-rank
                for spatial reasoning, VisualMRC [64] for reading com-                         adaptation [24] on the frozen LLM, and tune it along with
                prehension, NExT-QA [79] for temporal reasoning, and                           the visual encoder and QFormer by VTG loss. Moreover,
                CLEVRER [88] for spatiotemporal reasoning. (6) Classi-                         inspired by [11], we integrate instructions (i.e.,‘i’ of ‘QA’)
                ﬁcation aims at boosting robustness to object and action                       into QFormer, in order to extract instruction-relevant visual
                recognition. We sample data from ImageNet [14], COCO-                          tokens as input to LLM. However, different from [11], we
                ITM[43], Kinetics-710 [38] and SthSthV2 [21].                                  donotincorporatequestions(i.e.,‘q’of‘QA’)intoQFormer
                4.2. Progressive Multi-Modal Training                                          due to the subpar performances (see appendix.).
                Another critical factor in boosting MLLMs is how to effec-                     5. Experiments
                tively bridge the semantic gap between visual and linguistic
                representation. To tackle this problem, we adopt a progres-                    Implementation Details. For visual encoder and LLM,
                sive multi-modal training paradigm as shown in Fig. 4.                         we apply UMT-L [40] and Vicuna-7B v0 [66] by default.
                   Stage1: Vision-LanguageAlignment. Intheﬁrststage,                           Following BLIP2 [37], we deploy QFormer using the pre-
                we aim at aligning vision and text. To balance efﬁciency                       trained BERT           [15]. 32 queries are used in Stage1, and
                                                                                                                base
                and effectiveness, we freeze the visual encoder and train                      extra 64 queries are introduced in Stage2 and Stage3 when
                a ﬂexible QFormer [37], which compresses redundant vi-                         the visual encoder is unfrozen. For efﬁcient training, 4-
                sual tokens into fewer query tokens, and align these queries                   frame videos are processed through 10 epochs in Stage1
                with text tokens by multi-modal losses, i.e., Vision-Text                      and 1 epoch in Stage2. Transitioning to Stage3, we shift
                Contrastive learning (VTC), Vision-Text Matching (VTM),                        to 8-frame videos for 3 epochs. For evaluation, we input
                and Vision-grounded Text Generation (VTG). But different                       16-frame videos with elaborate prompts for better results.
                                                                                           22200
               Model                LLM          Avg AS AP AA FA UA OE OI OS MD AL ST AC MCMA SC FP CO EN ER CI
               Random               -           27.3 25.0 25.0 33.3 25.0 25.0 33.3 25.0 33.3 25.0 25.0 25.0 33.3 25.0 33.3 33.3 25.0 33.3 25.0 20.0 30.9
               ImageMLLMs: Following[11],all models take 4 frames as input, with the output embeddings concatenated before feeding into the LLM.
               mPLUG-Owl-I[87] LLaMA-7B29.425.020.044.527.023.536.024.034.023.024.034.534.522.031.540.024.037.025.521.037.0
               LLaMA-Adapter[96]LLaMA-7B31.723.028.051.030.033.053.532.533.525.521.530.529.022.541.539.525.031.522.528.032.0
               BLIP2[37]            FlanT5-XL 31.424.529.033.517.042.051.526.031.025.526.032.525.530.040.042.027.030.026.037.031.0
               Otter-I [36]         MPT-7B      33.5 34.5 32.0 39.5 30.5 38.5 48.5 44.0 29.5 19.0 25.5 55.0 20.0 32.5 28.5 39.0 28.0 27.0 32.0 29.0 36.5
               MiniGPT-4[97]        Vicuna-7B 18.816.018.026.021.516.029.525.513.011.512.0 9.5 32.515.5 8.0 34.026.029.519.0 9.9 3.0
               InstructBLIP [11]    Vicuna-7B 32.520.016.546.024.546.051.026.037.522.023.046.542.526.540.532.025.530.025.530.538.0
               LLaVA[44]            Vicuna-7B 36.028.039.563.030.539.053.041.041.523.020.545.034.020.538.547.025.036.027.026.542.0
               Video MLLMs: All models take 16 frames as input, with the exception of VideoChatGPT, which uses 100 frames.
               Otter-V [36]         LLaMA-7B26.823.023.027.527.029.553.028.033.024.523.527.526.028.518.038.522.022.023.519.019.5
               mPLUG-Owl-V[87] LLaMA-7B29.722.028.034.029.029.040.527.031.527.023.029.031.527.040.044.024.031.026.020.529.5
               VideoChatGPT[48] Vicuna-7B 32.723.526.062.022.526.554.028.040.023.020.031.030.525.539.548.529.033.029.526.035.5
               VideoLLaMA[94]       Vicuna-7B 34.127.525.551.029.039.048.040.538.022.522.543.034.022.532.545.532.540.030.021.037.0
               VideoChat [39]       Vicuna-7B 35.533.526.556.033.540.553.040.530.025.527.048.535.020.542.546.026.541.023.523.536.0
               VideoChat2text       Vicuna-7B 34.724.527.049.527.038.053.028.040.025.527.038.541.527.532.546.526.536.033.032.040.0
               VideoChat2           Vicuna-7B 51.166.047.583.549.560.058.071.542.523.023.088.539.042.058.544.049.036.535.040.565.5
              Table 2. Evaluations results on MVBench. Excluding BLIP2 and Otter, all models are built upon LLaMA 1 [67] for fair comparisons.
              “Random” refers to results from random guesses. “VideoChat2           ” denotes the model receiving blank videos and excludes LoRA
                                                                                text
              tuning, relying solely on the LLM’s capacity for responses. Notably, our MVBench exceeds the leading models, by over 15%.
               Evaluation Aspect       VideoChat[39] VideoChatGPT[48] VideoChat2
               Correctness of Information 2.23          2.40          3.02
               Detail Orientation         2.50          2.52          2.88
               Contextual Understanding   2.53          2.62          3.51             "Summarize the given video clip.
               Temporal Understanding     1.94          1.98          2.66             !The video shows a large bird standing in a dry, barren landscape 
               Consistency                2.24          2.37          2.81             with a few plants in the background. The bird is a golden eagle, and it is 
               Avg                        2.29          2.38          2.98             standing on the ground with its wings spread out. The bird is not moving 
                  Table 3. Results of video conversation benchmark [48].               and appears to be resting. The landscape is dry and barren, with no signs 
                                                                                       of life or vegetation. The bird is the only living creature in the scene.
               Model                MSVD-QA MSRVTT-QA ANet-QA                             The video shows a bird of prey, likely a hawk, standing on the ground 
                                     Acc Score Acc       Score    Acc Score            and looking around. It then spots a dead bird and starts to eat it. The 
               VideoLLaMA[94]       51.6    2.5   29.6    1.8    12.4   1.1            bird of prey is seen eating the dead bird and then flying away.
               VideoChat [39]       56.3    2.8   45.0    2.5    26.5   2.2               The video clip shows a bird of prey attacking a small animal. The bird 
               VideoChatGPT[48] 64.9        3.3   49.3    2.8    35.2   2.7            is seen swooping down and attacking the animal, which appears to be a 
               VideoChat2           70.0    3.9   54.1    3.3    49.1   3.3            rabbit. The bird is a hawk, and it is hunting for its prey in the desert.
                      Table 4. Zero-shot video QA results on [81, 92].                         !User    !VideoChat      VideoChatGPT     VideoChat2
                                                                                    Figure 5. Qualitative comparison. Green signiﬁes accurate de-
              5.1. Results on MVBench                                               scriptions, while red denotes incorrect or hallucinatory responses.
              Tab. 2 presents the evaluation results on MVBench, reveal-            [48]. Compared with VideoChatGPT [48], our VideoChat2
              ing that current image and video MLLMs are underper-                  exhibits superior performances across all metrics, with dis-
              forming. For instance, VideoChat [39], a top-performing               tinct advancements in terms of information correctness as
              video MLLM, only marginally surpasses VideoChat2text                  well as context and temporal understanding. This indicates
              by 0.8% in average accuracy (35.5% vs. 34.7%), with the               that our VideoChat2 is more adept at comprehending both
              latter generating responses from text alone. In contrast, our         spatial and temporal details and providing consistent and re-
              VideoChat2 markedly exceeds the leading model by over                 liable responses. (2) Zero-Shot Video QA: Tab. 4 lists re-
              15%, particularly shining in categories like action, object,          sults of typical video QA datasets [81, 91]. It is evident that
              scene, attribute, and pose recognition. However, it struggles         our VideoChat2 surpasses all other methods, particularly
              in position, count, and character tasks, performing less ef-          excelling in understanding long videos in ActivityNet [91].
              fectively than VideoChat2text, which could be attributed to               We further present a qualitative comparison in Fig. 5,
              the lack of exposure to these tasks during instruction tuning.        where VideoChat2 delivers a precise and thorough re-
              5.2. More Comparisons                                                 sponse. For more qualitative analyses, see the appendix.
              Following [48], we use ChatGPT [53] to conduct quantita-              5.3. Ablations of VideoChat2
              tive comparisons among video MLLMs. (1) Video Con-                    In this section, we conduct comprehensive analyses of the
              versation: Tab. 3 shows the results on the benchmark of               instruction data, model architecture, and prompt designs.
                                                                                22201
                DataSource            Type      Task      #Num       Avg             SystemPrompt                                              Avg
                VideoChat [39]        I+V    DC+R+C 17K           36.4               Carefully observe the video and choose the best option    49.9
                VideoChatGPT[48] V           DC           100K    34.3 #2.1          for the question.
                                      I      ALL          1.1M    42.1 "5.7          Carefully watch the video and pay attention to the cause,
                Ours                  V      ALL          0.9M    50.5 "14.1         sequence of events, and object details and movements.     50.5
                                      I+V† ALL            1.2M    50.7 "14.3         Based on your observations, select the best option that   "0.6
                                      I+V    ALL          2.0M    51.1 "14.7         accurately addresses the question.
              Table 5. Instruction Data. “I” and “V” denote “Image” and              Carefully watch the video and pay attention to the cause
              “Video”, while “DC”, “R”, “C” represent “Detailed Caption”,            and sequence of events, the detail and movement of        51.1
              “Reasoning”and“Conversation”. “†”symbolizestheversionwith              objects and the action and pose of persons.               "1.2
              fewer captions: 100K from COCO [43], 80K from WebVid [3].              Based on your observations, select the best option that
                Visual Encoder       LLM                 LoRA        Avg             accurately addresses the question.
                                                           7      42.4               Table 8. System Prompt. It should consider temporal evolution.
                EVA-CLIP-g[63]       Vicuna-7B v0          3      45.3 "2.9           Model                AnswerPrompt HitRatio            Avg
                                     Vicuna-7B v0          7      48.6                VideoChat [39]       ?                   78.2%     22.8
                                                           3      51.1 "2.5                                Best option: (      100%      35.5 "12.7
                UMT-L[40]            Vicuna-13B v0         3      51.4                VideoChatGPT[48] ?                       64.6%     22.0
                                     Vicuna-7B v1.5        7      48.1                                     Best option: (      100%      32.8 "10.8
                                                           3      51.2 "3.1           VideoChat2           ?                   96.4%     50.1
                                     Vicuna-13B v1.5       3      51.6                                     Best option: (      100%      51.1 "1.0
              Table6. VisualEncoder&LLM.Vicuna[66]v0andv1.5models                   Table 9. Answer Prompt. ‘?’ indicates directly matching the
              are tuned from LLaMA 1 [67] and LLaMA 2 [68] respectively.            option within responses, similar to [46]. Our simple yet effective
                        Stage2                   Stage3                             prompt enhances response precision across various MLLMs.
               Visual Encoder QFomer VisualEncoder QFomer             Avg
                                                                  38.5              jection while freezing the visual encoder and QFormer as
                                                                  47.0 "8.5         in MiniGPT-4 [97], but it yielded subpar results in Tab. 7.
                                                                  47.5 "9.0         By unfreezing QFormer as [11], we achieve an 8.5% per-
                                                                  51.1 "12.6        formance boost. Further, when we unfreeze the visual en-
              Table 7. Training Method.      and   refer to freezing and tuning.    coder, results consistently improved, emphasizing the value
              Weefﬁciently freeze the visual encoder in Stage1 and LLM in all       of more learnable parameters for visual adaptation.
              stages, while tuning the visual encoder and QFormer in Stage2&3.          Prompt Design. Tab. 8 reveals that a comprehensive
                                                                                    systemprompt,whichunderscoresthetaskrequirement,en-
                 Instruction Data. Tab. 5 demonstrates that the lim-                hances task completion effectiveness. Different from the
              ited instruction data proposed in VideoChat [39] (17K) and            unstable ChatGPT-extracting methods [46] and more time-
              VideoChatGPT [48] (100K) is insufﬁcient for temporal un-              consuming log-likelihood comparisons [35], we apply a
              derstanding. As we increase the data diversity and quantity,          simple yet effective answer prompt to extra the options.
              the performances are signiﬁcantly improved, wherein video             Results in Tab.     9 demonstrate that it accurately targets
              data contributes more than image data (50.5% vs. 42.1%).              the option and enhances response precision across various
              Considering the potential redundancy in the simple caption            MLLMs. More importantly, VideoChat2 follows the in-
              data of COCO[43]andWebVid[3],werandomlycompress                       structions better to return options even without the prompt.
              them. Thisresultsinonlyaminimalimpactonperformance                    6. Conclusion
              (50.7% vs. 51.1%), while accelerating the tuning by 1.7⇥.             This paper introduces MVBench, a comprehensive bench-
                 Architecture. (1) Visual Encoder: In Tab. 6, we ﬁrst
              apply EVA-CLIP-g [63] akin to VideoChat, which achieves               markforevaluatingthetemporalunderstandingcapabilities
              6.9% higher accuracy with our instruction data (42.4% vs.             of MLLMs. Moreover, we propose a robust video MLLM
              35.5%fororiginaloneinTab. 2). Furthersubstitutionswith                baseline, VideoChat2, outperforming the leading models by
              UMT-L improve the performance by an additional 6.2%,                  over 15% on MVBench. Our extensive analyses further di-
              whichdemonstrates the effectiveness of our visual encoder.            rect the designs of MLLMs for temporal understanding.
              (2) LLM: However, incorporating larger and newer LLMs                 Acknowledgement
              offers a marginal improvement in the results, indicating that
              MVBenchrelies predominantly on the visual encoder. No-                This work was supported in part by the National Key R&D
              tably, LoRA [24] consistently uplifts the results, potentially        Program of China (No. 2022ZD0160505), and the Na-
              due to its enhanced capacity for instruction following.               tional Natural Science Foundation of China under Grant
                 Training Method. Initially, we tune only the linear pro-           (62272450, 62076119).
                                                                                22202
               References                                                                         Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr
                 [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-                        Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
                                                                                                                             ´
                     toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur                      Brennan Saeta, Mark Dıaz, Orhan Firat, Michele Catasta,
                     Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,                        Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff
                     Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,                      Dean,SlavPetrov,andNoahFiedel. Palm: Scalinglanguage
                     Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-                        modeling with pathways. JMLR, 2022. 1, 2
                     bastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-                     [11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
                     hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,                       Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert Li,
                     Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.                         Pascale Fung, and Steven C. H. Hoi.           Instructblip:  To-
                     Flamingo: a visual language model for few-shot learning.                     wards general-purpose vision-language models with instruc-
                     ArXiv, abs/2204.14198, 2022. 1, 2                                            tion tuning. In NeurIPS, 2023. 2, 6, 7, 8, 15
                 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan                [12] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and
                                                                                                                  ´
                     Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren                         Christopher Re. FlashAttention: Fast and memory-efﬁcient
                     Zhou. Qwen-vl: Afrontierlargevision-languagemodelwith                        exact attention with IO-awareness. In NeurIPS, 2022. 13
                     versatile abilities. ArXiv, abs/2308.12966, 2023. 15                   [13] Pradipto Das, Chenliang Xu, Richard F. Doell, and Jason J.
                                                      ¨                                           Corso. A thousand frames in just a few words: Lingual de-
                 [3] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisser-                       scription of videos through latent topics and sparse object
                     man. Frozen in time: A joint video and image encoder for                     stitching. In CVPR, 2013. 6
                     end-to-end retrieval. In ICCV, 2021. 6, 8                              [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
                                               `    ´                ´              ´
                 [4] Ali Furkan Biten, Ruben Perez Tito, Andres Maﬂa, Lluıs                       and Li Fei-Fei. Imagenet: A large-scale hierarchical image
                       ´                    ˜
                     Gomez, Marc¸al Rusinol, Ernest Valveny, C. V. Jawahar, and                   database. In CVPR, 2009. 6
                     DimosthenisKaratzas. Scenetextvisualquestionanswering.
                     In ICCV, 2019. 6                                                       [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
                 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-                           Toutanova. Bert: Pre-training of deep bidirectional trans-
                     biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-                    formersforlanguageunderstanding. ArXiv,abs/1810.04805,
                     tan, PranavShyam,GirishSastry,AmandaAskell,etal. Lan-                        2018. 1, 2, 6
                     guage models are few-shot learners. In NeurIPS, 2020. 2                [16] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey
                 [6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu                        Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
                     Soricut. Conceptual12m: Pushingweb-scaleimage-textpre-                       Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong
                     training to recognize long-tail visual concepts.      In CVPR,               Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
                     2021. 6                                                                      worth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,
                 [7] DavidL.ChenandWilliamB.Dolan. Collectinghighlypar-                           Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,
                     allel data for paraphrase evaluation. In ACL, 2011. 2                        and Peter R. Florence. Palm-e: An embodied multimodal
                 [8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun                         language model. In ICML, 2023. 1, 2
                     Liu,   Pengchuan Zhang, Raghuraman Krishnamoorthi,                     [17] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
                     Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.                         MengdanZhang,XuLin,ZhenyuQiu,WeiLin,JinruiYang,
                     Minigpt-v2: large language model as a uniﬁed interface for                   Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A
                     vision-language multi-task learning. ArXiv, abs/2310.09478,                  comprehensive evaluation benchmark for multimodal large
                     2023. 15                                                                     language models. ArXiv, abs/2306.13394, 2023. 1, 2, 3
                 [9] Ke Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng                   [18] Difei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yezhou Yang,
                     Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm’s                       and Mike Zheng Shou. Mist : Multi-modal iterative spatial-
                     referential dialogue magic. ArXiv, abs/2306.15195, 2023.                     temporal transformer for long-form video question answer-
                     15                                                                           ing. In CVPR, 2022. 15
               [10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,                       [19] J. Gao, Chen Sun, Zhenheng Yang, and Ramakant Nevatia.
                     Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul                             Tall: Temporal activity localization via language query. In
                     Barham, Hyung Won Chung, Charles Sutton, Sebas-                              ICCV,2017. 3, 14
                     tian   Gehrmann,      Parker   Schuh,    Kensen Shi,      Sasha        [20] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
                     Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker                           Miao Zheng, Qianmengke Zhao, Kuikun Liu, Wenwei
                     Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prab-                            Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vi-
                     hakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner                    sion and language model for dialogue with humans. ArXiv,
                     Pope, James Bradbury, Jacob Austin, Michael Isard, Guy                       abs/2305.04790, 2023. 2
                     Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya,                    [21] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-
                     Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski,                             ski, Joanna Materzynska, Susanne Westphal, Heuna Kim,
                                  ´                                                                                         ¨
                     Xavier Garcıa, Vedant Misra, Kevin Robinson, Liam Fedus,                     ValentinHaenel,IngoFrund,PeterYianilos,MoritzMueller-
                     DennyZhou,DaphneIppolito,DavidLuan,HyeontaekLim,                             Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, and
                     Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David                       Roland Memisevic.        The “something something” video
                     Dohan, Shivani Agrawal, Mark Omernick, Andrew M.                             database for learning and evaluating visual common sense.
                     Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,                      In ICCV, 2017. 2, 6, 13
                                                                                        22203
             [22] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-           navigation in continuous environments. In ECCV, 2020. 3,
                  tra, and Devi Parikh. Making the v in vqa matter: Elevating       14
                  the role of image understanding in visual question answer-   [31] Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li
                  ing. In CVPR, 2017. 2, 6                                          Fei-Fei. A hierarchical approach for generating descriptive
             [23] Kristen Grauman,    Andrew Westbury, Eugene Byrne,                image paragraphs. In CVPR, 2017. 6
                  Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson     [32] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
                  Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Mar-           Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
                  tin, Tushar Nagarajan, Ilija Radosavovic, Santhosh K. Ra-         tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
                  makrishnan, Fiona Ryan, Jayant Sharma, Michael Wray,              Connecting language and vision using crowdsourced dense
                  Mengmeng Xu, Eric Z. Xu, Chen Zhao, Siddhant Bansal,              image annotations. IJCV, 2017. 6
                  DhruvBatra,VincentCartillier,SeanCrane,TienDo,Morrie         [33] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L. Berg.
                  Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano        Tvqa: Localized, compositional video question answering.
                  Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebre-             In EMNLP,2018. 3, 14, 15
                                       ´
                  selasie, Cristina Gonzalez, James M. Hillis, Xuhua Huang,                                                            ´
                                                                               [34] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herve Le
                                                       ´        ´
                  Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik                                         ´                     ´
                                                                                    Borgne, Romaric Besanc¸on, Jose G. Moreno, and Jesus
                  Kottur, Anurag Kumar, Federico Landini, Chao Li, Yang-                ´
                                                                                    Lovon-Melgarejo. Viquae, a dataset for knowledge-based
                  hao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Mod-           visual question answering about named entities. In SIGIR,
                  hugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu,           2022. 6
                  Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda       [35] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
                  Sari, Kiran K. Somasundaram, Audrey Southerland, Yusuke           iao Ge, and Ying Shan.      Seed-bench:   Benchmarking
                  Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu,               multimodal llms with generative comprehension.    ArXiv,
                                                     ´
                  TakumaYagi,YunyiZhu,PabloArbelaez,DavidJ.Crandall,                abs/2307.16125, 2023. 2, 8
                  Dima Damen, Giovanni Maria Farinella, Bernard Ghanem,        [36] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
                  Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris            Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model
                  Kitani, Haizhou Li, Richard A. Newcombe, Aude Oliva,              with in-context instruction tuning. ArXiv, abs/2305.03726,
                  Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi,            2023. 7, 15
                  Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani,        [37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H.
                  Mingfei Yan, and Jitendra Malik. Ego4d: Around the world          Hoi.   Blip-2: Bootstrapping language-image pre-training
                  in 3,000 hours of egocentric video. In CVPR, 2022. 6              with frozen image encoders and large language models. In
             [24] J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-          ICML,2022. 1, 6, 7
                  Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora:          [38] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang,
                  Low-rank adaptation of large language models. In ICLR,            Limin Wang, and Y. Qiao. Uniformerv2: Spatiotemporal
                  2021. 6, 8, 13                                                    learning by arming image vits with video uniformer. ArXiv,
             [25] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,                    abs/2211.09552, 2022. 6, 15
                  Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,           [39] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang,
                  OwaisKhanMohammed,QiangLiu,KritiAggarwal,Zewen                    Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat:
                  Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia           Chat-centric video understanding. ArXiv, abs/2305.06355,
                  Song, and Furu Wei. Language is not all you need: Aligning        2023. 1, 2, 5, 6, 7, 8, 13, 15, 17
                  perception with language models. ArXiv, abs/2302.14045,      [40] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He,
                  2023. 1                                                           Limin Wang, and Yu Qiao. Unmasked teacher: Towards
             [26] Drew A. Hudson and Christopher D. Manning. Gqa: A new             training-efﬁcient video foundation models. In ICCV, 2023.
                  dataset for real-world visual reasoning and compositional         2, 6, 8, 13, 14, 15
                  question answering. In CVPR, 2019. 6                         [41] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang,
             [27] Y. Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gun-           Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu
                  hee Kim. Tgif-qa: Toward spatio-temporal reasoning in vi-         Sun,LingpengKong,andQiLiu.M3it: Alarge-scaledataset
                  sual question answering. In CVPR, 2017. 6                         towards multi-modal multilingual instruction tuning. ArXiv,
             [28] Justin Johnson, Bharath Hariharan, Laurens van der Maaten,        abs/2306.04387, 2023. 5
                  Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick.       [42] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
                  Clevr: A diagnostic dataset for compositional language and        Zhao, and Ji rong Wen. Evaluating object hallucination in
                  elementary visual reasoning. In CVPR, 2017. 4, 6                  large vision-language models. ArXiv, abs/2305.10355, 2023.
                               ˜                                                    1
             [29] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
                  Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,     [43] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
                  Tim Green, Trevor Back, Apostol Natsev, Mustafa Suley-                                                  ´
                                                                                    Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence
                  man, and Andrew Zisserman. The kinetics human action              Zitnick. Microsoft coco: Common objects in context. In
                  video dataset. ArXiv, abs/1705.06950, 2017. 2                     ECCV,2014. 6, 8
             [30] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra,     [44] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
                  andStefanLee. Beyondthenav-graph: Vision-and-language             Visual instruction tuning. In NeurIPS, 2023. 1, 2, 6, 7, 15
                                                                            22204
              [45] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,            [59] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
                   Ling-Yu Duan, and Alex C Kot.        Ntu rgb+d 120: A                Kenneth Marino, and Roozbeh Mottaghi.       A-okvqa: A
                   large-scale benchmark for 3d human activity understanding.           benchmarkforvisualquestionansweringusingworldknowl-
                   TPAMI,2020. 3, 14                                                    edge. In ECCV, 2022. 6
              [46] Yuanzhan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,              [60] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
                   Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,                  Soricut. Conceptual captions: A cleaned, hypernymed, im-
                   Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mm-                  age alt-text dataset for automatic image captioning. In ACL,
                   bench: Is your multi-modal model an all-around player?               2018. 6
                   ArXiv, abs/2307.06281, 2023. 1, 2, 3, 5, 8                     [61] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and
              [47] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Ming-                 Amanpreet Singh. Textcaps: a dataset for image captioning
                   Hui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Val-               with reading comprehension. In ECCV, 2020. 6
                   ley: Video assistant with large language model enhanced        [62] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,
                   ability. ArXiv, abs/2306.07207, 2023. 2                              Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus
              [48] Muhammad Maaz, Hanoona Abdul Rasheed, Salman Khan,                   Rohrbach. Towards vqa models that can read. In CVPR,
                   and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed              2019. 6
                   video understanding via large vision and language models.      [63] Quan Sun, Yuxin Fang, Ledell Yu Wu, Xinlong Wang, and
                   ArXiv, abs/2306.05424, 2023. 2, 6, 7, 8, 15, 17                      Yue Cao. Eva-clip: Improved training techniques for clip at
              [49] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and                 scale. ArXiv, abs/2303.15389, 2023. 8
                   Roozbeh Mottaghi. Ok-vqa: A visual question answering          [64] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida.        Vi-
                   benchmark requiring external knowledge. In CVPR, 2019.               sualmrc: Machine reading comprehension on document im-
                   2, 6                                                                 ages. In AAAI, 2021. 6
              [50] Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and          [65] InternLM Team. Internlm: A multilingual language model
                   C. V. Jawahar. Docvqa: A dataset for vqa on document im-             with progressively enhanced capabilities.   https://
                   ages. In WACV, 2021. 6                                               github.com/InternLM/InternLM,2023. 2
              [51] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and         [66] Vicuna Team. Vicuna: An open-source chatbot impress-
                   Anirban Chakraborty. Ocr-vqa: Visual question answering              ing gpt-4 with 90% chatgpt quality. https://vicuna.
                   byreading text in images. In ICDAR, 2019. 6                          lmsys.org/,2023. 1,6,8
              [52] Mathew Monfort, Bolei Zhou, Sarah Adel Bargal, Alex An-        [67] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
                   donian, Tom Yan, Kandan Ramakrishnan, Lisa M. Brown,                                                       ´
                                                                                        Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste
                   QuanfuFan,DanGutfreund,CarlVondrick,andAudeOliva.                        `
                                                                                        Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-
                   Moments in time dataset: One million videos for event un-            lien Rodriguez, Armand Joulin, Edouard Grave, and Guil-
                   derstanding. TPAMI, 2020. 3, 14                                      laume Lample. Llama: Open and efﬁcient foundation lan-
              [53] OpenAI.    Chatgpt.  https://openai.com/blog/                        guage models. ArXiv, abs/2302.13971, 2023. 1, 7, 8
                   chatgpt/,2023. 1,4,5,7,14                                      [68] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Al-
              [54] OpenAI.    Gpt-4v(ision) system card.  https://api.                  bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
                   semanticscholar.org/CorpusID:263218031,                              lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
                   2023. 1, 15                                                                                                         ´
                                                                                        Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer,
              [55] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.                   Moya Chen, Guillem Cucurull, David Esiobu, Jude Fer-
                   Im2text: Describing images using 1 million captioned pho-            nandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
                   tographs. In NeurIPS, 2011. 6                                        Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn,
                                                                       `
              [56] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Re-            SagharHosseini, Rui Hou, HakanInan, MarcinKardas, Vik-
                   casens Continente, Larisa Markeeva, Dylan, Banarse, Ma-              tor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Ko-
                   teusz Malinowski, Yezhou Yang, Carl Doersch, Tatiana                 renev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut
                   Matejovicova, Yury Sulsky, Antoine, Miech, Skanda Kop-               Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning
                                     ´
                   pula, Alexander Frechette, Hanna Klimczak, R. Koster, Jun-           Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
                   lin Zhang, Stephanie, Winkler, Yusuf Aytar, Simon Osin-              Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
                                                                 ˜
                   dero, Dima Damen, Andrew Zisserman, and Joao Carreira.               stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
                   Perception test : A diagnostic benchmark for multimodal              Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh
                   models. In NeurIPS, 2023. 2, 3, 14                                   Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin
              [57] Bryan A Plummer, Liwei Wang, Chris M Cervantes,                      Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
                   Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-               Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,
                   nik. Flickr30k entities: Collecting region-to-phrase corre-          RobertStojnic, SergeyEdunov,andThomasScialom. Llama
                   spondences for richer image-to-sentence models. In ICCV,             2: Open foundation and ﬁne-tuned chat models.      ArXiv,
                   2015. 2                                                              abs/2307.09288, 2023. 2, 8
              [58] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,       [69] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge,
                   Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and               Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu
                   Peter J Liu. Exploring the limits of transfer learning with a        Qie, and Mike Zheng Shou. All in one: Exploring uniﬁed
                   uniﬁed text-to-text transformer. JMLR, 2020. 2                       video-language pre-training. In CVPR, 2023. 15
                                                                               22205
             [70] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua         [84] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
                  Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment             Cordelia Schmid. Just ask: Learning to answer questions
                  networks: Towards good practices for deep action recogni-        from millions of narrated videos. In ICCV, 2021. 6
                  tion. In ECCV, 2016. 13                                     [85] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
             [71] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yi-            Cordelia Schmid. Zero-shot video question answering via
                  nan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2:            frozen bidirectional language models. In NeurIPS, 2022. 15
                  Scaling video masked autoencoders with dual masking. In     [86] QinghaoYe,GuohaiXu,MingYan,HaiyangXu,QiQian,Ji
                  CVPR,2023. 13                                                    Zhang, and Fei Huang. Hitea: Hierarchical temporal-aware
             [72] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun               video-language pre-training. In ICCV, 2023. 15
                  Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun     [87] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
                  Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali          Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya
                  Wang, Limin Wang, and Yu Qiao. Internvideo: General              Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng
                  video foundation models via generative and discriminative        Tian, Qiang Qi, Ji Zhang, and Feiyan Huang. mplug-owl:
                  learning. ArXiv, abs/2212.03191, 2022. 14                        Modularization empowers large language models with mul-
             [73] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu,           timodality. ArXiv, abs/2304.14178, 2023. 2, 7
                  XinJian Ma, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei      [88] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun
                  Liu, Yali Wang, Limin Wang, and Y. Qiao. Internvid: A            Wu, Antonio Torralba, and Joshua B. Tenenbaum. Clevrer:
                  large-scale video-text dataset for multimodal understanding      Collision events for video representation and reasoning. In
                  and generation. ArXiv, 2023. 6, 15                               ICLR, 2020. 3, 6, 13, 14
             [74] Zhenhailong Wang, Ansel Blume, Sha Li, Genglin Liu,         [89] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.
                  Jaemin Cho, Zineng Tang, Mohit Bansal, and Heng Ji. Pax-         Self-chained image-language model for video localization
                  ion: Patching action knowledge in video-language founda-         and question answering. In NeurIPS, 2023. 14, 15
                  tion models. In NeurIPS, 2023. 3, 13, 14
             [75] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,         [90] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
                  AdamsWeiYu,Brian Lester, Nan Du, Andrew M. Dai, and              Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
                  QuocV.Le. Finetuned language models are zero-shot learn-         Mm-vet: Evaluating large multimodal models for integrated
                  ers. In ICLR, 2021. 2                                            capabilities. ArXiv, abs/2308.02490, 2023. 1, 2
             [76] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten            [91] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting
                  Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and          Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for
                  Denny Zhou. Chain of thought prompting elicits reasoning         understanding complex web videos via question answering.
                  in large language models. In NeurIPS, 2022. 13                   In AAAI, 2019. 2, 7
             [77] BoWu,ShoubinYu,ZhenfangChen,JoshuaB.Tenenbaum,              [92] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting
                  and Chuang Gan. Star: A benchmark for situated reasoning         Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for
                  in real-world videos. In NeurIPS, 2021. 3, 4, 14, 15             understanding complex web videos via question answering.
             [78] Weijia Wu, Yuzhong Zhao, Zhuangzi Li, Jiahong Li, Hong           In AAAI, 2019. 7
                  Zhou, Mike Zheng Shou, and Xiang Bai. A large cross-        [93] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu
                  modal video retrieval dataset with reading comprehension.        Lai, MingDing,ZhuoyiYang,YifanXu,WendiZheng,Xiao
                  ArXiv, abs/2305.03347, 2023. 6                                   Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai,
             [79] Junbin Xiao, Xindi Shang, Angela Yao, and Tat seng Chua.         WenguangChen,P.Zhang,YuxiaoDong,andJieTang.Glm-
                  Next-qa: Next phase of question-answering to explaining          130b: An open bilingual pre-trained model. In ICLR, 2022.
                  temporal actions. In CVPR, 2021. 2, 6, 14, 15                    2
             [80] Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuan-        [94] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An
                  han Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu.            instruction-tuned audio-visual language model for video un-
                  Funqa: Towards surprising video comprehension.  ArXiv,           derstanding. ArXiv, abs/2306.02858, 2023. 2, 5, 7, 15
                  abs/2306.14899, 2023. 2, 3, 14                              [95] Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua
             [81] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang,           Ling, Yali Wang, Limin Wang, and Yu Qiao. Movqa: A
                  Xiangnan He, and Yueting Zhuang. Video question answer-          benchmark of versatile question-answering for long-form
                  ing via gradually reﬁned attention over appearance and mo-       movie understanding. ArXiv, abs/2312.04817, 2023. 3, 14
                  tion. In ICME, 2017. 2, 7                                   [96] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
             [82] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large        Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Jiao
                  videodescriptiondatasetforbridgingvideoandlanguage. In           Qiao. Llama-adapter: Efﬁcient ﬁne-tuning of language mod-
                  CVPR,2016. 2                                                     els with zero-init attention. ArXiv, abs/2303.16199, 2023. 7
             [83] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo          [97] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
                  Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Jiao               hamed Elhoseiny. Minigpt-4: Enhancing vision-language
                  Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evalu-            understanding with advanced large language models. ArXiv,
                  ation benchmark for large vision-language models. ArXiv,         abs/2304.10592, 2023. 1, 2, 6, 7, 8
                  abs/2306.09265, 2023. 1, 2
                                                                          22206
