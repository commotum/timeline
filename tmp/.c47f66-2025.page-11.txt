                  DongfuJiang, Xuan He, Huaye Zeng, Cong Wei, Max          Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mo-
                    Ku, Qian Liu, and Wenhu Chen. 2024. Mantis:              hammadShoeybi,andSongHan.2024. Vila: Onpre-
                    Interleaved multi-image instruction tuning. ArXiv        training for visual language models. In Proceedings
                    preprint, abs/2405.01483.                                of the IEEE/CVF Conference on Computer Vision
                                                                             andPattern Recognition, pages 26689–26699.
                  Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai
                    Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin            Tsung-Yi Lin, Michael Maire, Serge Belongie, James
                    Tan, Zhenye Gan, et al. 2024. Efficient multimodal       Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
                    large language models: A survey. ArXiv preprint,         and C Lawrence Zitnick. 2014.      Microsoft coco:
                    abs/2405.10739.                                          Common objects in context. In Computer Vision–
                                                                             ECCV 2014: 13th European Conference, Zurich,
                  Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram              Switzerland, September 6-12, 2014, Proceedings,
                    Duvvur, Ming Lim, Po-Yu Huang, Graham Neu-               Part V 13, pages 740–755. Springer.
                    big, Shuyan Zhou, Russ Salakhutdinov, and Daniel
                    Fried. 2024. VisualWebArena: Evaluating multi-         Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen,
                    modal agents on realistic visual web tasks. In Pro-      Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.
                    ceedings of the 62nd Annual Meeting of the Associa-      2023a. Hallusionbench: You see what you think? or
                    tion for Computational Linguistics (Volume 1: Long       you think what you see? an image-context reasoning
                    Papers), pages 881–905, Bangkok, Thailand. Associ-       benchmarkchallenging for gpt-4v (ision), llava-1.5,
                    ation for Computational Linguistics.                     and other multi-modality models. ArXiv preprint,
                                                                             abs/2310.14566.
                  HugoLaurençon, Andrés Marafioti, Victor Sanh, and
                    LéoTronchon.2024. Building and better understand-      Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
                    ing vision-language models: insights and future di-      Lee. 2023b. Improved baselines with visual instruc-
                    rections. ArXiv preprint, abs/2408.12637.                tion tuning. In NeurIPS 2023 Workshop on Instruc-
                                                                             tion Tuning and Instruction Following.
                  BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,
                    Jingkang Yang, and Ziwei Liu. 2023.      Otter: A      Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan
                    multi-modalmodelwithin-contextinstructiontuning.         Zhang, Sheng Shen, and Yong Jae Lee. 2024a. Llava-
                    ArXiv preprint, abs/2305.03726.                          next: Improved reasoning, ocr, and world knowledge.
                  BoLi,YuanhanZhang,DongGuo,RenruiZhang,Feng               Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
                    Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei           Lee. 2023c.    Visual instruction tuning.   In Ad-
                    Liu, and Chunyuan Li. 2024a. Llava-onevision: Easy       vances in Neural Information Processing Systems,
                    visual task transfer. ArXiv preprint, abs/2408.03326.    volume36,pages34892–34916.CurranAssociates,
                                                                             Inc.
                  BohaoLi, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui
                    Wang,RuimaoZhang,andYingShan.2024b. Seed-              Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam,
                    bench: Benchmarking multimodal large language            GrahamNeubig,YuanzhiLi,andXiangYue.2024b.
                    models. InProceedingsoftheIEEE/CVFConference             Visualwebbench: How far have multimodal llms
                    onComputerVisionandPatternRecognition(CVPR),             evolved in web page understanding and grounding?
                    pages 13299–13308.                                       Conference on Language Modeling.
                  Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang,         Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
                    BoLi,WeiLi,ZejunMa,andChunyuanLi.2024c.                  Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi
                    Llava-next-interleave: Tackling multi-image, video,      Wang, Conghui He, Ziwei Liu, et al. 2023d. Mm-
                    and 3d in large multimodal models. ArXiv preprint,       bench: Is your multi-modal model an all-around
                    abs/2407.07895.                                          player? ArXiv preprint, abs/2307.06281.
                  Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,         Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
                    Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong              2019. Vilbert: Pretraining task-agnostic visiolinguis-
                    Hu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-       tic representations for vision-and-language tasks. In
                    semantics aligned pre-training for vision-language       Advances in Neural Information Processing Systems
                    tasks. In Computer Vision–ECCV 2020: 16th Euro-          32: Annual Conference on Neural Information Pro-
                    pean Conference, Glasgow, UK, August 23–28, 2020,        cessing Systems 2019, NeurIPS 2019, December 8-
                    Proceedings, Part XXX 16, pages 121–137. Springer.       14, 2019, Vancouver, BC, Canada, pages 13–23.
                  Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu,         Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-
                    RyanHsieh, HyeonJung Kim, Jin Hyuk Lim, Sungy-           yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei
                    oung Ji, Byungju Lee, Xifeng Yan, et al. 2024d.          Chang, Michel Galley, and Jianfeng Gao. 2023a.
                    Mmsci: A multimodal multi-discipline dataset for         Mathvista: Evaluating mathematical reasoning of
                    phd-level scientific comprehension. ArXiv preprint,      foundation models in visual contexts. ArXiv preprint,
                    abs/2407.04903.                                          abs/2310.02255.
                                                                      15144
