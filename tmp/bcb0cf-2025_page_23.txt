          Published as a conference paper at ICLR 2025
          Figure 6: The plot illustrates the evolution of training and testing loss for both CIFAR-10 and CIFAR-100
          datasets over 100 epochs. Each curve represents the mean loss across 10 different random seeds, with the
          shaded regions indicating one standard deviation from the mean
          CIFAR-100alsosuggests that our model is scalable and adaptable to different tasks and data distributions,
          aligning well with the core objectives of this research project.
          Theversatility of our MIND model architecture is further underscored by its robust performance irrespective
          of whether it undergoes pre-training (Han et al., 2021). To elucidate this, Figure 7 showcases a side-by-
          side comparison of key performance indicators—test accuracy and test loss—across 100 epochs for both
          pre-trained and non-pre-trained configurations.
          Figure 7: Temporal evolution of test accuracies and losses for pre-trained and non pre-trained configurations.
          Subplot (a) captures the test accuracies, while subplot (b) focuses on the test losses. Both metrics are plotted
          as functions of the epoch count.
          Theobservedmetricsrevealadiscernibleadvantagewhenemployingpre-training. Specifically,thepre-trained
          model consistently surpasses its non-pre-trained counterpart in both accuracy and loss metrics. This superior
          performance is attributed to a 20 epochs pre-training “warm-up” phase for the prediction network. This
                             23
