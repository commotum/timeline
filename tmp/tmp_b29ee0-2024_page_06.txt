                                 6       S. He et al.
                                 3D coordinates ∈ R3 and an auxiliary feature vector ∈ RF (e.g., color). The
                                 point cloud ipoint is fed into the point encoder E, which extracts point features
                                            N ×D
                                 f      ∈R 1      , where N ≪ N, D is the feature dimension. Concurrently, the
                                   point                     1
                                 text instruction i    undergoes tokenization via F          . These prepared inputs
                                                   txt                               tokenize
                                 are then fed into the Large Language Model F, resulting in a textual response
                                 y. The above process can be formulated as:
                                         f      =E(i       ),  f    =F          (i  ),   y =F(f        , f  ).    (1)
                                          point       point      txt    tokenize  txt             point   txt
                                     Building on the approach introduced by LISA [28], SegPoint enhances the
                                 segmentation capabilities of Large Language Models (LLMs) by expanding their
                                 vocabulary with a new special token, <SEG>. This modification enables the model
                                 to recognize and predict the <SEG> token within the output sequence as a sig-
                                 nal to identify segmentation targets. Upon detecting a <SEG> token, the corre-
                                 sponding output sequence belonging to <SEG> token is extracted and processed
                                 through an MLP layer γ, generating mask embeddings hseg. The final step in-
                                 volves computing each binary mask prediction m ∈ RN by performing a dot
                                 product between the mask embeddings h            and the upsampled per-point em-
                                                                              seg
                                 beddings derived from the point features fpoint. The formulation of the afore-
                                 mentioned process is given by:
                                                    hseg = γ(y[seg]),   m=hseg⊗UpS.(fpoint),                      (2)
                                 where UpS. denotes the upsampling operation following PointNet++ [46] on
                                 fpoint. The vanilla baseline represents an initial attempt to bridge the gap be-
                                 tween LLMs’ text comprehension and point cloud segmentation tasks. It encoun-
                                 ters two primary issues. Firstly, the point encoder is trained on a scene-level
                                 dataset for classification achieving alignment between text and point clouds, not
                                 specifically trained for dense prediction tasks. Besides, the point encoder’s first
                                 layer employs Farthest Point Sampling (FPS) [46] to reduce the point cloud to N
                                                                                                                    1
                                 points, risking the loss of details vital for accurate dense predictions. Secondly,
                                 the operation of directly upsampling from N to N points to obtain per-point
                                                                                 1
                                 embeddings is prone to losing structural information and introducing a notable
                                 degree of noise, undermining the model’s efÏcacy in segmentation tasks.
                                 3.3    Geometric Enhancer Module
                                 To adapt the pre-trained point encoder for dense prediction tasks while main-
                                 taining its superior scene recognition capability, our objective is to harness the
                                 geometricinformationacrosstheentirescenetoguidethefurtherfeaturelearning
                                 process. Drawing inspiration from recent advancements in 2D computer vision,
                                 where studies [5,41,62,64] demonstrate that convolutions enhance transformers’
                                 ability to capture local spatial information, we introduce the Geometric En-
                                 hancer Module (GEM). This module is specifically designed to grasp the local
                                 geometric contexts within point clouds while enabling the preservation of the
                                 point encoder’s foundational architecture and information integrity.
