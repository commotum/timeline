                  Figure3.8: PerformanceonSuperGLUEincreaseswithmodelsizeandnumberofexamplesincontext. Avalue
                  of K = 32 means that our model was shown 32 examples per task, for 256 examples total divided across the 8 tasks in
                  SuperGLUE.WereportGPT-3valuesonthedevset,soournumbersarenotdirectlycomparabletothedottedreference
                  lines (our test set results are in Table 3.8). The BERT-Large reference model was ﬁne-tuned on the SuperGLUE training
                  set (125K examples), whereas BERT++ was ﬁrst ﬁne-tuned on MultiNLI (392K examples) and SWAG (113K examples)
                  before further ﬁne-tuning on the SuperGLUE training set (for a total of 630K ﬁne-tuning examples). We ﬁnd the
                  difference in performance between the BERT-Large and BERT++ to be roughly equivalent to the difference between
                  GPT-3withoneexamplepercontextversus eight examples per context.
                  and MultiRC, we sampled a new set of examples to use in the context for each problem. For WSC and MultiRC, we
                  used the same set of randomly drawn examples from the training set as context for all of the problems we evaluated.
                  WeobserveawiderangeinGPT-3’sperformanceacrosstasks. OnCOPAandReCoRDGPT-3achievesnear-SOTA
                  performance in the one-shot and few-shot settings, with COPA falling only a couple points short and achieving
                  second place on the leaderboard, where ﬁrst place is held by a ﬁne-tuned 11 billion parameter model (T5). On WSC,
                  performance is still relatively strong, achieving 80.1% in the few-shot setting (note that GPT-3 achieves 88.6% on the
                  original Winograd dataset as described in Section 3.4). On BoolQ, MultiRC, and RTE, performance is reasonable,
                  roughly matching that of a ﬁne-tuned BERT-Large. On CB, we see signs of life at 75.6% in the few-shot setting.
                  WiCisanotableweakspotwithfew-shotperformanceat49.4%(atrandomchance). Wetriedanumberofdifferent
                  phrasings and formulations for WiC (which involves determining if a word is being used with the same meaning in two
                  sentences), none of which was able to achieve strong performance. This hints at a phenomenon that will become clearer
                  in the next section (which discusses the ANLI benchmark) – GPT-3 appears to be weak in the few-shot or one-shot
                  setting at some tasks that involve comparing two sentences or snippets, for example whether a word is used the same
                  wayintwosentences (WiC), whether one sentence is a paraphrase of another, or whether one sentence implies another.
                  This could also explain the comparatively low scores for RTE and CB, which also follow this format. Despite these
                  weaknesses, GPT-3 still outperforms a ﬁne-tuned BERT-large on four of eight tasks and on two tasks GPT-3 is close to
                  the state-of-the-art held by a ﬁne-tuned 11 billion parameter model.
                  Finally, we note that the few-shot SuperGLUE score steadily improves with both model size and with number of
                  examples in the context showing increasing beneﬁts from in-context learning (Figure 3.8). We scale K up to 32
                  examples per task, after which point additional examples will not reliably ﬁt into our context. When sweeping over
                  values of K, we ﬁnd that GPT-3 requires less than eight total examples per task to outperform a ﬁne-tuned BERT-Large
                  onoverall SuperGLUE score.
                  3.8  NLI
                  Natural Language Inference (NLI) [Fyo00] concerns the ability to understand the relationship between two sentences.
                  In practice, this task is usually structured as a two or three class classiﬁcation problem where the model classiﬁes
                                                                         20
