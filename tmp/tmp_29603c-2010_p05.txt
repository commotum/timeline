                               Policy Iteration with REPS
                 input: features œÜ(s), maximal information loss ,
                 initial policy œÄ0(a|s).
                 for each policy update k
                                                              0
                    Sampling: Obtain N samples (s ,a ,s ,r ) using
                                                       i   i  i  i
                    current policy œÄk(a|s) in an on-policy setting.
                    Critic: Evaluate policy for Œ∑ and Œ∏.
                       for every sample i = 0 to N do:                                   Figure 2: Performance on the mountain-car problem.
                           n (s ,a ) = n (s ,a ) + (r +œÜT Œ∏ ‚àíœÜT Œ∏)
                            Œ¥   i  i      Œ¥   i  i       i     0       s
                                                              s         i
                                                               i
                           n (s ,a ) = n (s ,a )+(œÜ 0 ‚àíœÜ )
                            Œõ i i          Œõ i i          si     si
                           d(s ,a ) = d(s ,a ) + 1
                              i   i        i   i
                       Bellman Error Function: Œ¥ (s,a) = nŒ¥(s,a)
                                                    Œ∏          d(s,a)
                       Feature Difference: Œõ(s,a) = nŒõ(s,a)
                                                         d(s,a)
                       ComputeDualFunction:
                                         P             1        
                                          1    N     Œµ+ Œ¥ (s ,a )
                       g(Œ∏,Œ∑) = Œ∑log                e   Œ∑ Œ∏ i i                        Figure 3: Simulated setup for learning robot table tennis.
                                          N    i=1
                       ComputetheDualFunction‚Äôs Derivative :
                                   N   Œµ+1Œ¥ (s ,a )
                                      e   Œ∑ Œ∏ i i Œõ(s ,a )
                       ‚àÇ g = Œ∑     i=1                i  i                            method, it differs signiÔ¨Åcantly in two parts, i.e., the critic of
                         Œ∏                 Œµ+1Œ¥ (s ,a )
                                       N e Œ∑ Œ∏ i i                                    SARSAconverges slower, and the additional multiplication
                                    i=1                 
                                     P          1
                                       N     Œµ+ Œ¥ (s ,a )                             by the previous policy results in a faster pruning of taken
                       ‚àÇ g = log           e    Œ∑ Œ∏ i i
                         Œ∑             i=1                                            bad actions in the REPS approach. As a result, REPS is
                               N    Œµ+1Œ¥ (s ,a )
                                   e  Œ∑ Œ∏ i i 1 Œ¥ (s ,a )
                           ‚àí i=1                Œ∑2 Œ∏ i i                              signiÔ¨ÅcantlyfasterthanSARSAascanbeobservedinFig.2.
                                         Œµ+1Œ¥ (s ,a )
                                     N e Œ∑ Œ∏ i i
                                     i=1                                              Primitive Selection in Robot Table Tennis
                       Optimize: (Œ∏‚àó,Œ∑‚àó) = fmin BFGS(g,‚àÇg,[Œ∏ ,Œ∑ ])
                                                                      0   0           Table tennis is a hard benchmark problem for robot learn-
                                                        ‚àó        T ‚àó
                       Determine Value Function: VŒ∏ (s) = œÜ Œ∏
                                                                 s                    ing that includes most difÔ¨Åculties of complex skill.        The
                    Actor: Compute new policy œÄk+1(a|s).                              setup is shown in Fig. 3. A key problem in a skill learn-
                                        œÄ (a|s)exp 1 Œ¥ ‚àó(s,a)                         ing system with multiple motor primitives (e.g., many dif-
                       œÄ     (a|s) =     k        (Œ∑‚àó Œ∏       ) ,
                         k+1              œÄ (a|s)exp( 1 Œ¥ ‚àó(s,b))                     ferent forehands, backhands, smashes, etc.) is the selection
                                         b k          Œ∑‚àó Œ∏                            of task-appropriate primitives triggered by an external stim-
                 Output: Optimal policy œÄ‚àó(a|s).                                      ulus. Here, wehavegeneratedalargesetofmotorprimitives
               Table 2: Algorithmic description of Policy Iteration based             that are triggered by a gating network that selects and gener-
               on Relative Entropy Policy Search. This version of the al-             alizes amongthemsimilartoamixtureofexperts. REPSim-
               gorithm extends the one in Table 1 for practical application.          proves the gating network by reinforcement learning where
               Note that N is not a Ô¨Åxed number but may change after ev-              any successful hit results as a reward of +1 and for failures
               ery iteration.                                                         no reward is given. REPS appears to be sensitive to good
                                                                                      initial sampling policies. The results vary considerably with
               thermore, we also observe that our REPS policy iteration               initial policy performance. When the system starts with an
               yields a signiÔ¨Åcantly higher performance. A comparison                 initial policy that has a success rate of ‚àº24%, it may quickly
               with PoWER (Kober and Peters 2009) was not necessary                   converge prematurely yielding a success rate of ‚àº39%. If
               as the episodic form of REPS appears to be equivalent to the           provided a better initialization, it can reach success rates of
               applicable version of PoWER. The performance of all three              upto‚àº59%.
               methods for all three problems is shown in Fig. 1 (a-c).                              Discussion & Conclusion
               Mountain-CarProblem                                                    In this paper, we have introduced a new reinforcement learn-
                                                                                      ing method called Relative Entropy Policy Search. It is de-
               Themountaincarproblem(SuttonandBarto1998)isawell-                      rived from a principle as previous covariant policy gradient
               knownprobleminreinforcement learning.                                  methods (Bagnell and Schneider 2003), i.e., attaining max-
                  Weadaptthecodefrom(Hernandez2010)andemploythe                       imal expected reward while bounding the amount of infor-
               same tile-coding features for both SARSA and REPS. We                  mationloss. Unlikeparametricgradientmethod,itallowsan
               implementouralgorithminthesamesettingsandareableto                     exact policy update and may use data generated while fol-
               show that REPS policy iteration also outperforms SARSA.                lowing an unknown policy to generate a new, better policy.
               While SARSAissuperÔ¨Åcially quite similar to the presented               It resembles the well-known reinforcement learning method
                                                                                1611
