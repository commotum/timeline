                 Method      Scheme        Information                                         Naive     Cross       I     T     mAP        NDS
                                          L      I     T     mAP      NDS              (a)                                       24.83      40.36
                                          ✓                 24.83     40.36            (b)       ✓                               27.64      43.09
                                          ✓           ✓ 26.74         42.97            (c)                                 ✓ 27.75          43.71
                   (I)        Concat      ✓ ✓               41.59     48.60            (d)                 ✓               ✓ 32.11          47.51
                                          ✓ ✓ ✓ 44.57 52.19                            (e)                          ✓ ✓ 47.04 54.40
                                          ✓           ✓ 27.75         43.71            (f)                 ✓        ✓ ✓ 51.78 58.96
                   (II)      Self-Attn    ✓ ✓               43.22     49.49
                                          ✓ ✓ ✓ 47.04 54.40                         Table 5. Effectiveness of data augmentation. Naive [35]: origi-
              Table 3. Analysis of information fusion and fusion schemes. Con-      nal copy-and-paste scheme on point cloud only. cross: our cross-
              cat: concatenate the grid-wise BEV features between different in-     sensor and cross-time augmentation. T: the sequential input of
              puts and fuse with convolution layers. Self-Attn: treat grid fea-     point cloud. I+T: the sequential input of both images and points.
              tures as separate tokens and fuse with self-attention. Inputs: Lidar             PA     PE      PC     Sparse      mAP        NDS
              points (L), images (I), and sequential information in time (T).
                                                                                       (g)                                       49.76      57.84
                    Length         Ours        ∆        Cat [20]       ∆               (h)     ✓                                 50.25      57.95
                     T=1           24.83        -        24.83          -              (i)     ✓       ✓                         50.50      58.44
                     T=2           27.75     +2.92       26.60       +1.57             (j)     ✓       ✓      ✓                  51.30      58.51
                     T=3           30.37     +5.54       25.64       +0.81             (k)     ✓       ✓      ✓         ✓        51.78      58.96
                     T=4           30.77     +5.94       25.52       +0.69          Table 6. Ablation results on architecture components. PA: the
                     T=5           30.97     +6.14       26.07       +1.24          point-wise attention operation in grid feature encoder. PE: our
                 T=2(+Img)         47.04    +22.21       43.74      +19.91          proposed 4D relative positional encoding . PC: the pyramid con-
              Table4. Comparisonsoftheinputsequencelengths. Cat: thepoint           text. Sparse: the sparse window partition for 4D attention.
              concatenation scheme [20] for sequential point clouds. Ours: the
              proposed fusion method using self-attention.                              (2) Benefits of fusion scheme (I, II): On top of the single-
                                                                                    frame point cloud detector, our proposed sensor-time 4D
              Figure 5. By introducing the cross-sensor information in              attention module (last line) achieves an overall +22.21%
              camera features, the 3D detector can better perceive small            performance gain. Besides, the proposed attention fusion
              objects and eliminate false detections. Besides, our method           scheme (II) consistently achieves better detection accu-
              can further enhance the 3D perception by exploiting the               racy than the simple concatenation fusion scheme (I), i.e.
              complementaryinformation across sensors and time, which               43.22 vs 41.59 for L+I input and 27.75 vs 26.74 for L+T
              is beneficial to more accurate and stable predictions.                input. The information misalignment is a crucial problem
                                                                                    for feature fusion, and cannot be well handled by straight-
              4.3. Ablation Studies                                                 forward concatenation. The superior performance demon-
                 Weconduct ablation studies on the nuScenes dataset to              strates the capability of our proposed attention mechanism
              validate each proposedcomponent. Forefficiency, weapply               to effectively model the information interaction across sen-
              1/8 subset of the training set to train the network and test on       sors and time.
              the whole validation set.                                                 In Table 4, we further illustrate the ability of our method
                                                                                    to model temporal correlations. As shown in the last line,
              Effects of informationfusion. Wecomparedifferentinfor-                replacing our attention mechanism with the point concate-
              mation fusion settings and fusion schemes in Table 3. We              nation scheme for temporal fusion [20] yields a 3.3% mAP
              summarize the following observations:                                 drop. Comparing Ours (second column) with Cat (fourth
                 (1) Benefits of information fusion (I): Based on a single-         column), we consistently observe larger discrepancy when
              frame point cloud detector (first line), the introduction of          increasing the length of the input sequence, which suggests
              camera feature (second line) and sequential point cloud               the superiority of our method to aggregate information over
              (third line) yields considerable improvements of +16.76%              a longer time period. Note that we set T = 2 throughout
              and +1.91% mAP respectively, illustrating the valuable                experiments to alleviate computational load.
              complementary information from cross-sensor and tempo-                Effects of sensor-time data augmentation. We validate
              ral data. Furthermore, combining the LiDAR and image                  theeffectivenessofourproposeddataaugmentationscheme
              streams together leads to a large gain of +19.74% mAP.                in Table 5. As illustrated in (a) and (b), the original copy-
              This motivates us to take the full advantage of all available         and-paste operation yields an improvement of +2.81%
              data across sensors and time.                                         mAP, indicating the importance of data augmentation on
                                                                                17178
