        4
        is unlikely for the gradient ∂E to be canceled out for a mini-batch, because in
                            P      ∂xl
        general the term ∂    L−1F cannot be always -1 for all samples in a mini-batch.
                         ∂xl  i=l
        This implies that the gradient of a layer does not vanish even when the weights
        are arbitrarily small.
        Discussions
        Eqn.(4) and Eqn.(5) suggest that the signal can be directly propagated from
        any unit to another, both forward and backward. The foundation of Eqn.(4) is
        two identity mappings: (i) the identity skip connection h(xl) = xl, and (ii) the
        condition that f is an identity mapping.
           These directly propagated information ﬂows are represented by the grey ar-
        rows in Fig. 1, 2, and 4. And the above two conditions are true when these grey
        arrows cover no operations (expect addition) and thus are “clean”. In the fol-
        lowing two sections we separately investigate the impacts of the two conditions.
        3    On the Importance of Identity Skip Connections
        Let’s consider a simple modiﬁcation, h(xl) = λlxl, to break the identity shortcut:
                                  x    =λx +F(x,W),                                (6)
                                    l+1    l l      l   l
        where λl is a modulating scalar (for simplicity we still assume f is identity).
        Recursively applying this formulation we obtain an equation similar to Eqn. (4):
               Q             P Q
        xL =( L−1λi)xl+        L−1( L−1 λj)F(xi,Wi), or simply:
                 i=l           i=l   j=i+1
                                     L−1        L−1
                                     Y           Xˆ
                              x =(       λ )x +     F(x ,W ),                      (7)
                                L         i  l          i   i
                                     i=l         i=l
                            ˆ
        where the notation F absorbs the scalars into the residual functions. Similar to
        Eqn.(5), we have backpropagation of the following form:
                                      L−1            L−1          !
                         ∂E      ∂E    Y          ∂ X ˆ
                         ∂xl = ∂xL    (    λi) + ∂xl     F(xi,Wi) .                (8)
                                        i=l          i=l
        Unlike Eqn.(5), in Eqn.(8) the ﬁrst additive term is modulated by a factor
        QL−1λi. For an extremely deep network (L is large), if λi > 1 for all i, this
          i=l
        factor can be exponentially large; if λ < 1 for all i, this factor can be expo-
                                              i
        nentially small and vanish, which blocks the backpropagated signal from the
        shortcut and forces it to ﬂow through the weight layers. This results in opti-
        mization diﬃculties as we show by experiments.
           In the above analysis, the original identity skip connection in Eqn.(3) is re-
        placed with a simple scaling h(xl) = λlxl. If the skip connection h(xl) represents
        more complicated transforms (such as gating and 1×1 convolutions), in Eqn.(8)
                                Q
                                  L−1 0          0
        the ﬁrst term becomes         h where h is the derivative of h. This product
                                  i=l  i
        may also impede information propagation and hamper the training procedure
        as witnessed in the following experiments.
