                             modelsandifanewlearningparadigmbeyondstackingoflayersisrequiredtounleashthecapabilities
                             of LLMsincontinual setups.
                             CurrentModelsonlyExperiencetheImmediatePresent. Asananalogyandtobetterillustrate the
                             static nature of LLMs, we use the example of anterograde amnesia–a neurological condition where a
                             person cannot form new long-term memories after the onset of the disorder, while existing memories
                             remain intact [45]. This condition limits the person’s knowledge and experiences to a short window
                             of present and long past–before the onset of the disorder–which results in continuously experiencing
                             the immediate present as if it were always new. The memory processing system of current LLMs
                             suffer from a similar pattern. Their knowledge is limited to either, the immediate context that fits into
                             their context window, or the knowledge in MLP layers that stores long-past, before the onset of “end
                             of pre-training.” This analogy, has motivated us to take inspiration from neurophysiology literature
                             and how brain consolidate its short-term memories:
                             1.1   HumanBrainPerspectiveandNeurophysiologicalMotivation
                             Humanbrainishighly efficient and effective when it comes to continual learning (a.k.a. effective
                             context management), which is often attributed to neuroplasticity—the brain’s remarkable capacity
                             to change itself in response to new experiences, memories, learning, and even damage [46, 47].
                             Recent studies support that the formation of Long-term memory involves at least two distinct but
                             complementary consolidation processes [48–50]: (1) A rapid “online” consolidation (also known as
                             synaptic consolidation) phase occurs immediately or soon after learning, even during wakefulness.
                             This is when new and initially fragile memory traces are stabilized and begin transferring from
                             short-term to long-term storage; (2) An “offline” consolidation (also known as systems consolidation)
                             process repeats the replay of the recently encoded patterns—during sharp-wave ripples (SWRs) in
                             the hippocampus, coordinated with cortical sleep spindles and slow oscillations—strengthens and
                             reorganizes the memory and supports transfer to cortical sites [51–53].
                             Comingbacktotheanalogyofanterograde amnesia, evidence indicates that the condition can impact
                             both stages, but especially the online consolidation phase, mainly due to the fact that hippocampus is
                             the gateway for encoding new declarative memories, and so its damage means new information never
                             will be stored in long-term memory. As mentioned above, the design of LLMs, and more specifically
                             Transformer-based backbones, suffers from a similar condition after the pre-training phase. That
                             is, the information provided in the context, never impacts the long-term memory parameters (e.g.,
                             feedforward layers), and so the model is not capable of acquiring new knowledge or skill, unless
                             the information is still stored in the short-term memory (e.g., attention). To this end, although the
                             second stage is equally, or even more, crucial for the consolidation of memories, and its absence can
                             damage the process and might cause loss of memory [54, 55], in this work, we focus on the first
                             stage: memory consolidation as an online process. We provide additional discussion on human brain
                             perspective and its connection to NL in Appendix A.
                                                        N×d
                             Notations. We let x ∈ R         in be the input, Mt represent the state of memory/model M at time t,
                             Kbethekeys,Vbethevalues,andQbethequerymatrices. Weuseboldlowercase letters with
                             subscript t to refer to the vector corresponds to the input t (i.e., k ,v , and q ). We further refer to
                                                                                                   t   t       t
                             the distribution of any entities f as p(f). Through the paper, we use simple MLPs with L              ≥1
                                                                                                                               M
                             layers and residual connection as the architecture of the memory module M(·). When it is needed,
                             weparameterized the memory module with θ             ⊇{W ,W ,...,W            }, which at least includes
                                                                               M         1    2        L
                                                                                                         M
                             the parameters of linear layers in the MLP. We use superscript with parenthesis to refer to parameters
                             in different levels of nested learning (different update frequency): i.e., W(ℓ).
                             2    Nested Learning
                             This section discusses the motivations, formal definitions, and general high-level implications of
                             Nested Learning (NL). We start with a formulation of associative memory and then by using step-
                             by-step examples, we build the intuition behind architecture decomposition and its connection to
                             modeling a neural network as an integrated system of optimization problems. We aim to first show
                             howexisting methods and concepts in deep learning fall under the NL paradigm and then we present
                             new formulations that go beyond traditional methods and/or provide insights on how to improve
                             existing algorithms and designs.
                                                                                 3
