                                                  MNIST, N =3               MNIST, N =5              MNIST, N =10              MNIST, N =20             MNIST, N =200
                                                             z                        z                        z                         z                        z
                                            100                       100                      100                       100                      100
                                            110                       110                      110                       110                      110
                                         L  120                       120                      120                       120                      120
                                            130                       130                      130                       130                      130
                                            140                       140                      140                       140                      140
                                            150 5     6     7     8   150 5    6     7      8  150 5     6     7     8   150 5     6     7     8  150 5     6     7     8
                                              10    10    10    10     10    10    10     10     10    10    10    10      10    10    10    10     10    10    10    10
                                            # Training samples evaluated
                                                                          Frey Face, N =2           Frey Face, N =5          Frey Face, N =10         Frey Face, N =20
                                                                     1600               z     1600                z     1600               z     1600               z
                                                                     1400                     1400                      1400                     1400
                                               Wake-Sleep (test)     1200                     1200                      1200                     1200
                                               Wake-Sleep (train)    1000                     1000                      1000                     1000
                                               AEVB (test)         L  800                      800                       800                      800
                                               AEVB (train)           600                      600                       600                      600
                                                                      400                      400                       400                      400
                                                                      200                      200                       200                      200
                                                                        0 5     6      7      8  0 5      6      7      8  0 5     6      7      8  0 5      6      7      8
                                                                       10     10     10     10   10     10     10    10    10    10     10     10   10     10     10     10
                                       Figure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the
                                       lower bound, for different dimensionality of latent space (N ). Our method converged considerably
                                                                                                                            z
                                       faster and reached a better solution in all experiments. Interestingly enough, more latent variables
                                       does not result in more overﬁtting, which is explained by the regularizing effect of the lower bound.
                                       Vertical axis: the estimated average variational lower bound per datapoint. The estimator variance
                                       was small (< 1) and omitted. Horizontal axis: amount of training points evaluated. Computa-
                                       tion took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an
                                       effective 40 GFLOPS.
                                       decoder output. Note that with hidden units we refer to the hidden layer of the neural networks of
                                       the encoder and decoder.
                                       Parameters are updated using stochastic gradient ascent where gradients are computed by differenti-
                                       ating the lower bound estimator ∇                    L(θ,φ;X)(seealgorithm 1),plusasmallweightdecayterm
                                                                                        θ,φ
                                       corresponding to a prior p(θ) = N(0,I). Optimization of this objective is equivalent to approxi-
                                       mate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower
                                       bound.
                                       We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the
                                       sameencoder(alsocalled recognition model) for the wake-sleep algorithm and the variational auto-
                                       encoder. All parameters, both variational and generative, were initialized by random sampling from
                                       N(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were
                                       adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01,
                                       0.02, 0.1} based on performance on the training set in the ﬁrst few iterations. Minibatches of size
                                       M=100wereused,withL=1samplesperdatapoint.
                                       Likelihood lower bound                  We trained generative models (decoders) and corresponding encoders
                                       (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case
                                       oftheFreyFacedataset(topreventoverﬁtting,sinceitisaconsiderablysmallerdataset). Thechosen
                                       number of hidden units is based on prior literature on auto-encoders, and the relative performance
                                       of different algorithms was not very sensitive to these choices. Figure 2 shows the results when
                                       comparing the lower bounds. Interestingly, superﬂuous latent variables did not result in overﬁtting,
                                       which is explained by the regularizing nature of the variational bound.
                                       Marginallikelihood               For very low-dimensional latent space it is possible to estimate the marginal
                                       likelihood of the learned generative models using an MCMC estimator. More information about the
                                       marginal likelihood estimator is available in the appendix. For the encoder and decoder we again
                                       usedneural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional
                                       latent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB
                                       and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo
                                       (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for
                                       the three algorithms, for a small and large training set size. Results are in ﬁgure 3.
                                                                                                             7
