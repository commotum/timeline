                            Under review as a conference paper at ICLR 2025
                   216      calculated as a learnable 1D encoding:
                   217
                   218                                    E     =W, E ∈Rd, W∈RL×d
                                                           pos       i      pos
                   219                                        i               i
                   220      where W is a learned matrix assigning a d-dimensional vector to each of the possible L positions;
                   221      Listhemaximuminputlength.
                   222      As seen in Figure 2, ARC tasks are generative and require mapping an input image to an output
                   223      image. Because image dimensions may vary across instances of the same task and even between
                   224      the input and output grids of the same instance, any model that generates candidate solutions to an
                   225      ARCinputmustbeableto“reason” at the pixel level. We adapt the ViT architecture to this setting
                   226      bymakingthefollowing key modifications:
                   227      – We introduce a decoder with cross-attention using the same positional encoding and attention
                   228         mechanisms of the encoder. After the final decoder layer N, the output embedding hN of
                   229                                                                                                       i
                               patch i is projected linearly and a softmax function is applied to predict pixel-wise values yˆ
                   230                                                                                                            i
                               as yˆ = Softmax(Linear(hN)). The cross-entropy loss is computed as the sum over pixels,
                                   i                        i
                   231         −Py log(yˆ).
                   232              i i      i
                   233      – To achieve the required pixel-level precision for the ARC task, we employ a patch size of 1 × 1,
                   234         effectively treating each pixel as an independent input token.
                   235      – To handle variable-sized grids, the flattened list of tokens is padded to a fixed maximum length.
                   236         This configuration enables the model to process and generate ARC task outputs pixel-by-pixel.
                   237
                   238      3.1    EXPERIMENTS
                   239
                   240      Data.    To evaluate ViT’s reasoning capabilities comprehensively, we treat each of the 400 public
                   241      training ARC tasks as an individual AVR problem. We generate a dataset of 1 million input-output
                   242      pairs per task using the RE-ARC generator (Hodel, 2024) and train all of our models (the vanilla
                   243      ViTand VITARCmodels)inasupervisedmannerfromscratch.
                   244
                   245      Hyperparameters and training protocol.          The ViT baseline consists of three layers with eight
                   246      attention heads and a hidden dimension of 128. We trained the model on various single-core GPU
                   247      nodes,includingP100,V100,andT4,usingabatchsizeof8foroneepoch. Wechosetotrainforone
                   248      epoch because most models showed signs of convergence within the epoch. Due to computational
                   249      resource limitations, we evaluated our major milestone models on the full set of 400 tasks. However,
                   250      for the ablation studies hereafter, we used a randomly sampled subset of 100 tasks. For more details
                   251      on the training process, please refer to Appendix B. Our code is available in the supplementary
                   252      materials and will be open-sourced upon publication.
                   253      Evaluation metric.     Weevaluate the model primarily on the percentage of solved instances, using
                   254      a strict criterion: an instance is considered solved only if all generated pixels, including padding
                   255      and border tokens, exactly match the ground truth. This approach is stricter than the original ARC
                   256      metric which permits up to three candidate solutions.
                   257
                   258      Results.    Figure 3 shows that the vanilla ViT performs poorly: a significant percentage of tasks
                   259      have a near 0% solve rate despite the million training examples per task. This points to fundamental
                   260      limitations of the ViT architecture that inhibit abstract visual reasoning. In the following sections,
                   261      weanalyze failure cases and investigate methods for enhancing the visual reasoning ability of ViT.
                   262
                   263
                   264      4    VISUAL TOKENS: A BETTER REPRESENTATION FOR VIT
                   265
                   266      Thebasicversionofour VITARCframeworkbuildsonthevanillaViTbutincludesthreesimpleyet
                   267      highly effective changes to the representation of the ARC grids. We refer to these changes as visual
                   268      tokens to emphasize a departure from the language-based tokenization perspective in the particular
                   269      setting of the ARC.
                                                                               5
