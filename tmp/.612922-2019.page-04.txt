                    discrete reasoning. Finally, we validate the devel-            Statistic                    Train       Dev      Test
                    opmentandtest portions of DROP to ensure their                 Numberofpassages              5565       582       588
                    quality and report inter-annotator agreement.                  Avg. passage len [words]    213.45    191.62    195.12
                                                                                   Numberofquestions           77,409     9,536     9,622
                    Passage extraction       Wesearched Wikipedia for              Avg. question len [words]    10.79     11.17     11.23
                    passages that had a narrative sequence of events,              Avg. questions / passage     13.91     16.38     16.36
                                                                                   Question vocabulary size    29,929     8,023     8,007
                    particularly with a high proportion of numbers, as
                    our initial pilots indicated that these passages were           Table 2: Dataset statistics across the different splits.
                    the easiest to ask complex questions about. We
                    found that National Football League (NFL) game                workers and gradually reduced our worker pool to
                    summaries and history articles were particularly              workers who understood the task and annotated it
                    promising, and we additionally sampled from any               well. Each HIT paid 5 USD and could be com-
                   Wikipedia passage that contained at least twenty               pleted within 30 minutes, compensating a trained
                    numbers.2 This process yielded a collection of
                    about 7,000 passages.                                         worker with an average pay of 10 USD/ hour.
                                                                                     Overall, we collected a total of 96,567 question-
                    Question collection       WeusedAmazonMechani-                answer pairs with a total Mechanical Turk budget
                    cal Turk3 to crowdsource the collection of question-          of 60k USD (including validation). The dataset
                    answer pairs, where each question could be an-                wasrandomlypartitioned by passage into training
                    sweredinthecontextofasingleWikipediapassage.                  (80%), development (10%) and test (10%) sets, so
                    In order to allow some ﬂexibility during the annota-          all questions about a particular passage belong to
                    tion process, in each human intelligence task (HIT)           only one of the splits.
                    workers were presented with a random sample of                Validation      In order to test inter-annotator agree-
                    5 of our Wikipedia passages, and were asked to                ment and to improve the quality of evaluation
                    produce a total of at least 12 question-answer pairs          against DROP, we collected at least two additional
                    on any of these.                                              answers for each question in the development and
                       Wepresented workers with example questions                 test sets.
                    from ﬁve main categories, inspired by ques-                      In a separate HIT, workers were given context
                    tions from the semantic parsing literature (addi-             passages and a previously crowdsourced question,
                    tion/subtraction, minimum/maximum,counting,se-                and were asked to either answer the question or
                    lection and comparison; see examples in Table 1),             mark it as invalid (this occurred for 0.7% of the
                    to elicit questions that require complex linguistic           data, which we subsequently ﬁltered out). We
                    understanding and discrete reasoning. In addition,            found that the resulting inter-annotator agreement
                    to further increase the difﬁculty of the questions            wasgoodandonparwithotherQAtasks;overall
                    in DROP, we employed a novel adverserial anno-                Cohen’s κ was 0.74, with 0.81 for numbers, 0.62
                    tation setting, where workers were only allowed               for spans, and 0.65 for dates.
                    to submit questions which a real-time QA model
                                               4
                    BiDAFcouldnot solve.                                          4    DROPDataAnalysis
                       Next, each worker answered their own question
                    with one of three answer types: spans of text from            In the following, we quantitatively analyze proper-
                    either question or passage, a date (which was com-            ties of passages, questions, and answers in DROP.
                    moninhistoryandopen-domaintext)andnumbers,                    Different statistics of the dataset are depicted in Ta-
                    allowed only for questions which explicitly stated            ble 2. Notably, questions have a diverse vocabulary
                    a speciﬁc unit of measurement (e.g., “How many                of around 30k different words in our training set.
                    yards did Brady run?”), in an attempt to simplify             Question analysis        To assess the question type
                    the evaluation process.                                       distribution, we sampled 350 questions from the
                       Initially, we opened our HITs to all United States         training and development sets and manually anno-
                       2We used an October 2018 Wikipedia dump, as well as        tated the categories of discrete operations required
                    scraping of online Wikipedia.                                 to answer the question. Table 1 shows the distri-
                       3www.mturk.com                                             bution of these categories in the dataset. In addi-
                       4WhileBiDAFisnolongerstate-of-the-art,performanceis
                    reasonable and the AllenNLP implementation (Gardner et al.,   tion, to get a better sense of the lexical diversity of
                    2017) made it the easiest to deploy as a server.              questions in the dataset, we ﬁnd the most frequent
                                                                             2371
