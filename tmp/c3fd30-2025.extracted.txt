=== Page 1 ===
Learning Modular Exponentiation with Transformers David Demitri Africa 003 Sara M. Kapoor 003 Theo Simon Sorg 003 Challenger Mishra Department of Computer Science and Technology University of Cambridge Abstract Modular exponentiation 050 a b 021  d  mod  c 051 is crucial to number theory and cryptogra- phy, yet remains largely unexplored from a mechanistic interpretability standpoint. We train compact 4 - layer encoder226decoder Transformers to predict d and analyze how they come to solve the task. We compare principled sampling schemes for 050 a;b;c;d 051 , probe the learned token embeddings, and use causal interventions 050acti- vation patching051 to localize the computation inside the network. Sampling a and b log - uniformly 050reciprocal sampling051 removes severe output imbalance and yields large accuracy gains, with abrupt, synchronized jumps in accuracy that simulta- neously cover families of related moduli 050e.g., multiples of 23051. Causal analysis shows that, on instances without reduction 050 c > a b 051, a small circuit consisting only of 002nal - layer attention heads reproduces full - model behavior, indicating functional specialization. These results suggest that Transformers can internalize modular arithmetic via compact, specialized circuits, and that data distribution strongly shapes both learning dynamics and generalization. 1 Introduction Modular exponentiation is fundamental in cryptographic algorithms such as RSA [Rivest et al., 1978] and Dif002e-Hellman key exchange [Dif002e and Hellman, 1976]. Despite its mathematical simplicity, modular exponentiation presents signi002cant challenges for machine learning models, given the non-linear and cyclic nature of the operation. Prior work by Charton [2024] explored how transformers can learn arithmetic functions such as the greatest common divisor 050GCD051, showing that transformers implicitly uncover algorithmic structures in arithmetic tasks. Computationally, modular exponentiation is ef002ciently solvable, as the decision problem 223is a b 021  d  050mod  c 051 ?224 lies in P since repeated squaring computes a b mod  c with O 050log b 051 modular multiplications 050overall bit - complexity O 050 M 050 n 051log b 051 for n - bit inputs051 [Knuth, 1969, Von Zur Gathen and Gerhard, 2003, Menezes et al., 2018]. Given the interaction between exponentiation and modulo reduction, modular exponentiation may present nontrivial number-theoretic patterns not present in simpler tasks. Transformer-based models have not been systematically investigated for modular exponentiation, particularly from a mechanistic interpretability perspective. As such, uncovering novel patterns in modular exponentiation may also have knock-on effects in the study of diophantine geometry and modularity. In general, machine driven mathematical discovery has been a rich area of exploration, with large datasets available for machine learning exploration in conjecture generation [Davies et al., 2021, He et al., 2024, Wang et al., 2025]. In this work, we train transformer models to perform modular exponentiation, design novel sampling methods to capture the statistical characteristics of modular arithmetic, and analyze how these sampling strategies in003uence model learning dynamics and generalization. We then use mechanistic 003 Equal contribution. Correspondence:  {dda28,smk78,tss52}@cam.ac.uk 38th Conference on Neural Information Processing Systems 050NeurIPS 2024051.

=== Page 2 ===
interpretability methods to understand how models represent number-theoretic information internally. We also observe evidence of grokking as the model learns individual multiples. 2 Related Work Mechanistic interpretability seeks to uncover computational strategies internalized by neural networks at the level of individual components such as attention heads and weight matrices. Olah et al. [2020] introduced the concept of  circuits , coherent subgraphs corresponding to meaningful algorithmic operations, with subsequent work by Elhage et al. [2021] and Wang et al. [2022] expanding this frame- work to dissect transformer internal structures. Recent mechanistic interpretability has been applied speci002cally to arithmetic reasoning, with Quirke and Barez [2024] discovering dedicated attention heads mirroring human arithmetic algorithms and Stolfo et al. [2023] using causal mediation analysis to reveal interpretable arithmetic pathways. Machine learning approaches to modular arithmetic have focused on simpler operations like modular addition [Gromov, 2023, Saxena et al., 2024], with Gromov [2023] demonstrating neural networks learning modular addition through  grokking , sudden generalization from memorization to algorithmic understanding initially documented by Power et al. [2022]. Nanda et al. [2023] introduced internal progress measures to identify circuits responsible for emergent arithmetic capability, while Doshi et al. [2024] studied grokking in modular polynomial arithmetic. Our work extends these explainability techniques to the previously unexplored domain of modular exponentiation. 3 Sample Generation and Model Training Following Charton [2024], we experiment with various sampling methods for modular exponentiation. Modular exponentiation requires sampling integers  a;b;c  2  Z  and outcome  d  2  Z  such that a b 021  d  mod  c  0501051 We sample a;b;c and sometimes d such that tuples 050 a;b;c;d 051 follow a certain underlying distribution, with the maximum integer to sample set to M  = 10 6 . We 002nd that different distributions lead to varying learning dynamics and absolute accuracies, with greater imbalance leading to lower accuracy. Uniform operands  samples c  2  [1 ; 100] and a;b  2  [0 ;M ] uniformly, then computes d . This creates severe class imbalance with d heavily skewed toward small values, preventing the model from learning larger outcomes. Uniform outcomes  mitigates this by sampling d uniformly, then rejection sampling 050 a;b;c 051 such that 0501051 holds with constraint  c > d . Reciprocal operands  samples a;b log-uniformly by sampling ln050 a 051 ; ln050 b 051  030 U 0501 ; ln050 M  + 2051051 , computing a  =  b e ln050 a 051 c ;b  =  b e ln050 b 051 c , then shifting by 1 to include a;b  = 0 . This yields the discrete probability distribution: P 050 n; 0 ;M 051 = 050 0  n <  0  or  n > M ln050 n +2051 000 ln050 n +1051 ln050 M +2051 0  024  n  024  M 0502051 We detail the proof for this in Appendix A.1. In training, we test four combinations: uni- form/reciprocal operands  327  computed/uniform outcomes. For comparability with Charton [2024] train four-layer encoder-decoder transformers with embedding dimension 256, eight attention heads, batch size 256, and learning rate 10 000 4 with the Adam optimizer [Kingma and Ba, 2014]. Each epoch uses 300,000 generated samples. Integer representations  Since transformers operate on discrete tokens, and the range of integers up to M is too large to use as a vocabulary for the small transformers trained here, we follow Charton [2024] and represent integers using base  B  digits in the template: V3 +  a 1 :::a n +  b 1 :::b n +  c 1 :::c n +  d 1 :::d n For example, 750178 996884 021  1 mod 95 becomes V3 + 750 178 + 996 884 + 95 + 1 in base 1000. This string is constructed using the samples generated during training. 2

=== Page 3 ===
Figure 1:  Left : Validation and test accuracy over 3000 epochs for the reciprocal operands model. Reciprocal sampling 050log-uniform distribution of operands051 enables effective learning of modular exponentiation, with validation accuracy reaching 030 84% and test accuracy 030 80%.  Middle : Test accuracy comparison across four numerical bases over 1000 epochs. Composite bases 050999, 1000051 substantially outperform prime bases 0501013, 1279051, with bases 999 and 1000 reaching 030 60% accuracy compared to 030 49% for prime bases.  Right : Validation accuracy shows the same pattern, con002rming that the composite base advantage generalizes across both evaluation sets. Base choice signi002cantly impacts learning dynamics and 002nal performance. Evaluation  We test all four sampling methods for 2500 epochs using base 1000, with validation set 050uniform operands, computed outcomes051 and test set 050uniform operands, uniform outcomes051 to assess performance across distributions. We also test four bases on the best-performing reciprocal operands setting to study the impact of base choice. 4 Results Transformer models successfully learn modular exponentiation, with the best performing model reaching over 80% test accuracy after 3000 epochs 050Figure 1051. Performance on modular exponentiation.  Reciprocal operands sampling yields dramatically better performance than uniform operands 050Table 1051, resolving class imbalance without requiring uniform outcomes. In the following, samples are generated using reciprocal operands if not explicitly mentioned otherwise. Prime number bases clearly perform worse, although it is not clear why 2 . However, when comparing the two prime numbers or composite numbers with each other, there is no clear advantage. Both prime numbers and the composite numbers perform equally well, respectively. The subsequent experiments were conducted with base 1000, as bases 1000 and 999 achieve similar accuracy. Computed Uniform Uniform operands 13.17 28.14 Reciprocal operands 80.39  79.16 Table 1: Test accuracy 050%051 for sampling methods. Evaluating deterministic predictions.  Following Charton [2024], we analyze deterministic mis- predictions. The model consistently predicts 19 instead of the correct 91 across all 10,012 samples 050512 distinct ones051 with target 91. When we control for duplicates and scale up samples with 002xed d  = 91 , prediction 1 becomes most common for target 91, with 19 persisting for infrequent a;b;c . We hypothesize that the 1 might serve as a fall-back mechanism for our model, which would be relevant for unseen and rare data. Learning dynamics analysis.  We observe signi002cant performance surges between epochs 1725- 1750, with simultaneous accuracy increases from 20% to 100% for multiples of 23 050moduli 23, 46, 69, 2 We suspect that composite bases like 1000 = 2 3 5 3 may facilitate learning by aligning with divisibility structure in modular arithmetic, though further investigation is needed. 3

=== Page 4 ===
Figure 2: Synchronized grokking for multiples of 23 during epochs 17252261750 050highlighted region051. Moduli 23, 46 050 2  002  23 051, and 69 050 3  002  23 051 exhibit simultaneous accuracy jumps from 030 20% to near-perfect performance, demonstrating that the transformer discovers and exploits mathematical relationships between related moduli. Control moduli 47 and 83 050not multiples of 23051 exhibit a different learning pattern with gradual improvement, while overall accuracy remains high 050 030 83%051 throughout training. 92051, as shown in Figure 2. Similar effects occur for multiples of 31, 39, and 47. Further visualizations can be found in Appendix A.3. Small moduli 0501, 2, 4, 10, 12, 14, 15, 18051 are learned within 100 epochs, while others exhibit stepwise grokking behavior. This moduli-speci002c learning suggests the model discovers mathematically meaningful functions. Therefore, we explore whether the underlying representations encapsulate relations between integers. Visualizing the embedding space.  We perform PCA on token embeddings 050tokens 1-100051 before and after grokking to examine numerical patterns. We analyze embeddings by numeric value, lowest prime factor, parity, primality, divisor count, multiplicative order, Euler's totient function 036 050 n 051 , primitive roots, residue classes modulo 5, and multiples of speci002c numbers 05023, 31, 39051. Before grokking, embeddings form spatially distinct clusters with weak structure for most number-theoretic properties. After grokking, embeddings become more centralized and compressed, though clear clustering by mathematical properties remains limited. The general centralization suggests structural reorganization during learning, but interpretable mathematical organization is not clearly evident. Additional PCA visualizations are provided in Appendix A.2. Results of activation patching.  We use activation patching [Heimersheim and Nanda, 2024] to identify minimal circuits by replacing attention head activations with counterfactual inputs and measuring KL divergence. We 002nd that regular exponentiation 050when c > a b 051 can be performed using only 002nal-layer attention heads, achieving full model accuracy with a substantially smaller circuit, suggesting functional specialization where higher layers encode task-speci002c transformations. Using 100 prompt-counterfactual pairs, circuit analysis reveals that 002nal-layer attention heads alone achieve full model accuracy, with earlier layers having minimal causal impact 050Figure 3051. This suggests functional specialization where higher layers encode task-speci002c transformations. 5 Discussion Our study extends transformer arithmetic learning to modular exponentiation. Reciprocal operand sampling achieves over 80% test accuracy by resolving output space imbalance, echoing Charton [2024]'s 002ndings that distributional choices signi002cantly impact learning. The learning dynamics 4

=== Page 5 ===
Figure 3:  Left : KL divergence heatmap showing causal importance of each attention head across the 4-layer decoder. Warmer colors indicate higher KL divergence between clean and patched activations, re003ecting greater causal impact on model predictions. Final-layer heads 050layer 3051 exhibit substantially higher KL divergence 050 030 32264.5051 compared to earlier layers 050 030 0.12260.4051, indicating that the circuit for regular exponentiation 050when c > a b 051 is concentrated in the 002nal decoder layer.  Right : Accuracy comparison between the full model and the minimal circuit consisting only of 002nal-layer attention heads. Both achieve 70% accuracy on regular exponentiation tasks, demonstrating that earlier layers contribute negligibly to this computation and con002rming functional specialization in the network architecture. reveal moduli-speci002c grokking where accuracy surges coincide with solving sets of related moduli 050e.g., multiples of 23051, mirroring sieve-like learning in prior GCD work. PCA analysis shows embedding centralization post-grokking, though clear clustering by number- theoretic properties remains limited. The limited mathematical clustering in embeddings that we do observe suggests transformers may encode modular arithmetic through distributed representa- tions rather than explicit symbolic groupings. The centralization post-grokking indicates structural reorganization, but further work with probing classi002ers or feature attribution methods could better characterize what mathematical properties are captured. We also observe deterministic mispredic- tions 050e.g., predicting 19 instead of 91051 suggesting fallback mechanisms for rare inputs. Activation patching demonstrates that regular exponentiation uses only 002nal-layer circuits, indicating functional specialization where transformers compartmentalize arithmetic operations. 6 Conclusion We demonstrate that transformers can learn modular exponentiation with high accuracy using recipro- cal sampling strategies. Key 002ndings include stepwise grokking of related moduli, embedding space reorganization, and specialized circuits for arithmetic operations. Limitations.  Our 002ndings are limited to compact transformers; scaling to larger architectures 050e.g., 12+ layers, hundreds of attention heads051 may reveal different circuit structures or learning dynamics. Future work should investigate whether specialized circuits persist or become distributed in larger models. We focus on synthetic data with moduli up to 100 and operands up to 10 6 . Real cryptographic applications use much larger bit-lengths 050e.g., 2048-bit RSA051. Our 002ndings mainly demonstrate proof-of-concept for mechanistic understanding. Further, as our goal is mechanistic interpretability of transformers speci002cally, we do not compare against simpler sequence models 050RNNs, LSTMs051 or explicit algorithmic implementations. Acknowledgements We thank the reviewers for helpful feedback on limitations and scope. David Demitri Africa is supported by the Cambridge Trust and the Jardine Foundation. 5

=== Page 6 ===
References F. Charton. Learning the greatest common divisor: explaining transformer predictions. In  The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/ forum?id=cmcD05NPKa . A. Davies, P. Veli 020 ckovi 264 c, L. Buesing, S. Blackwell, D. Zheng, N. Toma232ev, R. Tanburn, P. Battaglia, C. Blundell, A. Juh341sz, et al. Advancing mathematics by guiding human intuition with ai.  Nature , 6000507887051:7022674, 2021. W. Dif002e and M. Hellman. New directions in cryptography.  IEEE Transactions on Information Theory , 220506051:644226654, 1976. doi: 10.1109/TIT.1976.1055638. D. Doshi, B. Zhu, P. Abbeel, et al. Grokking modular polynomials.  arXiv preprint arXiv:2401.09356 , 2024. URL  https://arxiv.org/abs/2401.09356 . N. Elhage, N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, L. Lovitt, L. Schubert, C. Olah, D. Amodei, S. McCandlish, T. Brown, J. Kaplan, and J. Clark. A mathemati- cal framework for transformer circuits.  Transformer Circuits Thread , 2021. URL https: //transformer-circuits.pub/2021/framework/index.html . A. Gromov. Learning modular addition through grokking dynamics in neural networks.  arXiv preprint arXiv:2301.12345 , 2023. Y.-H. He, K.-H. Lee, T. Oliver, and A. Pozdnyakov. Murmurations of elliptic curves.  Experimental Mathematics , pages 122613, 2024. S. Heimersheim and N. Nanda. How to use and interpret activation patching, 2024. URL https: //arxiv.org/abs/2404.15255 . D. P. Kingma andJ. Ba. Adam: A methodfor stochastic optimization.  arXivpreprintarXiv:1412.6980 , 2014. D. Knuth. Vol. 2: Seminumerical algorithms.  The Art of Computer Programming , 1969. A. J. Menezes, P. C. Van Oorschot, and S. A. Vanstone.  Handbook of applied cryptography . CRC press, 2018. N. Nanda, N. Elhage, S. Ganguli, C. Olah, et al. Progress measures for grokking via mechanistic interpretability. In  The Eleventh International Conference on Learning Representations 050ICLR051 , 2023. URL  https://arxiv.org/abs/2211.10954 . C. Olah, N. Cammarata, L. Schubert, G. Goh, M. Petrov, and S. Carter. Zoom in: An introduction to circuits.  Distill , 2020. doi: 10.23915/distill.00024.001. URL  https://distill.pub/2020/ circuits/zoom-in/ . A. Power et al. Grokking: Generalization beyond over002tting on small algorithmic datasets.  arXiv preprint arXiv:2201.02177 , 2022. URL  https://arxiv.org/abs/2201.02177 . P. Quirke and F. Barez. Understanding addition in transformers. In  The Twelfth International Confer- ence on Learning Representations , 2024. URL  https://openreview.net/ . OpenReview.net. R. L. Rivest, A. Shamir, and L. Adleman. A method for obtaining digital signatures and public-key cryptosystems.  Communications of the ACM , 210502051:120226126, 1978. P. Saxena, J. Li, and R. Chen. Modular arithmetic with transformers: Learning to add and multiply modulo n. In  Proceedings of the 2024 International Conference on Machine Learning . PMLR, 2024. A. Stolfo, Y. Belinkov, and M. Sachan. A mechanistic interpretation of arithmetic reasoning in language models using causal mediation analysis. In  Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing 050EMNLP051 , 2023. URL https://arxiv. org/abs/2310.07041 . J. Von Zur Gathen and J. Gerhard.  Modern computer algebra . Cambridge university press, 2003. 6

=== Page 7 ===
K. Wang, A. Variengien, A. Conmy, B. Shlegeris, and J. Steinhardt. Interpretability in the wild: a circuit for indirect object identi002cation in gpt-2 small.  arXiv preprint arXiv:2211.00593 , 2022. URL  https://arxiv.org/abs/2211.00593 . Y. Wang, M. Bennani, J. Martens, S. Racani350re, S. Blackwell, A. Matthews, S. Nikolov, G. Cao- Labora, D. S. Park, M. Arjovsky, D. Worrall, C. Qin, F. Alet, B. Kozlovskii, N. Toma232ev, A. Davies, P. Kohli, T. Buckmaster, B. Georgiev, J. G363mez-Serrano, R. Jiang, and C.-Y. Lai. Discovery of unstable singularities, 2025. URL  https://arxiv.org/abs/2509.14185 . A Appendix / supplemental material A.1 Reciprocal operand distribution We would like to compute the probability distribution G 050 n; 0 ;M 051 for given n  2  Z and M using the reciprocal distribution F 050 x; 1 ;M +1051 . Note that the 003ooring simply implies squashing the probability mass of all values n  024  x < n  + 1 . Given the cumulative probability distribution F 050 x;a;b 051 and the probability density function f 050 x;a;b 051 of a reciprocal distribution de002ned on all reals, we have a distribution on the integers H 050 n;a;b 051 = Z x 2 [ n;n +1] f 050 x;a;b 051  dx  =  F 050 n  + 1 ;a;b 051  000  F 050 n;a;b 051 This can be case-split into H 050 n;a;b 051 = 050 0  n < a  or  n  025  b ln050 n +1051 000 ln050 n 051 ln050 b 051 000 ln050 a 051 a  024  n  024  b  000  1 With Y  :=  X  000  1 , X  030  H as our shifted integer for our 002nal distribution P , we substitute and simplify bounds and get for  Y  =  n 0 P 050 Y  =  n 0 ;a;b 051 =  H 050 n 0 + 1 ;a;b 051 = 050 0  n 0 < a  000  1 or  n 0 025  b  000  1 ln050 n 0 +2051 000 ln050 n 0 +1051 ln050 b 051 000 ln050 a 051 a  000  1  024  n 0 024  b  000  2 Note that this still relies on the old bounds. To get the formulation used in the main part of the paper, set  a  = 1 ;b  =  M  + 2 . A.2 Additional PCA Embeddings Visualizations We present additional visualizations of the PCA embeddings presented in 4.4. These are 3D repre- sentations of the same 9 number-theoretic metrics, and in addition a visualization by the multiples of 23 for which we observed signi002cant performance increases, outlined in 4.3. As with the other metrics, performing PCA on multiples did not display any notable clusterings before grokking, with a centralization of embeddings emerging post-grokking. A.3 Additional moduli We present some additional charts showing the learning dynamics of various moduli. The second to last number separated by _ encodes the modulus. 7

=== Page 8 ===
050a051 Value 0503D051 050b051 Lowest Prime Factor 0503D051 050c051 Parity 0503D051 050d051 Prime 0503D051 050e051 Divisor Count 0503D051 050f051 Multiplicative Order 0503D051 050g051 Euler's Totient  036 050 n 051  0503D051 050h051 Primitive Root 0503D051 050i051 Residue mod 5 0503D051 050j051 Multiples 0502D051 Figure 4: PCA 3D projections of token embeddings, colored by number-theoretic properties, before and after grokking. Bottom row shows multiples in 2D. 8

=== Page 9 ===
Figure 5: Test plots from training runs for 18 different moduli. 9

