                 for the WikiText-103 dataset, as it is a relatively    token prediction, comparing it against several rep-
                 small dataset. The evaluation metric is perplex-       resentative RPEs as well as commonly used APEs.
                 ity (PPL), and we train the models with sequence       Next, we analyze runtime and memory efficiency,
                 length of 1024. All the models are trained on eight    followed by a detailed comparison with another
                 H100 GPUs with 80G GPU RAM. Training set-              context-aware RPE, namely DAPE (Zheng et al.,
                 tings are the same as those used for GPT-2 (Rad-       2024b). We then provide a visualization of the
                 ford et al., 2019). Gradients are updated after pro-   positional biases learned by CABLE. Finally, we
                 cessing 524,288 tokens and vocab size is 50304.        conduct an ablation study, demonstrating that cer-
                 For training on the FineWeb-Edu-10B dataset, we        tain modifications can further enhance CABLE’s
                 run 19k steps (~1 epoch) with batch sizes of 64,       performance in specific scenarios.
                 32, and 16 for the tiny, small, and medium mod-        5.1   LengthExtrapolation
                 els, respectively. On WikiText-103, the tiny, small,
                 and medium variants are trained for 9k, 5k, and 3k     Ourmethoddemonstrates strong length extrapola-
                 steps(~10, 5, 3 epochs respectively). The learning     tion performance on the FineWeb-Edu-10B dataset,
                 rate starts at 0.0006, with a linear warmup over 750   whentrained with a sequence length of 1024 and
                 steps, followed by cosine decay to a minimum of        evaluated on shorter and longer sequences, as
                 0.00006.                                               shown in Figure 1. Table 1 further compares the
                 4.3   Baselines                                        extrapolation capabilities of CABLE against base-
                                                                        line methods on the test sets of FineWeb-Edu-10B
                 Wecompareourmethodagainstthefollowingpo-               and WikiText-103. 1 The sinusoidal method suf-
                 sitional encoding approaches:                          fers a sharp performance drop even with slight in-
                    Learnable (Vaswani, 2017): A trainable APE          creases in sequence length. RoPE shows a simi-
                 where each position is associated with a learned       lar trend—initial improvement followed by a sig-
                 embedding. The number of positions is fixed and        nificant decline at longer lengths. The learnable
                 predefined during training.                            method performs competitively at 512 and 1024
                    Sinusoidal (Vaswani, 2017): A fixed APE             tokens but lacks extrapolation ability beyond the
                 used in early Transformer models (Vaswani, 2017;       training length. T5-bias follows a similar trend, but
                 Baevski and Auli, 2018; Ott et al., 2018; Lewis        its performance degrades more gradually than Si-
                 et al., 2021).                                         nusoidal. It initially extrapolates well to sequences
                    RoPE(Su et al., 2024): A non-learnable non-         slightly longer than those seen during training,
                 additive RPE widely adopted in LLMs such as            thanks to its mechanism of learning relative po-
                 GPT-2 (Brown et al., 2020), LLaMA (Touvron             sitional information and reusing the maximum rela-
                 et al., 2023), PaLM (Chowdhery et al., 2023), and      tive distance for unseen positions. ALiBi performs
                 Gemma(Teametal.,2024a,b).                              well on longer contexts overall but experiences
                    ALiBi(Press et al., 2021): A non-learnable ad-      slightly degradation at extreme lengths.
                 ditive RPE used in models like BLOOM (Le Scao             In contrast, our method consistently achieves
                 et al., 2023) and Falcon (Almazrouei et al., 2023).    lower PPL on longer sequences. Specifically, for
                    T5-bias (Raffel et al., 2020): A learnable addi-    models trained on sequence length of 1024, our
                 tive RPE used in the T5 model.                         method achieves lower PPL even when extrapolat-
                    Kerple (Chi et al., 2022a): A learnable additive    ing to sequences 16 times longer.
                 RPEwithlogarithmic and power variants; we use             Moreover, on the FineWeb-Edu-10B dataset,
                 the logarithmic variant due to its superior perfor-    which contains far more tokens than WikiText-103,
                 mance.                                                 a model trained with ALiBi on T=1024 performs
                    Fire (Li et al., 2023): A learnable additive RPE    well on T=2048 but begins to degrade with longer
                 designed to give more weight to distant query-key      sequences. In contrast, CABLE shows consistent
                 pairs than other methods.                              improvement, even for T=15360, and achieves a
                                                                        better PPL than it does on T=1024, the sequence
                 5 Results                                              length seen during training.
                                                                           Ourresults demonstrate that the learned biases
                 Weevaluate the effectiveness of our method across         1For the longest sequences tested, we report results for
                 multiple settings. First, we examine its extrapo-      15,360 tokens instead of 16,384 due to computational con-
                 lation capability in decoder-only models for next-     straints.
                                                                   30368
