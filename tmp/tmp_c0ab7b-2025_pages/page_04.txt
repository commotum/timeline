                                                  0
                                                                                                        0
                                                 -1   0
                                                                                                             0
                                             ⊙
                                                                                                    ⊙
                                                 -2   -1  0
                                                                                                                 0
                                                 -3   -2  -1   0
                                                                                                                      0
                                                                                                      CABLE Positional biases
                                                                        Pre-softmax Attention Logits
                                             ALiBi Positional biases
                 Figure 2: Comparison of how ALiBi and CABLE compute final attention scores per head. Left: ALiBi adds
                 constant linear biases with head-specific slopes, fixed across tokens. Right: CABLE adds learned, token-specific
                 context-aware biases and weights to the scores.
                                                                                                      t
                 3 ProposedMethod                                      position and obtain S(X) ∈ R .
                                                                                                      +
                 In this section, we formally introduce CABLE                       S(X)=Lf (X),                      (3)
                 (Context-Aware Biases for Length Extrapolation),                            (θ
                 a novel additive relative positional encoding (RPE)                  Li,j =   1 ifj ≤ i              (4)
                 approach designed to enhance the length general-                              0 otherwise
                 ization capabilities of Transformer models.
                   CABLEcomputescontext-awarepositional bias           Where L ∈ Rt×t is a lower triangular matrix of
                 scores for each attention head and adds them to the   ones. Therefore, the relative biases B(X) ∈ Rt×t
                 pre-softmax attention logits. Unlike existing RPE     for each pair of tokens in the input sequence is
                 methods, which are typically static and indepen-      calculated by:
                 dent of the input sequence, our proposed biases are              B(X) =S(X) −S(X)                    (5)
                 dynamically conditioned on the input context. Sim-                     i,j         i        j
                 ilar to the ALiBi method, we incorporate relative     At this stage, we have obtained context-aware bi-
                 positional biases at the attention score level. How-  ases that can be directly added to the pre-softmax
                 ever, CABLEintroducestwokeymodifications: (1)         attention logits. In our experiments, we refer to
                 the biases are learned and explicitly dependent on    this version—without any additional learnable pa-
                 the input context, and (2) we learn distinct scalar   rameters for the biases—as CABLE       . To further
                 weights for these biases. To implement this, we                                           NW
                 employtwoseparatelinearlayers—onetogenerate           enhance flexibility, we introduce a linear layer that
                 the context-aware biases and another to compute       learnsabiasweightvectorgθ(X)foreachtoken,al-
                 their associated weights.                             lowing the model to modulate (dampen or amplify)
                   Lettanddbethesequencelengthandthedimen-             the positional biases based on the input context.
                 sion of embeddingsoneachhead,respectively. The                    g (X) = Softplus(Xw )              (6)
                                                                                    θ                       s
                 learned bias for each token in the input sequence
                        t×d                                                          t×d       t                       d
                 X∈R isasfollows:                                      Where g : R        →− R , and θ = {ws ∈ R }.
                                                                                θ              +
                                                                       Finally, we multiply each relative bias by its corre-
                              f (X) = ReLU(Xw )                 (2)
                               θ                    c                  sponding weight to produce the final CABLE bias
                 Where f : Rt×d →− Rt , and θ = {w          ∈ Rd}.     for each attention head, as follows:
                          θ              +               c
                 Hence,weobtaincontextdependentbiasesforeach                 B(X) =g (X)(S(X) −S(X) )                 (7)
                 token. Here ReLU ensures the biases are positive.                  i,j   θ     i      i         j
                 Then, by taking a cumlative sum of these biases we       CABLEexhibitsaninductivebiassimilartoslid-
                 inherently make these biases aware of how much        ing window attention by penalizing distant query-
                 positional bias should be incorporated until their    key pairs—penalties that increase with positional
                                                                  30366
