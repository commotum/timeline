                                             Context-aware Biases for Length Extrapolation
                                                         ∗                                      ∗                                   ∗
                                             Ali Veisi       HamidrezaAmirzadeh                     AmirM.Mansourian
                                                                                  Algonet
                                       {ali.veisi, h.amirzadeh, amir.mansurian}@algonetlabs.com
                                               Abstract                                     24         Extrapolation for models trained on sequence length of 1024
                                                                                                Pos Method
                                                                                                   Sinusoidal
                                                                                            22     Learnable
                          Transformers often struggle to generalize to                             T5-Bias
                                                                                                   Kerple
                          longer sequences than those seen during train-                    20     RoPE
                                                                                                   Fire
                          ing—a limitation known as length extrapola-                              ALiBi
                          tion. Most existing Relative Positional Encod-                    xity   CABLE
                                                                                            erple18
                          ing (RPE) methods attempt to address this by                      P
                          introducingeitherfixedlinearbiasesorglobally                      16
                          learned biases, which lack the capacity to adapt
                          to different input contexts. In this work, we pro-                14
                          pose an additive RPE, Context-Aware Biases                        12
                                                                                                512       1024       2048       4096       8192      15360
                          for Length Extrapolation (CABLE), a method                                                   Context Length
                          that learns token-specific, context-aware biases                Figure 1:        Next-token prediction perplexity on
                          for each attention head in transformers. By                     FineWeb-Edu-10B eval set with varying inference se-
                          dynamically adjusting positional biases based                   quence lengths. The models are GPT-2 Medium trained
                          onthe input sequence, CABLE overcomes the                       on a sequence length of 1024 on FineWeb-Edu-10B
                          rigidity of fixed RPEs. When evaluated on                       train set.
                          sequences longer than originally trained with,
                          GPT-2 Medium (334M parameters) with CA-
                          BLE achieves lower perplexity than counter-                     are shown to be position-agnostic and need po-
                          parts using other widely adopted positional
                          encoding methods. Additionally, by apply-                       sition information (Yun et al., 2019). However,
                          ing CABLE to the BERT base model we im-                         evenbyincorporating positional information, trans-
                          proved performance in long-context retrieval                    former models often experience a sharp decline in
                          tasks. Our method significantly enhances the                    accuracy when processing inputs longer than those
                          extrapolation performance of existing RPE                       seen during training (Press et al., 2021; Anil et al.,
                          methods tested on the FineWeb-Edu-10B and                       2022). This limitation arises because training is
                          WikiText-103 datasets. Our code is available                    typically performed on short sequences to mitigate
                          at: https://github.com/AlgonetLabs/Cable.                       the quadratic cost of attention. As a result, there is
                     1 Introduction                                                       increasing interest in the length extrapolation prob-
                     Transformer based language models (Vaswani,                          lem—namely, a model’s ability to generalize to
                     2017) have achieved state-of-the-art performance                     and accurately predict sequences longer than those
                     in many Natural Lnaguge Processing (NLP) tasks                       encountered during training (Press et al., 2021).
                     (Devlin et al., 2019; Liu, 2019; Chowdhery et al.,                      Manycommonlyusedpositionalencodingmeth-
                     2023; Team et al., 2023; Touvron et al., 2023;                       ods, such as Absolute Positional Encoding (APE)
                     Achiametal., 2023). This is related to its attention                 (Vaswani, 2017), fail to generalize effectively to
                     mechanismthatcaptures contextual information by                      sequencelengthsbeyondthoseseenduringtraining
                     considering inter-token interactions. However, in                    (Kazemnejad et al., 2024). To address the length
                     contrast to Convolutional Neural Networks (CNNs)                     extrapolation challenge in transformers, various
                     (Gehring et al., 2017) and Recurrent Neural Net-                     strategies have been proposed, including context
                     works (RNNs) (Sherstinsky, 2020), which implic-                      windowextension(Beltagyetal., 2020; Chen et al.,
                     itly consider positional information, transformers                   2023b;Pengetal.,2023;Zhuetal.,2023),memory
                                                                                          mechanisms (Dai, 2019; Bulatov et al., 2022; Wu
                          ∗All authors contributed equally to this work.                  et al., 2022; Tworkowski et al., 2024), context com-
                                                                                    30363
                            Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 30363–30384
                                                  November4-9,2025©2025AssociationforComputational Linguistics
