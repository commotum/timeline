                 Limitation                                              Tom Brown, Benjamin Mann, Nick Ryder, Melanie
                                                                            Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
                 While CABLE demonstrates strong performance                Neelakantan, Pranav Shyam, Girish Sastry, Amanda
                 in length extrapolation, it has several limitations.       Askell, et al. 2020. Language models are few-shot
                 First, it incurs higher training time compared to          learners. Advances in neural information processing
                 RoPEduetoitsdynamicbiascomputation, though                 systems, 33:1877–1901.
                 this overhead is negligible in inference. Second,       Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev.
                 CABLEoccasionally underperforms RoPE at base               2022. Recurrent memory transformer. Advances
                 sequence lengths (e.g., 1024 tokens in our exper-          inNeuralInformationProcessingSystems,35:11079–
                 iments), particularly in tasks where fixed posi-          11091.
                 tional patterns suffice, suggesting a trade-off be-     Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu
                 tween adaptability and consistency for shorter con-        Lian, and Zheng Liu. 2024. Bge m3-embedding:
                 texts. Additionally, the method’s computational            Multi-lingual, multi-functionality, multi-granularity
                                                                            text embeddings through self-knowledge distillation.
                 overhead, though minimal, may become more pro-             arXiv preprint arXiv:2402.03216.
                 nounced for extremely long sequences (>100K to-         Pu-Chin Chen, Henry Tsai, Srinadh Bhojanapalli,
                 kens), and its extrapolation capabilities remain de-       Hyung Won Chung, Yin-Wen Chang, and Chun-
                 pendent on the diversity of positional patterns in         Sung Ferng. 2021. A simple and effective posi-
                 training data. While empirical results are promis-         tional encoding for transformers.  arXiv preprint
                 ing, theoretical analysis of its attention dynamics at     arXiv:2104.08698.
                 arbitrary lengths remains an open question. Future      ShouyuanChen,ShermanWong,LiangjianChen,and
                 workcouldexplore optimizations for training effi-         Yuandong Tian. 2023a. Extending context window
                 ciency and head-specific bias adaptation to further        of large language models via positional interpolation.
                 enhance flexibility.                                       arXiv preprint arXiv:2306.15595.
                                                                         Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
                                                                            Zhijian Liu, Song Han, and Jiaya Jia. 2023b. Lon-
                 References                                                 glora: Efficient fine-tuning of long-context large lan-
                                                                            guage models. arXiv preprint arXiv:2309.12307.
                 Josh Achiam, Steven Adler, Sandhini Agarwal, Lama       Ta-Chung Chi, Ting-Han Fan, Peter J Ramadge, and
                    Ahmad, Ilge Akkaya, Florencia Leoni Aleman,             Alexander Rudnicky. 2022a. Kerple: Kernelized rel-
                    Diogo Almeida, Janko Altenschmidt, Sam Altman,          ative positional embedding for length extrapolation.
                    ShyamalAnadkat,etal.2023. Gpt-4technicalreport.        Advances in Neural Information Processing Systems,
                    arXiv preprint arXiv:2303.08774.                        35:8386–8399.
                 Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-      Ta-Chung Chi, Ting-Han Fan, Alexander I Rudnicky,
                    shamsi, Alessandro Cappelli, Ruxandra Cojocaru,         and Peter J Ramadge. 2022b. Dissecting transformer
                    Mérouane Debbah, Étienne Goffinet, Daniel Hess-         length extrapolation via the lens of receptive field
                    low, Julien Launay, Quentin Malartic, et al. 2023.      analysis. arXiv preprint arXiv:2212.10356.
                    The falcon series of open language models. arXiv
                    preprint arXiv:2311.16867.                           AakankshaChowdhery,SharanNarang,JacobDevlin,
                 Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor              MaartenBosma,GauravMishra,AdamRoberts,Paul
                    Lewkowycz, Vedant Misra, Vinay Ramasesh, Am-            Barham, Hyung WonChung,Charles Sutton, Sebas-
                    brose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam        tian Gehrmann, et al. 2023. Palm: Scaling language
                    Neyshabur. 2022. Exploring length generalization in     modeling with pathways. Journal of Machine Learn-
                    large language models. Advances in Neural Informa-      ing Research, 24(240):1–113.
                    tion Processing Systems, 35:38546–38556.             Zihang Dai. 2019. Transformer-xl: Attentive language
                 Alexei Baevski and Michael Auli. 2018. Adaptive input      modelsbeyondafixed-lengthcontext. arXiv preprint
                    representations for neural language modeling. arXiv     arXiv:1901.02860.
                    preprint arXiv:1809.10853.                           Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
                                                                            Kristina Toutanova. 2019. Bert: Pre-training of deep
                 Federico   Barbero,    Alex    Vitvitskyi,  Christos       bidirectional transformers for language understand-
                    Perivolaropoulos,  Razvan Pascanu, and Petar            ing. In Proceedings of the 2019 conference of the
                        ˇ    ´
                    Velickovic. 2024. Round and round we go! what           North American chapter of the association for com-
                    makes rotary positional encodings useful?   arXiv       putational linguistics: human language technologies,
                    preprint arXiv:2410.06205.                              volume 1 (long and short papers), pages 4171–4186.
                 Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.    Weiguo Gao. 2024. Mep: Multiple kernel learning
                    Longformer: The long-document transformer. arXiv        enhancing relative positional encoding length extrap-
                    preprint arXiv:2004.05150.                              olation. arXiv preprint arXiv:2403.17698.
                                                                    30372
