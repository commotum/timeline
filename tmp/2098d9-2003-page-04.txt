                                                   BENGIO,DUCHARME,VINCENTANDJAUVIN
                      (running,walking), we could naturally generalize (i.e. transfer probability mass) from
                                                 The cat is walking in the bedroom
                      to                         A dog was running in a room
                      and likewise to           The cat is running in a room
                                                 A dog is walking in a bedroom
                                                 The dog was walking in the room
                                                            ...
                      and manyother combinations. In the proposed model, it will so generalize because “similar” words
                      are expected to have a similar feature vector, and because the probability function is a smooth
                      function of these feature values, a small change in the features will induce a small change in the
                      probability. Therefore, the presence of only one of the above sentences in the training data will in-
                      crease the probability, not only of that sentence, but also of its combinatorial number of “neighbors”
                      in sentence space (as represented by sequences of feature vectors).
                      1.2 Relation to Previous Work
                      The idea of using neural networks to model high-dimensional discrete distributions has already
                      been found useful to learn the joint probability of Z1···Zn, a set of random variables where each is
                      possibly of a different nature (Bengio and Bengio, 2000a,b). In that model, the joint probability is
                      decomposed as a product of conditional probabilities
                                ˆ                               ˆ
                                P(Z =z ,···,Z =z )=            P(Z =z|g(Z         =z ,Z =z ,···,Z =z )),
                                    1    1       n     n    ∏ i i i i−1               i−1   i−2    i−2       1    1
                                                             i
                      where g(.) is a function represented by a neural network with a special left-to-right architecture,
                      with the i-th output block g () computing parameters for expressing the conditional distribution of
                                                    i
                      Zi given the value of the previous Z’s, in some arbitrary order. Experiments on four UCI data sets
                      show this approach to work comparatively very well (Bengio and Bengio, 2000a,b). Here we must
                      deal with data of variable length, like sentences, so the above approach must be adapted. Another
                      important difference is that here, all the Zi (word at i-th position), refer to the same type of object (a
                      word). Themodelproposedherethereforeintroduces asharing ofparameters acrosstime–thesame
                      g is used across time – that is, and across input words at different positions. It is a successful large-
                        i
                      scale application of the same idea, along with the (old) idea of learning a distributed representation
                      for symbolic data, that was advocated in the early days of connectionism (Hinton, 1986, Elman,
                      1990). More recently, Hinton’s approach was improved and successfully demonstrated on learning
                      several symbolic relations (Paccanaro and Hinton, 2000). The idea of using neural networks for
                      language modeling is not new either (e.g. Miikkulainen and Dyer, 1991). In contrast, here we push
                      this idea to a large scale, and concentrate on learning a statistical model of the distribution of word
                      sequences, rather than learning the role of words in a sentence. The approach proposed here is also
                      related to previous proposals of character-based text compression using neural networks to predict
                      the probability of the next character (Schmidhuber, 1996). The idea of using a neural network for
                      language modeling has also been independently proposed by Xu and Rudnicky (2000), although
                      experiments are with networks without hidden units and a single input word, which limit the model
                      to essentially capturing unigram and bigram statistics.
                           The idea of discovering some similarities between words to obtain generalization from training
                      sequences to new sequences is not new. For example, it is exploited in approaches that are based on
                      learning a clustering of the words (Brown et al., 1992, Pereira et al., 1993, Niesler et al., 1998, Baker
                                                                         1140
