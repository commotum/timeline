                           Published as a conference paper at ICLR 2022
                                      Context Memory XLcache arXiv PG19 C4(4K+) GitHub Isabelle
                                      512         None       None      3.29   13.71    17.20      3.05      3.09
                                      2048        None       None      2.69   12.37    14.81      2.22      2.39
                                      512         None        512      2.67   12.34    15.38      2.26      2.46
                                      2048        None       2048      2.42   11.88    14.03      2.10      2.16
                                      512         1536       None      2.61   12.50    14.97      2.20      2.33
                                      512         8192       None      2.49   12.29    14.42      2.09      2.19
                                      512         8192        512      2.37   11.93    14.04      2.03      2.08
                                      512         65K         512      2.31   11.62    14.04      1.87      2.06
                                      2048        8192       2048      2.33   11.84    13.80      1.98      2.06
                                      2048        65K        2048      2.26   11.37    13.64      1.80      1.99
                                  Table 4: Average token-level perplexities of each model when trained for 500k steps.
                           together into one long document. Unlike the Github corpus, we order the ﬁles according to their
                           import dependencies, so that later ﬁles use sub-theorems that are proved in earlier ﬁles.
                           C4(4K+) C4,thecolossalcleanedcommoncrawl,isaverylargecollectionofdocumentsthathave
                           been scraped from the internet (Raffel et al., 2020). We ﬁltered out all documents that have less than
                           4096tokens to focus on documents where memory can have an impact.
                           PG-19 PG-19isalargedatasetofEnglish-language books, published prior to 1919, which were
                           retrieved from the Project Gutenberg archive (Rae et al., 2020; Sun et al., 2021). PG-19 is one of the
                           few public datasets that only contains full-length books, and has become a benchmark for long-range
                           natural language text modeling.
                           4.2   EXPERIMENTAL METHOD
                           Weused a 12-layer decoder-only transformer (with and without Transformer-XL cache) with an
                           embedding size of 1024, 8 attention heads of dimension 128, and an FFN hidden layer of size 4096.
                           For all of our experiments, we used k = 32. Unless speciﬁed otherwise, we use the 9th layer as the
                           kNNaugmentedattention layer. We used a sentence-piece (Kudo & Richardson, 2018) tokenizer
                           with a vocabulary size of 32K.
                           WeusedtheAdafactoroptimizer (Shazeer & Stern, 2018). In preliminary experiments, we conducted
                           a hyperparameter search to determine the optimal learning rate among three choices ({3.0, 1.0,
                           3 · 10−1}), and found that 1.0 works best. We used a linear warmup schedule for the ﬁrst 1000
                           steps, followed by square root decay. We trained the models from scratch for 500K steps on all the
                           datasets, except for the Isabelle dataset. Isabelle is small, so we stopped training after 100K steps
                           whenthemodelbegantooverﬁt. Weranallofourexperiments on 32 TPU cores. Our models were
                           implemented in JAX (Bradbury et al., 2018) and Flax (Heek et al., 2020).
                           Whencomparingmodelswithdifferentcontext lengths, we adjusted the batch size (the number of
                                                                           17
                           documents in a batch) so that there are always 2   tokens in a batch. E.g., a model with a context
                           length of 512 has a batch size of 256, while the 2048 model has a batch size of 64.
                           Weexperimented with multiple implementations of approximate kNN lookup with different tradeoffs
                           between quality and computational cost. We did not observe a signiﬁcant degradation of the model
                           quality when switching to lower quality approximations of kNN, so the model appears to be quite
                           robust with respect to the quality of kNN retrieval. For a model with around 200M trainable
                           parameters the step time increased from 0.2s to 0.25s when we added a memory of size 8K, and to
                           0.6s when we added a memory of size 65K (measured on TPUv3).
                           4.3   EFFECT OF EXTERNAL MEMORY
                           Adding external memory results in substantial gains across datasets and architectures,          as
                           showninTable4. Across all ﬁve datasets, adding external memory to either the vanilla Transformer
                           or the Transformer-XL architecture improves perplexity by a substantial amount. For example, on
                                                                           6
