           sequence. Leveraging the DiT framework, DynamicCity seamlessly incorporates various conditions to guide
           the 4D generation process, enabling awiderangeofapplications including hexplane-conditional generation,
           trajectory-guided generation, command-driven scene generation, layout-conditioned generation, and dynamic
           scene inpainting.
           Our contributions can be summarized as follows:
              • We propose DynamicCity, a high-quality, large-scale 4D occupancy generation framework consisting of a
               tailored VAE for HexPlane fitting and a DiT-based network for HexPlane generation, which supports
               various downstream applications.
              • In the VAE architecture, DynamicCity employs a novel Projection Module to benefit in encoding
               4D scenes into compact HexPlanes, significantly improving HexPlane fitting quality. Following, an
               Expansion & Squeeze Strategy is introduced to decode the HexPlanes for reconstruction, which improves
               both fitting efÏciency and accuracy.
              • Building on fitted HexPlanes, we design a Padded Rollout Operation to reorganize HexPlane features
               into a masked 2D square feature map, enabling compatibility with DiT training.
              • Extensive experimental results demonstrate that DynamicCity achieves significantly better 4D re-
               construction and generation performance than previous SoTA methods across all evaluation metrics,
               including generation quality, training speed, and memory usage.
           2 RelatedWork
           3DObjectGeneration has been a central focus in machine learning, with diffusion models playing a significant
           role in generating realistic 3D structures. Many techniques utilize 2D diffusion mechanisms to synthesize
           3D outputs, covering tasks like text-to-3D object generation [20], image-to-3D transformations [43], and
           3D editing [31]. Meanwhile, recent methods bypass the reliance on 2D intermediaries by generating 3D
           outputs directly in three-dimensional space, utilizing explicit [1], implicit [19], triplane [44], and latent
           representations [30]. Although these methods demonstrate impressive 3D object generation, they primarily
           focus on small-scale, isolated objects rather than large-scale, scene-level generation [13, 46]. This limitation
           underscores the need for methods capable of generating complete 3D scenes with complex spatial relationships.
           UrbanSceneGeneration extends the scope to larger, more complex environments. Earlier works used VQ-
           VAE [50] and GAN-based models [5, 23] to generate LiDAR scenes. However, recent advancements have
           shifted towards diffusion models [14, 22, 24, 27, 47, 50], which better handle the complexities of expansive
           outdoor scenes. For example, [17] utilize voxel grids to represent large-scale scenes but often face challenges
           with empty spaces like skies and fields. While some recent works incorporate temporal dynamics to extend
           single-frame generation to sequences [41, 49], they often lack the ability to fully capture the dynamic nature
           of 4D environments. Thus, these methods typically remain limited to short temporal horizons or struggle
           with realistic dynamic object modeling, highlighting the gap in generating high-fidelity 4D scenes.
           4DGeneration represents a leap forward, aiming to capture the temporal evolution of scenes. Prior works
           often leverage video diffusion models [4, 35] to generate dynamic sequences [36], with some extending to
           multi-view [33, 45] and single-image settings [32] to enhance 3D consistency. In the context of video-conditional
           generation, approaches such as [16, 28, 29] incorporate image priors for guiding generation processes. While
           these methods capture certain dynamic aspects, they lack the ability to generate long-term, high-resolution
           4D scenes with versatile applications. Our method, DynamicCity, fills this gap by introducing a novel 4D
           generation framework that efÏciently captures large-scale dynamic environments, supports diverse generation
           tasks (e.g., trajectory-guided [2], command-driven generation), and offers substantial improvements in scene
           fidelity and temporal modeling.
           3 Preliminaries
           HexPlane [7, 11] is an explicit and structured representation designed for efÏcient modeling of dynamic
           3D scenes, leveraging feature planes to encode spacetime data. A dynamic 3D scene is represented as six
                                               3
