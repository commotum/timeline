                         Table 1: Maximumpathlengths,per-layercomplexityandminimumnumberofsequentialoperations
                         for different layer types. n is the sequence length, d is the representation dimension, k is the kernel
                         size of convolutions and r the size of the neighborhood in restricted self-attention.
                             Layer Type                Complexity per Layer   Sequential   MaximumPathLength
                                                                              Operations
                             Self-Attention                  O(n2·d)             O(1)              O(1)
                             Recurrent                       O(n·d2)             O(n)              O(n)
                                                                     2
                             Convolutional                  O(k·n·d )            O(1)           O(log (n))
                                                                                                      k
                             Self-Attention (restricted)    O(r·n·d)             O(1)             O(n/r)
                         bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d
                                                                                                                model
                         as the embeddings, so that the two can be summed. There are many choices of positional encodings,
                         learned and ﬁxed [8].
                         In this work, we use sine and cosine functions of different frequencies:
                                                                                 2i/dmodel
                                                      PE(pos,2i) = sin(pos/10000        )
                                                                                 2i/dmodel
                                                    PE           =cos(pos/10000         )
                                                        (pos,2i+1)
                         where pos is the position and i is the dimension. That is, each dimension of the positional encoding
                         corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000·2π. We
                         chose this function because we hypothesized it would allow the model to easily learn to attend by
                         relative positions, since for any ﬁxed offset k, PE  can be represented as a linear function of
                                                                        pos+k
                         PE .
                             pos
                         Wealsoexperimented with using learned positional embeddings [8] instead, and found that the two
                         versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
                         because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
                         during training.
                         4   WhySelf-Attention
                         In this section we compare various aspects of self-attention layers to the recurrent and convolu-
                         tional layers commonly used for mapping one variable-length sequence of symbol representations
                         (x ,...,x ) to another sequence of equal length (z ,...,z ), with x ,z ∈ Rd, such as a hidden
                           1     n                                       1     n         i  i
                         layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
                         consider three desiderata.
                         Oneisthetotal computational complexity per layer. Another is the amount of computation that can
                         be parallelized, as measured by the minimum number of sequential operations required.
                         Thethird is the path length between long-range dependencies in the network. Learning long-range
                         dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
                         ability to learn such dependencies is the length of the paths forward and backward signals have to
                         traverse in the network. The shorter these paths between any combination of positions in the input
                         and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare
                         the maximumpathlength between any two input and output positions in networks composed of the
                         different layer types.
                         AsnotedinTable1,aself-attentionlayerconnectsallpositionswithaconstantnumberofsequentially
                         executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of
                         computational complexity, self-attention layers are faster than recurrent layers when the sequence
                         length n is smaller than the representation dimensionality d, which is most often the case with
                         sentence representations used by state-of-the-art models in machine translations, such as word-piece
                         [31] and byte-pair [25] representations. To improve computational performance for tasks involving
                         very long sequences, self-attention could be restricted to considering only a neighborhood of size r in
                                                                      6
