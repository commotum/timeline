                             Published as a conference paper at ICLR 2017
                             program and the input-output values, and then call an SMT solver to ﬁnd a satisfying setting of
                             the program variables. This approach shines when special-purpose reasoning can be leveraged, but
                             complex DSLscanleadtoverylarge constraint problems where constructing and manipulating the
                             constraints can be a lot slower than an enumerative approach.
                             Finally, stochastic local search can be employed to search over program space, and there is a long
                             history of applying genetic algorithms to this problem. One of the most successful recent examples
                             is the STOKE super-optimization system (Schkufza et al., 2016), which uses stochastic local search
                             to ﬁnd assembly programs that have the same semantics as an input program but execute faster.
                             Ranking.      While we focus on the search problem in this work, we brieﬂy mention the ranking
                             problem here. A popular choice for ranking is to choose the shortest program consistent with input-
                             output examples (Gulwani, 2016). A more sophisticated approach is employed by FlashFill (Singh
                             &Gulwani,2015).It works in a manner similar to max-margin structured prediction, where known
                             ground truth programs are given, and the learning task is to assign scores to programs such that the
                             ground truth programs score higher than other programs that satisfy the input-output speciﬁcation.
                             3    LEARNING INDUCTIVE PROGRAM SYNTHESIS (LIPS)
                             In this section we outline the general approach that we follow in this work, which we call Learning
                             Inductive Program Synthesis (LIPS). The details of our instantiation of LIPS appear in Sect. 4. The
                             componentsofLIPSare(1)aDSLspeciﬁcation,(2)adata-generationprocedure,(3)amachinelearn-
                             ing model that maps from input-output examples to program attributes, and (4) a search procedure
                             that searches program space in an order guided by the model from (3). The framework is related to
                             the formulation of Menon et al. (2013); the relationship and key differences are discussed in Sect. 6.
                             (1) DSL and Attributes.        The choice of DSL is important in LIPS, just as it is in any program
                             synthesis system. It should be expressive enough to capture the problems that we wish to solve, but
                             restricted as much as possible to limit the difﬁculty of the search. In LIPS we additionally specify
                             an attribute function A that maps programs P of the DSL to ﬁnite attribute vectors a = A(P).
                             (Attribute vectors of different programs need not have equal length.) Attributes serve as the link
                             between the machine learning and the search component of LIPS: the machine learning model
                             predicts a distribution q(a | E), where E is the set of input-output examples, and the search procedure
                             aims to search over programs P as ordered by q(A(P) | E). Thus an attribute is useful if it is both
                             predictable from input-output examples, and if conditioning on its value signiﬁcantly reduces the
                             effective size of the search space.
                             Possible attributes are the (perhaps position-dependent) presence or absence of high-level functions
                             (e.g., does the program contain or end in a call to SORT). Other possible attributes include control
                             ﬂow templates (e.g., the number of loops and conditionals). In the extreme case, one may set A
                             to the identity function, in which case the attribute is equivalent to the program; however, in our
                             experiments we ﬁnd that performance is improved by choosing a more abstract attribute function.
                                                                                           (n)   (n)   (n)  N                   (n)
                             (2) Data Generation.       Step 2 is to generate a dataset ((P   , a   , E   ))     of programs P      in
                                                                                                            n=1
                             the chosen DSL, their attributes a(n), and accompanying input-output examples E(n). Different ap-
                             proaches are possible, ranging from enumerating valid programs in the DSL and pruning, to training
                             a more sophisticated generative model of programs in the DSL. The key in the LIPS formulation is
                             to ensure that it is feasible to generate a large dataset (ideally millions of programs).
                             (3) Machine Learning Model.           The machine learning problem is to learn a distribution of at-
                             tributes given input-output examples, q(a | E). There is freedom to explore a large space of models,
                             so long as the input component can encode E, and the output is a proper distribution over attributes
                             (e.g., if attributes are a ﬁxed-size binary vector, then a neural network with independent sigmoid
                             outputs is appropriate; if attributes are variable size, then a recurrent neural network output could be
                             used). Attributes are observed at training time, so training can use a maximum likelihood objective.
                             (4) Search.     The aim of the search component is to interface with an existing solver, using the
                             predicted q(a | E) to guide the search. We describe speciﬁc approaches in the next section.
                                                                                 3
