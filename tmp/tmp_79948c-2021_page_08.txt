                            Table 4: Model performance on ImageNet. 1K only denotes training on ImageNet-1K only; 21K+1K
                            denotes pre-training on ImageNet-21K and ﬁnetuning on ImageNet-1K; PT-RA denotes applying
                            RandAugmentduring21Kpre-training, and E150 means 150 epochs of 21K pre-training, which is
                            longer than the standard 90 epochs. More results are in Appendix A.3.
                                             Models                    Eval Size   #Params     #FLOPs     ImageNetTop-1Accuracy
                                                                                                          1Konly        21K+1K
                                                                             2
                                                 EfﬁcientNet-B7          600         66M         37B        84.7            -
                                 ConvOnly                                    2
                                                 EfﬁcientNetV2-L         480        121M         53B        85.7          86.8
                                                                             2
                                                 NFNet-F3                416        255M       114.8B       85.7            -
                                                                             2
                                                 NFNet-F5                544        377M       289.8B       86.0            -
                                                                             2
                                                 DeiT-B                  384         86M        55.4B       83.1            -
                                                                             2
                               ViT-Stem TFM      ViT-L/16                384        304M       190.7B        -            85.3
                                                                             2
                                                 CaiT-S-36               384         68M        48.0B       85.0            -
                                                                             2
                                                 DeepViT-L               224         55M        12.5B       83.1            -
                                                                             2
                                                 Swin-B                  384         88M        47.0B       84.2          86.0
                              Multi-stage TFM                                2
                                                 Swin-L                  384        197M       103.9B        -            86.4
                                                                             2
                                                 BotNet-T7               384        75.1M       45.8B       84.7            -
                                                                             2
                                                 LambdaResNet-420        320           -          -         84.8            -
                                Conv+TFM                                     2
                                                 T2T-ViT-24              224        64.1M       15.0B       82.6            -
                                                                             2
                                                 CvT-21                  384         32M        24.9B       83.3            -
                                                                             2
                                                 CvT-W24                 384        277M       193.2B        -            87.7
                                                                             2
                                                 CoAtNet-0               224         25M         4.2B       81.6            -
                                                                             2
                                                 CoAtNet-1               224         42M         8.4B       83.3            -
                                                                             2
                                                 CoAtNet-2               224         75M        15.7B       84.1          87.1
                                                                             2
                                                 CoAtNet-3               224        168M        34.7B       84.5          87.6
                                                                             2
                                                 CoAtNet-0               384         25M        13.4B       83.9            -
                                                                             2
                                                 CoAtNet-1               384         42M        27.4B       85.1            -
                                                                             2
                                Conv+TFM         CoAtNet-2               384         75M        49.8B       85.7          87.1
                                                                             2
                                   (ours)        CoAtNet-3               384        168M       107.4B       85.8          87.6
                                                                             2
                                                 CoAtNet-4               384        275M       189.5B        -            87.9
                                                                             2
                                                   +PT-RA                384        275M       189.5B        -            88.3
                                                                             2
                                                   +PT-RA-E150           384        275M       189.5B        -            88.4
                                                                             2
                                                 CoAtNet-2               512         75M        96.7B       85.9          87.3
                                                                             2
                                                 CoAtNet-3               512        168M       203.1B       86.0          87.9
                                                                             2
                                                 CoAtNet-4               512        275M       360.9B        -            88.1
                                                                             2
                                                   +PT-RA                512        275M       360.9B        -            88.4
                                                                             2
                                                   +PT-RA-E150           512        275M       360.9B        -            88.56
                            ImageNet-21K As we can see from Table 4 and Fig. 3, when ImageNet-21K is used for pre-
                            training, the advantage of CoAtNet becomes more obvious, substantially outperforming all previous
                            models. Notably, the best CoAtNet variant achieves a top-1 accuracy of 88.56%, matching the ViT-
                            H/14 performance of 88.55%, which requires pre-training the 2.3x larger ViT model on a 23x larger
                            proprietary weakly labeled dataset (JFT) for 2.2x more steps. This marks a dramatic improvement in
                            both data efﬁciency and computation efﬁciency.
                            JFT Finally, in Table 5, we further evaluate CoAtNet under the large-scale data regime with JFT-
                            300MandJFT-3B.Encouragingly, our CoAtNet-4 can almost match the best previous performance
                            with JFT-300M set by NFNet-F4+, while being 2x more efﬁcient in terms of both TPU training
                            time and parameter count. When we scale up the model to consume similar training resource as
                            NFNet-F4+, CoAtNet-5 reaches 89.77% on top-1 accuracy, outperforming previous results under
                            comparable settings.
                            Moreover, as we further push the training resource towards the level used by ViT-G/14 and utilize the
                            sameJFT-3Bdataset of an even larger size [26], with over 4x less computation, CoAtNet-6 is able to
                                                                               8
