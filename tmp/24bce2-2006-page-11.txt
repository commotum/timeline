             AFastLearningAlgorithmforDeepBeliefNets                    1537
               1. Learn W0 assumingall the weight matrices are tied.
               2. Freeze W0 and commit ourselves to using WT to infer factorial ap-
                                                        0
                  proximateposteriordistributionsoverthestatesofthevariablesinthe
                  ﬁrsthiddenlayer,evenifsubsequentchangesinhigher-levelweights
                  meanthatthisinferencemethodisnolongercorrect.
               3. Keeping all the higher-weight matrices tied to each other, but untied
                  from W0, learn an RBM model of the higher-level “data” that was
                  producedbyusingWT totransformtheoriginaldata.
                                    0
               If this greedy algorithm changes the higher-level weight matrices, it
             is guaranteed to improve the generative model. As shown in Neal and
             Hinton(1998),thenegativelogprobabilityofasingledatavector,v0,under
             the multilayer generative model is bounded by a variational free en-
             ergy, which is the expected energy under the approximating distribution,
             Q(h0|v0), minus the entropy of that distribution. For a directed model, the
             “energy”oftheconﬁgurationv0,h0 isgivenby
                 E(v0,h0) =−[log p(h0)+log p(v0|h0)],                  (4.1)
             so the bound is
                 log p(v0) ≥  Q(h0|v0)[log p(h0)+log p(v0|h0)]
                            allh0
                           − Q(h0|v0)logQ(h0|v0),                     (4.2)
                             allh0
             whereh0isabinaryconﬁgurationoftheunitsintheﬁrsthiddenlayer, p(h0)
             is the prior probability of h0 under the current model (which is deﬁned by
             the weights above H ), and Q(·|v0) is any probability distribution over
                               0
             the binary conﬁgurations in the ﬁrst hidden layer. The bound becomes an
             equality if and only if Q(·|v0) is the true posterior distribution.
               Whenalloftheweightmatrices are tied together, the factorial distribu-
             tionover H producedbyapplyingWT toadatavectoristhetrueposterior
                      0                     0
             distribution, so at step 2 of the greedy algorithm, log p(v0) is equal to the
             bound.Step2freezesboth Q(·|v0)andp(v0|h0),andwiththesetermsﬁxed,
             the derivative of the bound is the same as the derivative of
                  Q(h0|v0)logp(h0).                                   (4.3)
                 allh0
             So maximizing the bound with respect to the weights in the higher layers
             is exactly equivalent to maximizing the log probability of a data set in
             whichh0 occurswithprobability Q(h0|v0). If the bound becomes tighter, it
