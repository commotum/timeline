                     Published as a conference paper at ICLR 2017
                             – Concatenate the representations of the input types, the embeddings of integers in the
                               inputs, the representation of the output type, and the embeddings of integers in the
                               output into a single (ﬁxed-length) vector.
                             – Pass this vector through H = 3 hidden layers containing K = 256 sigmoid units each.
                          • Pool the last hidden layer encodings of each input-output example together by simple arith-
                            metic averaging.
                     Fig. 7 showsaschematicdrawingofthisencoderarchitecture,togetherwiththedecoderthatperforms
                     independent binary classiﬁcation for each function in the DSL, indicating whether or not it appears
                     in the ground truth source code.
                     Attribute Predictions
                                                                  Sigmoids
                      Final Activations
                         Pooled
                        Hiddens 3
                        Hiddens 2                                …
                        Hiddens 1
                     State Embeddings
                       Program State                             …
                                         Inputs 1     Outputs 1         Inputs 5     Outputs 5
                            Figure 7: Schematic representation of our feed-forward encoder, and the decoder.
                     While DeepCoder learns to embed integers into a E = 20 dimensional space, we built the system up
                     gradually, starting with a E = 2 dimensional space and only training on programs of length T = 1.
                     Such a small scale setting allowed easier investigation of the workings of the neural network, and
                     indeed Fig. 8 below shows a learned embedding of integers in R2. The ﬁgure demonstrates that
                     the network has learnt the concepts of number magnitude, sign (positive or negative) and evenness,
                     presumablyduetoFILTER(>0),FILTER(<0),FILTER(%2==0)andFILTER(%2==1)allbeing
                     amongtheprogramsonwhichthenetworkwastrained.
                     D DEPTH-FIRST SEARCH
                     WeuseanoptimizedC++implementationofdepth-ﬁrst search (DFS) to search over programs with
                     a given maximum length T. In depth-ﬁrst search, we start by choosing the ﬁrst function (and its
                     arguments) of a potential solution program, and then recursively consider all ways of ﬁlling in the
                     rest of the program (up to length T), before moving on to a next choice of ﬁrst instruction (if a
                     solution has not yet been found).
                     Aprogramisconsideredasolutionifitis consistent with all M = 5 provided input-output examples.
                     Notethatthis requires evaluating all candidate programs on the M inputs and checking the results for
                     equality with the provided M respective outputs. Our implementation of DFS exploits the sequential
                     structure of programs in our DSL by caching the results of evaluating all preﬁxes of the currently
                     considered program on the example inputs, thus allowing efﬁcient reuse of computation between
                     candidate programs with common preﬁxes.
                     This allows us to explore the search space at roughly the speed of ∼ 3 × 106 programs per second.
                                                         15
