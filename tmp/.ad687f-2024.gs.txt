                         ResonanceRoPE:ImprovingContextLengthGeneralizationof
                                                     LargeLanguageModels
                                      1,2                  3           1                             3                   1,2‚Ä†
                   SuyuchenWang ,IvanKobyzev ,PengLu ,MehdiRezagholizadeh and BangLiu
                     1DIRO,Universit√© de Montr√©al          2Mila - Quebec AI Institute       3Huawei Noah‚Äôs Ark Lab
                                      {suyuchen.wang, peng.lu, bang.liu}@umontreal.ca
                                      {ivan.kobyzev, mehdi.rezagholizadeh}@huawei.com
                                      Abstract                           of long texts. Specifically, the train-short-test-long
                     This paper addresses the challenge of train-        (TSTL) scenario (Press et al., 2022) highlights a
                     short-test-long (TSTL) scenarios in Large Lan-      limitation where LLMs, pre-trained on shorter se-
                     guage Models (LLMs) equipped with Rotary            quences, struggle with out-of-distribution (OOD)
                     Position Embedding (RoPE), where models             token positions in longer sequences, impacting
                     pre-trained on shorter sequences face difficulty    their performance in real-world applications (Zhao
                     with out-of-distribution (OOD) token positions      et al., 2023).
                     in longer sequences.   We introduce RESO-              Recent efforts to enhance TSTL performance
                     NANCE ROPE, a novel approach designed to            have focused on LLMs equipped with Rotary Posi-
                     narrow the generalization gap in TSTL scenar-       tion Embedding (RoPE) (Su et al., 2024), such
                     ios by refining the interpolation of RoPE fea-      as LLaMA (Touvron et al., 2023a,b) and Mis-
                     tures for OOD positions, significantly improv-      tral (Jiang et al., 2023), owing to their excep-
                     ing the model performance without additional        tional capabilities and widespread adoption. These
                     online computational costs. Furthermore, we
                     present POSGEN, a new synthetic benchmark           initiatives aim to refine the test-time computa-
                     specifically designed for fine-grained behav-       tion of RoPE position embedding by introducing
                     ior analysis in TSTL scenarios, aiming to iso-      a scaling factor to either the position index of
                     late the constantly increasing difficulty of to-    each token (Chen et al., 2023) or RoPE‚Äôs base
                     ken generation on long contexts from the chal-      value (Xiong et al., 2023; Liu et al., 2024; Peng
                     lenges of recognizing new token positions. Our      et al., 2024). These methods ensure that the po-
                     experiments on synthetic tasks show that after      sition embeddings for out-of-distribution (OOD)
                     applying RESONANCE ROPE, Transformers               positions remain within the range experienced dur-
                     recognize OOD position better and more ro-
                     bustly. Our extensive LLM experiments also          ing pre-training. This minimizes the need for the
                     showsuperior performance after applying RES-        model to adapt to new position embedding value
                     ONANCEROPEtothecurrentstate-of-the-art              ranges, a task that is inherently difficult.
                     RoPEscalingmethod,YaRN,onbothupstream                  In this paper, we introduce RESONANCE ROPE,
                     language modeling tasks and a variety of down-      a novel technique designed to further narrow the
                     stream long-text applications.1
                                                                         generalization gap on position embeddings in
                 1 Introduction                                          TSTLscenarios. Recognizing that RoPE‚Äôs position
                                                                         embedding is governed by a complex, non-linear
                  Recent advancements in Large Language Models           function, we posit that minimizing extrapolation on
                 (LLMs)havedemonstrated their potential across a         OODpositions, while crucial, is insufficient. We
                 widespectrumofnaturallanguageprocessingtasks,           argue that it is equally vital to address the inter-
                 showcasing their ability to handle complex interac-     polation of RoPE features at the OOD positions.
                 tions, document analyses, professional writing, and     Byimplementing RESONANCE ROPE,weslightly
                 advanced reasoning with a unified approach (Ope-        scale each RoPE feature to correspond to an inte-
                 nAI, 2023; Touvron et al., 2023a,b; Jiang et al.,       ger wavelength. This adjustment aligns each RoPE
                 2024). As these models are increasingly adapted         feature‚Äôs wavelength with a specific token span
                 for complex applications, challenges arise in sce-      length, enabling it to "resonate" with a particular
                 narios requiring the comprehension or generation        local context length. This simple modification ef-
                    1https://github.com/sheryc/resonance_rope.           fectively reduces the generalization gap for over
                     ‚Ä†Canada CIFARAIChair. Corresponding author.         half of the position embedding features in LLaMA
                                                                     586
                                  Findings of the Association for Computational Linguistics: ACL 2024, pages 586‚Äì598
                                         August 11-16, 2024 ¬©2024 Association for Computational Linguistics
                 and LLaMA2underTSTLscenarios. Furthermore,                   YaRN, RESONANCE ROPEfurtherimproves
                 our approach is compatible with RoPE and any                 LLM‚Äôs length extrapolation ability, as evi-
                 RoPE-based scaling techniques, enhancing their               denced by lower perplexity in upstream TSTL
                 performance in TSTL situations without the need              language modeling and enhanced outcomes in
                 for additional computational resources during train-         downstream tasks involving lengthy contexts.
                 ing or inference.
                    Additionally, to facilitate further research on      2 RelatedWork
                 position embeddings, we present a new syn-              2.1   Scaling of RoPE Position Encoding
                 thetic benchmark tailored for TSTL scenarios,
                 named POSGEN. Improving position embeddings             Recent efforts in extending LLMs‚Äô context window
                 for TSTL requires a detailed analysis of the cause      focus on manipulating position embedding (PE),
                 of failures in handling longer contexts. However,       particularly RoPE (Su et al., 2024), which is used
                 current benchmarks, such as those measuring per-        in LLMslikeLLaMA(Touvronetal.,2023a,b)and
                 plexity in long context (Rae et al., 2020; Huang        Mistral (Jiang et al., 2023). Main strategies include
                 et al., 2021; Wu et al., 2022) and most synthetic       embedding scaling (Chen et al., 2023; Liu et al.,
                 TSTL tasks (Liu et al., 2023; Kazemnejad et al.,        2024; Peng et al., 2024) and randomizing token
                 2023) face a common issue: the difficulty of gener-     positions (Ruoss et al., 2023; Zhu et al., 2024). Our
                 ating the next token increases with context length.     emphasis is on the embedding scaling strategies.
                 This makes it difficult to determine whether a             Existing embedding scaling strategies adjust po-
                 model‚Äôs failure is due to its inability to generate     sition embedding for longer sequences to match
                 more complex tokens or its failure to recognize         the pre-training range, avoiding feature extrapola-
                 out-of-distribution (OOD) positions. POSGEN ad-         tion. For instance, Chen et al. (2023) compresses
                 dresses this limitation by standardizing the diffi-     position indices to fit the pre-training range, ex-
                 culty level of token generation across all positions.   tending LLaMA‚Äôs (Touvron et al., 2023a) context
                 This ensures that any observed shortcomings are         to 16K with 1,000 steps of fine-tuning. Alterna-
                 directly related to the model‚Äôs inability to identify   tively, Liu et al. (2024); Rozi√®re et al. (2023);
                 and handle new token positions effectively.             Xiong et al. (2023) modify RoPE‚Äôs rotary base
                    Ourcontributions in this study are threefold:        and employ fine-tuning on extended sequences,
                                                                         termed Adjusted Base Frequency (ABF) or "NTK-
                   1. We propose RESONANCE ROPE,aninnova-                aware"scaling. CodeLLaMA(Rozi√®reetal.,2023)
                      tive modification to RoPE based on an in-          achieved 16K context length with this method af-
                      depth analysis of the wavelengths of RoPE          ter 10,000 fine-tuning steps. YaRN (Peng et al.,
                      features, aiming to narrow the generalization      2024) improved NTK-aware scaling by segment-
                      gap in TSTL scenarios across RoPE and sim-         ing RoPE features and applying tailored extrapo-
                      ilar RoPE-based scaling techniques, without        lation strategies, achieving 64K context length for
                      necessitating extra computational resources        LLaMA2 (Touvron et al., 2023b) with 400 fine-
                      during runtime.                                    tuning steps. Distinguishingly, our RESONANCE
                   2. We present POSGEN, a newly developed syn-          ROPEfocusonreducingfeatureinterpolation on
                      thetic benchmark tailored for TSTL scenarios.      OODpositions, which we argue is another impor-
                      This benchmark is specifically designed to         tant factor in improving the length extrapolation
                      disentangle the complexities associated with       capability of Transformer.
                      generating tokens in longer contexts from the      2.2   LongContextEvaluations
                      challenges posed by recognizing new posi-          Evaluations of Transformer-based LLMs‚Äô long-
                      tions or position embedding values.                context capabilities are twofold: synthetic task as-
                   3. Through rigorous testing of RESONANCE              sessments for length extrapolation strategies and
                       ROPE on both RoPE and YaRN within                 real-world task evaluations at the LLM scale. Syn-
                      the POSGEN benchmark, we demonstrate               thetic evaluations target simple tasks such as long
                      its ability to enhance performance on out-         sequence classification (Tay et al., 2021) and arith-
                      of-distribution (OOD) positions, surpassing        metic language modeling (Liu et al., 2023; Kazem-
                      existing methods that do not include RESO-         nejad et al., 2023). LLM scale evaluations mea-
                       NANCE ROPE. Moreover, when applied to             sure metrics such as perplexity (PPL) in extensive
                                                                     587
                            text corpora (e.g., PG19 (Rae et al., 2020), GovRe-                                          whereŒò={Œ∏0,Œ∏1,¬∑¬∑¬∑ ,Œ∏d                              }, and each R                   is
                                                                                                                                                                        ‚àí1                          Œ∏j,m
                            port (Huang et al., 2021), GitHub (Wu et al., 2022))                                         a 2 √ó 2 rotation matrix:                     2
                            and complex tasks including summarization, ques-                                                                          cosmŒ∏               ‚àísinmŒ∏ 
                            tion answering, and mathematical reasoning (An                                                            R           =                   j                    j     .        (3)
                                                                                                                                         Œ∏j,m             sinmŒ∏              cosmŒ∏
                            et al., 2023; Bai et al., 2023; Shaham et al., 2023).                                                                                    j                   j
                            3 Background                                                                                 RoPEcomputestheattention logit q‚ä§k as follows:
                                                                                                                                             q =Rd Wx                                                     (4)
                            3.1       Rotary Position Embedding (RoPE)                                                                         m            Œò,m        q    m
                                                                                                                                              k =Rd W x                                                   (5)
                            In Transformers (Vaswani et al., 2017), the self-                                                                   n           Œò,n       k n
                                                                                                                                           ‚ä§               ‚ä§            d
                            attention scores are softmax-normalized scaled at-                                                          q k =x W R                                  Wx                    (6)
                                                                                                                                           m n             m q Œò,n‚àím k n
                            tention logits q‚ä§k:                                                                          For each two dimensions [2j : 2j + 1] of q and
                                                                            q ‚ä§k                                       k, its corresponding Œ∏j reflects a temporal wave-
                                                                                  m n
                                              am,n = Softmax                       ‚àö                                     length Œª . This wavelength describes the token
                                                                                       d                                                j
                                                                                                                         length for the corresponding RoPE features to en-
                                 Suppose the input to a single attention head                                            counter approximately the same rotary angle mŒ∏
                                                                                                                                                                                                             j
                                                                   d
                            is x1,x2,...,xl ‚àà R , where l is the sequence                                                in Equation 3:
                            length and d is the dimension of an attention head.
                                                                                                                                                              2œÄ                2j
                            RoPEinjectsthepositioninformationofeachtoken                                                                            Œª =              =2œÄbd                                (7)
                                                                                                                                                      j        Œ∏
                            into the q and k vectors by the following equations                                                                                  j
                            in the complex space:                                                                        As an example, the wavelengths of LLaMA /
                                                                                        imŒ∏j                             LLaMA2‚ÄôsRoPEfeatures range from 2œÄ ‚âà 6.28
                                                q                    =Wx e                                                                               126/128
                                                  m,[2j:2j+1]                 q   m                                      for Œ∏0 to 2 ‚àó 10000                         œÄ ‚âà 54410.14 for Œ∏d‚àí1.
                                                                                        imŒ∏                                                                                                            2
                                               k                     =Wxme j
                                                  m,[2j:2j+1]                 k                                          3.2       Critical Dimensions of RoPE
                                                                           ‚àí2j
                                                                Œ∏j = b d ,                                   (1)         In a TSTL scenario (Press et al., 2022), one takes
                            where W ,W are trainable parameters, and b                                                   a model trained on texts with lengths up to L, and
                                             q        k                                                                  tests it on a task with input lengths up to L‚Ä≤ = sL,
                            is a constant called the rotary base, which is set                                           with the scaling factor s > 1. Recently, Liu et al.
                            to 10,000 (Su et al., 2024) or other integers or                                             (2024) discovered that there may exist two ‚Äúcrit-
                            fractions (Xiong et al., 2023; Peng et al., 2024).                                           ical dimensions‚Äù in RoPE features, which corre-
                            This form makes the dot product between the m-th                                             spond to the dimensions [2c : 2c + 1] that satis-
                            queryq andn-thkeyk onlydependontheinput
                                         m                            n                                                  fies Œª       ‚â• L and Œª                   < L. The dimensions of
                            xm,xnandtheirrelative distance (m‚àín):                                                                  c                      c‚àí1
                                                                                                                         RoPEfeatures above and below the critical dimen-
                                            ‚ü®q                   , k                  ‚ü©                                  sion (which wedenoteas‚Äúpost-criticaldimensions‚Äù
                                                m,[2j:2j+1]           n,[2j:2j+1]
                                        =‚Ñúhq‚àó                        k                  i                                and ‚Äúpre-critical dimensions‚Äù, respectively) have
                                                    m,[2j:2j+1] n,[2j:2j+1]                                              different behaviors in TSTL: for post-critical di-
                                               h                 ‚àó                    i(m‚àín)Œ∏ i                          mensions (i.e., j > c), since their wavelengths sat-
                                        =‚Ñú (W x ) (W x )e                                         j
                                                        q   m             k n                                            isfy Œª > L, the training corpus does not cover all
                                                                                                                                   j
                                        =g(x ,x ,m‚àín).                                                                   possible rotary angles mŒ∏ on a unit circle. Thus,
                                                  m n                                                                                                                 j
                            RoPE‚Äôs real-number implementation divides the d-                                             these dimensions will encounter OOD value range
                            dimension space into multiple 2-dimensional sub-                                             on longer sequences. This is not an issue for pre-
                            spaces and applies real rotation matrix to each of                                           critical dimensions due to their shorter temporal
                            them. Formally, define a d √ó d block-diagonal                                                wavelengths.
                            matrix:                                                                                          The concept of RoPE‚Äôs critical dimensions im-
                                               Ô£´                                                          Ô£∂              plicitly guides the development of RoPE scaling
                                                   R                 ¬∑ ¬∑ ¬∑       ¬∑ ¬∑ ¬∑           0                       methods. For example, previous RoPE scaling
                                               Ô£¨ Œ∏0,m                                                     Ô£∑              methods (Chen et al., 2023; Xiong et al., 2023;
                                               Ô£¨ 0                RŒ∏ ,m ¬∑¬∑¬∑                      0        Ô£∑
                                   d           Ô£¨                       1                                  Ô£∑              Peng et al., 2024) mainly focus on reducing or
                               RŒò,m =                   .              .         .               .            ,
                                               Ô£¨ .                     .           ..            .        Ô£∑
                                               Ô£≠ .                     .                         .        Ô£∏              avoiding value extrapolation on post-critical dimen-
                                                       0              0          ¬∑ ¬∑ ¬∑    R
                                                                                              Œ∏d‚àí1,m                     sions, and minimize post-training modifications to
                                                                                                2            (2)         the pre-critical dimensions.
                                                                                                                   588
                     3.3    Yet another RoPE extensioN (YaRN)                                      RoPE Features'           Resonance RoPE Features'
                                                                                                  Rotation Angles                Rotation Angles
                     YaRN(Pengetal.,2024)isthecurrent state-of-the-
                     art RoPE scaling method for TSTL. It introduces
                     the ‚ÄúNTK-by-parts‚Äù scaling for RoPE, which ap-
                     plies different scaling strategies to each RoPE fea-
                     ture according to its temporal wavelength.
                        In a TSTL scenario with scaling factor s, YaRN
                     scales the wavelength of the j-th RoPE feature Œª
                                                                                   j
                         ÀÜ
                     to Œª and further fine-tune the model:
                          j                                                                     Training Position Range       OOD Position Range
                                     ÀÜ                                                       0             32            64            96           128
                                    Œª =(1‚àíŒ≥ )sŒª +Œ≥ Œª ,
                                      j             j     j     j  j                                               Token Position
                     where Œ≥j is a piece-wise function depending on                       Figure 1: An illustration of RoPE‚Äôs rotation angles mŒ∏
                     its corresponding wavelength Œª , and two hyperpa-                                                                                  6
                                                            j                                                                               Àú
                                                                                          andRESONANCEROPE‚ÄôsrotationanglesmŒ∏ inEqn.3
                     rameters Œ± and Œ≤:                                                                                                       6
                                                                                          in a TSTL scenario with training max length 64 and test-
                                      Ô£±1,                 if Œª < L/Œ≤                      ing max length 128. RoPE‚Äôs non-integer feature wave-
                                      Ô£¥                        j                          lengths create a feature gap between the RoPE features
                                      Ô£¥
                                      Ô£¥
                                      Ô£≤0,                 if Œª > L/Œ±                      of the training and OOD testing positions, while RESO-
                               Œ≥ =                             j
                                 j    Ô£¥L/Œª ‚àíŒ±                                             NANCE ROPEreducesthisgapto0.
                                      Ô£¥        j
                                      Ô£¥
                                      Ô£≥ Œ≤‚àíŒ± ,                 otherwise                                                                 d
                                                                                         where i = 0,...,d ‚àí 1 and X ‚äÇ R is the set of
                        Empirically, for the LLaMA family, Peng et al.                    feature vectors to which we apply a position embed-
                     (2024) suggests using Œ± = 1 and Œ≤ = 32. This                         ding. Note that the formulation of the feature gap
                     setting avoids value range extrapolation on post-                    is similar to the ‚Äúembedded vector distance‚Äù metric
                     critical dimensions, while reducing modifications                    proposed by Xiong et al. (2023). However, these
                     to the original pre-critical dimensions.                             twometrics target totally different aspects of RoPE
                        In addition to the ‚ÄúNTK-by-parts‚Äù RoPE scaling                    scaling methods. A more detailed comparison can
                     strategy mentioned above, YaRN also comprises                        be found in Appendix B.
                     a scaling strategy on the attention scores, which                       Existing RoPE scaling methods (Xiong et al.,
                     reduces the change in the entropy of the attention                   2023; Peng et al., 2024) mainly focus on the post-
                     score on longer sequences. We maintain the com-                      critical dimensions of RoPE, since the rotary angle
                     plete design of YaRN in our experiments, but our                     mŒ∏ on these dimensions extrapolates on OOD
                                                                                              j
                     analysis will focus on its RoPE scaling strategy.                    positions, hence creating a feature gap. In this
                                                                                          section, we argue that reducing RoPE‚Äôs feature
                     4 ProposedMethod:RESONANCE ROPE                                      interpolation on the pre-critical dimensions is also
                     In this section, we introduce RESONANCE ROPE,a                       beneficial for better length extrapolation.
                     universal improvement for RoPE and RoPE-based                           Duetoanon-linear relationship between RoPE
                                                                                          feature RŒò andthetokenpositionminEquation3,
                     scaling methods to (further) improve their length                                m
                     extrapolation performance.                                           the interpolation on RoPE features is potentially
                        Suppose we abstract RoPE‚Äôs Equation 4, 5: for                     hard for the model to generalize to. We found
                                  d                                d                      that such potentially hard interpolation appears on
                     anyx‚ààR ,wedefinef(x,m)=R                           Wx.Ina
                                                                   Œò,m                    the pre-critical dimensions [0 : 2c ‚àí 1], which
                     TSTLscenario where we generalize an LLM from
                                               ‚Ä≤                                          have wavelengths Œª shorter than the pre-trained
                     length L to length L , let us denote a scaled RoPE                                            j
                                     Àú                                                    sequence length L. By default, the rotary base b
                     functionbyf. ToperformwellonOODpositionsit
                                                               Àú                          of RoPE features is an integer or a fraction, which
                     should reduce the feature gap h(f) between token                                                               2j
                     features seen during training and token features                     makestheir wavelength Œªj = 2œÄb d not an integer.
                     after scaling that we can define for each i-th feature              As the position index m ‚àà N increases, a phase
                     as:                                                                  shift of ‚àÜœï occurs for the rotary angle mŒ∏ after
                                                                                                                                                 j
                                                                                          each full rotation. This could potentially result in a
                          Àú                               Àú              Àú
                     h (f) = max             min        |f(x,m) ‚àíf(x,n) |,                large distribution gap between the RoPE features
                       i         x‚ààX m‚àà{0,¬∑¬∑¬∑,L‚àí1}                  i             i
                                                  ‚Ä≤                                       on positions seen during training and the OOD po-
                                       n‚àà{L,¬∑¬∑¬∑,L ‚àí1}                            (8)      sitions. This phenomenon is illustrated in Figure 1.
                                                                                     589
                 Algorithm 1 Pseudocode of RESONANCE ROPE.                  Because    of   its  simplicity,   RESONANCE
                 Require: Œ∏ ,Œ∏ ,¬∑¬∑¬∑ ,Œ∏d       ‚ààŒò                         ROPE can be applied on top of RoPE and all
                              0  1         ‚àí1
                                          2                              RoPE-based scaling methods to reduce their
                    for i ‚àà {0,1,¬∑¬∑¬∑ , d ‚àí 1} do
                                       2                                 feature gap in TSTL and further improve their
                        Œª =2œÄ/Œ∏
                         i         i                                     performance.      Meanwhile, this method only
                        Àú
                        Œª =round(Œª )       ‚ñ∑ Roundtointeger wavelength
                         i            i                                  involves an offline computation of the scaled Œ∏,
                        Àú        Àú
                        Œ∏ =2œÄ/Œª
                         i         i                                     thus introducing no online computation overhead.
                    endfor
                    Àú      Àú Àú         Àú
                    Œò={Œ∏0,Œ∏1,¬∑¬∑¬∑ ,Œ∏d‚àí1}                                  5 EvaluatingPosition Embeddings
                                        2
                    ComputeRd byEquation2                                    with POSGEN
                                 Àú
                                Œò
                    Computeq,kbyEquation4,5                              In this section, we propose our new position em-
                                                                         bedding evaluation suite: POSGEN, based on an
                    Wetackle this issue by developing a synergistic      analysis of common failure patterns on existing
                 modification to the conventional RoPE embedding,        position embedding evaluation methods.
                 referred to as RESONANCE ROPE. It aims to iden-            Weconsider a next token prediction task, where
                 tify the optimal angular frequency that minimizes       weexpect the model to generate the token xl given
                                                                         the input sequence {x0,¬∑¬∑¬∑ ,x      }. In TSTL sce-
                 the interpolation gap, which ensures the corre-                                         l‚àí1
                 sponding wavelength closely matches the original        narios, when a model succeeds in correctly generat-
                 one while imposing alignment of the wavelength          ing a token up to position L but fails systematically
                 to an integer. More specifically, for a given angular   afterwards, we observe two failure patterns:
                                                              	
                 frequencysetofRoPEŒò = Œ∏ ,Œ∏ ,...,Œ∏               , we
                                                 1  2       d/2             ‚Ä¢ Failure due to harder algorithmic difficulty
                 round their wavelengths to their nearest integer to          ongenerating later tokens. The rule of gen-
                 eliminate new rotary angles on each feature. We              erating a new token x may vary with the se-
                 provide a pseudocode for RESONANCE ROPE in                                          l
                                                                              quence length l. Generally, tokens placed
                 Algorithm 1.                                                 later in the sequence depend on more con-
                    Afterapplyingthistechnique,eachRoPEfeature                text tokens, which incurs a more complex de-
                               Àú
                 repeats after Œª tokens, and therefore ‚Äúresonates‚Äù
                                 i                                            pendency pattern. During training on shorter
                 with a specific span length and eliminates the in-           sequences, the model only learns the token
                 terpolation gap between pre-trained and OOD po-              dependency rules involving up to L tokens,
                 sitions on pre-critical dimensions. We illustrate            and might fail on longer sequences because it
                 the effect of RESONANCE ROPE on RoPE‚Äôs fea-                  has never been exposed to the more complex
                 ture gap on one of the pre-critical dimensions in            dependency rules.
                 Figure 1. Moreover, we can prove the feature gap
                 reducing ability of our method. As for above, we           ‚Ä¢ Failure due to unrecognized new token po-
                 formalize RESONANCE ROPE‚Äôs computation rule                  sitions. The difference between training and
                     Àú            d                                           testing lengths in the TSTL setting creates
                 as f(x,m) = RÀú       Wx.
                                  Œò,m                                         a feature gap between the position indices
                 Theorem1. ForaRoPE-equippedmodelwithcon-                     or position embeddings in training and infer-
                                                        Àú
                 text window L, RESONANCE ROPE f reduces the                  ence. Thisfeaturegapmakesitdifficultforthe
                 feature gap on pre-critical dimensions to 0. Specifi-        model to generalize to new positions due to
                 cally, ‚àÄx ‚àà X, ‚àÄn ‚àà N\{0,¬∑¬∑¬∑ ,L ‚àí1}, we have:                unrecognized features. RoPE scaling methods
                                     Àú            Àú                           mainly focus on reducing this type of length
                           min      |f(x,m) ‚àíf(x,n) | = 0
                       m‚àà{0,¬∑¬∑¬∑,L‚àí1}          i          i                    extrapolation failure.
                 for all i = 0,...,2c ‚àí 1.                                  Currently,   neither perplexity-based evalua-
                                                                         tions (Rae et al., 2020; Huang et al., 2021; Wu et al.,
                    See the proof in Appendix A. Note that although      2022) nor synthetic TSTL evaluations (Kazemne-
                 each pre-critical RoPE feature RÀú       repeats, the    jad et al., 2023; Liu et al., 2023) can effectively
                                                    Œ∏ ,m
                                                     j
                 combination of all {RÀú       }     only repeats af-     distinguish these two failure patterns, since the
                                          Œ∏ ,m j<c
                                           j
                 ter the least common multiple (LCM) of all pre-         token generation difficulty tends to increase with
                 critical dimensions‚Äôs wavelengths. For LLaMA2,          respect to the sequence length in these tasks. To fa-
                                                        51
                 this LCM value is greater than 7 √ó 10     .             cilitate research on better position representations,
                                                                     590
                                     Seed tokens                 Generated by the 
                                  (first Ì†µÌ±ó + Â†µÌ±ò tokens)        corresponding rules
                     Recursive     2    0    3    1      6    3    6    2     3    0    4         2            =‚Ñé(         ,     ,     ,     )
                           CoT 2        0    3    1      6    5    0    6     6    0    0         1       =        +      +      +        modÂ†µÌ±ö
                         Semi-     2    0    3    1      6    5    5    2     1    4    1         0
                     Recursive
                                                            Input                               Output
                    Figure 2: An example of the three subtasks of POSGEN. This figure shows the process of generating the 12th token
                    showninthe red boxes for each subtask. In this example, h is a modular addition task with the modulus m = 7 and
                    the difficulty-controlling parameters j = 1,k = 3. The output token depends on: (1) only the local j + k tokens in
                    the recursive task; (2) k local tokens and the beginning j tokens in the CoT task; and (3) k local tokens and j tokens
                    with a varied dependency distance in the semi-recursive task.
                    wedesign POSGEN,whichcontrolsthedifficulty                           Based on the equation for each subtask, when
                    in generating tokens throughout the sequence to be                given the first j + k tokens, one can generate a
                    identical, which effectively distinguishes the two                sequence with unlimited length as the ground truth
                    types of TSTL failures. Failures in this benchmark                sequence. We show an example of POSGEN in Fig-
                         2    Huawei Proprietary - Restricted Distribution
                    are only due to the inability to recognize new token              ure 2. As a TSTL benchmark, we train a model on
                    positions in TSTL scenarios.                                      a subtask with sequence length up to L, and evalu-
                       Our POSGEN framework comprises three sub-                      ate the model‚Äôs accuracy on a longer sequence with
                    tasks, with each extracting the general token de-                 length L‚Ä≤ > L generated by the same rule on the
                    pendency pattern of a different type of reason-                   unseen positions L < m ‚â§ L‚Ä≤, which we refer to
                    ing task. Suppose that we define a fixed function                 as the ‚ÄúOOD Accuracy‚Äù (OOD Acc). This met-
                            j+k                                                       ric measures how well a model can recognize the
                    h : V        ‚ÜíV,whereVisthe model‚Äôs vocabu-                       OODpositionsandcontinue following the genera-
                    lary and j,k are predefined constants controlling
                    the task‚Äôs difficulty. The three subtasks of POS-                 tion rule learned during training. As a benchmark
                    GENareasfollows:                                                  for position embeddings, a standard usage of this
                       1. Recursive. This task simulates the token de-                benchmarkis to train a small Transformer (e.g., a
                                                                                      2-layer Transformer as used in our experiments)
                           pendency pattern of generating a Fibonacci-                with different position embeddings on its training
                           style sequence, where new tokens depend                    set with only short sequences, and test its OOD
                           on j + k neighboring tokens only: x =
                                                                            l         Accuracy on the test set with longer sequences.
                           h(x           , ¬∑ ¬∑ ¬∑ , x  ) when l ‚â• j + k.
                               l‚àí(j+k))           l‚àí1                                 Weprovideourexperiment setting for POSGEN in
                       2. Chain-of-Thought (CoT). This task simu-                     moredetails in Section 6.1.1 and Appendix C.1.
                           lates the token dependency pattern of CoT rea-             6 Experiments
                           soning (Wei et al., 2022), where new tokens
                           depend on k neighboring tokens (simulating                 Weevaluate RESONANCE ROPE onthree different
                           the previous reasoning step) and j tokens in               TSTLtasks: a small-scale evaluation on our pro-
                           the front (simulating the original question):              posed POSGEN task, and LLM-scale evaluations
                           x =h(x ,¬∑¬∑¬∑ ,x            , x    , ¬∑ ¬∑ ¬∑ , x  ) when
                            l         0          j‚àí1    l‚àík          l‚àí1              withLLaMA2-Chat(Touvronetal.,2023b)onboth
                           l ‚â• j +k.                                                  language modeling perplexity and real-world long
                       3. Semi-recursive. This task simulates the to-                 context applications.
                           ken dependency pattern of the last-letter con-             6.1    Synthetic Task Evaluation
                           catenation task (Zhou et al., 2023), where                 6.1.1     ExperimentSetup
                           newtokens depend on both k neighboring to-
                           kens (simulating the current progress) and j               Wefirst apply RESONANCE ROPE on RoPE and
                           tokens with varied distances according to a                YaRN,assessing the model‚Äôs performance on POS-
                           specific rule (simulating the word sequence):               GENforunseen position recognition. We test on
                           x =h(x                     , ¬∑ ¬∑ ¬∑ , x               ,     a modular addition task, which was proved to be
                            l         ‚åäl‚àí(j+k)/2‚åã‚àíj            ‚åäl‚àí(j+k)/2‚åã‚àí1
                           x    , ¬∑ ¬∑ ¬∑ , x  ) when l ‚â• j + k.                        learnable by a one-layer Transformer (Nanda et al.,
                            l‚àík          l‚àí1                                          2023). We configured j = 1,k = 3, and defined
                                                                                  591
                                                    Recursive                                              CoT                                          Semi-Recursive
                               3.0                                                 3.0
                                                                                                                                       4.0
                               2.5                                                 2.5
                                                                                                                                       3.5
                               2.0                                                 2.0
                                                                                                                                       3.0
                               1.5                                                 1.5
                                                                                                                                       2.5
                              Loss (Val)1.0                                        1.0
                                                                                                                                       2.0
                               0.5                                                 0.5
                                                                                   0.0                                                 1.5
                               0.0
                                   0     20    40     60    80    100   120    140     0     20    40    60     80    100   120    140    0     20     40    60     80    100   120   140
                                                       Epoch                                               Epoch                                               Epoch
                                          RoPE                  Resonance RoPE (Ours)                          YaRN (s=4)                     Resonance YaRN (s=4) (Ours)
                          Figure 3: The validation loss curves of Transformers using RoPE and YaRN PEs with and without our RESONANCE
                          scaling on the three subtasks of POSGEN.
                                  Setting            Recursive             CoT           Semi-Rec.             in OODscenarios. This improvement indicates a
                                    RoPE             65.29¬±0.43        69.56¬±0.33        17.96¬±0.03            superior adaptation to OOD position embeddings
                             Res. RoPE (Ours)        62.64¬±0.15        75.25¬±0.10        29.78¬±0.07            through minimized Positional Encoding (PE) in-
                                    YaRN             95.93¬±0.04        98.71¬±0.00        33.70¬±0.04            terpolation. An exception is observed when apply-
                             Res. YaRN(Ours)         98.30¬±0.00        99.58¬±0.00        48.46¬±0.03            ing RESONANCE ROPE totheRecursivesubtask,
                                                                                                               likely due to the dominance of extrapolated post-
                          Table 1: The accuracy on OOD Positions (OOD Acc.)                                    critical dimensions in OOD positions. This issue
                          on POSGEN‚Äôs test set. All results are in percentage (%).                             canbemitigatedbyemployingaRoPEscalingtech-
                          Wereportboththemeanandvarianceacrossfive runs                                        nique such as YaRN, which effectively counters the
                          with different random seeds. We compare the same                                     extrapolation of post-critical dimensions. Among
                          RoPE-based PE with or without our RESONANCE scal-                                    all configurations, RESONANCE YARN exhibits
                          ing. The best performance for each pair of settings on                               the highest OOD performance, demonstrating the
                          each subtask is marked in Bold.
                                                          P                                                    synergy between RoPE scaling methods and the
                          h(x ,x ,x ,x ) =                    3     x mod17withvocab-
                                0     1     2    3            i=0 i                                            Resonance technique.
                          ulary V = {0,...,16}.                                                                    Figure 3 plots validation losses against training
                              Our experiments involved training a two-layer                                    epochs for different PEs, illustrating the training
                          Transformer. Each layer follows T5-Small‚Äôs config-                                   dynamics. The introduction of the Resonance tech-
                          urations (Raffel et al., 2020) except for the position                               nique leads to a reduction in the lowest validation
                          embeddings. In this model, each attention head has                                   loss for both RoPE and YaRN, with RESONANCE
                          64dimensions. WeapplydifferentRoPE-basedem-                                          ROPEachievingevenlowervalidation losses than
                          beddings with the rotary base equal to 10,000. The                                   YaRNintheSemi-Recursivesubtask. Furthermore,
                          models are trained on sequences of length L = 64,                                    the validation loss trajectories for RESONANCE
                          and evaluating on lengths of L‚Ä≤ = 256 for OOD                                        ROPEandRESONANCEYARNremainlowerthan
                          Accuracy. In this experiment setting, each head has                                  those of their counterparts in all subtasks, further
                          32RoPEfeatures, out of which the first 17 are pre-                                   demonstrating the enhanced OOD generalization
                          critical dimensions with a wavelength less than the                                  capability of our approach.
                          maximum training length. We generated 10,000
                          training sequences, and 1,000 each for validation                                    6.2      LLMFine-tuningEvaluation
                          and testing, and ensured that the first j + k = 4                                    6.2.1       ExperimentSetup
                          tokens in each sequence do not overlap to testify
                          whether the model learns the correct generation                                      In this section, we apply our proposed RESO-
                          mechanism. We averaged results over 5 seeds. A                                       NANCE ROPEtothecurrentstate-of-the-art RoPE
                          moredetailed setting is provided in Appendix C.1.                                    scaling method, YaRN (Peng et al., 2024). More
                                                                                                               specifically, we replace the original position em-
                          6.1.2       Results and Analysis                                                     beddings of LLaMA2 7B and 13B (Touvron et al.,
                          Table 1 displays the comparison of the OOD accu-                                     2023b)withaseriesofscaledpositionembeddings,
                          racy. In most cases, RESONANCE ROPE and RES-                                         including the NTK-Aware scaling (bloc97, 2023;
                          ONANCE YARN outperform their counterparts                                            Xiongetal.,2023;Liuetal.,2024),DynamicNTK-
                          lacking the Resonance technique, showcasing sig-                                     Aware Scaling (Peng et al., 2024; Rozi√®re et al.,
                          nificantly better performance and reduced variance                                   2023), and YaRN (Peng et al., 2024).
                                                                                                          592
                                        Setting                    Ctx Len.   Coursera   GSM QuALITY TOEFL CodeU SFiction                 Avg.
                                                                           LLaMA2-Chat7B
                                     DynamicNTK-Aware(noFT)           32K       31.98    32.00     34.65      59.11     1.11     36.72    32.59
                                        NTK-Aware(s = 8,noFT)         32K       36.77    3.00      26.73      34.2      1.11     50.78    25.43
                                 YaRN(s=8,FT@32K,50epcs.)             32K       36.05    19.00     33.17      50.56     4.44     56.25    33.24
                       Resonance YaRN(s = 8, FT@32K, 50 epcs.)        32K       36.48    22.00     34.16      55.76     0.00     57.03    34.24
                                 YaRN(s=8,FT@4K,400epcs.)             32K       35.03    24.00     37.62      57.62     4.44     60.94    36.61
                       Resonance YaRN(s = 8, FT@4K, 400 epcs.)        32K       36.34    27.00     40.59      56.51     3.33     61.72    37.58
                                                                          LLaMA2-Chat13B
                                     DynamicNTK-Aware(noFT)           16K       29.22    39.00     40.59      63.94     1.11     39.84    35.62
                                        NTK-Aware(s = 4,noFT)         16K       40.26    21.00     38.12      65.43     1.11     46.88    35.47
                                YaRN(s=4,FT@16K,100epcs.)             16K       38.08    39.00     43.07      65.43     0.00     63.28    41.48
                      Resonance YaRN(s = 4, FT@16K, 100 epcs.)        16K       38.66    39.00     43.56      65.06     1.11     62.50    41.65
                                 YaRN(s=4,FT@4K,400epcs.)             16K       41.72    34.00     41.09      66.91     2.22     48.44    39.06
                       Resonance YaRN(s = 4, FT@4K, 400 epcs.)        16K       41.86    35.00     42.57      65.80     5.56     48.44    39.87
                    Table 2: Long text evaluations on some closed-ended tasks in L-Eval. ‚ÄúCtx Len‚Äù means the target context length of
                    the model after scaling its PE. ‚ÄúFT@32K, 50 epcs‚Äù means the model is fine-tuned on 32K sequence length for 50
                    epochs. The settings with ‚Äúno FT‚Äù are not fine-tuned after modifying its position embedding. We highlight the best
                    and second-best performance for each base model in Bold and Underline, respectively.
                                GovReport                     Proofpile             length. We report the results in Figure 4. Of the
                       7
                                                    6                               tested methods, RESONANCE YARN achieves the
                       6                                                            lowest perplexity across all context lengths. Espe-
                                                    4                               cially, RESONANCE YARN achieves a lower per-
                       5                                                            plexity compared to YaRN with the same set of
                      Perplexity (PPL)4             2                               hyperparameters optimized for YaRN, demonstrat-
                             10000   20000   30000        10000   20000   30000     ingthebenefitofapplyingtheResonancetechnique
                              Context Length              Context Length            to existing RoPE scaling methods.
                           Dynamic NTK            NTK-Aware Scaling (s=8)
                           YaRN (s=8)             Resonance YaRN (s=8) (Ours)
                    Figure4: TheperplexityofLLaMA-Chat7Bwithdiffer-                 6.2.3    Real-world Task Evaluation
                    ent position embeddings on GovReport and Proofpile.
                       For YaRN and RESONANCE YARN, We use a                        Lastly, we test the real-world task performance of
                    scaling factor of 8 and 4 for LLaMA2 7B and 13B                 LLaMA2-Chat7Band13B‚Äôsperformancewithdif-
                    to extend their context window from 4K to 32K                   ferent RoPE scaling strategies on L-Eval (An et al.,
                    and 16K, respectively. For the configurations that              2023)‚Äôs close ended task suite, a long-text LLM
                    require fine-tuning, we fine-tune the LLM with the              benchmark covering a wide range of domains such
                    scaled position embedding on the training set of                as school lectures, long conversations and novels.
                    PG19 (Rae et al., 2020) with the fine-tuning set-               Wefine-tunethemodelwithdifferentRoPEscaling
                    ting and hyperparameters adopted directly from                  strategies using two different strategies: training on
                    YaRN(Pengetal., 2024), with the only difference                 shorter sequences (4K length) for more epochs, and
                    being that we control the total training token count            training on longer sequences (32K or 16K length)
                    to be approximately 100M. A more detailed fine-                 for less epochs. All settings requiring fine-tuning
                    tuning setting can be found in Appendix C.2. We                 keep the training token count to be approximately
                    test the model‚Äôs performance on two TSTL sce-                   100M.Theresults are listed in Table 2.
                    narios: language modeling evaluation on long-text
                    sequences and long-text downstream application                     Although no single setting in the experiment
                    performance.                                                    achieves the best result on all subtasks, we observe
                                                                                    that applying RESONANCE YARN achieves better
                    6.2.2    Perplexity on Long Sequence                            average performance in different training settings
                    Weevaluate the model‚Äôs language modeling per-                   and model sizes compared to its counterpart YaRN
                    formance on GovReport (Huang et al., 2021) and                  setting. This further proves the compatibility of the
                    Proofpile (Azerbayev, 2022). We randomly select                 Resonance technique and RoPE scaling methods,
                    50 samples from each dataset and report the final               and the better length extrapolation performance
                    perplexity in text fragments of gradually increased             brought by our proposed method.
                                                                                593
                  7 Conclusion                                            References
                 We introduce RESONANCE ROPE, a novel en-                 Chenxin An, Shansan Gong, Ming Zhong, Mukai Li,
                  hancement of RoPE that focuses on minimizing               Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.
                  the interpolation of RoPE features for OOD posi-           L-eval: Instituting standardized evaluation for long
                                                                             context language models. CoRR, abs/2307.11088.
                  tions, thereby reducing the generalization gap and
                  improving LLM‚Äôs performance on train-short-test-        Zhangir Azerbayev. 2022. zhangir-azerbayev/proof-
                  long (TSTL) scenarios. Additionally, we present            pile.
                  a novel synthetic benchmark, POSGEN, which              Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
                  provides a fine-grained analysis of the model‚Äôs            Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
                 TSTL performance regarding various token de-                Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
                  pendency patterns. Extensive experiments on our            and Juanzi Li. 2023. Longbench: A bilingual, mul-
                  proposed POSGEN and two LLM-based evalua-                  titask benchmark for long context understanding.
                                                                             CoRR,abs/2308.14508.
                  tions demonstrate RESONANCE ROPE‚Äôs efficacy
                  in identifying OOD positions and its compatibil-        bloc97.2023. NTK-AwareScaledRoPEallowsLLaMA
                  ity with current RoPE scaling strategies. Future           models to have extended (8k+) context size without
                 workincludes exploring RESONANCE ROPE‚Äôs per-                any fine-tuning and minimal perplexity degradation.
                  formance on other foundational models, and the          ShouyuanChen,ShermanWong,LiangjianChen,and
                  identification of more optimal wavelength combi-           YuandongTian. 2023. Extending context window of
                  nations for RoPE features.                                 large language models via positional interpolation.
                                                                             CoRR,abs/2306.15595.
                  Limitations                                             Tri Dao. 2023. Flashattention-2: Faster attention with
                  Ourproposed RESONANCE ROPE focusonreduc-                   better parallelism and work partitioning.  CoRR,
                                                                             abs/2307.08691.
                  ing the interpolation of only RoPE‚Äôs pre-critical
                  dimensions on OOD positions. However, this              LuyangHuang,ShuyangCao,NikolausNovaParulian,
                  method does not solve the extrapolation issue on           HengJi, and Lu Wang. 2021. Efficient attentions for
                  RoPE‚Äôs post-critical dimensions, which has been            long document summarization. In Proceedings of
                                                                             the 2021 Conference of the North American Chap-
                  shown to be also detrimental to LLM‚Äôs length               ter of the Association for Computational Linguistics:
                  extrapolation performance. Thus, the technique             Human Language Technologies, pages 1419‚Äì1436.
                  of RESONANCE ROPE needstobecombinedwith                    Association for Computational Linguistics.
                  another RoPE scaling method that can reduce ex-         Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
                  trapolation on RoPE‚Äôs post-critical dimensions,            sch, Chris Bamford, Devendra Singh Chaplot, Diego
                  e.g., YaRN, to achieve the full potential of LLM in        de Las Casas, Florian Bressand, Gianna Lengyel,
                 TSTLscenarios. Such combination has been our                Guillaume Lample, Lucile Saulnier, L√©lio Re-
                  focus in Section 6.2.                                      nard Lavaud, Marie-Anne Lachaux, Pierre Stock,
                                                                             Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-
                    Secondly, applying LLMs to long text sequences           th√©e Lacroix, and William El Sayed. 2023. Mistral
                  requires considerations of both performance and            7b. CoRR, abs/2310.06825.
                  efficiency due to the super-linear complexity of        Albert Q. Jiang, Alexandre Sablayrolles, Antoine
                 Transformers w.r.t input length. As an improve-             Roux, Arthur Mensch, Blanche Savary, Chris Bam-
                  ment of the position embeddings, we focus only             ford, Devendra Singh Chaplot, Diego de Las Casas,
                  on improving Transformers‚Äô performance in TSTL             Emma Bou Hanna, Florian Bressand, Gianna
                  scenarios. An interesting future direction would           Lengyel, Guillaume Bour, Guillaume Lample,
                  be to apply RESONANCE ROPE to efficient Trans-             L√©lio Renard Lavaud, Lucile Saulnier, Marie-
                                                                             AnneLachaux,Pierre Stock, Sandeep Subramanian,
                  formers for both performance and efficiency en-            Sophia Yang, Szymon Antoniak, Teven Le Scao,
                  hancements.                                                Th√©ophile Gervet, Thibaut Lavril, Thomas Wang,
                    Lastly, benchmarkingLLMsisstillanopenques-               Timoth√©eLacroix, and William El Sayed. 2024. Mix-
                  tion, as there is currently no benchmark to thor-          tral of experts. CoRR, abs/2401.04088.
                  oughly test the performance of LLMs, especially         Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan
                  on long-sequence tasks. We expect that a more              Natesan Ramamurthy, Payel Das, and Siva Reddy.
                  comprehensive long-text benchmark would further            2023. The impact of positional encoding on length
                  improve the validity of the experiment results.            generalization in transformers. In Advances in Neu-
                                                                             ral Information Processing Systems, volume 36,
                                                                             pages 24892‚Äì24928.
                                                                      594
                   Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Kr-          Anian Ruoss, Gr√©goire Del√©tang, Tim Genewein, Jordi
                      ishnamurthy, and Cyril Zhang. 2023. Transformers             Grau-Moya, R√≥bert Csord√°s, Mehdi Bennani, Shane
                      learn shortcuts to automata. In The Eleventh Interna-        Legg, and Joel Veness. 2023. Randomized positional
                      tional Conference on Learning Representations.               encodings boost length generalization of transform-
                                                                                   ers. In Proceedings of the 61st Annual Meeting of the
                   Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and              Association for Computational Linguistics (Volume
                      DahuaLin.2024. Scaling laws of roPE-based extrap-            2: Short Papers), pages 1889‚Äì1903, Toronto, Canada.
                      olation. In The Twelfth International Conference on          Association for Computational Linguistics.
                      Learning Representations.                                 Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant,
                   Ilya Loshchilov and Frank Hutter. 2019. Decoupled               and Omer Levy. 2023. ZeroSCROLLS: A zero-shot
                      weight decay regularization. In 7th International            benchmark for long text understanding. In Find-
                      Conference on Learning Representations.                      ings of the Association for Computational Linguis-
                                                                                   tics: EMNLP 2023, pages 7977‚Äì7989, Singapore.
                   Neel Nanda, Lawrence Chan, Tom Lieberum, Jess                   Association for Computational Linguistics.
                      Smith, and Jacob Steinhardt. 2023. Progress mea-          Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng
                      suresforgrokkingviamechanisticinterpretability. In           Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
                      TheEleventh International Conference on Learning             hanced transformer with rotary position embedding.
                      Representations.                                             Neurocomputing, 568:127063.
                   OpenAI. 2023.       GPT-4 technical report.       CoRR,      Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,
                      abs/2303.08774.                                              Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
                                                                                   Sebastian Ruder, and Donald Metzler. 2021. Long
                   BowenPeng,JeffreyQuesnelle,HongluFan,andEnrico                  range arena : A benchmark for efficient transform-
                      Shippole. 2024. YaRN: Efficient context window ex-           ers. In 9th International Conference on Learning
                      tension of large language models. In The Twelfth             Representations.
                      International Conference on Learning Representa-
                      tions.                                                    HugoTouvron,Thibaut Lavril, Gautier Izacard, Xavier
                                                                                   Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
                   Ofir Press, NoahA.Smith,andMikeLewis.2022. Train                Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal
                      short, test long: Attention with linear biases enables       Azhar, Aur√©lien Rodriguez, Armand Joulin, Edouard
                      inputlengthextrapolation. InTheTenthInternational            Grave, and Guillaume Lample. 2023a. Llama: Open
                      Conference on Learning Representations.                      and efficient foundation language models. CoRR,
                                                                                   abs/2302.13971.
                   Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,          Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
                      Chloe Hillier, and Timothy P. Lillicrap. 2020. Com-          bert, Amjad Almahairi, Yasmine Babaei, Nikolay
                      pressive transformers for long-range sequence mod-           Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
                      elling. In 8th International Conference on Learning          Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
                      Representations.                                             Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
                   Colin Raffel, Noam Shazeer, Adam Roberts, Katherine             Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
                      Lee, Sharan Narang, Michael Matena, Yanqi Zhou,              Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
                      WeiLi,andPeterJ.Liu. 2020. Exploring the limits              thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
                      of transfer learning with a unified text-to-text trans-      Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
                      former. J. Mach. Learn. Res., 21:140:1‚Äì140:67.               Isabel Kloumann,ArtemKorenev,PunitSinghKoura,
                                                                                   Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
                   Jie Ren, Samyam Rajbhandari, Reza Yazdani Am-                   anaLiskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
                      inabadi, Olatunji Ruwase, Shuangyan Yang, Min-               tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
                      jia Zhang, Dong Li, and Yuxiong He. 2021. Zero-              bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
                      offload: Democratizing billion-scale model train-            stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
                      ing. In 2021 USENIX Annual Technical Conference,             Ruan Silva, Eric Michael Smith, Ranjan Subrama-
                      pages 551‚Äì564. USENIX Association.                           nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
                                                                                   lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
                                                                                   ZhengYan,IliyanZarov, Yuchen Zhang, Angela Fan,
                   Baptiste Rozi√®re, Jonas Gehring, Fabian Gloeckle, Sten          Melanie Kambadur, Sharan Narang, Aur√©lien Ro-
                      Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,             driguez, Robert Stojnic, Sergey Edunov, and Thomas
                      Jingyu Liu, Tal Remez, J√©r√©my Rapin, Artyom                  Scialom. 2023b. Llama 2: Open foundation and
                      Kozhevnikov, Ivan Evtimov, Joanna Bitton, Man-               fine-tuned chat models. CoRR, abs/2307.09288.
                      ish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori,
                      Wenhan Xiong, Alexandre D√©fossez, Jade Copet,             Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                      Faisal Azhar, Hugo Touvron, Louis Martin, Nico-              Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
                      las Usunier, Thomas Scialom, and Gabriel Synnaeve.           Kaiser, and Illia Polosukhin. 2017. Attention is all
                      2023. Code llama: Open foundation models for code.           you need. In Advances in Neural Information Pro-
                      CoRR,abs/2308.12950.                                         cessing Systems 30: Annual Conference on Neural
                                                                            595
         Information Processing Systems 2017, pages 5998‚Äì
         6008.
        Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
         Bosma,Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,
         and Denny Zhou. 2022. Chain-of-thought prompting
         elicits reasoning in large language models. In Ad-
         vances in Neural Information Processing Systems 35:
         Annual Conference on Neural Information Process-
         ing Systems 2022.
        Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,
         andChristian Szegedy. 2022. Memorizing transform-
         ers. In The Tenth International Conference on Learn-
         ing Representations.
        WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,
         Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
         Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
         MadianKhabsa,HanFang,YasharMehdad,Sharan
         Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale,
         Sergey Edunov, Mike Lewis, Sinong Wang, and Hao
         Ma.2023. Effective long-context scaling of founda-
         tion models. CoRR, abs/2309.16039.
        LiangZhao,XiaochengFeng,XiachongFeng,BingQin,
         and Ting Liu. 2023. Length extrapolation of trans-
         formers: A survey from the perspective of position
         encoding. CoRR, abs/2312.17044.
        Denny Zhou, Nathanael Sch√§rli, Le Hou, Jason Wei,
         Nathan Scales, Xuezhi Wang, Dale Schuurmans,
         Claire Cui, Olivier Bousquet, Quoc V. Le, and Ed H.
         Chi. 2023. Least-to-most prompting enables com-
         plex reasoning in large language models. In The
         Eleventh International Conference on Learning Rep-
         resentations.
        DaweiZhu,NanYang,LiangWang,YifanSong,Wen-
         hao Wu, Furu Wei, and Sujian Li. 2024. PoSE: Ef-
         ficient context window extension of LLMs via po-
         sitional skip-wise training. In The Twelfth Interna-
         tional Conference on Learning Representations.
                              596
                A ProofofTheorem1                                    Thedistance calculation specifically compares the
                                                                                                                ÀÜ
                Proof. All we need is to prove that for each x ‚àà     original RoPE, f(¬∑,¬∑), to the scaled RoPE, f(¬∑,¬∑),
                  d                                                  with token positions beginning at zero. It aims to
                R , each n ‚àà N\{0,¬∑¬∑¬∑ ,L ‚àí 1} and each i =           quantify the alterations in position embedding due
                0,...,2c ‚àí 1 we can find m ‚àà {0,¬∑¬∑¬∑ ,L ‚àí 1} ,        to the scaling process.
                          Àú           Àú
                such that f(x,m) = f(x,n) . By definition, it is
                                  i          i                         In contrast, our feature gap metric is tailored
                equivalent to solving the equations:                 for a more practical and common scenario, where
                          (Rd    Wx) =(Rd Wx)                        modelsaretrainedorfine-tuned on short sequences
                             Àú         i      Àú        i
                             Œò,m              Œò,n                    using the already scaled RoPE embeddings. This
                for m, given i, n, and x.                            setting emphasizes the generalization gap of the
                   The RoPE feature matrix Rd       is defined as    RoPEfeatures between training and testing posi-
                                                Œò,m                  tion ranges. The core hypothesis is that a smaller
                block-diagonal with 2 √ó 2 blocks given by Equa-      discrepancy in the RoPE features of new token po-
                tion 3. Hence, given i, x and n, the equation re-
                ducestoequalityofalinearcombinationoftrigono-        sitions relative to those encountered during training
                metric functions:                                    correlates with enhanced model generalization to
                                                                     novel token positions. Our metric diverges from
                          Àú           Àú           Àú          Àú
                   acosmŒ∏i+bsinmŒ∏i =acosnŒ∏i+bsinnŒ∏i                  the ‚Äúembedded vector distance‚Äù in two significant
                for a,b ‚àà R, depending on x and i. This equality     aspects to better align with our use-case:
                                  Àú     Àú
                clearly holds if mŒ∏i ‚àí nŒ∏i is a multiple of 2œÄ:         ‚Ä¢ The distance computation shifts to compare
                                        Àú                                 scaled RoPE across different token positions,
                                (m‚àín)Œ∏i =2œÄk,                             reflecting the operational context where train-
                for some k ‚àà Z. By our construction, 2œÄ is a              ing involves short sequences (train-short) and
                                                           Àú
                                                          Œ∏               testing involves longer sequences (test-long).
                                                           i
                natural number. Hence, to finish the proof that we
                can solve our initial equation for m, we need to        ‚Ä¢ Wemodifythetokenposition ranges, k and j,
                showthat we can find integer k to satisfy:                to represent token positions observed during
                                                                        training (in-distribution) and testing (out-of-
                           n‚àí2œÄk ‚àà{0,¬∑¬∑¬∑ ,L‚àí1}                            distribution), respectively, to directly measure
                                Àú
                                Œ∏i                                        the generalization gap.
                for n ‚àà N\{0,¬∑¬∑¬∑ ,L ‚àí 1}. This is where we             This adaptation of the metric allows for a more
                use the pre-critical dimension condition: for i =    targeted evaluation of the model‚Äôs ability to gener-
                0,...,2c ‚àí 1, by definition of c, we have the in-    alize across different token positional distributions,
                equality 0 ‚â§ 2œÄ < L. Taking k = ‚åänŒ∏i‚åã will give      whichiscriticalinscenarioswheresequencelength
                              Àú                     2œÄ
                              Œ∏i
                us the required range for m and hence finish the     varies significantly between training and deploy-
                proof.                                               ment.
                                                                     C DetailedExperimentSettings
                B ComparisonBetweenFeatureGapand                     In this section, we provide the detailed experi-
                     EmbeddedVectorDistance                          ment settings for both our synthetic task evalua-
                Our proposed feature gap metric, as defined in       tion on POSGEN and LLM-based evaluations on
                Equation 8, shares similarities with the ‚Äúembed-     both upstream language modeling evaluation and
                ded vector distance‚Äù metric introduced by Xiong      downstream real-world application evaluations.
                et al. (2023):                                       C.1   Synthetic Task Evaluation on POSGEN
                      ÀÜ                                   ÀÜ
                 d(f,f) = max       min      dist[f(x,k),f(x,j)]     For the synthetic task experiments in Section 6.1.1,
                           x‚ààX k‚àà{0,¬∑¬∑¬∑,N‚àí1}                         we train a two-layer Transformer on each of the
                                        ÀÜ
                                j‚àà{0,¬∑¬∑¬∑,N‚àí1}                 (9)    subtasks, with each layer following the configu-
                               d
                where X ‚äÇ R represents the set of vectors re-        ration of a T5-Small model (Raffel et al., 2020).
                quiring positional embedding. This equation as-      For each subtask, we train the model with different
                sesses the discrepancy in Rotary Position Embed-     position embeddings on a training set with 10,000
                ding (RoPE) before and after a scaling operation.    sequence samples of length 64. The validation and
                                                                 597
                 test sets each contains 1,000 sequence samples with     32Kand16K,respectively, which corresponds to
                 length 256. The sequences in the training, valida-      a scaling factor s = 8 and s = 4 for the position
                 tion and test sets do not overlap in the first j + k    embeddings of the two models.
                 tokens. For all YaRN and RESONANCE YARN set-              For both the long-sequence perplexity evalua-
                 tings, we train the model with YaRN and RESO-           tion in Section 6.2.2 and real-world task evalua-
                 NANCE YARN applied to the model with a scal-            tions in Section 6.2.3, the hyperparameters for the
                 ing factor s = 4, which corresponds to the TSTL         LLMexperiments follow the configurations pro-
                                                                                                   2
                 setting of our evaluation. Each model is trained       videdinPengetal.(2024) ,withtheonlymodifica-
                 on each subtask for 150 epochs with a language          tion that we fine-tune the model on approximately
                 modeling-style cross-entropy loss. Training was        100M tokens. More specifically, we use Œ± = 1
                 done with AdamWoptimizer (Loshchilov and Hut-           and Œ≤ = 32 for YaRN and RESONANCE YARNas
                                                      ‚àí4
                 ter, 2019), using learning rate 2√ó10    and weight      suggested by Peng et al. (2024). The model was
                 decay 1 √ó 10‚àí2. We use a batch size of 128 for          trained with a language modeling-style cross en-
                 all experiments. All hyperparameters were tuned         tropy loss. Training was done with the AdamW op-
                 to maximize YaRN‚Äôs validation set performance           timizer (Loshchilov and Hutter, 2019) using learn-
                 onthe Semi-Recurrent subtask. All synthetic task        ing rate 2 √ó 10‚àí5 and weight decay 1 √ó 10‚àí2. We
                 evaluations were performed on a single NVIDIA           use a batch size of 1 on each of the GPUs. The
                 V10032GGPU.                                             learning rate warm-up is applied to the first 5% of
                 C.2    LLMEvaluations                                   the total training steps. Models were fine-tuned
                                                                        with BF16 precision, FlashAttention 2 (Dao, 2023)
                 For the LLM-based evaluations in Section 6.2,           and DeepSpeed ZeRO-3 Offload (Ren et al., 2021)
                 wefine-tune LLaMA2-Chat 7B or LLaMA2-Chat               onfour NVIDIAA10040GGPUs.
                 13B(Touvronet al., 2023b) after replacing its orig-       For the real-world task evaluations in Sec-
                 inal RoPE position embedding with RoPE scaled           tion 6.2.3, we further compare two different fine-
                 with different strategies:                              tuning strategies:
                    ‚Ä¢ NTK-Aware Scaling (bloc97, 2023; Xiong               1. Fine-tuning on long sequences for less
                      et al., 2023; Liu et al., 2024), which scales the       epochs. We directly fine-tune the model on
                      base b in Equation 1 to s ¬∑ b, where s is the           the target sequence lengths after applying the
                      scaling factor. We evaluate the performance             scaled position embeddings. For LLaMA2-
                      without fine-tuning as used in bloc97 (2023).           Chat 7B and 13B, we fine-tune the model on
                                                                              sequences with length 32,768 for 50 steps and
                    ‚Ä¢ Dynamic NTK-AwareScaling (Peng et al.,                  sequences with length 16,384 for 100 steps,
                      2024; Rozi√®re et al., 2023). This method dy-            respectively.
                      namically computes the scaling factor consid-
                      ering the current sequence length Lc and the         2. Finetuning on short sequences for more
                                                                  L
                      original context window length L: s =        c .        epochs. We fine-tune the model on the origi-
                                                                  L           nal pre-training sequence length after apply-
                      Duetothehighcostoffrequently recomput-
                      ing RoPE features, we evaluated its perfor-             ing the scaled position embeddings. For both
                      mancewithout fine-tuning.                               LLaMA2-Chat7Band13B,wefine-tunethe
                                                                              modelonsequenceswithlength4,096for400
                    ‚Ä¢ YaRN (Peng et al., 2024). We evaluate its               steps.
                      performance after fine-tuning.
                    For NTK-Aware scaling and Dynamic NTK-
                 Aware scaling settings, we replace the original
                 RoPEposition embeddings in the model with the
                 scaled ones and test their performance without fine-
                 tuning following (bloc97, 2023; Peng et al., 2024).
                 For YaRN and RESONANCE YARN settings, we
                 fine-tune the model for approximately 100M to-
                 kens on PG19‚Äôs training set (Rae et al., 2020). Our
                 target scaled length for the 7B and 13B models is          2https://github.com/jquesnelle/yarn.
                                                                     598
