                     Published as a conference paper at ICLR 2025
                     GaoHuang,YuSun,ZhuangLiu,DanielSedra,andKilianQ.Weinberger. Deepnetworkswithstochastic
                       depth. In ComputerVision–ECCV2016: 14thEuropeanConference,Amsterdam,TheNetherlands,October
                       11–14, 2016, Proceedings, Part IV 14, pp. 646–661. Springer, 2016.
                     Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens Van Der Maaten, and Kilian Q. Weinberger.
                       Multi-scale dense networks for resource efficient image classification. arXiv preprint arXiv:1703.09844,
                       2017a.
                     GaoHuang,ZhuangLiu,LaurensVanDerMaaten,andKilianQ.Weinberger. Denselyconnectedconvolu-
                       tional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
                       4700–4708, 2017b.
                     Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
                       internal covariate shift. In International Conference on Machine Learning, pp. 448–456. pmlr, 2015.
                     EricJang,ShixiangGu,andBenPoole. CategoricalreparameterizationwithGumbel-softmax. InInternational
                       Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=
                       rkE3y85ee.
                     YounghanJeon, Minsik Lee, and Jin Young Choi. Differentiable forward and backward fixed-point iteration
                       layers. IEEE Access, 9:18383–18392, 2021.
                     Diederik P. Kingma and Jimmy Ba.    Adam: A method for stochastic optimization.  arXiv preprint
                       arXiv:1412.6980, 2014.
                     Jonathan Krause, Jia Deng, Michael Stark, and Li Fei-Fei. Collecting a large-scale dataset of fine-grained
                       cars. 2013.
                     Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Department of
                       ComputerScience, University of Toronto, Toronto, ON, Canada, 2009.
                     YannLeCun,LéonBottou,YoshuaBengio,andPatrickHaffner. Gradient-basedlearningappliedtodocument
                       recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
                     YannLeCun,YoshuaBengio,andGeoffreyE.Hinton. Deeplearning. Nature, 521(7553):436–444, 2015.
                     Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and Xiaotong Zhang. Pruning and quantization for deep
                       neural network acceleration: A survey. Neurocomputing, 461:370–403, 2021.
                     Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow, Raquel Urtasun, and
                       Richard Zemel. Reviving and improving recurrent back-propagation. In International Conference on
                       Machine Learning, pp. 3082–3091. PMLR, 2018.
                     Timothy P. Lillicrap, Daniel Cownden, Douglas B. Tweed, and Colin J. Akerman. Random feedback weights
                       support learning in deep neural networks. arXiv preprint arXiv:1411.0247, 2014.
                     Yinhan Liu. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692,
                       2019.
                     ChengLu,YuhaoZhou,FanBao,JianfeiChen,ChongxuanLi,andJunZhu. DPM-Solver: AfastODEsolver
                       for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing
                       Systems, 35:5775–5787, 2022.
                     Matthew MacKay, Paul Vicol, Jimmy Ba, and Roger B. Grosse. Reversible recurrent neural networks.
                       Advances in Neural Information Processing Systems, 31, 2018.
                                                               13
