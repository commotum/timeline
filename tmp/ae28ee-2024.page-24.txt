                            Published as a conference paper at ICLR 2024
                             Repository             Summary                                                Count    License
                             marshmallow-code/      Parse complex objects to/from Python data-types           9     MIT
                                marshmallow
                             pylint-dev/astroid     Library for AST parsing, static analysis/inference       31     LGPL-2.1
                             pydicom/pydicom        Read/modify/write DICOM files w/ Python                  56     Custom
                             pvlib/pvlib-python     Simulate photovoltaic energy systems performance         63     Custom
                             pyvista/pyvista        3Dplotting, mesh analysis through interface              16     MIT
                             sqlfluff/sqlfluff      SQLlinter, supports multiple dialects, templates         50     MIT
                            Table 15: Summary and licenses for all GitHub repositories that development task instances were
                            extracted from.
                               Category      Count     Examples
                               Bug           127       “bug” (111); “Bug :cockroach:” (10); “rule bug” (6);
                               Feature       55        “enhancement”: 46; “Enhancement :star:”: 5; “feature-request”: 2;
                               Regression    4         “Regression” (4);
                               Other         95        “api”: 11, “documentation”: 7, “help wanted”: 6, “config options”: 5,
                                                       “io”: 5, “jinja”: 4, “good first issue”: 4, “parser” 3
                            Table 16: Categories of tags associated with issues from SWE-bench’s development task instances.
                            B ADDITIONALDETAILS ON TRAINING SWE-LLAMA
                            B.1   TRAINING DETAILS
                            Optimization. We finetune using LoRA (Hu et al., 2022) with r = 16, α = 16, dropout = 0.05,
                            on the query, key, value, and output projection matrices of every attention sublayer. We train with
                            a learning rate of 6e − 4 and a batch size of 32 sequences per gradient step for a maximum of
                            4 epochs. During training, we save checkpoints every 50 steps, and after training, select the best
                            checkpoint based on the validation loss on a held-out 100 instances. SWE-Llama 7b was initialized
                            with CodeLlama-Python 7b and trained in 20 hours on 4 NVIDIA A100s. SWE-Llama 13b was
                            initialized with CodeLlama-Python 13b and trained in 47 hours on 8 NVIDIA A100s. We used
                            DeepSpeed Ulysses (Jacobs et al., 2023) and Flash Attention (Dao et al., 2022) to enable long
                            context training.
                            C ADDITIONALRESULTS
                            C.1   RESULTS WITH “ORACLE” RETRIEVAL
                            Using the “oracle” retrieval method described in Section 4.1, we show the general performance
                            results in Table 18. Naturally, providing only the files edited by the reference solution’s pull request,
                            model performance improves compared to the noisier BM25 retrieval setting.
                            C.2   EVALUATION TEST SET
                            We include a repository-by-repository breakdown of model performance in Table 19 that corre-
                            sponds to Figure 4 in the main paper. As discussed, in the main paper, performance differs heavily
                            across repositories.
                            C.3   GPT-4 EVALUATION SUBSET RESULTS
                            In this section, we present the statistics shown in Table 5 for the 25% random subset that GPT-4 was
                            tested in Table 20. As the selection of the subset is random, we find that the % Resolved and %
                            Applyrates are consistent with the main results, and not significantly skewed towards being simpler
                            or more difficult than the general evaluation set.
                                                                             24
