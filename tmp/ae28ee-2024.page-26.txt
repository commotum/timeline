                         Published as a conference paper at ICLR 2024
                          Repo                    Claude 2  ChatGPT-3.5    GPT-4    SWE-Llama13b     SWE-Llama7b
                          astropy/astropy             3.23          0.00     0.00              1.06             3.16
                          django/django               6.15          1.32     2.50              5.19             4.00
                          matplotlib/matplotlib       3.05          3.33     0.00              3.12             1.11
                          mwaskom/seaborn             0.00          0.00     0.00              0.00             0.00
                          pallets/flask               0.00          0.00     0.00              9.09             0.00
                          psf/requests               15.91          2.33     8.33             13.64           18.18
                          pydata/xarray               6.90          0.00     0.00              5.81             3.00
                          pylint-dev/pylint           1.75          0.00     0.00              1.75             1.75
                          pytest-dev/pytest           5.93          0.00     0.00              5.04             4.20
                          scikit-learn/scikit-learn   5.41          0.00     0.00              3.12             0.88
                          sphinx-doc/sphinx           5.65          1.83     0.00              2.25             2.69
                          sympy/sympy                 1.94          0.00     0.00              3.01             1.59
                                   Table 19: % Resolved for models per repository represented in SWE-bench.
                                                         BM25Retrieval              “Oracle” Retrieval
                                  Model             %Resolved      %Apply       %Resolved        %Apply
                                  Claude 2           2.27 ↑ 0.31  45.72 ↑ 2.65  4.01 ↓ 0.79  62.65 ↓ 0.17
                                  ChatGPT-3.5        0.17 −0.00   26.53 ↑ 0.02  0.70 ↑ 0.18  21.64 ↓ 0.16
                                  GPT-4              0.00 −0.00   14.82 −0.00   1.74 −0.00   34.00 −0.00
                                  SWE-Llama7b        0.35 ↑ 0.35  49.04 ↓ 2.70  1.92 ↓ 1.09  63.70 ↓ 1.82
                                  SWE-Llama13b       0.70 −0.00   56.54 ↑ 2.92  4.54 ↑ 0.57  66.67 ↓ 0.11
                         Table 20: We compare models against each other using the BM25 and oracle retrieval settings as
                         described in Section 4 on a 25% random subset (574 instances) of SWE-bench in the “oracle” and
                         BM2527Kretrieversettings only. This is the same subset that GPT-4 is evaluated on, as mentioned
                         in Table 5. The difference relative to percentages in Table 5 and Table 18 is included as a subscript.
                         behavior is maintained (all P2P tests pass) but the issue remains completely unresolved (0 F2P tests
                         pass). Finally, if the issue is unresolved (0 F2P tests pass) and prior working behavior is reverted
                         (some P2P tests fail), the codebase is left in a worse state, which we define to be a “Regression”.
                         In Table 23, we categorize patch generations that successfully applied according to these six cases.
                         Wefindthat of non-“Resolved” issues, the majority of patch generations proposed by the model do
                         not solve a single F2P test case from the corresponding task instance (“No-Op” and “Regression”).
                         Within the subset of these cases, the majority (60% to 70%) of cases are a No-Op, while the model
                         breaks existing behavior for the remainder of these situations.
                         Generally, the cases where model generations pass some, but not all tests (“Breaking Resolved”,
                         “Partially Resolved”, “Work in Progress”) cumulatively represent a smaller subset of problems rel-
                         ative to the other three categories. From manual inspection of several of these cases, it is clear that
                         the model demonstrates some understanding of the task requirements. However, due to the baseline
                         methods’limitedviewofthecodebasethatdoesnotincludeinformationsuchasinter-filedependen-
                         cies and functions’ relationships, for many of these task instances often fail because a change that
                         correctly resolves the immediate issue does not account for other modules that use and are affected
                         by the changed entity. We include several case studies that directly highlight these shortcomings in
                         Section F Overall, these results highlight not just the difficulty of SWE-bench, but also point to the
                         potential value of providing feedback via an execution environment that would allow models to run
                         fixes against existing tests, then decide whether to continue editing or submit the patch for review.
                         C.6   PATCH GENERATION EXTENDED ANALYSIS
                         In this section, we present a statistics to quantify various facets of patch generations following
                         the metrics laid out in Table 8. In Table 24, we recalculate these values for all patch generations
                         in the oracle retrieval setting for all models, regardless of whether or not the patch was applied
                                                                    26
