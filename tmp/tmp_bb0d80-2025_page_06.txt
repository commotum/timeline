                                                                                                     The GeneCA model iteratively updates the state.
                                                                                                                                                                         Visible channels of the final state are compared against 
                                                                                                   Followed by the GenePropCA model doing the same.
                                                                                                                                                                    the target ARC grid to compute the mean square error to be used 
                                                                                                  Backprop-through-time updates both sets of parameters.                            as the loss for backpropagation.
                                                                                                                                                                                The new state is added back to the pool.
                                                                                                                  GeneCA Parameters
                                                          The initial state is sampled 
                                                         without replacement from a            Step 1                Step 2                       Step N
                                                                 pool of states
                                                                                                                                                                                                     Target Image
                                                                           .                Stage 1    Stage 2    Stage 1    Stage 2           Stage 1    Stage 2                          L2 Loss                                  Pool
                                                                                                               GenePropCA Parameters
                                                             Figure 5: One backpropagation step of training EngramNCA for solving ARC problems.
                          takes to reach the final solution. Thus, we take the loss to be                                                                                                             Results
                          MSEPixelWiseLossasin(Mordvintsevetal.,2020).                                                                                 General Results
                               Todetermine whether a problem is solved, we look at the                                                                 In this section, we present the results of each CA in the form
                          mean pixel error across the generated NCA. An evaluation                                                                     of Mean log(loss) and the CA solve rate. The same results
                          loss of log(MSEPixelWiseLoss) ≤ −7, where this loss                                                                          were obtained for the union of different CA. As a reminder
                          wasevenlydistributedamongpixels,wasexperimentallyde-                                                                         to the reader, two answers may be submitted when solving
                          termined to produce exact solutions to the ARC problems.                                                                     ARC; thus, by taking the union (each model produces one
                                                                                                                                                       output) we still produce a valid submission.
                          ModelTraining
                          We choose to solve ARC via test-time training. As stated                                                                                                                    CAResults
                          by (Chollet, 2019; Chollet et al., 2024), program generators                                                                    Model                                     Meanlog(loss)                        Solve Rate
                          must be able to learn from new information. We take this                                                                        NCA                                       -4.31                                10.7%
                          to mean that our program generator, the system that trains                                                                      EngramNCAv1                               -3.63                                6.5%
                          NCAs,cantrainanewCAperproblem. Foreveryproblem,                                                                                 EngramNCAv2⋆                              -4.03                                9.2%
                          wetrainanewCAfromscratchonthe2-3trainingexamples                                                                                EngramNCAv3                               -4.35                                12.9%
                          and evaluate its performance on the unseen sample. All our                                                                      EngramNCAv4                               -4.20                                10.3%
                          experiments are run on the ARC-AGI public evaluation set.                                                                       Chat GPT4.5⋆⋆                             N/A                                  10.3%
                               Figure 5 shows one training iteration of the training pro-                                                              Table 2: Mean log(loss) and solve rate for all four CA vari-
                          cedure for EngramNCA versions. The training procedure                                                                        ations. The best results are highlighted in green, and the
                          mirrors that of (Guichard et al., 2025) with one key modifi-                                                                 worst results are highlighted in red. ⋆Due to space con-
                          cation. Due to training both the GeneCA and GenePropCA                                                                       straints in the paper, and the fact that the results are very
                          from scratch for each problem, the GeneCA weights are                                                                        similar to EngramNCA v4, we omit EngramNCA v2 from
                          not frozen, and both sets of weights are co-optimized. The                                                                   manyoftheresultdiscussions, including the unions of mod-
                          standard NCA was instead trained with the same procedure                                                                     els. ⋆⋆TheresultsforChatGPT4.5aretakenfromtheARC-
                          shownin(Mordvintsev et al., 2020).                                                                                           AGIleaderboard (ARC Team, 2025). Note that such results
                                                                                                                                                       were obtained on the ARC-AGI private evaluation set, in-
                                                                                                                                                     stead of the public evaluation set as for our results.
                                                                         P P P                                                      2
                                                                  1          H       W        C                       ˆ
                               PixelWiseMSE = H×W×C                          i=0     j=0      k=0 I(i,j,k)−I(i,j,k)(11)
                                                                                                                                                            Table 2 shows the mean loss(log) and solve rate for each
                               WhereH,W,Carethedimensionsoftheimage,I isthe                                                                            CA.EngramNCAv3performsbestinbothcategorieswitha
                                                                   ˆ
                          reference image, and I is the final state of the NCA.                                                                        near 13% solve rate. In contrast, EngramNCA v1 performs
                               We use AdamW as the optimizer, with a learning rate                                                                     the worst in both metrics, with a solve rate of 6.5%.
                          (LR) of 1e − 3. For each problem, the CA are trained for                                                                          Table 3 shows the cost comparison between the CA mod-
                          3000 iterations, with a 66% reduction in LR at 2000 itera-                                                                   els we experimented with and Chat GPT 4.5. We chose to
                          tions.                                                                                                                       comparetoChatGPT4.5asithassolveratessimilartoours
