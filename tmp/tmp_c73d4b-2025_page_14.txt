           [50] Wannan Yang, Chen Sun, Roman Huszár, Thomas Hainmueller, Kirill Kiselev, and György
              Buzsáki. Selection of experience for memory by hippocampal sharp wave ripples. Science, 383
              (6690):1478–1483, 2024.
           [51] Daoyun Ji and Matthew A Wilson. Coordinated memory replay in the visual cortex and
              hippocampus during sleep. Nature neuroscience, 10(1):100–107, 2007.
           [52] Adrien Peyrache, Mehdi Khamassi, Karim Benchenane, Sidney I Wiener, and Francesco P
              Battaglia. Replay of rule-learning related neural patterns in the prefrontal cortex during sleep.
              Nature neuroscience, 12(7):919–926, 2009.
           [53] DavidJFosterandMatthewAWilson. Reversereplayofbehaviouralsequencesinhippocampal
              place cells during the awake state. Nature, 440(7084):680–683, 2006.
           [54] Sean PA Drummond, Gregory G Brown, J Christian Gillin, John L Stricker, Eric C Wong,
              and Richard B Buxton. Altered brain response to verbal learning following sleep deprivation.
              Nature, 403(6770):655–657, 2000.
           [55] Seung-Schik Yoo, Peter T Hu, Ninad Gujar, Ferenc A Jolesz, and Matthew P Walker. A deficit
              in the ability to form new human memories without sleep. Nature neuroscience, 10(3):385–392,
              2007.
           [56] WScott Terry. Learning and memory: Basic principles, processes, and procedures. Routledge,
              2017.
           [57] Hideyuki Okano, Tomoo Hirano, and Evan Balaban. Learning and memory. Proceedings of the
              National Academy of Sciences, 97(23):12403–12404, 2000.
           [58] Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. It’s all connected: A
              journey through test-time memorization, attentional bias, retention, and online optimization.
              arXiv preprint arXiv:2504.13173, 2025.
           [59] Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. Longhorn: State
              space models are amortized online learners. arXiv preprint arXiv:2407.14207, 2024.
           [60] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers
              are rnns: Fast autoregressive transformers with linear attention. In International conference on
              machine learning, pages 5156–5165. PMLR, 2020.
           [61] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang,
              and Furu Wei. Retentive network: A successor to transformer for large language models. arXiv
              preprint arXiv:2307.08621, 2023.
           [62] Juergen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent
              nets. accepted for publication in. Neural Computation, 1992.
           [63] Imanol Schlag, Kazuki Irie, and Juergen Schmidhuber. Linear transformers are secretly fast
              weight programmers. In International Conference on Machine Learning, pages 9355–9366.
              PMLR,2021.
           [64] DLPradosandSCKak. Neuralnetworkcapacityusingdelta rule. Electronics Letters, 25(3):
              197–199, 1989.
           [65] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois,
              Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with
              expressive hidden states. arXiv preprint arXiv:2407.04620, 2024.
           [66] Nicholas J Higham. Functions of matrices: theory and computation. SIAM, 2008.
           [67] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2
              with delta rule. arXiv preprint arXiv:2412.06464, 2024.
           [68] Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear trans-
              formers with the delta rule over sequence length. Advances in Neural Information Processing
              Systems, 37:115491–115522, 2024.
                               14
