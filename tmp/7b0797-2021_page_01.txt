                    Training data-efﬁcient image transformers & distillation through attention
                                                   HugoTouvron12 MatthieuCord12 MatthijsDouze1
                                                                   1                              1       ´   ´    1
                                               Francisco Massa        Alexandre Sablayrolles        HerveJegou
                                          Abstract                                                                  ⚗↑
                     Recently, neural networks purely based on atten-                                                           ⚗
                     tion were shown to address image understanding
                     tasks such as image classiﬁcation. These high-
                     performing vision transformers are pre-trained                                                                        ⚗
                     with hundreds of millions of images using a large
                     infrastructure, thereby limiting their adoption.
                     In this work, we produce competitive convolution-                                    ⚗
                     free transformers trained on ImageNet only us-
                     ing a single computer in less than 3 days. Our
                     reference vision transformer (86M parameters)
                     achieves top-1 accuracy of 83.1% (single-crop)
                     onImageNetwithnoexternal data.
                     Wealsointroduce a teacher-student strategy spe-
                     ciﬁc to transformers. It relies on a distillation
                     token ensuring that the student learns from the
                     teacher through attention, typically from a con-                Figure 1. Throughput and accuracy on Imagenet of our method
                     vnet teacher. The learned transformers are com-                 (no external training data). The throughput is measured as the
                     petitive (85.2% top-1 acc.) with the state of the art           numberofimagesprocessed per second on a V100 GPU. DeiT-B
                     on ImageNet, and similarly when transferred to                  is identical to ViT-B, but with training adapted to a data-starving
                     other tasks. We will share our code and models.                 regime. It is learned in a few days on one machine. The symbol ⚗
                                                                                     refers to models trained with our transformer-speciﬁc distillation.
                                                                                     See Table 5 for details and more models.
                1. Introduction                                                      to image classiﬁcation with raw image patches as input.
                Convolutional neural networks have been the main design              Their paper presented excellent results with transformers
                paradigm for image understanding tasks, as initially demon-          trained with a large private labelled image dataset contain-
                strated on image classiﬁcation tasks. One of the ingredient          ing 300 millions images. The paper concluded that vision
                to their success was the availability of a large training set,       transformers “do not generalize well when trained on in-
                namely Imagenet. Motivated by the success of attention-              sufﬁcient amounts of data”. The training of these models
                based models in Natural Language Processing, there has               involved extensive computing resources.
                been an increasing interest in architectures leveraging atten-       In our paper, we train a vision transformer on a single 8-
                tion mechanisms within convnets. More recently several               GPUnodein two to three days (53 hours of pre-training,
                researchers have proposed hybrid architecture transplanting          and optionally 20 hours of ﬁne-tuning) that is competitive
                transformer ingredients to convnets to solve vision tasks.           with convnets having a similar number of parameters and
                The vision transformer (ViT) introduced by Dosovitskiy               efﬁciency. It uses Imagenet as the sole training set. We
                et al. (2020) is an architecture directly inherited from Natu-       build upon the visual transformer architecture from Doso-
                ral Language Processing (Vaswani et al., 2017), but applied          vitskiy et al. (2020) and improvements included in the timm
                                                                                     library (Wightman, 2019). With our Data-efﬁcient image
                   1FacebookAI2SorbonneUniversity. Correspondence to: Hugo
                Touvron <htouvron@fb.com>.                                           Transformers (DeiT), we report large improvements over
                                                                                     previous results, see Figure 1. Our ablation study details
                Proceedings of the 38th International Conference on Machine          the hyper-parameters and key ingredients for a successful
                Learning, PMLR 139, 2021. Copyright 2021 by the author(s).           training, such as repeated augmentation.
