contrast, process supervision provides a richer signal: it specifies both how
many of the first steps were in fact correct, as well as the precise location of
he incorrect step. Process supervision makes credit assignment easier, and we
yelieve that this explains its strong performance.

6.2 Alignment Impact

Process supervision has several advantages over outcome supervision related
o AI alignment. Process supervision is more likely to produce interpretable
reasoning, since it encourages models to follow a process endorsed by humans.
Process supervision is also inherently safer: it directly rewards an aligned chain-
of-thought rather than relying on outcomes as a proxy for aligned behavior
(Stuhlmiiller and Byun, 2022). In contrast, outcome supervision is harder to
scrutinize, and the preferences conveyed are less precise. In the worst case,
he use of outcomes as an imperfect proxy could lead to models that become
misaligned after learning to exploit the reward signal (Uesato et al., 2022; Cotra,
2022; Everitt et al., 2017).

In some cases, safer methods for AI systems can lead to reduced performance
(Ouyang et al., 2022; Askell et al., 2021), a cost which is known as an alignment
tax. In general, any alignment tax may hinder the adoption of alignment meth-
ods, due to pressure to deploy the most capable model. Our results show that
process supervision in fact incurs a negative alignment tax. This could lead to
increased adoption of process supervision, which we believe would have positive
alignment side-effects. It is unknown how broadly these results will generalize
beyond the domain of math, and we consider it important for future work to
explore the impact of process supervision in other domains.

6.3. Test Set Contamination

The test set of the MATH dataset contains problems that are discussed in
several online venues, and it is likely that some of these problems appear in
the pretraining dataset for our models. We attempted to remove all MATH
problems from our MathMix dataset using string-matching heuristics, but since
humans can post hard-to-detect rephrasings of a problem online, it is difficult
to make any strong guarantees about the overlap between MathMix and the
MATH dataset.
n our experience inspecting model-generated solutions, we saw no clear signs
of our models memorizing MATH problems. However, it is impossible to rule
out subtle forms of memorization that would slip past manual inspection, and
it is still possible that some degree of contamination has slightly inflated our
performance on the MATH test set. Even in that case, we would expect any
contamination to manifest similarly across all methods, and that the relative
comparisons made throughout this work would remain mostly unaffected.

We also note that the PRM regularly surfaces correct solutions to MATH
problems that have a low single-digit percentage solve-rate under the genera-
tor, some examples of which can be seen in Appendix I. The generatorâ€™s low

11
