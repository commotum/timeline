Published as a conference paper at ICLR 2021

Untie [CLS]

Figure 4: Illustration of untying [CLS]. vj; denotes the positional correlation of pair (i, j). The first
row and first column are set to the same values respectively.

3.3. IMPLEMENTATION DETAILS AND DISCUSSIONS

In the two subsections above, we propose several modifications to untie the correlations between
positions and words (Eq. (7) and Eq. (8)), and untie the [CLS] symbol from other positions (Eq. (9)).
By combining them, we obtain a new positional encoding method and call it TUPE (Transformer
with Untied Positional Encoding). There are two versions of TUPE. The first version is to use the
untied absolute positional encoding with the untied [CLS] symbol (Eq. (7) + Eq. (9)), and the second
version is to use an additional relative positional encoding (Eq. (8) + Eq. (9)). We call them TUPE-A
and TUPE-R respectively and list the mathematical formulations as below.

a= (ei WE) (AWE)? + resete(T5(piU%)(pjU*)", 3,3) (10)

aR = Fe (eWe)(a WK)? + reseto( 5 (piU)(pjU*)” + bj-1,4,9), AD

The multi-head version, parameter sharing, and efficiency. TUPE can be easily extended to the
multi-head version. In our implementation, the absolute positional embedding p; for position 7 is
shared across different heads, while for each head, the projection matrices U@ and U* are different.
For the relative positional encoding, b;_; is different for different heads. The reset parameter @ is also
not shared across heads. For efficiency, we share the (multi-head) projection matrices U@ and U* in
different layers. Therefore, in TUPE, the number of total parameters does not change much. Taking
BERT-Base as an example, we introduce about 1.18M (2 x 768 x 768) new parameters, which is
only about 1% of the 110M parameters in BERT-Base. Besides, TUPE introduces little additional
computational costs. As the positional correlation term Jaq (Pi U®)(p;U* )* is shared in all layers,

we only need to compute it in the first layer, and reuse its outputs in other layers.

Are absolute/relative positional encoding redundant to each other? One may think that both
the absolute/relative positional encoding in Eq. (11) describe the content-free correlation, and thus
one of them is redundant. To formally study this, we denote B as an n x n matrix where each
element B;,; = b;_;. By definition, B is a Toeplitz matrix (Gray, 2006). We also denote P as an
nm X n matrix where the i-th row is p;, and thus the absolute positional correlation in matrix form is
Yeql(PUS)(PU*)?. We study the expressiveness of B and = Jaq (PU®)(PU*)? by first showing

B can be factorized similarly from the following proposition.

Proposition 1. Any Toeplitz matrix B © C"*" can be factorized into B = GDG", where D is a
2n x 2n seen matrix. G is an x 2n Vandermonde matrix in the complex space, where each
element Gj, = HeitG+Dk/n and G* is the conjugate transpose of G.

The proof can be found in Appendix A. The two terms B and —= Jaq (PU?) (PU* )* actually form

different subspaces in R”*”. In the multi-head version, the stage of the matrix U@ and U* are
dx 4. Therefore, (PU@)(PU* )? can characterize low-rank matrices in R"*”. But from the
proposition, we can see that B forms a linear subspace in R”*” with only 2n — 1 freedoms, which

is quite different from the space of yaq(PU®)(PU* )?. There are also some practical reasons
which make using both terms together essential. As discussed previously, in Raffel et al. (2019), the
range of the relative distance 7 — i will be clipped up to an offset beyond which all relative positions
will be assigned the same value. In such a situation, the relative positional encoding may not be
able to differentiate words faraway and Tea (p:U®)(pj;U* )* can be used to encode complementary
information.
