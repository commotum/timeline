              tion on ShapeNetPart (86.6% instance mIoU). Our full im-           putation and memory costs due to the cubic growth in the
              plementation and trained models will be released upon ac-          number of voxels as a function of resolution. The solution
              ceptance. In summary, our main contributions include the           is to take advantage of sparsity, as most voxels are usually
              following.                                                         unoccupied. For example, OctNet [29] uses unbalanced
                • WedesignahighlyexpressivePointTransformerlayer                 octrees with hierarchical partitions. Approaches based on
                   for point cloud processing.     The layer is invariant        sparse convolutions, where the convolution kernel is only
                   to permutation and cardinality and is thus inherently         evaluated at occupied voxels, can further reduce computa-
                   suited to point cloud processing.                             tion and memory requirements [9, 3]. These methods have
                                                                                 demonstrated good accuracy but may still lose geometric
                • Based on the Point Transformer layer, we construct             detail due to quantization onto the voxel grid.
                   high-performing Point Transformer networks for clas-          Point-based networks. Rather than projecting or quantiz-
                   sification and dense prediction on point clouds. These        ingirregular point clouds onto regular grids in 2D or 3D, re-
                   networks can serve as general backbones for 3D scene          searchers have designed deep network structures that ingest
                   understanding.                                                point clouds directly, as sets embedded in continuous space.
                • We report extensive experiments over multiple do-              PointNet [25] utilizes permutation-invariant operators such
                   mains and datasets. We conduct controlled studies to          as pointwise MLPs and pooling layers to aggregate features
                   examine specific choices in the Point Transformer de-         across a set. PointNet++ [27] applies these ideas within a
                   sign and set the new state of the art on multiple highly      hierarchical spatial structure to increase sensitivity to local
                   competitive benchmarks, outperforming long lines of           geometric layout. Such models can benefit from efficient
                   prior work.                                                   sampling of the point set, and a variety of sampling strate-
                                                                                 gies have been developed [27, 7, 46, 50, 11].
              2. Related Work                                                       A number of approaches connect the point set into
                                                                                 a graph and conduct message passing on this graph.
                 For 2D image understanding, pixels are placed in regu-          DGCNN[44]performsgraphconvolutionsonkNNgraphs.
              lar grids and can be processed with classical convolution.         PointWeb [55] densely connects local neightborhoods.
              In contrast, 3D point clouds are unordered and scattered           ECC[31]usesdynamicedge-conditionedfilterswherecon-
              in 3D space: they are essentially sets. Learning-based ap-         volution kernels are generated based on edges inside point
              proaches to processing 3D point clouds can be classified           clouds. SPG [15] operates on a superpoint graph that rep-
              intothefollowingtypes: projection-based,voxel-based,and            resents contextual relationships. KCNet [30] utilizes kernel
              point-based networks.                                              correlation and graph pooling. Wang et al. [40] investigate
              Projection-based networks. For processing irregular in-            the local spectral graph convolution. GACNet [41] employs
              puts like point clouds, an intuitive way is to transform ir-       graphattentionconvolutionandHPEIN[13]buildsahierar-
              regular representations to regular ones.    Considering the        chical point-edge interaction architecture. DeepGCNs [19]
              success of 2D CNNs, some approaches [34, 18, 2, 14, 16]            explore the advantages of depth in graph convolutional net-
              adoptmulti-viewprojection,where3Dpointcloudsarepro-                works for 3D scene understanding.
              jected into various image planes. Then 2D CNNs are used               Anumberofmethodsarebasedoncontinuous convolu-
              to extract feature representations in these image planes, fol-     tions that apply directly to the 3D point set, with no quan-
              lowed by multi-view feature fusion to form the final output        tization. PCCN [42] represents convolutional kernels as
              representations. In a related approach, TangentConv [35]           MLPs. SpiderCNN [49] defines kernel weights as a fam-
              projects local surface geometry onto a tangent plane at ev-        ily of polynomial functions. Spherical CNN [8] designs
              ery point, forming tangent images that can be processed by         spherical convolution to address the problem of 3D rota-
              2D convolution. However, this approach heavily relies on           tion equivariance. PointConv [46] and KPConv [37] con-
              tangent estimation. In projection-based frameworks, the ge-        struct convolution weights based on the input coordinates.
              ometric information inside point clouds is collapsed during        InterpCNN [22] utilizes coordinates to interpolate point-
              the projection stage. These approaches may also underuti-          wisekernelweights. PointCNN[20]proposestoreorderthe
              lize the sparsity of point clouds when forming dense pixel         input unordered point clouds with special operators. Um-
              grids on projection planes. The choice of projection planes        menhofer et al. [38] apply continuous convolutions to learn
              may heavily influence recognition performance and occlu-           particle-based fluid dynamics.
              sion in 3D may impede accuracy.                                    Transformer and self-attention. Transformer and self-
              Voxel-based networks. An alternative approach to trans-            attention models have revolutionized machine translation
              forming irregular point clouds to regular representations is       and natural language processing [39, 45, 5, 4, 51]. This
              3D voxelization [23, 32], followed by convolutions in 3D.          has inspired the development of self-attention networks for
              Whenappliednaively, this strategy can incur massive com-           2D image recognition [10, 28, 54, 6]. Hu et al. [10] and
                                                                             16260
