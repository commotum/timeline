       1546               G.Hinton,S.Osindero,andY.-W.Teh
       Figure 9: Each row shows 10 samples from the generative model with a par-
       ticular label clamped on. The top-level associative memory is initialized by an
       up-passfromarandombinaryimageinwhicheachpixelisonwithaprobability
       of 0.5. The ﬁrst column shows the results of a down-pass from this initial high-
       level state. Subsequent columns are produced by 20 iterations of alternating
       Gibbssamplingintheassociativememory.
       in mind. This use of the word mind is not intended to be metaphorical.
       Webelievethat a mental state is the state of a hypothetical, external world
       in which a high-level internal representation would constitute veridical
       perception. That hypothetical world is what the ﬁgure shows.
       8Conclusion
       Wehave shown that it is possible to learn a deep, densely connected be-
       lief network one layer at a time. The obvious way to do this is to assume
       that the higher layers do not exist when learning the lower layers, but
       this is not compatible with the use of simple factorial approximations to
       replace the intractable posterior distribution. For these approximations to
       work well, we need the true posterior to be as close to factorial as pos-
       sible. So instead of ignoring the higher layers, we assume that they exist
       but have tied weights that are constrained to implement a complementary
       prior that makes the true posterior exactly factorial. This is equivalent to
       havinganundirectedmodelthatcanbelearnedefﬁcientlyusingcontrastive
       divergence. It can also be viewed as constrained variational learning be-
       cause a penalty term—the divergence between the approximate and true
