                           Preprint, Under Review.
                           3.3   TRAINING THE DYNAMIC PLANNING AGENT
                           To enable the agent to learn when to plan, thereby implicitly performing the cost-benefit analysis
                           outlined in Section 3.1, we use RL fine-tuning. The agent’s policy parameters θ (governing the
                           decision policy ϕ , planning policy ψ and acting policy π ) are optimized to maximize the expected
                                           θ                   θ                   θ
                           discounted sum of task rewards, adjusted by a penalty for the computational cost of planning:
                                                                 " H                                    #
                                             θ∗ = argmaxE         Xγt(R         (s ,a ) − d · C        )
                                                        θ    τ∼θ            task  t  t     t   tokens,t
                                                                   t=0
                           Other detrimental effects of poor planning strategies, such as those arising from excessive latency
                           or instability (conceptualized as Clatency and Cnoise), are implicitly discouraged as they naturally
                           lead to lower task rewards R     (s ,a ). Thus, by optimizing this objective, the decision policy
                                                        task  t  t
                           ϕ , planning policy ψ , and acting policy π should jointly learn to optimally decide when to plan
                            θ                   θ                    θ
                           (d =1),howtooutputplans(p ) that are beneficial, and how to output actions (a ) that effectively
                             t                            t                                                t
                           follow these plans, ensuring that the expected improvement in future task rewards (i.e., the empirical
                           benefit corresponding to the conceptual A    (c )) outweighs the explicit cost C       as well as
                                                                    plan  t                               tokens,t
                           any implicit degradation of R    due to poor planning.
                                                        task
                           4   EXPERIMENTAL SETUP
                           Our experiments evaluate planning agents across diverse settings. In this section, we detail the
                           environments used, the core evaluation protocol, and the specific setups for evaluation, SFT, and RL.
                           4.1   ENVIRONMENTS
                           Toevaluatedynamicplanningacrossdifferentconditions,weselecttwocomplementaryenvironments.
                           First, Partially Observable Graph Search (POGS) is our custom synthetic environment designed
                           to isolate planning under uncertainty. Agents navigate procedurally generated graph using only local
                           observations, which require adaptive replanning upon discovery of new nodes or dead ends. POGS
                           allows measurement of exploration efficiency via backtracking statistics. Second, Crafter (Hafner,
                           2022) is a complex 2D grid-world, long-horizon benchmark inspired by Minecraft. It demands
                           multi-scale planning for survival, resource management, and crafting, testing both short-term tactical
                           decisions and long-term strategic choices. Interaction in both environments occurs via natural
                           language. Full technical details and figures for the environments are provided in the Appendix B.
                           4.2   EVALUATION PROTOCOL
                           Weutilize the BALROG benchmark (Paglieri et al., 2025a) for standardized agent evaluation and
                           environment interaction. At each timestep t, the agent receives its history and current observation o
                                                                                                                          t
                           within a chat template, guided by a system prompt outlining the task (Fig. 4 in Appendix A). The
                           agent’s response must include a natural language action command at. Our dynamic planning agents
                           are instructed to decide at each step whether to plan. If they choose not to plan, they output only the
                           action command [Action]. If they choose to plan, they output the plan followed by the action,
                           using the format <plan> [natural language plan] </plan> [Action]. BALROG
                           parses this output, identifying a planning decision (d = 1) if the <plan> block is present and
                                                                                t
                           using its content as the current plan p in subsequent context. The [Action] command is always
                                                               t
                           extracted and executed. Fallback mechanisms ensure robustness against invalid outputs. Appendix A
                           provides detailed prompts.
                           4.3   ZERO-SHOT EVALUATION
                           Tounderstand baseline capabilities and the raw effect of planning frequency, we perform zero-shot
                           evaluations using Llama-3.3-70B-Instruct (Grattafiori et al., 2024) on POGS and Crafter (100 seeds
                           each). We compare different prompting strategies without any fine-tuning. We test a Naive Agent,
                           prompted to only output actions and thus never plan. We also test Fixed-Frequency Planners,
                           which are prompted to plan-every-k-steps for various k ∈ {1,2,4,8,...}; these agents are
                                                                          6
