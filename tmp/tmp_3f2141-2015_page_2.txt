                           short-term memory (LSTM) cells [1, 5] can learn some transductions when tested on inputs of the
                           same length as seen in training, they fail to consistently generalise to longer strings. In contrast,
                           our sequential memory-based algorithms are able to learn to reproduce the generating transduction
                           algorithms, often generalising perfectly to inputs well beyond those encountered in training.
                           2   Related Work
                           String transduction is central to many applications in NLP, from name transliteration and spelling
                           correction, to inﬂectional morphology and machine translation. The most common approach lever-
                           ages symbolic ﬁnite state transducers [6, 7], with approaches based on context free representations
                           also being popular [8]. RNNs offer an attractive alternative to symbolic transducers due to their sim-
                           ple algorithms and expressive representations [9]. However, as we show in this work, such models
                           are limited in their ability to generalise beyond their training data and have a memory capacity that
                           scales with the number of their trainable parameters.
                           Previous work has touched on the topic of rendering discrete data structures such as stacks con-
                           tinuous, especially within the context of modelling pushdown automata with neural networks
                           [10, 11, 3, 12]. We were inspired by the continuous pop and push operations of these architec-
                           tures and the idea of an RNN controlling the data structure when developing our own models.
                           The key difference is that our work adapts these operations to work within a recurrent continu-
                           ousStack/Queue/DeQue-likestructure, the dynamics of which are fully decoupled from those of the
                           RNNcontrolling it. In our models, the backwards dynamics are easily analysable in order to obtain
                           the exact partial derivatives for use in error propagation, rather than having to approximate them as
                           done in previous work.
                           In a parallel effort to ours, researchers are exploring the addition of memory to recurrent networks.
                           The NTMandMemoryNetworks[4,13,14]provide powerful random access memory operations,
                           whereas we focus on a more efﬁcient and restricted class of models which we believe are sufﬁcient
                           for natural language transduction tasks. More closely related to our work, [15] have sought to
                           develop a continuous stack controlled by an RNN. Note that this model—unlike the work proposed
                           here—rendersdiscretepushandpopoperationscontinuousby“mixing”informationacrosslevelsof
                           the stack at each time step according to scalar push/pop action values. This means the model ends up
                           compressing information in the stack, thereby limiting its use, as it effectively loses the unbounded
                           memorynatureoftraditional symbolic models.
                           3   Models
                           In this section, we present an extensible memory enhancement to recurrent layers which can be set
                           up to act as a continuous version of a classical Stack, Queue, or DeQue (double-ended queue). We
                           begin by describing the operations and dynamics of a neural Stack, before showing how to modify
                           it to act as a Queue, and extend it to act as a DeQue.
                           3.1  Neural Stack
                           Let a Neural Stack be a differentiable structure onto and from which continuous vectors are pushed
                           and popped. Inspired by the neural pushdown automaton of [3], we render these traditionally dis-
                           crete operations continuous by letting push and pop operations be real values in the interval (0,1).
                           Intuitively, we can interpret these values as the degree of certainty with which some controller wishes
                           to push a vector v onto the stack, or pop the top of the stack.
                                                                          2
