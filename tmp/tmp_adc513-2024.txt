                                                                                                                                                                                                                                                                                                 This CVPRpaperisthe Open Access version, provided by the Computer Vision Foundation.
                                                                                                                                                                                                                                                                                                                                                                                                         Except for this watermark, it is identical to the accepted version;
                                                                                                                                                                                                                                                                                                                                                                   the final published version of the proceedings is available on IEEE Xplore.
                                                                                                                                                                                      SEED-Bench: BenchmarkingMultimodalLargeLanguageModels
                                                                                                                                                                                                                                                        3,1⋆                                                                                                                                                                                      1⋆                                                                                                                                                                      1,2†                                                                                                                                                                                                                                       2                                                                                                                                                             1
                                                                                                                                                                BohaoLi                                                                                                                                                                      Yuying Ge                                                                                                                                                                    Yixiao Ge                                                                                                                                                                           Guangzhi Wang                                                                                                                                                                                                           Rui Wang
                                                                                                                                                                                                                                                                                                                                                                                                            RuimaoZhang3†                                                                                                                                                                                                                                               Ying Shan1,2
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1Tencent AI Lab
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                2ARCLab,TencentPCG
                                                                                                                                                                                                                                         3School of Data Science, The Chinese University of HongKong, Shenzhen
                                                                                                                                                                                                                                                                                                                                         Input: Interleaved Image-Text                                                           Output: Interleaved Image-Text                                                                                                                                                                                                                                                                                                                                                                                                                        Te                                        ing
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Ne                                Cr       xt                             e        d
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Pr        x                              ea       -Ima                           n       an                            e 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 t                              t                                     e        t                            c        y
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        e                                       io                                             s                           n        it
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Te                              dict      Im                            n        ge                          Sc      er                          a       nt                              
                                                                                                                                                                                                                                                                         L4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Ge       xt                              i        a                                                                       d                        nst       e                             e
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                -to                           o        ge                                                                                             I        Id                          nc          e
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ne          -Ima                       n                                                                             Un                                                          a       but
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            r                                                                                                                                                                         st        i
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              a                                                                                                                                                                     n          r
                                                                                                                                                                                                                              el                                                                                                                                        Input: Interleaved Image-Text                                                                        Output: Image(s) & Text                                                                                                                                                                                                                                                                           In                              t                                                                             Ima                                                                                   I          t
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   t                             io         ge                                                      Ge                                                                                                    At                                  e 
                                                                                                                                                                                                                         v                                            NE                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Ima er                                     n                                                                                                                                                                                                    nc
                                                                                                                                                                                                                                                                               xt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ge lea                                                                                              n        ge                                                                                                                      a         ion
                                                                                                                                                                                                                                                                                    -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 An                       ve                                                                                     e                                                                                                                         nst           t
                                                                                                                                                                                                               Le                                                                      GP                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             -Te           d                                                                                 r        &T                                                                                                             I             a
                                                                                                                                                                                                                                                              Em                                T                                                              h                                                                                                                                                                                                                                                                                                                                                                                                            aly             x                                                  Ge         Ima                         at                                                                                                                            Loc
                                                                                                                                                                                                       k                             3                                  u                                                                                   c                                                                                                                                                                                                                                                                                                                                                                                                                     s            t                                                   n                                   io       e
                                                                                                                                                                                                                                   L                                                                                                                    n                                                                                                                                                                                                                                                                                                                                                                                                                           is                                                              e          ge                       n       x                                                                                                                                                   e 
                                                                                                                                                                                                                                                                                 JA                                                             Be                                                                                                                                                                                                                                                                                                                                                                                                   In                                                                                               r                                          t                                                                                                                                               c
                                                                                                                                                                                              as                                                     Dr                                 M                                                D-                                                                                                                                                                                                                                                                                                                                                                                                               -Co                                                                                          at                                                                                                                                                                                  an             g
                                                                                                                                                                                                                                                             e                                                                       E                                                                                                                                                                                                                                                                                                                                                                                                          Ca                n                                                                                      i                                                                                                                                                                             st            in
                                                                                                                                                                                                                                                                a                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     t                                                                                   o                                                                                                                                                                         n              t
                                                                                                                                                                                         T                                                                          mL                                                        SE                                                                                  Input: Interleaved Image-Text                                                                                   Output: Text                                                                                                                                                                                                        pt                ext                          Int                                                    n                                                                                                                                                                     I          un
                                                                                                                                                                                                                                                                             L                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ion                                Te          er                                                                                                                                                                                                                         Co
                                                                                                                                                                               al                                                                                                M                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ing                              xt         leav
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          e
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Com               d 
                                                                                                                                                                        ic                                                      ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   p           Ima                                                        3                                                                                                                                                                        l 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         r                                                                                                                                                                                                                                        a
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        t                                                                                                                                                                       i
                                                                                                                                                                                                                                       EF                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Pr                                                                                    e              ge                                          r                                                                                                                                                                    at
                                                                                                                                                                                                                                              ICS                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          oced                                                                              he                                                Pa                                                                                                                                                                   Sp
                                                                                                                                                                                               2          Qw                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Un                           ur                                                                        n             &                                                                                                                                                                                                                   on
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               e                                                                                                                                                                                                                                                                                                   i
                                                                                                                                                             ch                             L                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       der                                                                                                sio                                                                                                                                                                                                                  lat
                                                                                                                                                                                                                     e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        st                                                                                             n                                                                                                                                                                                                       Re
                                                                                                                                                                                                                         n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         and                                                                                                         2
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             in                                                                                                
                                                                                                                                                                                                                             -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     g                                                                                         t
                                                                                                                                                    ar                                                                         VL                                        GP                                                                                                                                                                                     Input: Image(s) & Text                                                                                   Output: Text                                                                                                                                                                                                                                                                  Par
                                                                                                                                                                                                                                                                                  T
                                                                                                                                                                                                                                                                                     -
                                                                                                                                                                                                                                                 Ot                                    4V
                                                                                                                                                                                                                                                         t
                                                                                                                                                                                                                                                           e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                e 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  anc
                                                                                                                                                                                                                                                               r                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         nst
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Act                                                                                                                                                                                                                                                                                                                  I
                                                                                                                                  Hier                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ion                                                                                                                                                                                                                                                                                                                            ion
                                                                                                                                                                                                                                                                            Fl                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    act
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            er
                                                                                                                                                                                                                                                                                 a                                                                                                                                                                                                                                                                                                                                                                                                                                           Pr                                                                                                                                                                                                                                                                                                                      Int
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   edict
                                                                                                                                                        1                                                                                                                            mi                                                                                                                                                                                                                                                                                                                                                                                                                                                          ion                                                                   
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                & 
                                                                                                                                                      L                                                                                                                                      ng                                                                                                            M                                      ne                                                                                                                                                                                                                                                                                                                                         Video 
                                                                                                                                                                                                                                                                                                     o                                                                                               M                                      to                                                                                                                                                                                                                                                                                                                                                                      ension                                                                          !                                                                              Single
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             eh                                                                                           !                                                                                         -
                                                                                                                                                                                                                                                                                                                                                                                                                                        S                                                                                                                                                                                                                                                                                                                                                 Compr                                                                                                                                                                                                       Ima
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         
                                                                                                                                                                                                          LLaM                                                                                                                                                                               LA                                     h                                                                                                                                                                                                                                                                                                                                       Text                                                                                                                                                                                         Te                                      ge 
                                                                                                                                                                                                                                                                                                                                                                                                                                 c                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              xt                                       & 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                                                                                                                             u                                                                                                                                                                                                                                                                                                                 on                                                                                                                                                                                                                                      Compr                                                              Visual 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Input: Text                                                                              Output: Text                                                                                                                i                                                                                                                                                     !                                                                                                     ehe
                                                                                                                                                                                                                           A                                                                                                                                                                                          To                                                                                                                                                                                                                                                                                                            Act                                                                                                                                                                  "                                                                                                           nsio                          Re
                                                                                                                                                                                                                               -                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                n                         a
                                                                                                                                                                                                                                 Ad                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           s
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ion                                                                                                                                                                                                                                                                                                      onin
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               nit                                                                                                                                                                                                                                                                                                                           g
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        og
                                                                                                                                                                                                                                          a                                                                                                                                                           h                                                                                                                                                                                                                                                                                                                              c
                                                                                                                                                                                                                                             p                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Re                                                                                                                                                                                                                1
                                                                                                                                                  GP                                                                                             t                                                                                                 MME                                         nc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   !                                           
                                                                                                                                                                                                                                                    e                                                                                                                                      e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             #                                   t
                                                                                                                                                          T                                                                                             r                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Par
                                                                                                                                                              3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
                                                                                                                 0                                                .5                                                                                                                                                                                                      MMB                                                                                   C                                                                                                                                                                                                                                                                                                 eo                                                                          & on
                                                                                                               L            T5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Vid                                                                           es          si                                                                                                                                                                                               Te
                                                                                                                                                                                                                                                                                                                                                                                                                                                  QAS                                         A                                                                                                                                                                                                                                                    al                    ng                                                        ag            n                                                                                                                                                                                     Re                  xt
                                                                                                                                                                                       Co                                                                                                                                                                b                                                                                                                                                                                                                                                                                                                                                                                   ob                     di                                                                         e                                                                                                                                                                                              c                  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     hQ                                                                                                                                                                                                                                                                      an                                                                m            h                                                                                                                                                                                                   og
                                                                                                                                                                                               d                                                                                                                                                                                                                                                                                  t                                                                                                                                                                                                                                                    Gl                st                                                                 -I            e                                                                                                                                                                                                            ni
                                                                                                                                                                                                   e                                                                                                                                        eHu                                                                                                                                                                                                                                                                                                                                                                                                       r                                                                    i            r                                                                                                                                                                                                                   t
                                                                                                                                                         LLaM                                          G                                                                                                                                  -                                                                                    C                                        Ma                                                                                                                                                                                                                                                                      de                                                                      lt           p                                                                                                                                                                                                                        ion
                                                                                                                                                                                                            e                                                                                                                   LM                                                                                     AR                                                                                                                                                                                                                                                                                                               Un                                                                                       m
                                                                                                                                                                                                                n                                                                                                        LV                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Mu Co
                                                                                                                                                                                                                                                                                                                                                                                                                                                       Q                                                                                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                         A                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    n                        xt                                                                                                                                                                                                     Ce
                                                                                                                                                                                                                                   CP                                                                                                                                                                                                        CW                                                                                                                                                                                                                                                                                                                               e          io                                                                                                                                                                                                                          Re             le
                                                                                                                                                                                                                                           M                                                                                                                                                 QA                                                                                                            rk                                                                                                                                                                                                                                                            m ns                                     Te                                                                                                                                                                                                       c            br
                                                                                                                     Co                                                                                                                                                                                                                                                                PI                                                                                                          a                                                                                                                                                                                                                                                             Me              e                                                                                                                                                                                                                                            og             it
                                                                                                                                                                                                 Pa                                                                                                                                                                                                                         D                                                                                                                                                                                                                                                                                                                                              eh                                                                                                                                                                                                                                                      ni           y 
                                                                                                                                                                                                        n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 r                                                                                                                                                                                                                                                             t
                                                                                                                                     rr                                                                     Gu                                                       We                                                                                     lQA                                            SQuA                                                                hm                                                                                                                                                                                                                                                                                 mp                                       e                                                                                                                                                                                 Re La                                        ion
                                                                                                                                                es                                                                                                                             LM                                                              thfu                                                                                                              nc                                                                                                                                                                                                                                                                                         Co                                        nc        g                                                                                                                                                                                 c       ndm
                                                                                                                                                             p                                                                       Vic                                                                                                   u                                         QA                                                                  e                                                                                                                                                                                                                                                                                                                                      ere         in                                                                                                                                                   Un                                 og             a
                                                                                                                                                                     o                                                                        una                                                                                    Tr                                                                                                                                                                                                                                                                                                                                                                                                                                                        f           t                                                                                                                                                                                            n            r
                                                                                                                                                                             n                                                                                                                                                                                            MK                                                                     B                                                                                                                                                                                                                                                                                                                                           f         ot                                       s                                                                                                                    d         Ch                          it         k
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Di                                         l         ic                                                                                            Vi                         e                                     ion         
                                                                                                                                                                                     d                                                                                                                                                                                                                                                g                                                                                                                                                                                                                                                                                                                                                           Sp                              a          t                                                                                                s                         r          a
                                                                                                                                                                                             i                                                                          Spa                                                                          A                                                                        n                                                                                                                                                                                                                                                                                                                                                                                                  u         a                                                                                         Ex       u                           st        r
                                                                                                                                                                                                 n                                                                                                                                                                                                                         i                                                                                                                                                                                                                                                                                                                                                                                                   s         m                                                                                                     a                            a         t
                                                                                                                                                                                                         g                                                                                                                                      Q                                                                  d                                                                                                                                                                                                                                                                                                                                                                                                                                                 n        on                                e                      p        l                             n         
                                                                                                                                                                                                                                                                                   r                                                        e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Vi        e                                     i                                                                    
                                                                                                                                                                                                                                                                                      r                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              h                              o        t                                  g                       r        R                              d
                                                                                                                                                                                                                                                                                        o                                            nc                                                                    n                                                                                                                                                                                                                                                                                                                                                                                                                        t                               i       i                          e                                 e                                       i
                                                                                                                                                                                                                   M                                                                        w                                  ie                                                                 o                                                                                                                                                                                                                                                                                                                                                                                                                                                                t        n                          c        d                         s       e                               n
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  o                                    n        e                                  f                                g
                                                                                                                                                                                                                                o                                                                                       Sc                                                                p                                                                                                                                                                                                                                                                                                                                                                                                                                     Ma                                        og                           e        l                         si        er
                                                                                                                                                                                                                                        d                                                                                                                                            s                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Em       c                             i        w                           o        r
                                                                                                                                                                                                                                                e                                                                                                                            e                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Re                              Sc       o                            n        ing
                                                                                                                                                                                                                                                        l                                                                                         Corr                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Kn                                       
                                                                                                  Figure 1. (left) Overview of hierarchical capability levels of MLLMs from L to L , where higher level encompasses lower capability
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0                                           4
                                                                                                  tiers. Models and corresponding evaluation benchmarks at each pyramid tier are illustrated. SEED-Bench-2 covers the assessment of
                                                                                                  MLLMsuptoL . (right)Overviewof27evaluation dimensions in SEED-Bench-2, which consists of three parts, with part-1 constituting
                                                                                                                                                                                                                       3
                                                                                                  L , part-1&2 constituting L , and part-1&2&3 constituting L .
                                                                                                               1                                                                                                                                                                                        2                                                                                                                                                                                                                                            3
                                                                                                                                                                                                                                                                                              Abstract                                                                                                                                                                                                                                                                                                                      covering the limitations of current MLLMs. In this work,
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            we categorize the capabilities of MLLMs into hierarchical
                                                                                                                          Multimodal large language models (MLLMs), building                                                                                                                                                                                                                                                                                                                                                                                                                                                levels from L to L based on the modalities they can ac-
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0                                                4
                                                                                                  upon the foundation of powerful large language models                                                                                                                                                                                                                                                                                                                                                                                                                                                                     cept and generate, and propose SEED-Bench, a compre-
                                                                                                  (LLMs), have recently demonstrated exceptional capabili-                                                                                                                                                                                                                                                                                                                                                                                                                                                                  hensive benchmark that evaluates the hierarchical capa-
                                                                                                  ties in generating not only texts but also images given in-                                                                                                                                                                                                                                                                                                                                                                                                                                                               bilities of MLLMs.                                                                                                                                                       Specifically, SEED-Bench comprises
                                                                                                  terleaved multimodal inputs (acting like a combination of                                                                                                                                                                                                                                                                                                                                                                                                                                                                 24K multiple-choice questions with accurate human anno-
                                                                                                  GPT-4V and DALL-E 3). However, existing MLLM bench-                                                                                                                                                                                                                                                                                                                                                                                                                                                                       tations, which span 27 dimensions, including the evalua-
                                                                                                  marksremainlimitedtoassessingonlymodels’comprehen-                                                                                                                                                                                                                                                                                                                                                                                                                                                                        tion of both text and image generation. Multiple-choice
                                                                                                  sion ability of single image-text inputs, failing to keep up                                                                                                                                                                                                                                                                                                                                                                                                                                                              questions with ground truth options derived from human
                                                                                                  with the strides made in MLLMs. A comprehensive bench-                                                                                                                                                                                                                                                                                                                                                                                                                                                                    annotation enable an objective and efficient assessment of
                                                                                                  mark is imperative for investigating the progress and un-                                                                                                                                                                                                                                                                                                                                                                                                                                                                 model performance, eliminating the need for human or
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            GPT intervention during evaluation. We further evaluate
                                                                                                                        *Equal Contribution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                the performance of 22 prominent open-source MLLMs and
                                                                                                                        †Correspondence Author.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 13299
                   Table 1. Comparisons between existing MLLM benchmarks. “H/G Evaluation” denotes whether human or GPT is used for evaluation.
                      Benchmark             Visual Modality       Evaluation Level   Customized Question    #AnswerAnnotation      AnswerType      H/GEvaluation     #Models
                   LLaVA-Bench[24]              Image                   L                     ✓                     150              free-form          GPT             4
                                                                          1
                    OCR-Bench[26]               Image                   L                     ✗                       -              free-form          N/A             6
                                                                          1
                       MME[11]                  Image                   L                     ✓                     2194               Y/N              N/A             10
                                                                          1
                     M3Exam[46]                 Image                   L                     ✓                    12317             A/B/C/D            N/A             7
                                                                          1
                      LAMM[42]          Image(s) & Point cloud          L                     ✗                       -              free-form          GPT             4
                                                                          1
                   LVLM-eHub[40]                Image                   L                     ✗                       -              free-form         Human            8
                                                                          1
                     MMBench[25]               Image(s)                 L                     ✓                     2974             free-form          GPT             14
                                                                          1
                    VisIT-Bench [5]             Images                  L                     ✓                     592              free-form      Human/GPT           14
                                                                          1
                     MM-VET[43]                 Image                   L                     ✓                     200              free-form          GPT             9
                                                                          1
                     Touchstone [3]            Image(s)                 L                     ✓                     908              free-form          GPT             7
                                                                          1
                   SciGraphQA[20]               Image                   L                     ✓                      3K              free-form          N/A             4
                                                                          1
                          Ours             Image(s) & Video             L                     ✓                    24371             A/B/C/D            N/A             22
                                                                          3
                 summarize valuable observations. By revealing the limi-                           ligence (AGI) since a human-level AI should be able to ef-
                 tations of existing MLLMs through extensive evaluations,                          fortlessly digest and create multimodal content. In the ca-
                 we aim for SEED-Bench to provide insights that will mo-                           pability pyramid, higher levels inherently include the capa-
                 tivate future research toward the goal of General Artificial                      bilities of lower tiers. This hierarchical categorization not
                 Intelligence. Dataset and evaluation code are available at                        only clearly illustrates the current progress of MLLMs, but
                 https://github.com/AILab-CVC/SEED-Bench.                                          also provides a well-defined roadmap for future research.
                                                                                                       Wepropose SEED-Bench, a comprehensive benchmark
                 1. Introduction                                                                   that evaluates the hierarchical capabilities of MLLMs up
                                                                                                   to L3, including the generation of both texts and images
                 In recent years, Large Language Models (LLMs) [7, 10,                             given interleaved image-text inputs. As shown in Fig. 1,
                 30, 31, 37] have exhibited remarkable capabilities to under-                      SEED-Bench consists of three parts, where part-1 consti-
                                                                                                   tutes capability level L         for images and texts comprehen-
                 stand, reason, and generate texts across a variety of open-                                                      1
                 ended tasks. Leveraging the strong generality of LLMs,                            sion, part-1&2 constitute capability level L2 for interleaved
                 Multimodal Large Language Models (MLLMs) [2, 8, 15–                               image-text comprehension, and part-1&2&3 constitute ca-
                 19, 23, 24, 27, 28, 32, 32, 34, 41, 45, 47] have demon-                           pability level L3 for image and text generation. To the best
                 strated exceptional capabilities in comprehending multi-                          of our knowledge, SEED-Bench is the first benchmark that
                 modal data through predicting open-form texts.                    Recent          provides hierarchical evaluations of MLLMs, which effec-
                 work [9, 13, 14, 21, 36, 39] further empower LLMs with                            tively showcases the range of model capabilities.
                 the ability to generate images beyond texts (acting like                              Specifically, SEED-Bench consists of 24K multiple-
                 a combination of GPT-4V [1] and DALL-E 3 [4]), since                              choice questions with ground truth answers derived from
                 they contend that the premise for the emergence of multi-                         humanannotation(×10largerthanMME[11]and×8larger
                 modalcapabilities is that text and image can be represented                       than MMBench [25] as shown in Tab. 1). SEED-Bench
                 and processed interchangeably in a unified autoregressive                         spans 27 evaluation dimensions, enabling a comprehen-
                 Transformer. However, despite the extensive capabilities of                       sive assessment of MLLMs’ performance across diverse
                 MLLMs, existing MLLM benchmarks [3, 11, 25, 40, 42]                               aspects. We employ three approaches for the generation
                 primarily focus on evaluating single image-text comprehen-                        of multiple-choice questions, including (1) a sophisticated
                 sion, thus failing to fully demonstrate the progress and lim-                     pipeline utilizing foundation models, (2) the adaptation of
                 itations of current MLLMs. The lag of benchmarks behind                           existing datasets, and (3) a combination of human cre-
                 the rapid development of MLLMs hinders the exploration                            ation and GPT assistance. We further incorporate an au-
                 and evolution of models.                                                          tomated filtering mechanism and manual verification pro-
                    In this work, we categorize the capabilities of MLLMs                          cess to ensure the quality of questions and the accuracy
                 into hierarchical levels ranging from L                to L based on              of ground truth answers. Different from existing MLLM
                                                                     0        4
                 the modalities they can accept and generate, as depicted                          benchmarks [3, 5, 24, 25, 40, 42, 43] that employ hu-
                 in Fig. 1. Building upon LLMs, the lowest-tier capability                         man annotators or GPT to evaluate open-form output, re-
                 L involves generating texts given text inputs, while the                          sulting in compromised efficiency, increased subjectivity,
                   0
                 highest-tier capability L4 entails producing open-form in-                        and reduced assessment accuracy, SEED-Bench provides
                 terleaved image and text output given arbitrary interleaved                       multiple-choice questions, which restricts the model’s out-
                 image-text inputs.        Reaching the capability L             is a cru-         puttoA/B/C/Doptions. Thisapproachfacilitatestheconve-
                                                                              4
                 cial milestone on the path towards General Artificial Intel-                      nient computation of accuracy, serving as an objective met-
                                                                                               13300
              ric for evaluation.                                                   of human annotators during evaluation not only introduces
                 Based on SEED-Bench, we comprehensively evaluate                   bias but also incurs significant costs. LLaVA-Bench [24],
              22 prominent open-source MLLMs. Our evaluation results                LAMM[42]andTouchstone[3]utilizeGPTtoevaluatethe
              yield the following three key findings: (1) Existing MLLMs            answers’ relevance and accuracy to the ground truth. The
              havenotyetreachedtheceilinglevelofcapabilityL1 forthe                 reliance on entity extraction and GPT metrics can impact
              comprehension of fixed-form images and texts, with even               the accuracy and reliability of the evaluation. MME [11]
              the top-ranked model achieving only a 60% accuracy rate.              and MMBench [25] aim to enhance the objective evalu-
              MLLMs, in particular, exhibit poor performance in certain             ation of MLLMs by constructing 2194 True/False Ques-
              dimensions, such as understanding charts and visual math-             tions and 2974 Multiple Choice Questions across a variety
              ematics. (2) MLLMs achieve less satisfactory performance              of ability dimensions respectively. Considering the limited
              at capability L2 than that at L1, which indicates that it is          scale of these benchmarks, their evaluation results may ex-
              morechallenging for MLLMs to comprehend free-form in-                 hibit instability. In this work, we introduce SEED-Bench
              terleaved image-text inputs since most MLLMs are trained              to evaluate the hierarchical capabilities of MLLMs includ-
              on structured image-caption pairs. (3) At present, only a             ing the generation of both texts and images, which contains
              few MLLMscanattain capability L3, which requires mod-                 24K human-annotated multiple-choice questions covering
              els to output content in multiple modalities. A universal             27evaluation dimensions.
              MLLM that unifies the generation of images and texts is               3. SEED-Bench
              currently underexplored. We will launch an evaluation plat-
              form and consistently maintain a leaderboard for assessing            3.1. Hierarchical Capability Levels
              and comparing model performance.
                                                                                    Wecategorize the capabilities of MLLMs into hierarchical
              2. Related Work                                                       levels from L to L4, based on input and output modali-
                                                                                                    0
              Multimodal Large Language Models. With the impres-                    ties, where the higher level encompasses the lower capabil-
              sive success of Large language models (LLM) [7, 10, 37],              ity tiers, as illustrated in Fig. 1. SEED-Bench covers the
                                                                                    assessment of MLLMs up to L . The detailed categoriza-
              recent studies work on generative Multimodal Large Lan-                                                  3
              guage Models (MLLMs) [2, 8, 15–18, 23, 24, 32, 34,                    tion of capability level is illustrated below,
              41, 45, 47] to improve multimodal comprehension through               Level L : Building upon LLMs, the most fundamental ca-
                                                                                             0
              aligning visual features of pre-trained image encoder with            pability of MLLMs generating text based on provided text
              LLMs on image-text datasets.        Some work [19, 27, 28]            inputs, whichdoesnotnecessitatespecificevaluationwithin
              further considers video inputs and leverages the vast ca-             the MLLMbenchmark.
              pabilities of LLMs for video understanding tasks. Recent              Level L : MLLMs at this capability level should possess
              work[9,13,14,21,36,39]takesignificantstridesinequip-                           1
              ping MLLMs with the capacity for generating images be-                the ability to comprehend multimodal inputs in a fixed for-
              yond texts. In SEED-Bench, we provide a comprehensive                 mat, i.e., image or multiple images (video input can be re-
              and objective evaluation of these models to thoroughly as-            garded as multiple images) and then texts. Current MLLM
              sess their hierarchical capabilities.                                 benchmarksonlyevaluate this capability level with a single
              Benchmarks for Multimodal Large Language Models.                      image and text as inputs.
              WiththerapiddevelopmentofMultimodalLargeLanguage                      Level L2: MLLMsatthis capability level should be able to
              Models (MLLMs), some concurrent works [3, 11, 25, 40,                 understand multimodal inputs with open-form interleaved
              42] propose various benchmarks for evaluating MLLMs.                  image-text data, which aligns with the multimodal inputs
              However, they remain limited to assessing only the model’s            encountered in real-life scenarios.
              ability to predict texts given single image-text inputs, failing      Level L : Besides the inherent ability of LLMs to generate
              to keep up with the strides made in multimodal model capa-                     3
              bilities. For example, GVT [38] constructs a benchmark by             texts, MLLMs at this capability level should also be pro-
              aggregating two semantic-level understanding tasks (VQA               ficient in producing images, as advanced MLLMs are ex-
              and Image Captioning) and two fine-grained tasks (Ob-                 pected to process and represent multimodal content on both
              ject Counting and Multi-class Identification). However, its           input and output sides.
              evaluation is constrained to limited aspects of visual un-            Level L4: MLLMs at the highest capability level should
              derstanding. LVLM-eHub [40] combines multiple existing                possess the ability to process and generate interleaved
              computer vision benchmarks and develops an online plat-               image-text content in an open-form format, which is an es-
              form, where two models are prompted to answer a question              sential step towards achieving general artificial intelligence.
              related to an image and human annotators are employed                 We will incorporate evaluations of this capability level in
              to compare the predictions of models. The involvement                 our future work.
                                                                                13301
               Difference Spotting
                                              What are the differences between the two image?
                                              A. In the second image, there are two people standing on the sidewalk instead of three and a car is just entering the parking lot.
                                              B. In the second image, there are four people standing on the sidewalk instead of three and a car is just leaving the parking lot.
                                              C. In the second image, there are three people standing on the sidewalk instead of two and a car is just entering the parking lot.
                                              D. In the second image, there are two people standing on the sidewalk instead of three and a car is just leaving the parking lot.
               Meme Comprehension
                                                                          What is funny about this comic strip?
                                                                          A. The polar bear entered the bus pavilion with a Dalmatian, but the bus pavilion was a dog without Dalmatian.
                                                                          B. The Dalmatian and bear are in the rain.
                                                                          C. This is a fake Dalmatian.
                                                                          D. The rain cleaned the Dalmatian's spots.
               Global Video Understanding
                                                                                                 Whatis the main activity the woman is performing in the kitchen?
                                                                                                 A. Filling a kettle with water, and then pouring the water into a pot on the stove.
                                                                                                 B. Pouring water from a kettle into a pot, and then adding ingredients to the pot.
                                                                                                 C. Turning on the stove, and then pouring water from the kettle into a pot on the stove.
                                                                                                 D. Pouring water from a kettle into a pot on the stove.
                                                                                                 time
               Action Recognition
                                                                                                     Whatistheactionbeingcarriedoutinthevideo?
                                                                                                     A. Throwing something in the air and letting it fall
                                                                                                     B. Throwing something in the air and catching it
                                                                                                     C. Lifting up one end of something, then letting it drop down
                                                                                                     D. Poking something so that it falls over
                                                                                                 time
               Action Prediction
                                                                                                     What action do you anticipate following the end of this video?
                                                                                                     A. Stir potatoes
                                                                                                     B. Wash potatoes
                                                                                                     C. Add potatoes
                                                                                                     D. Slice potatoes
                                                                                                 time
               Procedure Understanding
                                                                                                     Can you recognize the actions that occur in this video and list them in order?
                                                                                                     A. Cook breakfast, switch stove on, close fridge, carry milk, peel banana
                                                                                                     B. Scoop ice cream, squeeze chocolate syrup, pour sprinkles, close fridge
                                                                                                     C. Close fridge, carry milk, screw open milk cap, pour milk, screw close milk cap
                                                                                                     D. Reach for cereal box, grab bowl, pour milk, stir cereal, close fridge
                                                                                                 time
              Figure 2. Data samples from a subset of evaluation dimensions in part-1 with multiple images or videos as inputs, which encompasses
              capability L1 in SEED-Bench.
              3.2. Evaluation Dimensions                                          inputs: (1) Single-Image & Text, (2) Multiple-Images &
              As shown in Fig. 1, SEED-Bench comprises a total of 27              Text, (3) Video & Text.
              evaluation dimensions, which constitute three capabilities          • Single-Image & Text Comprehension. This sub-part con-
              levels, from L1 to L3. Since the higher level encompasses             sists of diverse evaluation dimensions including Scene
              the lower capability tiers, we further divide the evaluation          Understanding, Instance Identity, Instance Attribute, In-
              dimensions of L3 into three non-overlapping parts: part-              stance Location, Instance Counting, Spatial Relation, In-
              1 forms level L , part-2 combined with part-1 constitutes
                               1                                                    stance Interaction, Visual Reasoning, Text Recognition,
              level L , part-3, part-2 and part-1 form level L together.
                     2                                           3                  Celebrity Recognition, Landmark Recognition, Chart
              Weintroduce the dimensions of each part in detail below.              Understanding, Visual Referring Expression, Science
                                                                                    Knowledge, Emotion Recognition and Visual Mathemat-
              3.2.1   Part-1                                                        ics.  These dimensions assess MLLMs’ comprehension
                                                                                    of image-text pairs from extensive aspects, encompassing
              Thedimensionsofpart-1evaluateMLLMs’comprehension                      global/object-level understanding, recognition/reasoning,
              of multimodal inputs in a fixed format, and can be further            and various specialized domains.
              grouped into three sub-parts based on the types of visual           • Multiple-Images & Text Comprehension. This sub-part
                                                                               13302
                                    In-Context Captioning                                                              Interleaved Image-Text Analysis                                                    Text-to-Image Generation
                                                                                                                                                                                                                   Generateanimageofthiscaption:Abrownpurseissittingonagreenbench.
                                                                                                                                As shown in the picture, this is an image of a girl eating a
                                                                                                                                burgeratMcDonald's.
                                                                                                                                                                                                                               A                                        B                                        C                                        D
                                                                                                                                                                                                          Next Image Prediction
                                                     There are three suitcases in the pictures
                                                                                                                                As shown in the picture, this is the menu of McDonald's in 
                                                                                                                                St. Petersburg, Russia.                                                            Please generate the following image.
                                                         There are one chair in the image.
                                                                                                                                                                                                                               A                                        B                                        C                                        D
                                                                                                                                                                                                          Text-Image Creation
                                                                                                                                                                                                                   WhatdoestheSydneyOperaHouselooklike?Tellmetheanswerandshowmeapicture.
                                                                                                                                If I want to buy two of the burgers this girl is eating at
                                      A. Most of the basketball players in the image are wearing blue shorts.                   McDonald'sin St. Petersburg, how much would it cost me?
                                      B. The relative height between the basketball hoop and the players in the                 A. 130 rubles
                                      imagecannotbedetermined                                                                   B. 260 rubles
                                      C. The stripe on the basketball is blue.                                                  C. 75 rubles                                                                 The Sydney Opera House is a multi-       The Sydney Opera House is a tall         The Sydney Opera House is a large         TheSydneyOperaHouseisabridge
                                                                                                                                                                                                             venue performing arts center in          skyscraper with a rectangular shape.     circular stadium with an open roof.       with a large steel arch.
                                      D. There are four basketball players playing in the image.                                D. 520 rubles                                                                Sydney,Australia.
                                                                                                                                                                                                                               A                                        B                                        C                                        D
                                  Figure 3. (left) Data samples of evaluation dimensions in part-2 with interleaved image-text as inputs, which encompasses capability
                                  L together with dimensions in L . (right) Data samples of evaluation dimensions in part-3 with images and texts as outputs, which
                                       2                                                                                1
                                  encompasses capability L together with dimensions in L .
                                                                                                 3                                                                             2
                                        consists of Difference Spotting and Meme Comprehen-                                                                                                                            image based on a caption prompt, and Next Image Gen-
                                        sion, which evaluates MLLMs’ capability of extracting                                                                                                                          eration, where the model is required to generate a subse-
                                        information and discerning differences from multiple im-                                                                                                                       quent image based on previous images.
                                        ages.                                                                                                                                                                    • Text-Image creation. Given a question, the model is re-
                                  • Video & Text Comprehension. This sub-part consists of                                                                                                                              quired to provide a text-based answer and subsequently
                                        GlobalVideoUnderstanding,ActionRecognition,Action                                                                                                                              generate a corresponding image as an illustration.
                                        Prediction, and Procedure Understanding, which assesses                                                                                                                  3.3. Construction of Multiple-choice Questions
                                        MLLMs’abilityfor fine-grained action recognition, tem-
                                        poralrelationshipunderstanding, andtemporalreasoning.                                                                                                                    We employ three approaches to construct multiple-choice
                                                                                                                                                                                                                 questions covering 27 evaluation dimensions: (1) an au-
                                  3.2.2                Part-2                                                                                                                                                    tomatic pipeline to generate questions for specific evalua-
                                  The dimensions of part-2 evaluate MLLMs’ comprehen-                                                                                                                            tion dimensions, (2) tailoring existing datasets for the for-
                                  sionofarbitraryinterleavedimage-textinputs, includingIn-                                                                                                                       mat of multiple-choice questions, and (3) human creation
                                  Context Captioning, where two examples of image-caption                                                                                                                        combined with GPT.
                                  pairs and an image are given, and the model is expected to
                                  describe the specific aspect of the image, and Interleaved                                                                                                                     Automatic pipeline.                                              As shown in Fig. 4, our pipeline
                                  Image-Text Analysis, where the model answers questions                                                                                                                         for generating multiple-choice questions involves ques-
                                  based on images and texts with varying quantities and posi-                                                                                                                    tion/answer generation and verification.                                                                               For generating
                                  tions.                                                                                                                                                                         question/answer pairs, we first leverage various founda-
                                                                                                                                                                                                                 tion models to extract visual information including image-
                                  3.2.3                Part-3                                                                                                                                                    level captions, instance-level descriptions, and textual ele-
                                                                                                                                                                                                                 ments. Based on specially designed prompts corresponding
                                  The dimensions of part-3 evaluate MLLMs’ capability of                                                                                                                         to specific evaluation dimensions, ChatGPT/GPT-4 subse-
                                  generating images in addition to texts, and can be divided                                                                                                                     quentlygeneratesquestionsandfourcandidateoptionswith
                                  into two sub-parts including (1) Image generation and (2)                                                                                                                      one ground truth answer. For verifying question/answer
                                  Image &Textgeneration.                                                                                                                                                         pairs, we filter out questions that can be answered correctly
                                  • Imagegeneration. Thissub-partcomprisesText-to-Image                                                                                                                          by multiple LLMs without resorting to visual information,
                                        Generation, where the model is expected to generate an                                                                                                                   since such questions are not helpful to evaluate the visual
                                                                                                                                                                                                        13303
             (a) Ques�on/Answer Genera�on
                                                                                          A person holding a board standing on a street
                                                               Image Cap�oning
                                                                                          A person is holding a white board and another person...
                                                               (BLIP2 & Tag2text) 
                                                               Dense Cap�oning
                                                                                          A person holding a white board [0.4, 0.05, 0.65, 1.0]
                                                                   (GRiT)
                                                                                          A white board with texts on it [0.2, 0.4, 0.7, 0.95]
                                                                                          Person [0.1, 0.5, 0.15, 0.5]
                                                               Object Detec�on
                                                                                          Person [0.1, 0.1, 0.15, 0.5]...
                                                                   (SAM)
                                                                                                                                         
                                                                                          Person [0.1, 0.1, 0.15, 0.5] old, standing
                                                               A�ribute Detec�on
                                                                                                                                                     
                                                                                          Street [0.0, 0.1, 0.15, 1.0] grey, empty ...
                                                                   (VinVL)
                                                                Text Detec�on
                                                                                          "Tax the rich" [0.25, 0.5, 0.62, 0.5]
                      Image From CC3M
                                                                 (PaddleOCR)
                                                                                          "20 Brackets-$20 Million" [0.18, 0.85, 0.75, 0.84] ...
               Prompts for Ques�on Genera�on
                                                                                   Visual Informa�on
                                                                                                                                                            
                  Based on the above informa�on, create several 
                  mul�ple-choice ques�ons.  Each ques�on should 
                                                                                  What is the main topic of the sign held by the man in the image?
                  have four choices with one correct answer ...
                                                                                                           
                                                                                  A. Environmentalism  B. An�-government
                                                                                                               
                                                                                  C. Taxa�on        D. Educa�on      Answer: C
               Prompts for each 
                                  Create ques�ons that is related 
                                                                ChatGPT/GPT-4
               evalua�on dimension
                                  to the texts in the image ...
             (b) Ques�on/Answer Veriﬁca�on
                What is the main topic of the sign held by the man in the image?
                                          
                A. Environmentalism  B. An�-government
                                              
                C. Taxa�on        D. Educa�on      Answer: C
                                                                           Automa�c Filtering
                                                                                          
                        Ques�ons and answers generated in Step (a)
                                                                                                  Human Annota�on 
                                                                                                                          SEED-Bench
             Figure4. OverviewofautomaticpipelineinSEED-Benchforgeneratingmultiple-choicequestions. (a)Wefirstleveragevariousfoundation
             models to extract visual information including image-level captions, instance-level descriptions, and textual elements. Based on specially
             designed prompts corresponding to specific evaluation dimensions, ChatGPT/GPT-4 subsequently generates questions and four candidate
             options with one ground truth answer. (b) We further filter out questions by utilizing LLMs and employ human annotators to select the
             correct option and classify each question into one evaluation dimension.
             comprehension capability of MLLMs. We further employ            3.4. Evaluation Strategy
             human annotators to select the correct option and classify      Evaluation    of  text  output.   Different   from MM-
             each question into one evaluation dimension.                    Bench [25] that employs ChatGPT to match a model’s
                                                                             prediction to one of the choices in a multiple-choice ques-
                                                                             tion (achieves only 87.0% alignment rate), we adopt the
             Tailoring existing datasets.   For existing datasets with       answer ranking strategy [6, 8, 22] for evaluating existing
             annotated labels, we first prompt ChatGPT/GPT-4 to gen-         MLLMs with multiple-choice questions. Specifically, for
             erate questions based on the provided information. We then      each choice of a question, we compute the likelihood that
             construct distracting choices either from the annotated la-     an MLLM generates the content of this choice given the
             bels of other samples or by utilizing ChatGPT to generate       question. We select the choice with the highest likelihood
             three distractors. For distractors generated by ChatGPT, we     as the model’s prediction.   Our evaluation strategy does
             additionally utilize human annotators to filter out options     not rely on the instruction-following capabilities of models
             that are too similar to the ground truth answer.                to output “A” or “B” or “C” or “D”. Furthermore, this
                                                                             evaluation strategy eliminates the impact of the order of
                                                                             multiple-choice options on the model’s performance.
             Human creation combined with GPT. For evaluation                Evaluation of image output.     Since not all MLLMs with
             dimensions lacking suitable data, e.g. Interleaved Image-       imagegenerationcapabilitiesemployvisualautoregression,
             Text Analysis and Text-Image Creation, we employ human          adopting an answer ranking strategy for image evaluation
             annotators to meticulously design questions, retrieve corre-    is impractical. Instead, we calculate the CLIP similarity
             sponding images, and construct distracting choices with the     score [33] between the generated image and each candidate
             assistance of ChatGPT.                                          image option, selecting the highest-scoring option as the fi-
                                                                         13304
                                                                                                         ¯
              Table 2. Evaluation results of various MLLMs in different capability levels of SEED-Bench. T denotes the averaged accuracy across
              corresponding dimensions, and R¯ denotes the rank based on the the averaged accuracy. The evaluation dimensions of part-2, together
                                             T
              with L , encompass L , while the evaluation dimensions of part-3, together with L , encompass L .
                    1              2                                                       2              3
                                                                       L (Part-1)       Part-2          L            Part-3          L
                            Model                 Language Model         1                                2                            3
                                                                         ¯             ¯              ¯             ¯              ¯
                                                                         T     R¯      T     R¯      T      R¯      T     R¯      T     R¯
                                                                                 T             T              T             T             T
                         BLIP-2 [18]                 Flan-T5-XL        41.0     8     35.3    9     40.5     7      -      -       -      -
                       InstructBLIP [8]              Flan-T5-XL        42.2     6     35.7    5     41.7     6      -      -       -      -
                   InstructBLIP Vicuna [8]           Vicuna-7B         41.4     7     29.7    18    40.5     8      -      -       -      -
                         LLaVA[24]                   LLaMA-7B          38.7    11     30.2    17    38.0    12      -      -       -      -
                       MiniGPT-4[47]                 Vicuna-7B         39.4     9     34.1    12    39.0     9      -      -       -      -
                       VPGTrans[44]                  LLaMA-7B          36.2    19     23.9    20    35.2    18      -      -       -      -
                    MultiModal-GPT[15]               Vicuna-7B         37.4    14     34.9    11    37.1    13      -      -       -      -
                          Otter [17]                 LLaMA-7B          36.4    17     36.6    4     36.4    16      -      -       -      -
                     OpenFlamingo [29]               LLaMA-7B          37.3    15     35.5    8     37.1    14      -      -       -      -
                  LLaMA-AdapterV2[12]                LLaMA-7B          37.5    13      -       -      -      -      -      -       -      -
                          GVT[38]                    Vicuna-7B         34.4    21     38.6    3     34.8    19      -      -       -      -
                      mPLUG-Owl[41]                  LLaMA-7B          39.4    10     28.9    19    38.5    10      -      -       -      -
                        Kosmos-2[32]             Decoder only 1.3B     46.3     3     23.3    21    44.4     3      -      -       -      -
                      Qwen-VL-Chat[2]                 Qwen-7B          43.1     4     35.5    7     42.5     4      -      -       -      -
                       LLaVA-1.5[23]                 Vicuna-7B         47.3     2     30.8    16    46.0     2      -      -       -      -
                  IDEFICS-9B-Instruct [16]           LLaMA-7B          38.0    12     40.3    2     38.2    11      -      -       -      -
                InternLM-Xcomposer-VL [45]          InternLM-7B        59.2     1     32.1    14    56.9     1      -      -       -      -
                       VideoChat [19]                Vicuna-7B         37.0    16     35.3    9     36.8    15      -      -       -      -
                     Video-ChatGPT [28]              LLaMA-7B          36.4    18     31.0    15    35.9    17      -      -       -      -
                          Valley [27]               LLaMA-13B          34.5    20     32.2    13    34.3    20      -      -       -      -
                          Emu[35]                   LLaMA-13B          42.5     5     41.1    1     42.4     5    41.4     1     42.3     1
                       NExt-GPT[39]                  Vicuna-7B         30.7    22     35.6    6     31.1    21    33.9     2     31.4     2
              nal prediction of the given multiple-choice question.              plementations. For each model, we first determine its ca-
                                                                                 pability level and then evaluate the corresponding dimen-
              Evaluationoftextandimageoutput.           Forquestionswith         sions. Note that we have confirmed with the authors that the
              text and image answers, we first employ an answer rank-            LLaMA-AdapterV2’scapabilitylevelisL1. SomeMLLMs
              ing strategy to select the most likely text prediction. If it      can reach the capability level L3, but they are not available
              matches the ground truth, we evaluate the image output us-         as open-source.
              ing the CLIP similarity score [33] between the generated           4.2. Main Results
              image and each candidate. The model is deemed correct
              only if both text and image predictions match the ground           Theevaluation results of various MLLMs in different capa-
              truth.                                                             bility levels of SEED-Bench are listed in Tab. 2. The de-
              4. Evaluation Results                                              tailed leaderboard of each evaluation dimension is provided
                                                                                 in the supplemental materials. InternLM-Xcomposer-VL
              4.1. Models                                                        outperforms a large number of MLLMs, achieving the best
                                                                                 performance based on the averaged accuracy in capability
              We evaluate a total of 22 open-source MLLMs includ-                level L and L , and Emu ranks top-1 in capability level L
                                                                                        1       2                                            3
              ing BLIP-2 [18], InstructBLIP [8], InstructBLIP Vi-                with only one competitor. Because InternLM-Xcomposer-
              cuna [8], LLaVA [24], MiniGPT-4 [47], VPGTrans [44],               VL retrieves images from the available image pool rather
              MultiModal-GPT [15], Otter [17], OpenFlamingo [29],                than generate images, it does not reach the capability level
              LLaMA-Adapter V2 [12], GVT [38], mPLUG-Owl [41],                   L . To better showcase the capabilities of models across
                                                                                   3
              Kosmos-2 [32], Qwen-VL-Chat [2], LLaVA1.5 [23],                    different evaluation dimensions, we further visualize the
              IDEFICS-9B-Instruct [16], InternLM-Xcomposer-VL [45],              ranking of each model within each evaluation dimension
              VideoChat [19],     Video-ChatGPT [28],        Valley   [27],      in Fig. 5, where darker colors represent higher ranks and
              Emu [35], and NExt-GPT [39] based on their official im-            grey color indicates that the model has not yet reached the
                                                                             13305
                                                                                                                                                                            High Rank
                                     BLIP-2
                                InstructBLIP
                         InstructBLIP Vicuna
                                     LLaVA
                                 MiniGPT-4
                                  VPGTrans
                            MultiModal-GPT
                                      Otter
                             OpenFlamingo
                          LLaMA-Adapter V2
                                       GVT
                   Model        mPLUG-Owl
                                  Kosmos-2
                              Qwen-VL-Chat
                                  LLaVA-1.5
                         IDEFICS-9B-Instruct
                     InternLM-Xcomposer-VL
                                 VideoChat
                             Video-ChatGPT
                                     Valley
                                       Emu
                                  NExt-GPT
                                                                                                                                                                             Low Rank
                                                   InstanceIdentityInstancettributesInstanceLocationInstanceCountingSpatialRelationsInstanceVisualTextVisualReferringSciencenowledgeVisualSpottingActionActionAnalysisNextImageCreation
                                                         A                      Reasoning         Chart         Emotion            Video     Prediction              Prediction
                                              Scene                         InteractionRecognitionCelebrityRecognitionLandmarkRecognitionExperssionKRecognitionMathematicsDifferenceMemeGlobalRecognitionProcedureIn-ContextCaptioningInterleavedImage-TextGenerationText-Image
                                                Understanding                                      Understanding               ComprehensionUnderstandingUnderstandingText-to-Image
                                                                                              Evaluation Dimension
                 Figure 5. Illustration of each model’s performance across different evaluation dimensions, where darker colors represent higher ranks.
                 Grayindicates that the model has not yet reached the capability level required for evaluating that dimension.
                 capability level required for evaluating that dimension. The                          and generation simultaneously.                    Although NExt-GPT
                 championMLLMInternLM-Xcomposer-VLachievescom-                                         reaches the capability level L , which can generate both
                                                                                                                                                3
                 petitive results in a large number of evaluation dimensions                           texts and images, it shows poor performance in capabil-
                 of capability level L and L . Although NExt-GPT reaches                               ity L for multimodal comprehension. Equipping MLLMs
                                            1         2                                                       1
                 the capability level L3, it performs poorly in multiple eval-                         with image generation ability without compromising their
                 uation dimensions at levels L1 and L2.                                                inherent text output performance remains to be addressed.
                 4.3. Observations                                                                     5. Conclusion
                 Throughthecomprehensionandobjectiveevaluationofvar-                                   In this work, we introduce SEED-Bench, a large-scale
                 ious MLLMsindifferent capability levels of SEED-Bench,                                benchmark for evaluating Multimodal Large Language
                 wehaveuncoveredinsights that can inform future work.                                  Models (MLLMs) in terms of hierarchical capabilities, in-
                 Existing MLLMs have yet to reach the ceiling level of                                 cluding the generation of both texts and images. SEED-
                 capability L . Even the top-ranked MLLM achieves only
                                  1                                                                    Benchconsistsof24Kmultiple-choicequestionswithaccu-
                 a 60% averaged accuracy in capability L , which evaluates
                                                                       1                               rate human annotations, which cover 27 evaluation dimen-
                 the comprehension of multimodal inputs in a fixed format,                             sions. We conduct a thorough evaluation of 22 prominent
                 i.e., images or multiple images (videos) and then texts.                              open-source MLLMs, analyzing and comparing their per-
                 The comprehension of Interleaved Image-Text data is                                   formances to provide insights for future research. We plan
                 moredifficult. The majority of MLLMs achieve worse re-                                to launch and maintain a leaderboard, offering a platform
                 sults on part 2, which consists of multiple-choice questions                          for the community to assess model performance.
                 with interleaved image-text inputs, than on L1 with fixed-                                Ackoweledge           The      work      is    partially     supported
                 form image and text as inputs.                                                        by the Young Scientists Fund of the National Nat-
                 OnlyasmallnumberofMLLMscanreachthecapabil-                                            ural    Science Foundation of China under grant No.
                 ity L . Only two open-source MLLMs possess the ability                                62106154, by the Natural Science Foundation of Guang-
                        3
                 to generate images, besides the inherent ability of LLMs to                           dong Province, China (General Program) under grant
                 output texts. A universal MLLM that unifies the generation                            No.2022A1515011524, and by Shenzhen Science and
                 of images and texts is currently underexplored.                                       Technology         Program        JCYJ20220818103001002                   and
                 It is challenging to address multimodal comprehension                                 ZDSYS20211021111415025
                                                                                                  13306
               References                                                                         Shan. Planting a seed of vision in large language model.
                 [1] Gpt-4v(ision) system card. 2023. 2                                           arXiv preprint arXiv:2307.08041, 2023. 2, 3
                                                                                            [14] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li,
                 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan                      Xintao Wang, and Ying Shan. Making llama see and draw
                     Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren                         withseedtokenizer. arXiv preprint arXiv:2310.01218, 2023.
                     Zhou. Qwen-vl: Afrontierlargevision-languagemodelwith                        2, 3
                     versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2,         [15] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
                     3, 7                                                                         Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping
                 [3] Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan                     Luo, and Kai Chen. Multimodal-gpt: A vision and language
                     Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, and Jin-                      modelfor dialogue with humans, 2023. 2, 3, 7
                     gren Zhou. Touchstone: Evaluating vision-language mod-                                                              ´
                                                                                            [16] Hugo Laurenc¸on, Lucile Saulnier, Leo Tronchon, Stas Bek-
                     els by language models. arXiv preprint arXiv:2308.16890,                     man, Amanpreet Singh, Anton Lozhkov, Thomas Wang,
                     2023. 2, 3                                                                   Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela,
                 [4] James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng                      Matthieu Cord, and Victor Sanh. Obelics: An open web-
                     Wang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee,                        scale filtered dataset of interleaved image-text documents,
                     YufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu,                         2023. 7
                     YunxinJiao, and Aditya Ramesh. Improving image gener-                  [17] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
                     ation with better captions. 2                                                Jingkang Yang, and Ziwei Liu.           Otter:  A multi-modal
                 [5] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao,                      model with in-context instruction tuning.        arXiv preprint
                     Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori,                       arXiv:2305.03726, 2023. 7
                     and Ludwig Schimdt. Visit-bench: A benchmark for vision-               [18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
                     language instruction following inspired by real-world use.                   Blip-2:   Bootstrapping language-image pre-training with
                     arXiv preprint arXiv:2308.06595, 2023. 2                                     frozen image encoders and large language models. ICML,
                 [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-                           2023. 3, 7
                     biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-              [19] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai
                     tan, PranavShyam,GirishSastry,AmandaAskell,etal. Lan-                        Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
                     guage models are few-shot learners. Advances in neural in-                   Videochat: Chat-centric video understanding. arXiv preprint
                     formation processing systems, 33:1877–1901, 2020. 6                          arXiv:2305.06355, 2023. 2, 3, 7
                 [7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret                        [20] ShengzhiLiandNimaTajbakhsh. Scigraphqa: Alarge-scale
                     Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,                           synthetic multi-turn question-answering dataset for scientific
                     Mostafa Dehghani, Siddhartha Brahma, et al.              Scaling             graphs. arXiv preprint arXiv:2308.03349, 2023. 2
                     instruction-finetuned language models.          arXiv preprint         [21] Yu Lili, Shi Bowen, Pasunuru Ram, Miller Benjamin,
                     arXiv:2210.11416, 2022. 2, 3                                                 Golovneva Olga, Wang Tianlu, Babu Arun, Tang Binh, Kar-
                 [8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat                        rer Brian, Sheynin Shelly, Ross Candace, Polyak Adam,
                     Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale                         Howes Russ, Sharma Vasu, Xu Jacob, Singer Uriel,
                     Fung, and Steven Hoi.         Instructblip:  Towards general-                Li (AI) Daniel, Ghosh Gargi, Taigman Yaniv, Fazel-Zarandi
                     purpose vision-language models with instruction tuning.                      Maryam, Celikyilmaz Asli, Zettlemoyer Luke, and Agha-
                     arXiv preprint arXiv:2305.06500, 2023. 2, 3, 6, 7                            janyan Armen. Scaling autoregressive multi-modal models:
                 [9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng                        Pretraining and instruction tuning. 2023. 2, 3
                     Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,               [22] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa:
                     Haoran Wei, et al. Dreamllm: Synergistic multimodal com-                     Measuring how models mimic human falsehoods.               arXiv
                     prehension and creation. arXiv preprint arXiv:2309.11499,                    preprint arXiv:2109.07958, 2021. 6
                     2023. 2, 3                                                             [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
               [10] FastChat. Vicuna. https://github.com/lm-sys/FastChat, 2023.                   Improved baselines with visual instruction tuning.         arXiv
                     2, 3                                                                         preprint arXiv:2310.03744, 2023. 2, 3, 7
                                                                                            [24] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
               [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,                            Visual instruction tuning. arXiv preprint arXiv:2304.08485,
                     MengdanZhang,XuLin,ZhenyuQiu,WeiLin,JinruiYang,                              2023. 2, 3, 7
                     Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A                  [25] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang
                     comprehensive evaluation benchmark for multimodal large                      Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
                     language models. arXiv preprint arXiv:2306.13394, 2023.                      Ziwei Liu, et al. Mmbench: Is your multi-modal model an
                     2, 3                                                                         all-around player? arXiv preprint arXiv:2307.06281, 2023.
               [12] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie                         2, 3, 6
                     Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-                   [26] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin
                     angyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2:                      Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan
                     Parameter-efficient visual instruction model. arXiv preprint                 Li, Lianwen Jin, et al. On the hidden mystery of ocr in large
                     arXiv:2304.15010, 2023. 7                                                    multimodalmodels. arXivpreprintarXiv:2305.07895,2023.
               [13] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying                       2
                                                                                        13307
              [27] RuipuLuo,ZiwangZhao,MinYang,JunweiDong,Minghui                    [42] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingn-
                    Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley:                 ing Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang,
                    Video assistant with large language model enhanced ability.           Zhiyong Wang, et al.     Lamm: Language-assisted multi-
                    arXiv preprint arXiv:2306.07207, 2023. 2, 3, 7                        modal instruction-tuning dataset, framework, and bench-
              [28] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFa-                          mark. arXiv preprint arXiv:2306.06687, 2023. 2, 3
                    had Shahbaz Khan. Video-chatgpt: Towards detailed video          [43] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
                    understanding via large vision and language models. arXiv             Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
                    preprint arXiv:2306.05424, 2023. 2, 3, 7                              Mm-vet: Evaluating large multimodal models for integrated
              [29] ml foundations.                              Openflamingo.             capabilities. arXiv preprint arXiv:2308.02490, 2023. 2
                    https://github.com/mlfoundations/open flamingo,      2023.       [44] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu,
                    7                                                                     and Tat-Seng Chua. Transfer visual prompt generator across
              [30] OpenAI.                          Introducing        chatgpt.           llms. abs/23045.01278, 2023. 7
                    https://openai.com/blog/chatgpt, 2022. 2                         [45] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,
              [31] OpenAI. Gpt-4 technical report, 2023. 2                                Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang
              [32] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan                 Zhang, Haodong Duan, Hang Yan, et al.             Internlm-
                    Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-                    xcomposer: A vision-language large model for advanced
                    ing multimodal large language models to the world. arXiv              text-image comprehension and composition. arXiv preprint
                    preprint arXiv:2306.14824, 2023. 2, 3, 7                              arXiv:2309.15112, 2023. 2, 3, 7
              [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya                [46] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao,
                    Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,                 YewKenChia,andLidongBing. M3exam: A multilingual,
                    AmandaAskell,PamelaMishkin,JackClark,etal. Learning                   multimodal, multilevel benchmark for examining large lan-
                    transferable visual models from natural language supervi-             guage models. arXiv preprint arXiv:2306.05179, 2023. 2
                    sion. In International conference on machinelearning, pages      [47] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
                    8748–8763. PMLR, 2021. 6, 7                                           hamed Elhoseiny. Minigpt-4: Enhancing vision-language
              [34] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and               understanding with advanced large language models. arXiv
                    Deng Cai. Pandagpt: One model to instruction-follow them              preprint arXiv:2304.10592, 2023. 2, 3, 7
                    all. arXiv preprint arXiv:2305.16355, 2023. 2, 3
              [35] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
                    Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun
                    Huang, and Xinlong Wang. Generative pretraining in multi-
                    modality. arXiv preprint arXiv:2307.05222, 2023. 7
              [36] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
                    Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun
                    Huang, and Xinlong Wang. Generative pretraining in multi-
                    modality. arXiv preprint arXiv:2307.05222, 2023. 2, 3
              [37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
                                                           ´
                    Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste
                        `
                    Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
                    Llama: Open and efficient foundation language models.
                    arXiv preprint arXiv:2302.13971, 2023. 2, 3
              [38] Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankan-
                    halli, and Ying Shan.    What makes for good visual to-
                    kenizers for large language models?         arXiv preprint
                    arXiv:2305.12223, 2023. 3, 7
              [39] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng
                    Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint
                    arXiv:2309.05519, 2023. 2, 3, 7
              [40] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo
                    Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao,
                    and Ping Luo.    Lvlm-ehub: A comprehensive evaluation
                    benchmarkforlarge vision-language models. arXiv preprint
                    arXiv:2306.09265, 2023. 2, 3
              [41] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
                    Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
                    Yaya Shi, et al.   mplug-owl: Modularization empowers
                    large language models with multimodality. arXiv preprint
                    arXiv:2304.14178, 2023. 2, 3, 7
                                                                                 13308
