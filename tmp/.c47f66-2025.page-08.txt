                                                                                            tences. As shown in Figure 8, GPT-4o generates
                                                                                            significantly shorter responses but uses moretokens
                                                                                            for “Descriptive”ratherthan“Analytical”. Onepos-
                                      Reasoning            Lack of                          sible reason is that the increased cognition work-
                                         Error           Knowledge                          load of the vision inputs requires the model to fo-
                                         46%                 25%
                                                                                            cus more on visual processing, which distracts the
                                                                                            model from generating extensive reasoning chains.
                                               Perceptual Error
                                                      27%                                   4 GuideforFutureModelTraining
                                                                 Annotation 
                                  OCR Error                         Error                  The results of MMMU-Pro provide valuable in-
                                      0%                              2%
                                                                                            sights into the challenges faced by current multi-
                      Figure 7: Distribution of 60 annotated GPT-4o errors.                 modal models and suggest several promising direc-
                                                                                            tions for future model development.
                      to better understand the error reasons (Figure 7).                    Scaling of LLM Backbones. As demonstrated
                      Consistent with MMMU findings, the errors are                         in Table 1, increasing the scale of large language
                      broadly categorized into three main types: per-                       model (LLM) backbones consistently enhances
                      ception errors, knowledge errors, and reasoning                       both perception and reasoning capabilities. For
                      errors. However, reasoning errors account for 46%                     example, larger models such as GPT-4o outper-
                      of cases, a significant increase from the original                    form their smaller counterparts like GPT-4o mini,
                      MMMUdistribution(26%). Within perception er-                         while LlavaOneVision-72B achieves better results
                      rors, text recognition and OCR do not prove to                        than LlavaOneVision-7B. Similarly, InternVL2-
                      be the primary bottleneck. Instead, the main chal-                   78Bdemonstrates superior performance compared
                      lenges lie in the integration and interpretation of                   to InternVL2-8B. This trend underscores the im-
                     visual and textual information. This shift in error                    portance of scaling as a critical factor in improving
                      distribution highlights the increased difficulty for                  multimodal understanding and reasoning.
                      models in transitioning from accurate perception                      MoreCapableVisionEncodersthatHighlights
                      to complex multimodal reasoning.                                     Visual Representation Learning. We train two
                                                                                            Cambrian (Tong et al., 2024a) models on 1M Cam-
                      3.7    Response Length Comparison                                     brian data with two different vision encoders to
                                                                                            explore their impact (more details of the setup
                                                                                            are in Appendix E).              As shown in Table 3,
                                              Standard     Vision                           encoders such as Siglip ViT-SO400M-14 (Zhai
                                500
                                                                  409                       et al., 2023), trained with extensive language su-
                                400
                                                                      366
                                                      360                                   pervision, perform well on MMMU (Val) but
                                300
                                                          258                               struggle on MMMU-Pro (Vision). In compari-
                                200                                                         son, self-supervised encoders like DINOv2 ViT-
                              # Tokens100 49 108                                            G-14 (Oquab et al., 2023) achieve better results
                                  0                                                         on the Vision input setting. These findings sug-
                                        Descriptive  Analytical     Total                   gest future work may focus on further enhancing
                                         Response Sentence Types                           visual feature learning while exploring the inte-
                      Figure 8: GPT-4o outputs’ length comparison between                   gration of language-based training objectives with
                      the Standard and Vision settings.                                     self-supervised training objectives.
                                                                                            Better Integration of Vision and Text Modali-
                         One interesting observation we have from the                       ties. Integration of visual and textual information
                      previous qualitative examples is that responses (es-                  remains a key challenge for multimodal models.
                      pecially the reasoning sentences) of GPT-4o under                     Current architectures often struggle with tasks re-
                      the Vision Input setting seem to be shorter than the                  quiring deep cross-modal understanding. Develop-
                      Standard setting. We quantify this phenomenon by                      ing models with better cross-modal attention and
                      asking another LLM (Qwen2-72B-Instruct (Yang                          effective feature fusion is critical to bridge this gap.
                      et al., 2024)) to classify the GPT-4o’s responses                     CoTDataGeneration. TheCoTpromptingtech-
                      into “Descriptive” sentences and “Analytical” sen-                    niqueshowssignificant benefits in reasoning-heavy
                                                                                     15141
