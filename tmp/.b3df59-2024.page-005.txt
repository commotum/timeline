                                                                                        TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                                                         x0     x1      x3
                                                                                                                                                     Hierarchical                                                                         Data Ablations                  Optimization Ablations
                                                                                                                                                    Majority Vote
                   30                               29
                                                                                              Identity
                                                                                                                              LM 
5
                                                                                                                                                           (top-2)
                                                                                                                              transform-1                                                                                                                                                         26
                                                                                       ?
                                                                                                                        ?
                                                                         y0     y1
                                         Test Task                                                                                                                                                                                                                             22
                                  x0     x1    x2      x3
                                                                          x2    x1      x3
                                                                                                                                                                                     20
                                                                                                                              LM 
5                                                                                                                         18
                                                                                             Horizonta
v
                                                                                               Flip
                                                                                                                              transform-1
                                                       ?
                                                                                                                        ?
                                                                                       ?
                                  y0     y1    y2
                                                                         y2     y1                                                                                                                                                       13
                                                                         x2     x0
                                                                                        x3                                                                                         asks Solved10
                                                                                                                                                                                   T
                                                                                                                              LM 
5
                                                                                              Vertica
v
                                                                                               Flip                                                                                                 5
                                                                                                                              transform-1
                                                                                       ?
                                                                                                                        ?
                                                                         y2     y0
                                                                      Leave-one-out Queries           Rule Based Augmentations                                                         0
                                                                                                                                                                                                   FT                 TT                                                       TT
                               Figure 4. Augmented inference and hierarchical voting. We                                                                                                                                                                                                        Loss
                                                                                                                                                                                                               FT + T               mations          Direct I/O       Shared T            No Demo
                                use leave-one-out tasks and invertible geometric transformations                                                                                                                              No Transfor
                                to obtain multiple equivalent versions of the task for augmented                                                                              Figure 5. Accuracy of different data and optimization ablations
                                inference. Predictions from these versions are aggregated with a                                                                              in TTTonARC.OurdataablationsrevealthattheICLdatafor-
                                hierarchical voting strategy: first, voting is performed within each                                                                          mat is crucial for effective TTT, and that applying transformations
                                transformation, and then the top candidates from each transforma-                                                                             to augment the TTT dataset notably enhances performance. For
                                tion undergo global voting to yield the top two predictions.                                                                                  optimization, learning task-specific adapters significantly outper-
                                                                                                                                                                              forms using a single adapter and taking a loss on the in-context
                                                                                                                                                                              demonstrations provides a minor performance boost.
                                obtain multiple responses and select the best according to
                                a ranker, called self-consistency (Wang et al., 2023). How-
                                ever, this is not viable in ARC (where the output grid is
                                directly predicted) as there is no way to directly enforce
                                diversity across samples while ensuring coherence within
                                samples. As an alternative self-consistency approach, we
                                try an augmented inference strategy that combines greedy
                                decoding with multiple versions of the input. Specifically,
                                wegeneratemultiplepredictioncandidatesbyusinggeomet-
                                ric transformations. We then employ a hierarchical voting
                                strategy to determine the final prediction from the set of
                                generated candidates. This approach involves two stages of                                                                                    Figure 6. Performance results across model sizes. Fine-tuned
                                voting to progressively narrow down the best candidates: (1)                                                                                  modelperformance improves with increasing size. However, the
                                Intra-transformation voting: Group predictions by their                                                                                       scaling behavior after TTT is less clear. For instance, the final
                                corresponding transformation t. Within each group, select                                                                                     performance of the 1B and 3B models is identical after TTT.
                                the top-3 most frequent predictions. (2) Global voting:
                                Take the selected transformation-specific candidates from                                                                                     portant; using the direct input-output data to construct D
                                the previous step and select the top-2 most frequent predic-                                                                                                                                                                                                              TTT
                                tions across all transformations. The augmented inference                                                                                     causes an 11-task drop (38%). Removing transformations
                                pipeline is summarized in Figure 4 and full details of the                                                                                    causes a 16 task drop (55%). Regarding optimization, per-
                                pipeline are in Appendix E.                                                                                                                   task LoRA adapters outperform a single shared adapter by 7
                                                                                                                                                                              tasks (24%). Including losses on the demonstration outputs
                                4.3. Impact of TTT Design                                                                                                                     yields a modest but consistent gain (26% → 29%).
                                In this section, we compare the final implementation of our                                                                                   4.4. Impact of Model Size
                                method with different design choices for TTT. FT serves as                                                                                    We perform full fine-tuning of 1B and 3B Llama 3.2
                                the baseline, using only the fine-tuned model with demon-                                                                                     (instruction-tuned) and 8B Llama 3 (instruction-tuned) us-
                                strations in-context. No Transformations omits the aug-                                                                                       ing synthetically generated data, as detailed in Appendix B,
                                mentation step. Direct I/O Data replaces in-context tasks                                                                                     and then use our default TTT implementation. We show
                                with the direct input-output task formulation (Section 3.1).                                                                                  results using different model sizes in Figure 6. Increasing
                                Shared TTT uses a single LoRA adapter across all tasks                                                                                        the model size consistently improves FT performance, with
                                instead of learning one per task. No Demonstration Loss                                                                                       the 8B model achieving the highest accuracy of 36%. At
                                removes the loss on demonstration outputs (Section 3.1).                                                                                      all model sizes, TTT leads to significant improvements in
                                Results are presented in Figure 5. Our TTT method is effec-                                                                                   performance. We also observe that for smaller model sizes,
                                tive, improving fine-tuned model accuracy approximately                                                                                       TTTeffectively closes the performance gap, with the 1B
                                6×(5%→29%). In-contextformatting is especially im-                                                                                            and 3B models achieving similar accuracy after TTT.
                                                                                                                                                                       5
