                      To control for these issues, and to distill the core challenges of visual QA, the CLEVR visual QA
                   dataset was developed [15]. CLEVR contains images of 3D-rendered objects, such as spheres and
                   cylinders (Figure 2). Each image is associated with a number of questions that fall into diﬀerent
                   categories. For example, query attribute questions may ask “What is the color of the sphere?”,
                   while compare attribute questions may ask “Is the cube the same material as the cylinder?”.
                      For our purposes, an important feature of CLEVR is that many questions are explicitly relational
                   in nature. Remarkably, powerful QA architectures [46] are unable to solve CLEVR, presumably
                   because they cannot handle core relational aspects of the task. For example, as reported in the
                   original paper a model comprised of ResNet-101 image embeddings with LSTM question processing
                   and augmented with stacked attention modules vastly outperformed other models at an overall
                   performance of 68.5% (compared to 52.3% for the next best, and 92.6% human performance) [15].
                   However, for compare attribute and count questions (i.e., questions heavily involving relations
                   across objects), the model performed little better than the simplest baseline, which answered questions
                   solely based on the probability of answers in the training set for a given question category (Q-type
                   baseline).
                      We used two versions of the CLEVR dataset: (i) the pixel version, in which images were
                   represented in standard 2D pixel form, and (ii) a state description version, in which images were
                   explicitly represented by state description matrices containing factored object descriptions. Each
                   row in the matrix contained the features of a single object – 3D coordinates (x, y, z); color (r, g,
                   b); shape (cube, cylinder, etc.); material (rubber, metal, etc.); size (small, large, etc.). When we
                   trained our models, we used either the pixel version or the state description version, depending on
                   the experiment, but not both together.
                   3.2    Sort-of-CLEVR
                   To explore our hypothesis that the RN architecture is better suited to general relational reasoning as
                   compared to more standard neural architectures, we constructed a dataset similar to CLEVR that
                                           1
                   we call “Sort-of-CLEVR” . This dataset separates relational and non-relational questions.
                      Sort-of-CLEVR consists of images of 2D colored shapes along with questions and answers about
                   the images. Each image has a total of 6 objects, where each object is a randomly chosen shape
                  (square or circle). We used 6 colors (red, blue, green, orange, yellow, gray) to unambiguously identify
                   each object. Questions are hard-coded as ﬁxed-length binary strings to reduce the diﬃculty involved
                   with natural language question-word processing, and thereby remove any confounding diﬃculty
                   with language parsing. For each image we generated 10 relational questions and 10 non-relational
                   questions. Examples of relational questions are: “What is the shape of the object that is farthest from
                   the gray object?”; and “How many objects have the same shape as the green object?”. Examples of
                   non-relational questions are: “What is the shape of the gray object?”; and “Is the blue object on the
                   top or bottom of the scene?”. The dataset is also visually simple, reducing complexities involved in
                   image processing.
                   3.3    bAbI
                   bAbI is a pure text-based QA dataset [41]. There are 20 tasks, each corresponding to a particular
                   type of reasoning, such as deduction, induction, or counting. Each question is associated with a set
                   of supporting facts. For example, the facts “Sandra picked up the football” and “Sandra went to
                   the oﬃce” support the question “Where is the football?” (answer: “oﬃce”). A model succeeds on
                   a task if its performance surpasses 95%. Many memory-augmented neural networks have reported
                   impressive results on bAbI. When training jointly on all tasks using 10K examples per task, Memory
                   Networks pass 14/20, DNC 18/20, Sparse DNC 19/20, and EntNet 16/20 (the authors of EntNets
                   report state-of-the-art at 20/20; however, unlike previously reported results this was not done with
                   joint training on all tasks, where they instead achieve 16/20) [42, 9, 34, 13].
                     1The “Sort-of-CLEVR” dataset will be made publicly available online.
                                                                  4
