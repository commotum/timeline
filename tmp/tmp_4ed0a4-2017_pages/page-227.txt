                214               7.  SHANNON  ENTROPY AND KOLMOGOROV COMPLEXITY
                The number p* will be called the frequency or probability of letter a^.  For each code 
                ci,..., Ck  (for alphabet ai,..., ak) its average length is defined as
                                                              ^PiKci)-
                Now we can formulate our goal:  for a given pi,... ,Pk  we want to find a code of 
                minimal average length inside some class of codes, e.g., a uniquely decodable code 
                of minimal average length.
                      217  Which injective code has minimal average length (among injective codes)
                for given pi,... ,pn?
                     (.Hint:  Put all letters in order of decreasing frequency and all binary strings in 
               order of increasing length.)
                     7.1.2.  The  definition  of Shannon  entropy.  Shannon entropy provides a 
               lower bound for the average length of a uniquely decodable code.  It is defined (for 
               given non-negative pi such that YliPi = 1) as
                                   H — p\{— logpi) + p2(— logp2) H----+Pfc(-bgPfc).
                (We assume that plogp = 0 for p = 0, making the function plogp continuous at 
               the point p = 0.)
                     There is some motivation for this definition.  Letter a* appears with frequency 
               P i,  and each occurrence of           carries — logp*  “bits of information”, so the average 
               number of bits per letter is H.  But then we should also explain why we believe 
               that  each  occurrence  of the  letter  that  has  frequency pi  carries  — logpj  bits  of 
               information.  Imagine that somebody has in mind one of 2n possible numbers, and 
               you want to guess this number by asking yes-or-no questions.  Then you need n 
               questions,  and  each  answer gives  you  one  bit  of information;  so  when  an  event 
               having probability l/2n happens, it brings us n bits of information.
                     Of course,  the previous paragraph is just  a mnemonic rule for the definition 
               of entropy.  The formal reason to introduce this notion is given by the following 
               theorem:
                     Theorem 138.  Let pi,... ,pn be non-negative reals such that pi + - • -+pn — 1-
                     (a) The average length of every prefix code cq,..., c^ is at least H ( the entropy ):
                                                          ^ 2 р А съ)  >  H .
                                                            i
                     (b)  There exists a prefix code such that
                                                       'Ери*) <H + 1.
                                                         i
                    Proof.  Note that this theorem deals only with the lengths of codewords (but 
               not the codewords itself).  So it is important to know when given integers ni,..., nk 
               could be lengths of codewords in a prefix code.  Here is the criterion:
                    Lemma  (Kraft inequality).  Assume that non-negative integers                                    are
               fixed and we want to find binary strings c\,..., Ck  of these lengths  (/(q) = щ)  that 
               form a prefix code  (i.eC i  is not a prefix of Cj for i ф fi).  This is possible if and 
               only if
                                                           ^ 2 " ni  ^ 1.
