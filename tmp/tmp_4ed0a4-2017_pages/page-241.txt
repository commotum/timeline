             228           7.  SHANNON  ENTROPY AND KOLMOGOROV COMPLEXITY
                 7.3.2.  Expected  complexity.  Let  us  fix  k,  a  ^-letter  alphabet  A,  and  к 
             positive numbers Pi, ■ ■ ■ ,Pk whose sum is 1 (for simplicity we assume that all pi are 
             rational numbers).
                 Consider a random variable £, whose values are letters of A and probabilities are 
            Pi,... ,Pk-  For each N consider a random variable £jv consisting of N independent 
             identically distributed copies of £.  Its values are Л-strings of length N.  Now we may 
             ask a question:  What is the expected complexity of a string generated according 
             to this distribution?
                 Theorem 147.  The expected value of K(£n\N) is NH(£) + 0( 1) (the constant 
             in 0(1)  may depend on £ but not on N).
                 Note that  (for positive pi) all Л-strings of length N are among the values of 
            £n .  Some of them have complexity much greater than NH (except for the case of 
            uniform distribution), but others have complexity much less than NH.
                 PROOF.  For each Л-string of length N (i.e., for each value of £jv) consider its 
            shortest description (with respect to some fixed prefix-stable decompressor).  These 
            descriptions form a prefix code (in the sense of Section 7.1.1).  The average length 
            of the codeword is exactly the expected value of K(£N).  Therefore, Theorem 138 
             (p.  214) guarantees that this expected value cannot be less than H(ÇN) = NH(£). 
            The lower bound is proved (and even the 0(l)-term can be omitted).
                 The same theorem is useful for the upper bound, too.  Indeed, it guarantees that 
            there exists a prefix code that has average length of a codeword at most H(£N) +1. 
            Such a code can be constructed by an algorithm if N (and numbers pi, which are 
            fixed)  is  given.  For example,  one may  use the construction used in the proof of 
            Theorem 138, or use Huffman code, or even just try all codes until a good one is 
            found.
                 Anyway, the constructed code can be used as a conditional decompressor (with 
            N as the condition) such that average length of the shortest description of     does 
            not exceed H(£N) + 1 = NH(£) + 1.  Replacing this decompressor by an optimal 
            one, we increase the average length by 0(1).                                       □
                 236  Show that one can slightly improve the upper bound and prove that the 
            average value of monotone complexity KM(ÇN) does not exceed NH(£) + 0(1).
                 (Hint:  Apply Theorem 89 to the distribution of £°°.)
                We assumed that pi, ■ ■ ■ ,Pk are fixed rational numbers.  One may wish to get a 
            uniform bound that is true for all tuples pi,... ,pk-  Then we should add pi,... ,pk 
            to  the condition and prove bounds for the expected value of K(£N\N,pi,... ,pk) 
            instead of K(£n\N).  The lower bound is not  affected at  all,  since it  is true for 
            any prefix code, and for the code construction the information in the condition is 
            sufficient.  (We assume that pi are rational numbers.  This is not very important, 
            since one may replace arbitrary reals by their approximations with sufficiently small 
            error.)
                 237  Formulate the exact statement and prove it.
                This theorem says that  average  complexity equals entropy though individual 
            values of complexity could be much smaller or much larger.  In fact,  a stronger 
            statement it true:  most values of     have complexity close to NH(£).  More for­
            mally, the event  “the complexity of string ÇN differs significantly from NH(£)” has 
            small probability.  This statement could be considered as an algorithmic version of
