                         INTRODUCTION.  WHAT IS THIS BOOK ABOUT?
          8
          its  elements as équiprobable, then we obtain a ridiculously small value (less than 
          1000 bits); for the non-uniform distributions the entropy is even less.
             So  we  see  that  in  these  contexts  Kolmogorov  complexity  looks  like  a  more 
          adequate tool than Shannon entropy.
                             Complexity and randomness
             Let  us  recall  the  inequality  C(x)  <  l(x) + 0(1)  (Theorem  2).  For  most  of 
          the strings its left-hand side is close to the right hand side.  Indeed, the following 
          statement is true:
             Theorem 5.  Let n  be an integer.  Then there are less than 2n  strings x  such 
          that C(x) < n.
             Proof.  Let D be the optimal description mode used in the definition of Kol­
          mogorov complexity.  Then only strings D(y)  for all y,  such that  l(y) < n,  have 
          complexity less than n.  The number of such strings does not exceed the number of 
         strings y such that l(y) < n, i.e., the sum
                            1 + 2 + 4 + 8 + ... + 2n_1 = 2n -  1 
          (there are 2k strings for each length к < n).                 □
             This implies that the fraction of strings of complexity less than n — c among all 
         strings of length n is less than 2n-c/2n = 2“c.  For instance, the fraction of strings 
         of complexity less than 90 among all strings of length 100 is less than 2“10,
             Thus the majority of strings (of a given length)  are incompressible or almost 
         incompressible.  In  other words,  a randomly chosen string of the given length is 
         almost  incompressible.  This can be illustrated by the following mental  (or even 
         real) experiment.  Toss a coin, say, 80000 times, and get a sequence of 80000 bits. 
         Convert it into a file of size  10000 bytes  (8 bits =  1  byte).  One can bet that no 
         compression software (existing before the start of the experiment) can compress the 
         resulting file by more than  10 bytes.  Indeed,  the probability of this event is less 
         than 2“80 for every fixed compressor, and the number of (existing) compressors is 
         not so large.
             It  is  natural to consider incompressible strings as  “random”  ones:  informally 
         speaking, randomness is the absence of any regularities that may allow us to com­
         press the string.  Of course,  there is no strict  borderline between  “random”  and 
          “non-random”  strings.  It is ridiculous to ask which strings of length 3  (i.e.,  000, 
         001, 010, Oil,  100,  101,  110,  111)  are random and which are not.
             Another example:  assume that we start with a “random” string of length 10000 
         and replace its bits by all zeros (one bit at a step).  At the end we get a certainly 
         non-random string (zeros only).  But it would be naive to ask at which step the 
         string has become non-random for the first time.
             Instead, we can naturally define the  “randomness deficiency”  of a string x as 
         the difference l(x) — C(x).  Using this notion, we can restate Theorem 2 as follows: 
         the  randomness  deficiency  is  almost  non-negative  (i.e.,  larger  than  a  constant). 
         Theorem 5 says that the randomness deficiency of a string of length n is less than 
         d with probability at least 1 — l/2d (assuming that all strings are équiprobable).
             Now consider the Law of Large Numbers.  It says that most of the n-bit strings 
         have frequency of ones close to  1/2.  This law can be translated into Kolmogorov 
         complexity language as follows:  the frequency of ones in every string with small
