            224           7.  SHANNON  ENTROPY AND KOLMOGOROV COMPLEXITY
                             Figure 20.  Entropies of two random variables
                For three variables a, ß, 7 we get a more complicated diagram (Figure 21).  The 
            central region carries a number that is denoted by I (a : ß : 7).  It can be defined 
            as I (a : ß) — I (a : ß\j) or, equivalently, as I(a : 7) — I (a : 7|/3), etc.  In terms of 
            unconditional entropies we get the following expression:
                            F ig u r e   21.  Entropies of three random variables
                Note that  (unlike the other six values shown) the value of I (a : ß : 7) can be 
            negative.  For example, this happens if variables a are ß independent, but become 
            dependent when 7 is known.
                227 Construct three variables a, ß, 7 with this property.
                (Hint  Following the example given on p.  51,  consider uniformly distributed
            independent variables a and ß with range {0,1}, and let 7 = (a + ß) mod 2.)
                228 (Fano inequality) Prove that if the random variables a and ß differ with
            probability at most e < 1/2 and a takes at most a values, then
                                        H(a\ß) < £loga + h(e),
