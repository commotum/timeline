        482        2.  FOUR  ALGORITHMIC  FACES  OF  RANDOMNESS
          One could add a requirement that there is an algorithm that for a given x says 
        whether the equality p(Qx)  — 0 holds or not.  That  requirement gives a strictly 
        smaller class of measures that are called strongly computable measures in the sequel.
          An  important  subclass  of the  class  of computable  measures  is  the  class  of 
        computable-rational measures where the measure of each ball 0 is a rational number 
        that can be computed (the corresponding fraction presented) given x.  Note that it 
        is not the same as a computable measure whose values (on balls) are rational num­
        bers:  in the latter case we are only able to provide arbitrarily close approximations 
        to the rational number which is the measure of the ball, and this is not enough to 
        produce this number entirely (as a fraction of two integers).
          Recalling that probability distributions are those measures for which the mea­
        sure of Q equals  1,  we may speak about computable probability distributions on 
        Q,.  Many definitions and statements about randomness for the uniform distribution 
        can be generalized naturally to arbitrary computable probability distributions.  In 
        particular,  one can prove a general version of Martin-Löf’s theorem  (saying that 
        the  intersection  of all  effectively  large  sets  is  an  effectively  large  set  itself),  and 
        Levin’s theorem (saying that typicalness is equivalent to chaoticness defined using 
        monotone complexity).22
          Stochasticness.  Recall  our  notation:  the  kth  term  of some  sequence  e  is 
        denoted by ek or (to avoid subscripts) by e(k).
          For the case of uniform distribution,  stochasticness was understood as global 
        frequency stability,  i.e.,  the stability of frequencies in all admissible subsequences. 
        Those subsequences were obtained by application of Kolmogorov-admissible selec­
        tion rules.  For the general case of an arbitrary computable measure this scheme 
        remains the same, but frequency stability should be replaced by some more general 
        property derived from the strong law of large numbers in probability theory.
          For the Bernoulli distribution with parameter p 6 (0,1) the definition is clear: 
        we require that every admissible subsequence has the frequency stability property 
                    p.  In other words, for every admissible subsequence the fraction 
        with limit frequency 
        of ones in its n-bit prefixes tends to p as n —> oo.  We also treat the cases p = 0 and 
        p —  1  in  a special way:  only the sequence that contains only zeros  (respectively, 
        ones) is stochastic.
          Can we consider an even more general case of non-Bernoulli distribution?  This 
        definitely goes beyond the original idea of von Mises:  he tries to define the notion 
        of probability as limit frequency in random sequences.  Still one can try to follow 
        this path, starting with quasi-Bernoulli sequences.
          One could not expect the existence of limit frequency in the subsequences of 
        a  quasi-Bernoulli  sequence  (and  different  subsequences  may  have  different  limit 
        frequencies even if they exist).  So the stochasticity requirement should take into 
        account the selection rules  (which terms were selected).  But  first  let  us exclude 
        the  case when  a bit  appears  that  has probability  zero:  we declare a sequence a 
        non-stochastic if there exists some к such that a(k) = 0 and p(k) = 1, or a(k) = 1 
        and p(k) = 0.  Assuming this does not happen, we call a sequence a stochastic with
          22In the main part of the book this result is called the  “Levin-Schnorr theorem”; Schnorr’s 
        paper  was  published  earlier  and  considered  some  special  notion  of complexity  called  “process 
        complexity”.  It can differ significantly from monotone complexity (see the section about history 
        below and the bibliography at the end of this appendix), but the underlying ideas are similar and 
        the proof for one of them can be easily adapted for the other one.
