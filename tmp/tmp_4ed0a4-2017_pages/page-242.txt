                                         7.3.  COMPLEXITY AND  ENTROPY                              229
              the  Shannon theorem on  (noiseless)  channel capacity,  and we will return to this 
              question in Section 7.3.4.
                  7.3.3.       Prefixes of random sequences and their complexity.  In this sec­
             tion we consider infinite ML-random sequences and compare complexities of their 
             prefixes with the entropy of a generating distribution.  Again, let A be an alphabet 
             that has к letters, and let pi,... ,p/~ be a probability distribution on A.  We assume 
             that pi,... ,Pk are computable positive reals.
                  Consider the space A°° of infinite А-sequences and the probability distribution 
             on this space that corresponds to independent identically distributed variables with 
             distribution pi,... ,Pfc.  This is a computable probabilistic measure on A°°, so the 
             Martin-Löf definition of randomness can be used.  (In fact, we have defined Martin- 
             Löf randomness for a two-letter alphabet,  but essentially the same definition can 
             be used for any finite alphabet.)
                  Theorem 148.  Let to  be an ML-random sequence with respect to this distribu­
             tion.  Let (co)n  be its prefix of length N.  Then
                                                lim           = tf,
             where H is the Shannon entropy, i.e., H — ^2pi(— logpi).
                   238 Prove that for uniform distribution this statement is an immediate con­
             sequence of the randomness criterion (Theorem 90, p.  146).
                  (It is a rare occasion when the uniform case is really special.)
                  The statement refers to the plain complexity C\ however, this is not important, 
             since different versions of complexity differ only by O(logiV) = o(N).  So we may 
             use monotone complexity in the proof, and this is convenient.
                  P r o o f.  The  Levin-Schnorr randomness criterion  (Theorem 90,  p.  146)  says 
             that complexity of a prefix of a random sequence is close to the negated logarithm 
             of probability that this prefix appears.  The probability refers to the distribution on 
             A°° considered above, and the negated logarithm equals N ^  qi( — logp*) where qi 
             is the frequency of ith letter in (cj)n -  It remains to use the SLLN, which guarantees 
             that qi tends to pi as N —> oo for a random sequence.                                   □
                  Looking at this proof, we see that the difference between the complexity (per 
             letter)  and  entropy  has  three  reasons:  first,  the  randomness  deficiency  from  the 
             Levin-Schnorr theorem that gives an 0(1)/N difference; second, the difference be­
             tween the plain and monotone complexities (of order 0(\ogN/N))-, and, finally, the 
             difference between frequencies and probabilities which makes the most important 
             term.  (The law of iterated logarithm says that this term typically is a bit larger 
             than 0{VN)/N.)
                  We have assumed that pi are computable reals, otherwise the notion of Martin- 
             Löf randomness cannot be used.  If they are not computable, we can still consider 
             the set of sequences such that complexity of their prefixes (per letter) do not have 
             entropy as a limit.  Then we can prove that this set has measure zero (with respect 
             to the corresponding distribution).
                  239  Prove this statement.
                  {Hint:  For an upper bound we can use some approximations forp*; the precision 
             1/N2  is enough if we consider prefixes of length N.  The additional information
