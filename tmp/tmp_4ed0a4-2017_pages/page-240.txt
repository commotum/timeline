                          7.3.  COMPLEXITY AND  ENTROPY         227
         This computation was performed  (for к = 2) when we proved the Strong Law of 
         Large Numbers  (Theorem 27,  p.  56).  The general case  (for  arbitrary  k)  can  be 
         treated in the same way.
            Finally, note that we need about к log TV bits to specify TV, p\,..., pk (we need to 
         specify к integers whose sum is TV), so by deleting the condition in C(x\N,pi,... ,pk) 
         we increase the complexity by 0(log TV)  (and the constant in О (log Annotation is 
         close to k).                                           □
           Another proof uses the upper bound for monotone complexity (Theorem 89, 
         p.  144).  Consider  a probability  distribution  on  infinite  А-sequences  that  corre­
         sponds to independent trials with probabilities Pi, ■ ■ ■ ,Pk hi each trial.
           The event “a sequence with prefix 2 appears”, where 2 is an А-string of length TV 
         that has frequencies q\,..., qk, equals
         (letter ai has probability pi  and appears qiN times).  The binary logarithm of this 
         probability is equal to
                        -N  - (çi(— logpi) + ----bç^-logpfc))-
         For the special case qi — pi we get —Nh(pi,... ,Pk)', therefore the monotone com­
         plexity has upper bound Nh(p\,... ,Pk)-  (Recall also that monotone complexity 
         differs from other complexity versions by a term О (log TV) for strings of length TV.)
           In fact, this argument is flawed.  When we proved the upper bound for mono­
         tone complexity,  we had  assumed that  distribution  is fixed.  The constant  term, 
         therefore,  may depend on the distribution.  And now we try to estimate KM(x) 
         using a measure that depends on the letter frequencies in the string x.  So formally 
         Theorem 89 is not applicable.  But if we recall its proof, we see that it provides a 
         bound for conditional monotone complexity when pi,■■■,Pk are given.  The differ­
         ence between this conditional complexity and the unconditional one is О (log TV), so 
         we indeed get another proof for Theorem 146.
            234 What is a value of a constant hidden in О (log TV)  (as a function of к)?
           (Hint:  Both proofs give k(l + o(l)) log TV.)
            235 Show that when all frequencies pi,... ,Pk  are not  very close to 0,  the 
         statement of the previous problem could be improved up to (k/2 + 0(1)) log TV.
           (Hint:  In the first proof one should take into account the square roots in Stir­
         ling’s approximation—most of them are in the denominator.  The second proof can 
         also be modified:  instead of exact values of frequencies, one can consider approxi­
         mate frequencies with an error of order 0(l/v/TV).  This gives a weaker bound, but 
         the difference is  bounded  by  a constant.  (Recall that  a smooth function is qua­
         dratic near its minimum.)  In this way we can save half of the bits when specifying 
        Pi,--- ,Pk-)
           Note that the inequality provided by Theorem 146 may be very far from equal­
         ity.  Indeed, if A has two letters and they alternate in a string x, then the right-hand 
        size equals 1 and the left-hand size is of order (log TV)/iV.  This is not surprising and 
        fits well into the general picture:  the complexity is small since it reflects all the reg­
         ularities  (not only frequencies).  In the next sections we prove that the complexity 
        of a randomly generated string is close to the entropy with high probability.
