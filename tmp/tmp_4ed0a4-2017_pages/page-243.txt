                              7.  SHANNON  ENTROPY  AND  KOLMOGOROV COMPLEXITY
              230
              needed to specify these approximate values is of size О (log N).  The lower bound 
              does not use at all the algorithmic properties of pp, for example, we can get a bound 
              for relativized complexity with any oracle A that makes all Pi computable.)
                   7.3.4.  The complexity deviations.  Theorem 148 is asymptotic.  One may 
              look for a bound of difference between complexity and entropy of frequencies for 
              finite  sequences.   (This  follows  the  example  provided  by  the  probability  theory 
              that has the SLLN for the limit values as well as large deviation bounds for finite 
              sequences.)
                   Let A be a ^-letter alphabet, and let p\,... ,pk  be a distribution on A.  Again 
              we assume for simplicity that pi  are rational  (or at least computable).  Consider 
              the  product  distribution  on  AN  that  corresponds  to  N  independent  trials  with 
              probabilities p\, ... ,p^.  So each А-string of length N has certain probability (and 
              certain complexity).  We already know from Theorem 147 that the average value of 
              complexity is close to NH, where H — ^p*(—logp*).  But we want to know also 
              how far this complexity deviates from its average value.
                  The simplest case of two équiprobable letters (which is quite untypical, as we 
              shall see) gives a uniform distribution on all binary strings of length N.  We know 
              that all these strings have complexity at most N + 0(1) and the (overwhelming) 
              majority of strings has complexity close to  N:  The fraction of strings that  have 
              complexity less than N — c is at most 2~c.  So in this case the significant difference 
              between complexity and entropy has an exponentially small probability.
                  The case of uniform distribution on a ^-letter alphabet is similar.  However, if 
              not all the letters have the same probability, the situation changes significantly.
                  Here is the key observation.  For any string x of length N we compare proba­
              bilities pi with empirical frequencies qi(x) (frequencies of letters in x).  It turns out 
              that with high probability the complexity of a random string (with respect to our 
              distribution on AN)  is close to k(x)  = N                   logpi).  Indeed,  Theorem 89
              (p.  144)  says  that  monotone  complexity  can  exceed  k(x)  by  at  most  0(1).  On 
              the other hand, the argument used in the proof of Levin-Schnorr theorem (p. 146, 
              Lemma 1) shows that for any c the probability of the event  KM(x)  <  k(x) — c 
              (according to the distribution considered) does not exceed 2~c.
                  Therefore, the question about complexity reduces to a question about the dis­
              tribution of empirical frequencies.  This question has been studied in probability 
              theory for centuries.  It is known (Moivre-Laplace theorem) that this distribution is 
              close to a normal (Gaussian) one:  the expectation of frequency equals the probabil­
              ity, and the variance is proportional to 1/N.  This is the main term, since it is much 
              larger than terms caused by the О (log A") difference between different complexity 
              versions and by using N as a condition.  This argument (made precise) gives us the 
              proof of the following statement:
                  Theorem  149.  Let £  be  a random variable  with к  values.  For each positive 
              £ > 0 there exists c such that for all N the probability of the event
                                     NH{£) -  cy/N < C(x) < NH{£) + cVN 
              is  at least 1 — e.  (Here x  is a string formed by N independent copies of £.)
                  In fact our arguments assumed that pi are computable.  However, this assump­
              tion can be dropped if we replace pi by their approximations with sufficiently small 
              error (the precision 1/A2 is enough and requires only 0(logN) additional bits).
