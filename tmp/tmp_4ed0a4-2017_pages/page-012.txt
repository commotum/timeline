                                ACKNOWLEDGMENTS                     xiii
                   The meaning of the new definition is very simple.  Entropy 
                H(x\y) is the minimal [bit] length of a [...] program P that per­
                mits construction of the value of x, the value of y being known,
                               #(я|г/)=  min  1{P).
                                       A(P,y)—x
                This concept is supported by the general theory of “computable”
                (partially  recursive)  functions,  i.e.,  by  theory  of algorithms  in 
                general.
                   [...]  The preceding rather superficial discourse should prove 
                two general theses.
                     1)  Basic  information  theory  concepts  must  and  can  be 
                founded without recourse to the probability theory, and in such 
                a manner that “entropy” and “mutual information” concepts are 
                applicable to individual values.
                     2) Thus introduced, information theory concepts can form 
                the basis of the term random,, which naturally suggests that ran­
                domness is the absence of regularities.2
            And earlier (April 23, 1965), giving a talk “The notion of information and the 
         foundations of the probability theory”  at the Institute of Philosophy of the USSR 
         Academy of Sciences, Kolmogorov said:
                So the two problems arise sequentially:
                     1.  Is  it  possible to free the information theory  (and the 
                notion of the  “amount of information” ) from probabilities?
                     2.  It is possible to develop the intuitive idea of randomness 
                as  incompressibility  (the  law  describing  the  object  cannot  be 
                shortened)?
         (The transcript of his talk was published in [85] on p.  126).
            So Kolmogorov uses the term  “entropy”  for the same notion that was named 
         “complexity”  in his first paper, and denotes it by letter H instead of К .
            Later the same notion was denoted by C (see, e.g.,  [103]) while the letter К 
         is  used  for  prefix  complexity  (denoted  by  KP(x)  in  Levin’s  papers  where  prefix 
         complexity was introduced).
            Unfortunately, attempts to unify the terminology and notation made by differ­
         ent people (including the authors) have lead mostly to increasing confusion.  In the 
         English version of this book we follow the terminology that is most used nowadays, 
         with few exceptions,  and we mention  the other  notation used.  For the reader’s 
         convenience, a list of notation used (p. xv) and index (p. 505) are provided.
                                Acknowledgments
            In the beginning of the 1980s Kolmogorov (with the assistance of A. Semenov) 
         initiated  a  seminar  at  the  Mathematics  and  Mechanics  Department  of Moscow 
         State  (Lomonosov)  University called  “Description and computation complexity”; 
         now the seminar (still active) is known as the “Kolmogorov seminar”.  The authors 
         are deeply grateful to their colleagues working in this seminar, including A. Zvonkin,
            2The published  English version of this paper  says  “random  is the absence of periodicity”, 
         but this evidently is a translation error, and we correct the text following the Russian version.
