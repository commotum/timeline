        466        2.  FOUR ALGORITHMIC FACES OF  RANDOMNESS
           Somehow the strings  (I)  and  (II)  are perceived  as  “random”  while  (III)  and 
        (IV) are not.
          But what does it  mean to be  “perceived  as  random”?  Classical probability 
        theory  says  nothing  about  this  natural  question.  Sometimes  they  say  that  the 
        outcomes (III) and (IV) have very small probability 2~ 20 to appear in a fair coin 
        tossing, so the chances to get them are less than one in a million.  Still, (I) and (II) 
        have exactly the same probability!
          Let us start with three important remarks.
            •  First, the intuitive idea of randomness depends on the assumed probability 
              distribution.  If the coin is very asymmetric and one side is much heavier, 
              or  if  it  is  tossed  in  a  very  special  way,  (III)  or  (IV)  may  not  surprise 
              us.  So,  for simplicity,  we will speak mostly about  fair coin tossing,  i.e., 
              independent trials with success probability 1/2.
            •  Second,  the  intuitive  idea of randomness has  sense only  if the string is 
              long enough.  It would be stupid to ask which of four strings 00, 01,  10, 
              11 looks more random than the others.
            •  Finally, there is no sharp boundary between (intuitively) random and non- 
              random strings.  Indeed,  changing one bit  in a random string,  we get a 
              string that  is random,  too.  But  in several steps we can obtain  (III)  or 
              (IV)  from any string.  This well-known effect is sometimes called  “heap 
              paradox”.
          So,  trying  to  define  randomness,  one  should  consider  very  long  strings,  or, 
        even better, infinite bit sequences (in general infinite objects are  “approximations 
        from above”  for large finite objects).  For infinite sequences one may try to draw a 
        meaningful sharp division between random and non-random objects, i.e., to define 
        rigorously a mathematical notion of a random  bit sequence.  In this survey we 
        describe several attempts to provide such a definition, made by different authors. 
        However,  a general disclaimer is needed:  for all practical purposes only finite se­
        quences (strings) matter, so these definitions are necessarily far from “real life”.  In 
        fact, even very long finite sequences never appear in real life, so it is hard to extend 
        our intuition of randomness even to long finite strings.  This said, we now switch to 
        mathematical definitions.
          Let us start with some useful notation and terminology.
          We consider finite bit strings, i.e., finite sequences of zeros and ones.  (They are 
        also called  binary words.)  A string x = x \.... ,xn  has length n,  denoted also by 
        \x\?  A string may have zero length, i.e., may contain no bits; it is then called an 
        empty string and is denoted by A.
          The set of all binary strings is denoted by H.  The set of all infinite bit sequences 
        is  denoted by fi.  An infinite sequence ai, й2, аз, •..  has finite string ai, a2,..., an 
        as  its  n-bit prefix.  For every string x  we  consider  the set  Qx  C  fl of all infinite 
        sequences that have prefix x.  This set is called a ball, and the volume of this ball 
        is defined as 2“^   and denoted by v(x).2 3
          Each sequence from Q is considered as a record of an (infinite) coin tossing.  Let 
        us repeat that for now we assume that the coin is fair.  Mathematically speaking, it
          2 We used the notation l{x) for the length of x in the main part of the book.
          3In the rest of the book we call Clx  an interval,  not a ball,  and speak about its length,  not 
        volume.
