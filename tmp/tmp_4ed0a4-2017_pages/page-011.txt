        xii                    PREFACE
        (in  particular,  the  results  that  relate  complexity  and  randomness  with  classical 
        recursion theory).
          Our book does not try to be comprehensive (in particular, we do not say much 
        about the recent  results  mentioned  above).  Instead,  we tried to select  the  most 
        important topics and results (both from the technical and philosophical viewpoints) 
        and to explain them clearly.  We do not say much about the history of the topic: 
        as is usually done in textbooks, we formulate most statements without references, 
        but this does not mean (of course) any authorship claim.
          We start the book with a section  “What is this book about?”  where we try to 
        give a brief overview of the main ideas and topics related to Kolmogorov complexity 
        and algorithmic randomness so the reader can browse this section to decide whether 
        the book is worth reading.
          As an  appendix we reproduce the  (English translation)  of a small  brochure 
        written by one of the authors  (V.U.),  based on his talk for high school students 
        and undergraduates (July 23,  2005)  delivered during the  “Modern Mathematics” 
        Summer  School  (Dubna near  Moscow);  the  brochure  was  published  in  2006  by 
        MCCME publishing house (Moscow).  The lecture was devoted to different notions 
        of algorithmic randomness, and the reader who has no time or incentive to study 
        the corresponding chapters of the book in detail can still get some acquaintance 
        with this topic.
          Unfortunately, the notation and terminology related to Kolmogorov complexity 
        is not very logical (and different people often use different notation).  Even the same 
        authors use different notation in different papers.  For example, Kolmogorov used 
        both the letters К and H in his two basic publications [78,  79].  In [78]  he used 
        the term  “complexity”  and denoted the complexity of a string x by K(x).  Later 
        in  [79]  he used the term  “entropy”  (borrowed from Shannon information theory) 
        for  the  same  notion  that  was  called  “complexity”  in  [78].  Shannon  information 
        theory is based on probability theory; Kolmogorov had an ambitious plan to con­
        struct a parallel theory that does not depend on the notion of probability.  In [79] 
        Kolmogorov wrote, using the same word entropy in this new sense:
              The ordinary definition of entropy uses probability concepts, and 
              thus does not pertain to individual values,  but to random val­
              ues,  i.e.,  to  probability  distributions  within  a group  of values. 
              [...]  By far,  not all applications of information theory fit ratio­
              nally into such an interpretation of its basic concepts.  I believe 
              that the need for attaching definite meanings to the expressions 
              H(x\y) and I(x\y), in the case of individual values x and у that 
              are not  viewed as a result of random tests with a definite law 
              of distribution,  was realized long ago  by many who dealt with 
              information theory.
                As far as I know,  the first paper published on the idea of 
              revising information theory so as to satisfy the above conditions 
              was the article of Solomonoff [187].  I came to similar conclu­
              sions, before becoming aware of Solomonoff’s work in 1963-1964, 
              and published my first article on the subject [78] in early 1965.
