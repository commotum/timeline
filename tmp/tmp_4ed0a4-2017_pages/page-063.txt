           48          2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL  COMPLEXITY
                62  Prove that in Theorem 25 the weaker assumption “C(x\z) and C(y\x,z)
           do not exceed n”  is sufficient.
               We also relativize the definition of mutual information and let I(x:y\z) be the 
           difference C(y\z) — C(y\x,z).  As for the case of (unconditional) information, this 
           quantity is non-negative (with 0(1) precision).  Replacing conditional complexities 
           by the expressions involving unconditional  ones  (with logarithmic precision),  we 
           can rewrite the inequality I(x:y\z) ^ 0 as
                    C(y I z) - C(y IX, z) = C(y, z) - C(z) - C(y, x, z) + C(x, z) ^ 0.
           So we get the basic inequality of Theorem 24 again.
               In fact,  almost all known equalities and inequalities that involve complexities 
           (unconditional and conditional)  and information (and have logarithmic precision) 
           are immediate consequences of Theorems 21 and 24.  The first examples of linear 
           inequalities for complexities that do  not follow from basic inequalities were found 
           fairly recently (see [222, 223]) and they are rather complicated and not very intu­
           itive.  We discuss them in Section 10.13; we conclude our discussion here with two 
           simple corollaries of basic inequalities.
               Independent  strings.  We say that strings  x  and  y  are  “independent”  if 
           I(x:y) ~ 0.  We need to specify what we mean by      but we always ignore the 
           terms of order O(logn) where n is the maximal length (or complexity) of the strings 
           involved.
               Independent strings could be considered as some counterpart of the notion of 
           independent random variables,  which is central in probability theory.  There is a 
           simple observation:  if a random variable £ is independent with the pair of random 
           variables (a,ß), then £ is independent with a and with ß (separately).
               The  Kolmogorov  complexity  counterpart  of this  statement  (if a string  x  is 
           independent with a pair (y,z), then x is independent with y and x is independent 
           with z) can be expressed as the inequality
                                        I(x : (y, z)) ^ I{x:y)
           (and the similar inequality for z instead of y).  This inequality is indeed true (with 
           logarithmic precision), and it is easy to see if we rewrite it in terms of unconditional 
           complexities,
                         C{x) + C{y, z) -  C(x, y, z) ^ C(x) + C(y) -  C(x, y),
           which after cancellation of similar terms gives a basic inequality (Theorem 24).  (In 
           classical  probability  theory  one may  also  apply  a similar  inequality  for  Shannon 
           entropies.)
               Complexity of pairs and triples.  On the other hand, to prove the following 
           theorem (which we have already mentioned on p.  12), it is convenient to replace 
           unconditional complexities by conditional ones:
               Theorem 26.
                         2C{x,y,z) ^ C(x,y) + C(x,z) + C(y,z) + O(logn)
           for all strings x, y, z  of complexity at most n.
               PROOF.  Moving  C(x,y)  and  C(x, z)  to  the  left-hand  side  and  replacing  the 
           differences C(x, y, z) — C(x, y) and C(x, y, z) — C(x, z) by conditional complexities
