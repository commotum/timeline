                                                                                           1                    Layer Ì†µÌ±Å
                                                                                           r                                                       r             LN
                                                                                                                                        ‚Ä¶
                                                                    +                      Laye‚Ä¶ LN              MSA       LN     MLP              Laye          Last       MLP Head
                                                          Position      Token                              Visualization                  Visualization
                                                        Embedding Embedding                                                                                                        After Last Layer
                                       Layer1                       Layer4                     Layer7                       Layer9                    Layer11                                               T
                                                                                                                                                                                                            o
                                                                                                                                                                                                            k
                                                                                                                                                                                                            e
                                                                                                                                                                                                            n
                                                                                                                                                                                       Dimension
                                                                                                                    (a) Original
                                                                                                               Layer Ì†µÌ±Å                                                                After Last Layer
                                                                                          r 1                                                     r             LN
                                                                                                                                       ‚Ä¶
                                                                                          Laye‚Ä¶ LN              MSA       LN     MLP              Laye          Last       MLP Head           w/o PE
                                                           Position     Token  
                                                          Embedding Embedding
                                                                                                          Visualization                  Visualization
                                                     Layer1                     Layer4                    Layer7                     Layer9                    Layer11
                                  Token  
                               Embedding
                                                         +                         +                          +                          +                          +                         with PE
                                 Position
                               Embedding
                               Ì†µÌ∞ÇÌ†µÌ∞®Ì†µÌ∞´Ì†µÌ∞´Ì†µÌ∞ûÌ†µÌ∞•Ì†µÌ∞öÌ†µÌ∞≠Ì†µÌ∞¢Ì†µÌ∞®Ì†µÌ∞ß  -0.37                      -0.67                     -0.82                     -0.85                        -0.95
                                                                                      (b) Layer-wise, with PE / without PE in Last LN
                      Figure 2: The heatmaps depict the characteristics of each layer in both the original structure and the Layer-wise structure with
                      the GAPmethod.FortheLayer-wisestructure, the heatmaps illustrate cases both with and without PE in the Last LN. For each
                      heatmapbasedonDeiT-Ti,thex-axisrepresentsthedimensionofDeiT-Ti(256),andthey-axisrepresentsthenumberoftokens
                      (196). In both (a) and the top row (token embedding) of (b), the heatmaps represent the average value of token embedding in
                      each layer, while the bottom row of (b) shows the heatmap of PE. The correlation in (b) refers to the correlation coefficient
                      between token embedding and position embedding.
                      2019) proposed a 2-D relative position encoding for image                                                 effectively leverages the characteristics of PE in the Layer-
                      classification that showed superior performance compared                                                  wise structure.
                      to traditional 2-D sinusoidal embedding. This relative en-
                      coding captures spatial relationships between tokens more                                                 Preliminary: Absolute Position Embedding
                      effectively. In related research, iRPE (Wu et al. 2021) im-                                               The method of absolute position embedding used in vision
                      proves relative PE by incorporating query interactions and                                                transformers is as follows. As shown in Fig 3-(a), PE is
                      relative distance modelinginself-attention. RoPE(Heoetal.                                                 added to the token embedding before they are input into the
                      2024) introduces flexible sequence lengths, decaying inter-                                               layer. This can be expressed as follows:
                      token dependency, and relative position encoding in linear
                      self-attention.                                                                                                                                  1      2            N
                                                                                                                                                  x0 = [xcls; p ; p ; ... p ;] + pos,                                     (1)
                                                        Methodology                                                             where p and pos represent the patch and position embed-
                                                                                                                                ding, respectively. N represents the number of patches, cal-
                      In this section, we first explain the absolute position em-                                               culated as HW/P2, where H and W are the height and
                      bedding and then provide a detailed overview of the Layer-                                                width of the image, and P √ó P is the resolution of each
                      wise structure (Yu et al. 2023). Next, we introduce PVG, an                                               patch. The combined token embedding and PE, denoted as
                      improved Layer-wise structure, along with MPVG, which                                                     x, can be expressed in a layer as follows:
