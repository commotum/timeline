                                   Token Category           Tokens                      Purpose
                                   Alphabet                 A-Z, a-z (excl. I,O,i,o)    Learned pre-prompt tokens
                                   Numbers                  0-9                         Encoding the 10 colors
                                   Newline token            \n                          Signals end of each grid line
                                   Input/Output             I, O                        Signals start of problem input/output
                                   Begin token              ⟨bos⟩                       Inserted once at the beginning
                                   End token                ⟨eos⟩                       Inserted after each output
                                   Padding token            ⟨pad⟩                       Internal usage (e.g. batching)
                                                Table 2: Reduced Token Set for ARC-AGI-specific LLM Model
                                 Visual Representation of a Task Instance:
                                 Compact String Format of same Instance:
                                                             <bos> A ... Z a ... z I 2 1 1 0 1 2 1 \n
                                                                                              1 2 1 0 2 2 2 \n
                                                                                              2 1 1 0 1 1 1 \n
                                                                                                        O 1 1 1 \n
                                                                                                           1 3 1 \n
                                                                                                           1 1 1 \n
                                                                                                                 <eos>
                                 Figure 3: Our standard tokenization approach. Note that we use one token per cell instead
                                 of compressing the problem more. We also try to not include any unnecessary delimiters.
                                 The Pre-prompt(green) is only included for the first example. Depending on the model
                                 and run there might be some small changes to the Pre-prompt and Input/Output tokens.
                                 This reduction offers key benefits. It significantly decreases the model size,
                                 as we can remove the majority of rows from the embedding layer. Further,
                                 token merges that typically occur during text tokenization are no longer pos-
                                 sible. This ensures that the model can focus precisely on the data without
                                 the interference of digit separators.
                                 Asillustrated in Figure 3, we add a small number of extra tokens to the start
                                 of a task. Surprisingly, this addition improves the model’s performance. We
                                 believe that during fine-tuning (where the embedding layers are also trained),
                                 the model learns to use these extra tokens as a form of computational buffer,
                                 which influences every subsequent token, thereby enhancing overall perfor-
                                 mance.
                                 We also experimented with adding extra tokens between input and output
                                                                                  7
