                             Published as a conference paper at ICLR 2015
                             the alignment is not considered to be a latent variable. Instead, the alignment model directly com-
                             putes a soft alignment, which allows the gradient of the cost function to be backpropagated through.
                             Thisgradientcanbeusedtotrainthealignmentmodelaswellasthewholetranslationmodeljointly.
                             Wecan understand the approach of taking a weighted sum of all the annotations as computing an
                             expected annotation, where the expectation is over possible alignments. Let αij be a probability that
                             the target word y is aligned to, or translated from, a source word x . Then, the i-th context vector
                                               i                                                    j
                             ci is the expected annotation over all the annotations with probabilities αij.
                             The probability α , or its associated energy e , reﬂects the importance of the annotation h with
                                                ij                            ij                                               j
                             respect to the previous hidden state s     in deciding the next state s and generating y . Intuitively,
                                                                    i−1                             i                  i
                             this implements a mechanism of attention in the decoder. The decoder decides parts of the source
                             sentence to pay attention to. By letting the decoder have an attention mechanism, we relieve the
                             encoder from the burden of having to encode all information in the source sentence into a ﬁxed-
                             length vector. With this new approach the information can be spread throughout the sequence of
                             annotations, which can be selectively retrieved by the decoder accordingly.
                             3.2   ENCODER: BIDIRECTIONAL RNN FOR ANNOTATING SEQUENCES
                             The usual RNN, described in Eq. (1), reads an input sequence x in order starting from the ﬁrst
                             symbol x to the last one x      .  However, in the proposed scheme, we would like the annotation
                                       1                   Tx
                             of each word to summarize not only the preceding words, but also the following words. Hence,
                             we propose to use a bidirectional RNN (BiRNN, Schuster and Paliwal, 1997), which has been
                             successfully used recently in speech recognition (see, e.g., Graves et al., 2013).
                                                                                                        →−
                             ABiRNNconsistsofforwardandbackwardRNN’s. TheforwardRNN f readstheinputsequence
                                                                                                                    →−        →−
                             as it is ordered (from x to x   ) and calculates a sequence of forward hidden states ( h ,··· , h     ).
                                                     1    Tx                                                           1         Tx
                                                    ←−
                             The backward RNN f reads the sequence in the reverse order (from xT to x1), resulting in a
                                                                    ←−        ←−                            x
                             sequence of backward hidden states ( h 1,··· , h T ).
                                                                                  x
                                                                                                                         →−
                             Weobtain an annotation for each word xj by concatenating the forward hidden state hj and the
                                                            h           i
                                            ←−                →−   ←−    >
                             backward one h , i.e., h =       h>; h>       . In this way, the annotation h contains the summaries
                                               j        j       j     j                                    j
                             of both the preceding words and the following words. Due to the tendency of RNNs to better
                             represent recent inputs, the annotation hj will be focused on the words around xj. This sequence
                             of annotations is used by the decoder and the alignment model later to compute the context vector
                             (Eqs. (5)–(6)).
                             See Fig. 1 for the graphical illustration of the proposed model.
                             4    EXPERIMENT SETTINGS
                             Weevaluate the proposed approach on the task of English-to-French translation. We use the bilin-
                                                                                    3
                             gual, parallel corpora provided by ACL WMT ’14.          As a comparison, we also report the perfor-
                             mance of an RNN Encoder–Decoder which was proposed recently by Cho et al. (2014a). We use
                             the same training procedures and the same dataset for both models.4
                             4.1   DATASET
                             WMT’14 contains the following English-French parallel corpora: Europarl (61M words), news
                             commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively,
                             totaling 850Mwords. FollowingtheproceduredescribedinChoetal.(2014a),wereducethesizeof
                                                                                                                                    5
                             the combinedcorpustohave348MwordsusingthedataselectionmethodbyAxelrodetal.(2011).
                             Wedonotuseanymonolingual data other than the mentioned parallel corpora, although it may be
                             possible to use a much larger monolingual corpus to pretrain an encoder. We concatenate news-test-
                                3 http://www.statmt.org/wmt14/translation-task.html
                                4 Implementations are available at https://github.com/lisa-groundhog/GroundHog.
                                5 Availableonlineathttp://www-lium.univ-lemans.fr/ schwenk/cslm_joint_paper/.
                                                                                                ˜
                                                                                4
