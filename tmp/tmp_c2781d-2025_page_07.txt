                         Preprint, Under Review.
                         instructed to output a plan followed by an action every k steps, and only an action otherwise. These
                         evaluations measure performance trade-offs associated purely with inference-time planning strategies.
                         4.4  SUPERVISED FINE-TUNING (SFT)
                         Toprepare Llama-3.1-8B-Instruct for RL, we first perform SFT priming as in (Gandhi et al., 2025).
                         DataGeneration: We created a dataset of 1024 Crafter trajectories using Llama-3.3-70B-Instruct as
                         a teacher. To ensure diversity, the teacher planned every K steps (K ∼ U[2,12] per trajectory) using
                         16different planning prompts (Appendix A).
                         SFTPriming: TheLlama-3.1-8Bmodelwasfine-tuned on this data, aligning the SFT process with
                         the target RL configuration. For the SFT+RL plan dynamically agent, SFT targets use the dynamic
                         format – <plan>...</plan> [Action] if the teacher planned at that step, otherwise just
                        [Action]–alongwithadynamicpromptencouragingthischoice. Conversely,fortheSFT+RLno
                         plan agent, SFT targets include only the actions ([Action], with all plan blocks removed), paired
                         with a naive prompt focused solely on action prediction. This prepares the model appropriately for
                         the subsequent RL phase. More details on the prompt in the Appendix A.
                         4.5  REINFORCEMENT LEARNING (RL)
                         WethenusedProximalPolicyOptimization (PPO) (Schulman et al., 2017) to fine-tune Llama-3.1-
                         8B-Instruct agents in Crafter, optimizing task rewards possibly adjusted for planning costs (Sec. 3.3).
                         Wecomparefourkeyconfigurations:
                               • Base+RLplandynamically: RLonthebasemodelusingadynamicplanningprompt.
                               • Base+RLnoplan: RLonthebasemodelusingthenaive(action-only) prompt.
                               • SFT+RLplandynamically: RLontheSFT-primeddynamicplanningmodel,usingthe
                                 dynamic planning prompt.
                               • SFT+RLnoplan: RLonthenaiveSFT-primednaivemodel,usingthenaiveprompt.
                         This isolates learned dynamic planning from fixed strategies, and the benefit of SFT priming with
                         versus without explicit plan information. Further training details are in the Appendix C.
                         5   RESULTS
                         Wepresent findings analyzing the impact of planning frequency in zero-shot settings and the effec-
                         tiveness of our SFT priming approach, and the RL results.
                         5.1  ZERO-SHOT EVALUATION
                         Toestablish baseline capabilities and characterize the trade-offs between planning frequency, compu-
                         tational cost, and task performance, we conduct zero-shot evaluations. Specifically, we assess the
                         zero-shot performance of Llama-3.3-70B-Instruct in the POGS and Crafter environments over 100
                         seeds. We systematically vary the agent’s planning frequency, from never planning to planning at
                         fixed intervals, and measured task progression against the mean number of output tokens (log scale),
                         which serves as a proxy for the computational budget.
                         Contrary to the intuitive assumption that performance would scale monotonically with computational
                         effort to form a Pareto frontier, our findings reveal a non-monotonic relationship, which we term
                         the “Goldilocks” zone for planning frequency. As shown in Figure 1 (a-b), performance in both
                         environments peaks at intermediate planning frequencies before declining as planning becomes
                         morefrequent. The degradation in performance associated with excessive planning aligns with the
                         instability cost (Cnoise) introduced in our conceptual framework (Section 3). Quantitative analysis of
                         agent trajectories in the POGS environment (Appendix B) supports this view; the always-plan agent
                         exhibited the highest rate of backtracking, suggesting a tendency to oscillate or revisit states rather
                         than explore efficiently. We hypothesize that this phenomenon may be compounded by other factors.
                                                                    7
