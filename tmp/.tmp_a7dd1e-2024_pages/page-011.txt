                  Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,          Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi
                     Carroll Wainwright, Pamela Mishkin, Chong Zhang,           Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023b.
                     Sandhini Agarwal, Katarina Slama, Alex Ray, et al.         Mint: Evaluating llms in multi-turn interaction
                     2022. Training language models to follow instruc-          with tools and language feedback. arXiv preprint
                     tions with human feedback. Advances in Neural              arXiv:2309.10691.
                     Information Processing Systems, 35:27730–27744.         Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi
                  Junran Peng, Xingyuan Bu, Ming Sun, Zhaoxiang                 Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,
                     Zhang, Tieniu Tan, and Junjie Yan. 2020. Large-            Rui Xie, Jindong Wang, Xing Xie, et al. 2023c.
                     scale object detection in the wild from imbalanced         Pandalm: An automatic evaluation benchmark for
                     multi-labels. In Proceedings of the IEEE/CVF con-          llm instruction tuning optimization. arXiv preprint
                     ference on computer vision and pattern recognition,        arXiv:2306.05087.
                     pages 9709–9718.                                        Zekun Moore Wang, Zhongyuan Peng, Haoran Que,
                  JunranPeng,QingChang,HaoranYin,XingyuanBu,Ji-                 Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu,
                     ajun Sun, Lingxi Xie, Xiaopeng Zhang, Qi Tian, and         Hongcheng Guo, Ruitong Gan, Zehao Ni, Man
                     Zhaoxiang Zhang. 2023. Gaia-universe: Everything           Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu,
                     is super-netify. IEEE Transactions on Pattern Analy-       Wenhu Chen, Jie Fu, and Junran Peng. 2023d.
                     sis and Machine Intelligence, 45(10):11856–11868.          Rolellm: Benchmarking, eliciting, and enhancing
                  Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-       role-playing abilities of large language models. arXiv
                     pher D Manning, Stefano Ermon, and Chelsea Finn.           preprint arXiv: 2310.00746.
                     2024. Direct preference optimization: Your language     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
                     modelis secretly a reward model. Advances in Neu-          Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
                     ral Information Processing Systems, 36.                    et al. 2022. Chain-of-thought prompting elicits rea-
                  William A Scott. 1955. Reliability of content analysis:       soninginlargelanguagemodels. AdvancesinNeural
                     The case of nominal scale coding. Public opinion           Information Processing Systems, 35:24824–24837.
                     quarterly, pages 321–325.                               Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui
                  Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,             Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai,
                     AbuAwalMdShoeb,AbubakarAbid,AdamFisch,                     HaibinChen,TiezhengGe,etal.2024. Conceptmath:
                     Adam R Brown, Adam Santoro, Aditya Gupta,                  Abilingual concept-wise benchmark for measuring
                     Adrià Garriga-Alonso, et al. 2022.      Beyond the         mathematical reasoning of large language models.
                     imitation game: Quantifying and extrapolating the          arXiv preprint arXiv:2402.14660.
                     capabilities of language models.     arXiv preprint     CanwenXu,DayaGuo,NanDuan,andJulianMcAuley.
                     arXiv:2206.04615.                                          2023.   Baize: An open-source chat model with
                  Tao Sun, Linzheng Chai, Yuwei Yin Jian Yang,                  parameter-efficient tuning on self-chat data. arXiv
                     Hongcheng Guo, Jiaheng Liu, Bing Wang, Liqun               preprint arXiv:2304.01196.
                     Yang, and Zhoujun Li. 2024. Unicoder: Scaling code      Yi. 2023. Yi: Building the next generation of open-
                     large language model via universal code. ACL.              source and bilingual llms. https://github.com/
                  Yuchong Sun, Che Liu, Jinwen Huang, Ruihua Song,              01-ai/Yi.
                     Fuzheng Zhang, Di Zhang, Zhongyuan Wang, and            Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao,
                     Kun Gai. 2023. Parrot: Enhancing multi-turn chat           DanielZhang-Li,XinLv,HaoPeng,ZijunYao,Xiao-
                     models by learning to ask questions. arXiv preprint        han Zhang, Hanming Li, et al. 2023. Kola: Carefully
                     arXiv:2310.07301.                                          benchmarking world knowledge of large language
                  InternLM Team. 2023. Internlm: A multilingual lan-            models. arXiv preprint arXiv:2306.09296.
                     guage model with progressively enhanced capabili-       Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle
                     ties. https://github.com/InternLM/InternLM.                Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
                  Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-            Zhuohan Li, Zi Lin, Eric Xing, et al. 2023. Lmsys-
                     bert, Amjad Almahairi, Yasmine Babaei, Nikolay             chat-1m: A large-scale real-world llm conversation
                     Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti         dataset. arXiv preprint arXiv:2309.11998.
                     Bhosale, et al. 2023.     Llama 2: Open founda-         Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
                     tion and fine-tuned chat models.     arXiv preprint        Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
                     arXiv:2307.09288.                                          Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.
                  Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong             Judging llm-as-a-judge with mt-bench and chatbot
                     Wang,BinLiang,RuifengXu,andKam-FaiWong.                    arena. Advances in Neural Information Processing
                     2023a. Cue-cot: Chain-of-thought prompting for             Systems, 36.
                     responding to in-depth dialogue questions with llms.    WanjunZhong,RuixiangCui,YiduoGuo,YaoboLiang,
                     In Findings of the Association for Computational           Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen,
                     Linguistics: EMNLP 2023, pages 12047–12064.                and Nan Duan. 2023. Agieval: A human-centric
                                                                         7431
