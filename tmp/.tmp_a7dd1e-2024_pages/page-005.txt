                  preceding turns. This phenomenon is especially            DPO) (Jiang et al., 2023a), Qwen-Chat (7B,
                  significant in tasks with strong interactivity such      14B) (Bai et al., 2023), Yi-Chat (6B, 34B) (Yi,
                  as instruction clarification and proactive interac-       2023), ChatGLM2-6B/ChatGLM3-6B (Du et al.,
                  tion.  Hence, we leverage our meticulously cu-            2022), InternLM2-Chat (7B, 20B, RLHF) (Team,
                  rated dataset as the golden context for dialogue          2023), Vicuna-13B-v1.5 (Chiang et al., 2023),
                  history, as opposed to relying on self-predicted con-     Baichuan2-Chat-13B (Baichuan, 2023)), UltraLM-
                  text from LLM subjects. This approach facilitates        13B-v2.0 (Ding et al., 2023), and Baize-v2-
                  the creation of smoother, more rational dialogues.       13B(Xuetal., 2023). More details of these evalu-
                  Moreover, evaluating only the newest response of          ated models can be seen in the Appendix E.
                  the LLMs while maintaining consistency with the
                  conversation history also promotes fair evaluation.       4.2   MainResults
                     Following MT-Bench (Zheng et al., 2024), we            TaskDimensionalAnalysis         Table 3 presents the
                  employ GPT-4 for evaluation in our benchmark.             performance of different language models on the
                  We tailor different evaluation prompts (see Ap-          13 multi-turn dialogue tasks in our MT-Bench-101.
                  pendix C) for each task and develop fine-grained          Among all the tasks, content confusion and for-
                  scoring guidelines detailing what is required for         mat rephrasing are relatively less difficult, while
                  each score level or grade. Then GPT-4 scores each         the mathematical reasoning task is the most chal-
                  turn of the chatbotâ€™s responses from 1 to 10 and          lenging. Furthermore, closed-source models con-
                  gives detailed justifications. Additionally, our eval-    sistently exhibit superior performance compared
                  uation process utilizes a minimum-score-taking            to open-source counterparts across all evaluated
                  metric, where the lowest score of a turn is con-          tasks. GPT-4 emerges as the top-performing model
                  sidered the final score for the entire dialogue. This     across the entire spectrum of tasks with an average
                  approach is consistent with human intuition, as           score of 8.86, while Yi-34B with an average score
                  discussed in section 4.5, because a single failed         of 8.10 ranks as the second-best performer overall.
                  response can compromise the entire dialogue in
                  closely related conversational contexts. Moreover,        Ability Dimensional Analysis         Table 3 further
                  this metric prevents models from achieving inflated       indicates that model performances across tasks
                  scores by simply learning patterns from the golden        within the same ability tend to be similar, inspir-
                  context. This phenomenon will be further explored         ing us to assess the overall performance of various
                  in section 4.2.                                           models from the perspective of the abilities. Fig-
                     Li et al. 2024a; He et al. 2022 point out that         ure 3 illustrates the performance of different LLMs
                  there is self-bias in LLM judges (e.g., GPT-4 Judge       acrosssevenabilitydimensions,wherethescorefor
                  prefers GPT-4 answers). We also provide a leader-         each ability is the average score across its respec-
                  board with Qwen-72B-Chat as the judge model               tive tasks. Most LLMs demonstrate a widespread
                  in the Appendix D, showing that this problem is           proficiency in rephrasing and resistance to inter-
                  minor in our benchmark, with the rankings of GPT-         ference. However, the reasoning and questioning
                  4-Judge and Qwen-72B-Judge being consistent.              abilities of LLMs are still in need of enhancement.
                  4 Experiments                                             In addition, the performance of models in mem-
                  4.1   Experimental Setup                                  ory surpasses that in understanding ability. This
                  Settings    Weutilize the golden contexts as dia-         discrepancy arises because memory is primarily
                                                                            concerned with the recall of information, whereas
                  logue histories in all experiments unless otherwise       understanding encompasses the grasping of mean-
                  specified. For each LLM, we apply the correspond-         ing, representing a deeper level of cognitive pro-
                  ingchatformatandthesystempromptwhilesetting               cessing. Furthermore, reflection and questioning
                  the temperature to 0. Additional details on the ex-       abilities play pivotal roles in how models interact
                  perimental setup and implementation can be found          with users during multi-turn dialogues and are es-
                  in the Appendix C.                                        sential for maintaining communication coherence.
                  Models Weevaluate 21 popular LLMs on MT-                  Consequently, models that excel in reflection and
                  Bench-101, including 2 close-sourced LLMs (i.e.,          questioning not only show proficiency in individ-
                  GPT-3.5/ GPT-4 (OpenAI, 2023)) and 19 open-               ual tasks but also suggest a higher level of overall
                  sourced LLMs (i.e., Llama2-Chat (7B, 13B) (Tou-           conversational intelligence and are often rewarded
                  vron et al., 2023), Mistral-Instruct (7B, 8x7B,           with higher overall scores.
                                                                       7425
