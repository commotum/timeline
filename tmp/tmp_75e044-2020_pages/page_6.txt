                                                REALM:Retrieval-AugmentedLanguageModelPre-Training
               4. Experiments                                                      evant documents (e.g., 20). These documents are typically
                                                                                   then re-ranked using a learned model, but coverage may be
               We now evaluate our approach on the Open-QA task. In                limited by the initial heuristic retrieval step. Approaches
               this section, we describe in detail the benchmarksused and          such as DrQA (Chenet al., 2017), HardEM (Min et al.,
               the different approaches to which we compare empirically.           2019a), GraphRetriever (Min et al., 2019b), and PathRe-
                                                                                   triever (Asai et al., 2019) in Table 1 are in this category.
               4.1. Open-QABenchmarks
                                                                                   Somerecentapproacheshaveproposedtoimplementlearn-
               A number of benchmarks have been proposed for Open-                 ableretrievalusingaMIPSindex. ORQA(Leeet al.,2019)
               QA. In this work, we focus on datasets where the ques-              formulates Open-QA using a similar latent variable model
               tion writers did not already know the answer. This yields           as REALM, and also trains by maximizing the marginal
               questions that reﬂect more realistic information-seeking            likelihood.   However, REALM adds a novel language
               needs, and also avoids artifacts that can arise if the ques-        modelpre-trainingstep, and backpropagatesinto the MIPS
               tion is formulated with a particular answer in mind. A              index, rather than using a ﬁxed index. In Table 1, we di-
               deeper justiﬁcation is given in Lee et al. (2019). In all           rectly compare the two. It is also important to note that
               cases, the predicted answer is evaluated via exact match            the retrievers for both REALM pretraining and ORQA are
               with any reference answer, following previous Open-QA               initialized using the Inverse Cloze Task, described in Sec-
               work(Chenetal., 2017).                                              tion 3.4.
               NaturalQuestions-Open The NaturalQuestions dataset
               (Kwiatkowski et al., 2019) consists of naturally occurring          Generation-based Open-QA An emerging alternative
               Googlequeriesandtheiranswers. Eachansweralsocomes                   approach to Open-QA is to model it as a sequence pre-
               withan“answertype”: followingLee et al.(2019),weonly                diction task: simply encode the question, and then decode
               keep questions that are categorized as “short answer type”          the answer token-by-token based on the encoding. While
               with at most ﬁve tokens. The dataset also provides a sug-           it was initially unclear how large amounts of knowledge
               gested Wikipedia document to retrieve; like all models we           could be injected into the model, GPT-2 (Radford et al.,
               compareagainst, we do not provide this to our model.                2019) hinted at the possibility of directly generating an-
                                                                                   swers without using any given context via sequence-to-
                                                                                   sequence. However, their performance was not competi-
               WebQuestions The WebQuestions dataset (Berant et al.,               tive possibly due to the lack of ﬁne-tuning. Orthogonally,
               2013) was collected from the Google Suggest API, using              T5(Raffel et al., 2019) showed that directly generating an-
               one seed question and expanding the set to related ques-            swers without explicit extraction from the given context is
               tions. We follow the setting deﬁned by Chen et al. (2017).          viable approach, but they only experimented on the read-
                                                                                   ing comprehension task, where a context document is pro-
               CuratedTrec The CuratedTrec dataset is a collection of              vided.
               question-answer pairs drawn from real user queries issued
               onsitessuchasMSNSearchandAskJeeves. Toaccountfor                    Forthemostcompetitiveandcomparablegeneration-based
               multiplecorrectanswersordifferentspellingvariations,the             baseline, we compare to concurrent work which ﬁne-tunes
                                                                                   T5 for Open-QA (Robertsetal., 2020).4            We compare
               answers in this dataset are deﬁned as regular expressions
               that match all correct answers. It is unclear how to train          against the Base, Large, and even larger 11-billion parame-
               generation-based models with this type of supervision, so           ter model to measure the effect of model size.
               wedonotevaluatethemonthisdataset.
                                                                                   4.3. Implementation Details
               4.2. Approaches compared                                            Fine-tuning     We reuse all         hyperparameters     from
               Retrieval-based Open-QA          Mostexisting Open-QAsys-           Leeet al. (2019), to enable direct comparison.            Our
               tems answer the input question by ﬁrst retrieving poten-            knowledge corpus is derived from the December 20, 2018
               tially relevant documents from a knowledge corpus, and              snapshot of English Wikipedia. Documents are greedily
               then using a reading comprehension system to extract an             split into chunks of up to 288 BERT wordpieces, resulting
               answer from the documents. In this paradigm, the knowl-             in just over 13 million retrieval candidates. During ﬁne-
               edgeis stored explicitly in the corpus. We wish to compare          tuning inference, we consider the top-5 candidates, and the
               different methods for implementing retrieval.                          4We initially conducted our own T5 experiments using
               Many approaches use non-learned heuristic retrieval such            the code from https://tinyurl.com/t5-openqa-colab (Raffel et al.,
                                                                                   2019).   We now report results from the concurrent work of
               as sparse bag-of-words matching (Robertson et al., 2009)            Roberts et al. (2020), which has an improved ﬁne-tuning proce-
               or entity linking on the question to select a small set of rel-     dure.
