                                              REALM:Retrieval-AugmentedLanguageModelPre-Training
              decidewhatknowledgetoretrieveanduseduringinference.             CURATEDTREC) and compare to state-of-the-art Open-QA
              Before making each prediction, the language model uses          models, including both extremely large models that store
                                                   1
              the retriever to retrieve documents from a large corpus         knowledge implicitly (such as T5) as well as previous ap-
              such as Wikipedia, and then attends over those documents        proaches that also use a knowledge retriever to access ex-
              to help inform its prediction. Learning this model end-to-      ternal knowledge,but implementretrieval in a more heuris-
              end requires backpropagating through a retrieval step that      tic fashion (Lee et al., 2019; Min et al., 2019a; Asai et al.,
              considers an entire corpus of textual knowledge, as shown       2019). REALMachievesnewstate-of-the-artresults on all
              in Figure 1.                                                    three benchmarks, signiﬁcantly outperformingall previous
              The key intuition of REALM is to train the retriever us-        systems by 4-16%absolute accuracy. We also demonstrate
              ing a performance-based signal from unsupervised text:          qualitative beneﬁts of REALM, including interpretability
              a retrieval that improves the language model’s perplex-         and modularity.
              ity is helpful and should be rewarded, while an un-
              informative retrieval should be penalized.       For exam-      2. Background
              ple, in Figure 1, if the model needs to ﬁll the blank           Language model pre-training        The goal of language
              in “the        at the top of the pyramid”, the re-              modelpre-training is to learn useful representations of lan-
              triever should be rewarded for selecting a document con-        guage, usually from unlabeled text corpora. The resulting
              taining “The pyramidion on top allows for less                  pre-trained model can then be further trained (ﬁne-tuned)
              material higher up the pyramid”. We achievethis                 for a downstream task of primary interest (in our case,
              behavior by modeling our retrieve-then-predict approach         Open-QA),oftenleadingtobettergeneralizationthantrain-
              as a latent variable language model and optimizing the          ing from scratch (Dai & Le, 2015; Radford et al., 2019).
              marginallikelihood.
                                                                                                                        2
              Incorporating a large-scale neural retrieval module during      We focus on the masked language model (MLM) variant
              pre-training constitutes a signiﬁcant computational chal-       of pre-training popularized by BERT (Devlin et al., 2018).
              lenge, since the retriever must consider millions of candi-     In its basic form, an MLM is trained to predict the miss-
              date documents for each pre-training step, and we must          ing tokens in an input text passage. Given an unlabeled
              backpropagate through its decisions. To address this, we        pre-training corpus X (e.g., Wikipedia text), a training ex-
              structure the retriever such that the computation performed     ample (x,y) can be generated by randomly masking to-
              for each document can be cached and asynchronously up-          kens in a sampled piece of text (e.g., x = “The [MASK]
              dated, and selection of the best documents can be formu-        is the currency [MASK] the UK”; y = (“pound”,
              lated as Maximum Inner Product Search (MIPS).                   “of”)). The model uses its representation of the masked
                                                                              input x to predict the token that should go in each mask.
              Numerous prior works have demonstrated the bene-                AgoodMLMmustlearntoencodesyntacticandsemantic
              ﬁt of adding a discrete retrieval step to neural net-           information (e.g., to predict “of”) as well as some world
              works (Miller et al., 2016; Chen et al., 2017), but did not     knowledge(e.g., to predict “pound”).
              apply the framework to language model pre-training and
              employed non-learned retrievers to handle large-scale doc-
              umentcollections. In the language modeling literature, the      Open-domainquestionanswering(Open-QA) Tomea-
              k-Nearest Neighbor Language Model (Khandelwalet al.,            sure a model’s ability to incorporate world knowledge, we
              2019) (kNN-LM) retrieves similar LM examples to im-             need a downstream task where world knowledge is criti-
              prove memorization.     However, kNN-LM was not ﬁne-            cal. Perhaps one of the most knowledge-intensive tasks in
              tuned for downstream tasks, perhaps because it is unclear       natural language processing is open-domain question an-
              howtoadapttheretrievalmechanism: a kNN can only use             swering (Open-QA):given a question x such as “What is
              examples labeled for the target task—during ﬁne-tuning,         the currency of the UK?”, a model must output the
              this precludes LM examples, which contain the desired           correctanswerstringy,“pound”. The“open”partofOpen-
              world knowledge. In contrast, REALM’s retriever is de-          QArefersto the fact that the model does not receive a pre-
              signed to transfer to other tasks, and the retrieval is just    identiﬁed document that is known to contain the answer,
              text, not a labeled example.                                    unlike traditional reading comprehension (RC) tasks such
              We evaluate our approach by ﬁne-tuning the mod-                 as SQuAD (Rajpurkaret al., 2016; 2018). While RC mod-
              els pre-trained with REALM on the task of Open-                    1We use the term “document” loosely to refer to a passage
              domain Question Answering (Open-QA), one of the most            from the knowledge corpus, not necessarily a whole article.
              knowledge-intensive tasks in natural language process-             2Strictly speaking, MLM is not a standard language model,
              ing.    We evaluate on three popular Open-QA bench-             since it does not deﬁne a distribution over the entire sequence
              marks (NATURALQUESTIONS-OPEN, WEBQUESTIONS, and                 of tokens. In the paper we sometimes abuse the term “language
                                                                              model” slightly to make the phrase shorter.
