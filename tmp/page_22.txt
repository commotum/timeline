50.

51.

52.

53.

54.

55.
56.

57.

58.

59.

60.

6l.
62.

63.
64.

65.

66.

67.

68.

69.

Shuo Xie and Zhiyuan Li. Implicit bias of adamw: L inf norm constrained optimization.
ArXiv, abs/2404.04454, 2024.

Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of
numerical stability. In The Thirteenth International Conference on Learning Representations,
2025.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998-6008, 2017.

Meta AI. Llama 3: State-of-the-art open weight language models. Technical report, Meta,
2024. URL https: //ai.meta.com/llama/.

Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:
Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
Noam M. Shazeer. Glu variants improve transformer. ArXiv, abs/2002.05202, 2020.

Biao Zhang and Rico Sennrich. Root mean square layer normalization. ArXiv,
abs/1910.07467, 2019.

Giinter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-
normalizing neural networks. In Neural Information Processing Systems, 2017.

JAX Developers. jax.nn.initializers.lecun_normal. Google Research, 2025. URL
https: //docs. jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_
normal.html. Accessed June 22, 2025.

Yann LeCun, LÃ©on Bottou, Genevieve B Orr, and Klaus-Robert Miiller. Efficient backprop.
In Neural networks: Tricks of the trade, pages 9-50. Springer, 2002.

Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak,
Peter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jachoon Lee, and
Jeffrey Pennington. Scaling exponents across parameterizations and optimizers. In Forty-first
International Conference on Machine Learning, 2024.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.

Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In Neural
Information Processing Systems, 2017.

Jieyi Long. Large language model guided tree-of-thought. ArXiv, abs/2305.08291, 2023.

Yilun Du, Jiayuan Mao, and Josh Tenenbaum. Learning iterative reasoning through energy
diffusion. ArXiv, abs/2406.11179, 2024.

Kyubyong Park. Can convolutional neural networks crack sudoku puzzles? https:
//github.com/Kyubyong/sudoku, 2018.

Single-digit techniques. https: //hodoku.sourceforge.net/en/tech_singles php.
Accessed: 2025-06-16.

Tom Dillion. Tdoku: A fast sudoku solver and generator. https: //t-dillon.github.io/
tdoku/, 2025.

Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudoku-bench:
Evaluating creative reasoning with sudoku variants. arXiv preprint arXiv:2505. 16135, 2025.

Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, and Llion Jones. Continuous
thought machines. arXiv preprint arXiv:2505.05522, 2025.

22
