                   Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Kr-          Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi
                      ishnamurthy, and Cyril Zhang. 2023. Transformers             Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane
                      learn shortcuts to automata. In The Eleventh Interna-        Legg, and Joel Veness. 2023. Randomized positional
                      tional Conference on Learning Representations.               encodings boost length generalization of transform-
                                                                                   ers. In Proceedings of the 61st Annual Meeting of the
                   Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and              Association for Computational Linguistics (Volume
                      DahuaLin.2024. Scaling laws of roPE-based extrap-            2: Short Papers), pages 1889–1903, Toronto, Canada.
                      olation. In The Twelfth International Conference on          Association for Computational Linguistics.
                      Learning Representations.                                 Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant,
                   Ilya Loshchilov and Frank Hutter. 2019. Decoupled               and Omer Levy. 2023. ZeroSCROLLS: A zero-shot
                      weight decay regularization. In 7th International            benchmark for long text understanding. In Find-
                      Conference on Learning Representations.                      ings of the Association for Computational Linguis-
                                                                                   tics: EMNLP 2023, pages 7977–7989, Singapore.
                   Neel Nanda, Lawrence Chan, Tom Lieberum, Jess                   Association for Computational Linguistics.
                      Smith, and Jacob Steinhardt. 2023. Progress mea-          Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng
                      suresforgrokkingviamechanisticinterpretability. In           Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
                      TheEleventh International Conference on Learning             hanced transformer with rotary position embedding.
                      Representations.                                             Neurocomputing, 568:127063.
                   OpenAI. 2023.       GPT-4 technical report.       CoRR,      Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,
                      abs/2303.08774.                                              Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
                                                                                   Sebastian Ruder, and Donald Metzler. 2021. Long
                   BowenPeng,JeffreyQuesnelle,HongluFan,andEnrico                  range arena : A benchmark for efficient transform-
                      Shippole. 2024. YaRN: Efficient context window ex-           ers. In 9th International Conference on Learning
                      tension of large language models. In The Twelfth             Representations.
                      International Conference on Learning Representa-
                      tions.                                                    HugoTouvron,Thibaut Lavril, Gautier Izacard, Xavier
                                                                                   Martinet, Marie-Anne Lachaux, Timothée Lacroix,
                   Ofir Press, NoahA.Smith,andMikeLewis.2022. Train                Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
                      short, test long: Attention with linear biases enables       Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
                      inputlengthextrapolation. InTheTenthInternational            Grave, and Guillaume Lample. 2023a. Llama: Open
                      Conference on Learning Representations.                      and efficient foundation language models. CoRR,
                                                                                   abs/2302.13971.
                   Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,          Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
                      Chloe Hillier, and Timothy P. Lillicrap. 2020. Com-          bert, Amjad Almahairi, Yasmine Babaei, Nikolay
                      pressive transformers for long-range sequence mod-           Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
                      elling. In 8th International Conference on Learning          Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
                      Representations.                                             Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
                   Colin Raffel, Noam Shazeer, Adam Roberts, Katherine             Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
                      Lee, Sharan Narang, Michael Matena, Yanqi Zhou,              Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
                      WeiLi,andPeterJ.Liu. 2020. Exploring the limits              thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
                      of transfer learning with a unified text-to-text trans-      Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
                      former. J. Mach. Learn. Res., 21:140:1–140:67.               Isabel Kloumann,ArtemKorenev,PunitSinghKoura,
                                                                                   Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
                   Jie Ren, Samyam Rajbhandari, Reza Yazdani Am-                   anaLiskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
                      inabadi, Olatunji Ruwase, Shuangyan Yang, Min-               tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
                      jia Zhang, Dong Li, and Yuxiong He. 2021. Zero-              bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
                      offload: Democratizing billion-scale model train-            stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
                      ing. In 2021 USENIX Annual Technical Conference,             Ruan Silva, Eric Michael Smith, Ranjan Subrama-
                      pages 551–564. USENIX Association.                           nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
                                                                                   lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
                                                                                   ZhengYan,IliyanZarov, Yuchen Zhang, Angela Fan,
                   Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten          Melanie Kambadur, Sharan Narang, Aurélien Ro-
                      Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,             driguez, Robert Stojnic, Sergey Edunov, and Thomas
                      Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom                  Scialom. 2023b. Llama 2: Open foundation and
                      Kozhevnikov, Ivan Evtimov, Joanna Bitton, Man-               fine-tuned chat models. CoRR, abs/2307.09288.
                      ish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori,
                      Wenhan Xiong, Alexandre Défossez, Jade Copet,             Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                      Faisal Azhar, Hugo Touvron, Louis Martin, Nico-              Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
                      las Usunier, Thomas Scialom, and Gabriel Synnaeve.           Kaiser, and Illia Polosukhin. 2017. Attention is all
                      2023. Code llama: Open foundation models for code.           you need. In Advances in Neural Information Pro-
                      CoRR,abs/2308.12950.                                         cessing Systems 30: Annual Conference on Neural
                                                                            595
