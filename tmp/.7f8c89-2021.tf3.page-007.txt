                PCT: Point cloud transformer                                                                                              193
                Fig. 4 Left: Neighbor Embedding architecture. Middle: SG module with N   input points, d input channels, k neighbors, N output
                                                                                      in              in                             out
                sampled points, and dout output channels. Top-right: example of sampling (colored balls represent sampled points). Bottom-right: example of
                grouping with k-NN neighbors. Number above LBR: number of output channels. Number above SG: number of sampled points and its output
                channels.
                layers to gradually enlarge the receptive ﬁeld during            which can be achieved by setting the output at each
                feature aggregation, as is done in CNNs. The SG                  stage to still be of size N.
                layer aggregates features from the local neighbors for
                each point grouped by k-NN search using Euclidean                4    Experiments
                distance during point cloud sampling.
                  More speciﬁcally, assume that SG layer takes a                 We now evaluate the performance of naive PCT
                point cloud P with N points and corresponding                    (NPCT, with point embedding and self-attention),
                features F as input and outputs a sampled point cloud            simple PCT (SPCT, with point embedding and
                Ps with Ns points and its corresponding aggregated               oﬀset-attention), and full PCT (with neighbor
                features Fs.     First, We adopt the farthest point              embedding and oﬀset-attention) on two public
                sampling (FPS) algorithm [21] to downsample P                    datasets, ModelNet40[30]andShapeNet[31], givinga
                to Ps. Then, for each sampled point p ∈ Ps, let                  comprehensive comparison with other methods. The
                knn(p,P) be its k-nearest neighbors in P. We then                same soft cross-entropy loss function as Ref. [26] and
                compute the output feature Fs as follows:                        the stochastic gradient descent (SGD) optimizer with
                      ∆F(p)=concat                  (F(q)−F(p))                  momentum0.9wereadopted for training in each case.
                                         q∈knn(p,P)                              Other training parameters, including the learning
                         e
                        F(p) = concat(∆F(p),RP(F(p),k))                 (10)     rate, batch size, and input format, were particular to
                                                     e
                       Fs(p) = MP(LBR(LBR(F(p))))                                each speciﬁc dataset and are given later.
                where F(p) is the input feature of point p, Fs(p) is             4.1    Classiﬁcation on ModelNet40 dataset
                the output feature of sampled point p, MP is the                 ModelNet40 [30] contains 12,311 CAD models in 40
                max-pooling operator, and RP(x,k) is the operator                object categories; it is widely used in point cloud
                for repeating a vector x k times to form a matrix.               shape classiﬁcation and surface normal estimation
                The idea of concatenating the feature among sampled              benchmarking. For a fair comparison, we used the
                point and its neighbors is drawn from EdgeConv [26].
                  We use diﬀerent architectures for the tasks of                 oﬃcial split with 9843 objects for training and 2468
                point cloud classiﬁcation, segmentation, and normal              for evaluation. The same sampling strategy as used in
                estimation. For the point cloud classiﬁcation, we only           PointNet [1] was adopted to uniformly sample each
                need to predict a global class for all points, so the            object to 1024 points. During training, a random
                sizes of the point cloud are decreased to 512 and 256            translation in [−0.2,0.2], a random anisotropic scaling
                points within the two SG layer.                                  in [0.67,1.5], and a random input dropout were
                  For point cloud segmentation or normal estimation,             applied to augment the input data. During testing, no
                weneedtodeterminepoint-wisepartlabels or normal,                 data augmentation or voting methods were used. For
                so the process above is only used for local feature              all the three models, the mini-batch sizes were 32.250
                extraction without reducing the point cloud size,                training epochs were used and the initial learning
