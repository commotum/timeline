discrete reasoning. Finally, we validate the devel-
opment and test portions of DROP to ensure their
quality and report inter-annotator agreement.

Passage extraction We searched Wikipedia for
passages that had a narrative sequence of events,
particularly with a high proportion of numbers, as
our initial pilots indicated that these passages were
the easiest to ask complex questions about. We
found that National Football League (NFL) game
summaries and history articles were particularly
promising, and we additionally sampled from any
Wikipedia passage that contained at least twenty
numbers.” This process yielded a collection of
about 7,000 passages.

Question collection We used Amazon Mechani-
cal Turk? to crowdsource the collection of question-
answer pairs, where each question could be an-
swered in the context of a single Wikipedia passage.
In order to allow some flexibility during the annota-
tion process, in each human intelligence task (HIT)
workers were presented with a random sample of
5 of our Wikipedia passages, and were asked to
produce a total of at least 12 question-answer pairs
on any of these.

We presented workers with example questions
from five main categories, inspired by ques-
tions from the semantic parsing literature (addi-
tion/subtraction, minimum/maximum, counting, se-
lection and comparison; see examples in Table 1),
to elicit questions that require complex linguistic
understanding and discrete reasoning. In addition,
to further increase the difficulty of the questions
in DROP, we employed a novel adverserial anno-
tation setting, where workers were only allowed
to submit questions which a real-time QA model
BiDAF could not solve.*

Next, each worker answered their own question
with one of three answer types: spans of text from
either question or passage, a date (which was com-
mon in history and open-domain text) and numbers,
allowed only for questions which explicitly stated
a specific unit of measurement (e.g., “How many
yards did Brady run?”), in an attempt to simplify
the evaluation process.

Initially, we opened our HITs to all United States

"We used an October 2018 Wikipedia dump, as well as
scraping of online Wikipedia.

3www.mturk.com

4While BiDAF is no longer state-of-the-art, performance is
reasonable and the AllenNLP implementation (Gardner et al.,
2017) made it the easiest to deploy as a server.

Statistic Train Dev Test
Number of passages 5565 582 588
Avg. passage len[words] 213.45 191.62 195.12
Number of questions 77,409 9,536 9,622
Avg. question len [words] 10.79 11.17 11.23
Avg. questions / passage 13.91 16.38 16.36
Question vocabulary size 29,929 8,023 8,007

Table 2: Dataset statistics across the different splits.

workers and gradually reduced our worker pool to
workers who understood the task and annotated it
well. Each HIT paid 5 USD and could be com-
pleted within 30 minutes, compensating a trained
worker with an average pay of 10 USD/ hour.

Overall, we collected a total of 96,567 question-
answer pairs with a total Mechanical Turk budget
of 60k USD (including validation). The dataset
was randomly partitioned by passage into training
(80%), development (10%) and test (10%) sets, so
all questions about a particular passage belong to
only one of the splits.

Validation In order to test inter-annotator agree-
ment and to improve the quality of evaluation
against DROP, we collected at least two additional
answers for each question in the development and
test sets.

In a separate HIT, workers were given context
passages and a previously crowdsourced question,
and were asked to either answer the question or
mark it as invalid (this occurred for 0.7% of the
data, which we subsequently filtered out). We
found that the resulting inter-annotator agreement
was good and on par with other QA tasks; overall
Cohen’s « was 0.74, with 0.81 for numbers, 0.62
for spans, and 0.65 for dates.

4 DROP Data Analysis

In the following, we quantitatively analyze proper-
ties of passages, questions, and answers in DROP.
Different statistics of the dataset are depicted in Ta-
ble 2. Notably, questions have a diverse vocabulary
of around 30k different words in our training set.

Question analysis To assess the question type
distribution, we sampled 350 questions from the
training and development sets and manually anno-
tated the categories of discrete operations required
to answer the question. Table 1 shows the distri-
bution of these categories in the dataset. In addi-
tion, to get a better sense of the lexical diversity of
questions in the dataset, we find the most frequent

2371
