                 neighbours’ values. This is basically what mkatkov (#55) suggested.
                 On the other hand Kaleberg (#52) suggested that the Fourier spectrum of a function tells us
                 something about its complexity (=interestingness). Going back to boolean functions, one can
                 deﬁne the degree of a boolean function as a real polynomial, using its Fourier coe㘠陦cients.
                 We know that those two notions are related (at least for boolean functions), more precisely, that
                 $deg(g) \leq sensitivity(f)$ [Nisan & Szegedy, 94].
                 Juan Ramón González Álvarez Says:
                 Comment #71 September 26th, 2011 at 5:43 am
                 Entropy  does  not  increases  monotonically.  Even  if  be  entropy  you  really  mean  “average
                 entropy”, it does not increase monotonically outside the Markowian approximation.
                 It is not true that the Second Law says that the entropy of any closed system tends to increase
                 with  time  until  it  reaches  a  maximum  value.  You  seem  to  confound  closed  with  isolated
                 systems. Indeed, the Wikipedia article that you link uses the correct term “isolated”.
                 The proof that the middle picture is the more complex structure is easy, when one notices that
                 left  and rigth pictures correspond to equilibrium states. Macroscopically, complexity can be
                 measured as deviation from equilibrium. And in very far from equilibrium regimes, appear
                 dissipative structures, which are highly complex structures. Even a Nobel Prize was given for
                 that. A microscopic characterization of dissipative structures is also possible.
                 I would add that the characterization low complexity –> high complexity –> low complexity of
                 your example is an special case, valid when the external ﬂows are small, there is not bifurcation
                 points, neither memory ef㘶ects. Really complex system are much more complex.
                 Olothreutes Says:
                 Comment #72 September 26th, 2011 at 6:57 am
                 Walter J Freeman III had a really good idea about complexity in his book Neurodynamics; An
                 Exploration of Mesoscopic Brain Dynamics. [W. J. Freeman (2000)London UK: Springer-Verlag].
                 He  observed  that  “self  organizing”  phenomena  seemed  to  contradict  the  Second  Law  of
                 Thermodynamics by taking on more order as time progresses. What he observed was that these
                 phenomena were not destroying entropy, but rather they were able to organize because they
                 were  a  locally  cheaper  way  to  create  it.  He  used  as  an  example  a  hurricane.  This  is  an
                 exaggeration of the convection rolls that are typically used as examples of self organizing
                 phenomena. Hurricanes brew up whenever there is a su㘠陦cient amount of heat in ocean water.
                 Without that heat and its attendant potential dif㘶erence, there is no hurricane. As far as I have
                 been able to observe, all organized systems function as accelerated entropy creation exploits.
                 Scott Says:
                 Comment #73 September 26th, 2011 at 8:50 am
                 Terry Bollinger #65:
                 Have we met?
                 If we have, I don’t remember.
                 Do you know Russ T?
                 Sure.
                 The trouble is that on ﬁrst whack, I just don’t get where you are heading in trying to link that
                 particular idea with the emergence of “interesting” complexity in our universe.
                 I should have said this explicitly in this post, but the idea of linking Kolmogorov complexity
                 with thermodynamics isn’t new at all. I didn’t invent it. In fact, in the standard textbook An
                 Introduction to Kolmogorov Complexity and Its Applications, by Li and Vitanyi, there’s an entire
                 chapter about this connection.
                 Personally,  I’ve  long  thought  that  Kolmogorov  complexity  provides  probably  the  most
                 “objective,” mathematically clearest way to understand the notion of entropy—since basically,
                 what it lets you do is talk about the “entropy” of an individual object, without reference to any
                 hypothesized ensemble from which the object was drawn, or any hypothesized coarse-grained
                 structure  on  the  object.  (Though  actually,  what  you  really  want  here  is  resource-bounded
                 Kolmogorov  complexity—see  the  several  paragraphs  in  the  post  about  this.)  So  it  was
                 completely natural for me to try to answer Sean’s question using some variation on Kolmogorov
                 complexity as well.
                 My blog question to you would be this: If you had to give a two-line pitch of your most critical
                 insight or postulate to an expert — not just to some random person on an elevator! — what
                 would it be?
                 Thinking this question over, I quickly realized two things:
                 (1) I don’t have a “most critical insight or postulate.”
                 (2) It’s just as well that I don’t, since the people I can think of who do have a “most critical
                 insight or postulate,” are the ones who I’d consider spouters and shnoods!
                 I  mean, obviously I think things like computational complexity and quantum computing are
                 pretty important, since otherwise I wouldn’t spend my career studying them. But I wouldn’t call
                 the  importance  of  these  topics  my  “most  critical  insight  or  postulate”—since  among  other
                 things, I didn’t invent them and am far from the only person who studies them!
                 If you’re willing to read a few hundreds or thousands of lines instead of two, you can try:
                 NP-complete Problems and Physical Reality
                 Why Philosophers Should Care About Computational Complexity
                 My research statement (from about 5 years ago)
                 Scott Says:
                 Comment #74 September 26th, 2011 at 9:05 am
                 Peter #68:
                 Nice work. But please don’t corrupt it with cosmological theories that might be wrong.
                 As opposed to cosmological theories that are almost certainly wrong? 
                 Seriously, FWIW nothing I wrote depends on speciﬁc details of the Big Bang model—just that the
                 universe starts in a low-entropy state and then heads toward thermal equilibrium.
                 Scott Says:
                 Comment #75 September 26th, 2011 at 9:12 am
                 Juan #71:
                 The proof that the middle picture is the more complex structure is easy, when one notices that
                 left and rigth pictures correspond to equilibrium states.
                 [Game show buzzer] Sorry, completely wrong, try again! The left picture does not correspond
                 to an equilibrium state, and can’t possibly do so, since it evolves into the second picture.
                 As for the distinction between a “closed” and an “isolated” system: I don’t know what that
                 distinction is; would anyone care to explain it? In any case, whatever the distinction is, I really
                 don’t see how anything else in the post could have depended on it—just replace the word
                 “closed” by the word “isolated” the one incidental place where it appears.
                 Sean Matthews Says:
                 Comment #76 September 26th, 2011 at 10:48 am
                 > Why aren’t theoretical computer science conferences
                 > ever held on cruises? If nothing else, it certainly
                 > cuts down on attendees sneaking away from the
                 > conference venue.
                 I memorably attended a theoretical computer science conference (well sort of: CADE, one year)
                 which was advertised as in a hotel on the beach at the Barrier Reef.
                 When you got there, you realised that this was all true, but that actually to get to the reef
                 required an organised tour in a large boat (the conference outing of course) and the hotel was
                 on the beach in a mining town where there was _nothing_ to do but attend the conference (or
                 play video poker in the next room).
                 It is still my deﬁnition of perfect conference organisation.
                 Juan Ramón González Álvarez Says:
                 Comment #77 September 26th, 2011 at 1:39 pm
                 Sorry, but the left picture represents an *unstable* equilibrium and any initial ﬂuctuation will
                 almost surely trigger the departure of the system from this initial state towards the ﬁnal stable
                 equilibrium state (the right picture). This is the reason which it evolves 
                 The condition of equilibrium is dS=0 and the condition of stability is d^2S<0. This is similar to
                 mechanics where both stable and unstable equilibrium states exist.
                 Indeed, the process that you are considering is just a mixing process. Virtually any textbook on
                 *equilibrium* thermodynamics explain how to obtain the mixing functions (e.g. entropy of
                 mixing, enthalpy of mixing…) from the initial and ﬁnal equilibrium states.
                 The  intermediate  states  of  the  mixing  process  can  be  studied  using  the  tools  of
                 *nonequilibrium* thermodynamics.
                 Regarding your doubt, an isolated system is one in which neither energy nor mass can ﬂow in
                 or out. In a closed system, mass cannot ﬂow in or out but energy can be added or removed.
                 Contrary to what is said in this blog, the Second Law does not say that the entropy of any
                 *closed* system tends to increase with time; the law says that (average) entropy can increase,
                 decrease, or remain constant in function of the heat ﬂow with the surrounds. As said above, the
                 Wikipedia link correctly uses the term "isolated" in its discussion of the law.
                 As said also above, complexity can be characterized as departure from equilibrium. And in very
                 farm  from  equilibrium  regimes,  very  complex  structures  named  dissipative  arise.
                 Microscopically  this  is  also  true.  At  equilibrium  regimes,  the  dynamics  is  trivial  and  the
                 physicochemical properties of the systems are well-described by statistical methods. Outside
                 equilibrium that is not true. The more you depart from equilibrium, the more non-trivial is the
                 dynamics and statistical methods lose importance.
                 This is the reason that works by Kolmogorov, Sinai, and others have very little impact on the
                 physics and chemistry of nonequilibrium (The KS entropy is almost not used). Whereas works
                 by Shanon are known to give incorrect answers to well-studied problems.
                 A quantitative measure of complexity is given by the degree of contraction needed to describe
                 the system. Roughly the right picture needs a description that is about a half of the left picture,
                 and this itself is about a half of the needed for the intermediate states.
                 Scott Says:
                 Comment #78 September 26th, 2011 at 2:22 pm
                 Juan  #77:  No,  the  system  on  the  left  doesn’t  represent  an  equilibrium  at  all,  not  even  an
                 unstable one. It’s going to start to mix even in “ideal” conditions, because of the random
                 molecular motion of both the milk particles and the cof㘶ee particles—so it’s not at all analogous
                 to (say) a needle balanced on its tip. This is a simple point about which you’re completely,
                 totally, ﬂat-out wrong.
                 As  for  the  distinction  you’ve  drawn  between  “closed”  and  “isolated”—that  the  ﬁrst  allows
                 transfer of energy but not mass, while the second doesn’t allow transfer of either—that seems
                 to  me like a somewhat silly distinction, one that doesn’t even make sense in a relativistic
                 universe. It seems obvious that the sense of “closed” that’s relevant for the Second Law is that
                 nothing should go in or out.
                 Alex Says:
                 Comment #79 September 26th, 2011 at 4:09 pm
                 Scott, Sean:
                 I  am just a layman, so much of this is above my head, however it would seem that Cosma
                 Shalizi would be the person to ask on issues of complexity:
                 http://cscs.umich.edu/~crshalizi/notebooks/complexity-measures.html
                 Terry Bollinger Says:
                 Comment #80 September 26th, 2011 at 5:11 pm
                 Scott #73:
                 Ah, that helps. If your piece felt a bit open-ended to me, from your answer I gather it’s because
                 you intended it more as a mini-intro to a topic that already has a large and fairly mature
                 literature. I’ll read it as such.
                 “… you can try:
                 – NP-complete Problems and Physical Reality
                 – Why Philosophers Should Care About Computational Complexity
                 – My research statement (from about 5 years ago)”
                 Well, you may not be a shnood, but you just did an awfully good job of answering my question
                 in a short, three-line response that is fully understandable to experts. Hmm!
                 (A serious question on that: Wouldn’t Maxwell and many other famous physicists qualify as
                 big-time  shnoods  under  that  deﬁnition?  A  set  of  only  four  equations  is  pretty  doggone
                 presumptuous… what a shnood! And then there was that guy who stirred up such a ruckus with
                 an equation that had only two variables and one constant…)
                 BTW, to describe myself I had to extend your list of ad hominem phrases a bit. I am clearly an
                 extralusionary faux-intelligence. I an expert in nothing, and I apply nothing to everything,
                 always!
                 As an EFI (e㘠陦es always create acronyms) the question of whether or not for a given random
                 string there exists an e㘠陦cient program-like compression has always struck me — and still
                 strikes me — as a question that is likely a lot close to the leaf nodes of physical reality that to
                 its core. I realize you are using such ideas very dif㘶erently, however, so I’ll see of any of your
                 materials can help me see it dif㘶erently.
                 Cheers,
                 Terry Bollinger
                 P.S. — Wolfram? Seriously? Wow.
                 OK: Science is all about prediction. So a serious question: What exactly did Wolfram’s massive
                 tome (or the massive international follow-up) predict about the physical world?
                 I’m not aware of a single physically meaningful prediction anywhere in his book, and I went
                 through every blasted page of it when it ﬁrst came out. To the best of my knowledge, follow-
                 up work has not changed this status.
                 Some terminology here:
                 When you specify how to predict something, AND reﬁne that prediction by using physical
                 evidence, it’s called science.
                 When you instead postulate that you have found a principle that is the key to understanding the
                 entire universe, but you then realize you are missing the critical piece needed to make it work,
                 so you train thousands of people about your insight and encourage them to have faith that the
                 missing piece is out there waiting for them to help ﬁnd, it’s called…
                 [Please, no shouts of “string theory” from the peanut gallery, I’m trying to make a real point
                 here!…     ]
                 Scott Says:
                 Comment #81 September 26th, 2011 at 5:50 pm
                 Terry Bollinger #80:
                 Actually,  I  was  trying  to  address  Sean’s  new  question  about  ﬁnding  a  natural  complexity
                 measure that increases and then decreases in examples like the cof㘶ee cup. As I said in the
                 third paragraph:
                      My purpose, in this post, is to sketch a possible answer to Sean’s question, drawing on
                      concepts from Kolmogorov complexity.
                 But  in  order  to  do  that,  I  ﬁrst  had  to  discuss  the  already-known  connection  between
                 Kolmogorov complexity and entropy. I’m sorry if that wasn’t clearer.
                 I completely disagree that Einstein and Maxwell were shnoods by my deﬁnition! As it happens,
                 journalists did constantly ask Einstein to do things like “summarize your main idea in one
                 sentence,” and he gave them humorous non-answers in response. That’s because he realized
                 that  translating  a  nontrivial  idea  in  math  or  physics  into  “slogan”  form  could  only  lead  to
                 egregious misconceptions in someone who didn’t already know the idea. E.g., suppose he told
                 the journalists “my main ideas are that mass is energy, and that space and time are part of the
                 same structure,” like a salesman or politician repeating his catchphrases. People would then get
                 an  illusion  of  understanding  that  was  much  worse  than  no  understanding  at  all.  If  you’re
                 comfortable with abstract concepts, you can probably go from never having heard of special
                 relativity  to  genuinely  understanding  something about it in only an hour, but most people
                 aren’t interested enough to spend even that hour.
                 As for Wolfram, nine years ago I actually wrote the ﬁrst critical academic-style review of his
                 book, one month after it came out. In the review, I mostly focused on some incorrect claims of
                 Wolfram’s  about  quantum  mechanics  (in  the  process,  proving  a  simple  corollary  of  Bell’s
                 Theorem that Conway and Kochen would later call the “Free Will Theorem”     ), but there’s also
                 some stuf㘶 in there about complexity and pseudorandomness.
                 Dániel Varga Says:
                 Comment #82 September 26th, 2011 at 5:51 pm
                 I don’t think it is hard to answer Sean’s question about the three cof㘶ee cups using the notion
                 of coarse-graining. I don’t think it is possible to deﬁne entropy without referring to the notion
                 of  coarse-graining.  (I  know  you  attempted  that  here,  but  I  am  not  convinced  about  your
                 deﬁnition. I even checked Li-Vitanyi, but they don’t seem to get rid of coarse-graining either.)
                 Personally,  I’ve  long  thought  that  Kolmogorov  complexity  provides  probably  the  most
                 “objective,” mathematically clearest way to understand the notion of entropy—since basically,
                 what it lets you do is talk about the “entropy” of an individual object, without reference to any
                 hypothesized ensemble from which the object was drawn, or any hypothesized coarse-grained
                 structure on the object.
                 The coarse-grained structure is not hypothesized. A crucial property of any kind of observer is
                 that she can’t distinguish between all microstates of her surroundings. I am not very familiar
                 with  statistical  mechanics,  but  I  have  a  quite  strong  intuition  that  statistical  mechanics  is
                 nothing but understanding all the logical implications of this single fact. Of which there are
                 many.
                 Scott Says:
                 Comment #83 September 26th, 2011 at 5:55 pm
                 Alex #79:
                 it would seem that Cosma Shalizi would be the person to ask on issues of complexity
                 I agree that Cosma is a great person to ask! But what would make anyone think that there could
                 be one person to ask, on any subject as complex as complexity? 
                 Brigan Says:
                 Comment #84 September 26th, 2011 at 6:24 pm
                 Hi there!
                 I think I came up with an easy way to measure what is intended to measure. I was trying to read
                 all the comments to check out if someone had had the same idea, so I wouldn’t repeat it; but
                 there are really a lot of comments and tomorrow I must wake up early… so I just hope I’m not
                 repeating anything. If so: sorry for the bother 
                 Interesting complexity shows up when dynamics of dif㘶erent scales are mixed up together, so
                 let’s  take  a  look  at  the  scales  of  the  milk-cof㘶ee  by  Fourier  transforming  the  milk  density
                 $M(h)$ and the cof㘶ee density $C(h)$, where $h$ is the height. We should obtain in either case
                 the  Fourier  coe㘠陦cients  $B_w$  of  the  step  function,  where  $B_w$  is  the  coe㘠陦cient
                 corresponding to the frequency $w$. Let’s normalize the coe㘠陦cients such that $sum(B_w)=1
                 ==> b_w = B_w/sum(B_w)$ and reinterpret them as some weird probabilities (just imagine that
                 we have a device which is capable of measuring one and only one spatial-frequency $w$ of the
                 density distributions each time that it measures, and that the probability of measuring each
                 frequency is given by $P(w)=b_w$). Then, I propose as a measure of the complexity of the
                 system the entropy of $P(w)$.
                 My guess is that this quantity will obey the desired behavior. At the beginning we would have
                 the series expansion of the step function, which has very concrete values ($P(w)$ would be a
                 sum of Dirac’s deltas) and a relatively low entropy (low compared with what’s coming). As the
                 milk dif㘶uses down, the sharp distributions $P(w)$ should degenerate yielding a $P(w)$ where
                 more $b_w$ are dif㘶erent from zero, so the entropy of $P(w)$ should be larger. Eventually, milk
                 and cof㘶ee are equally distributed and the density is ﬂat all over the cup. The Fourier transform
                 of this thing would only have one component and the entropy would be minimal. This state will
                 also have a lower complexity than the initial one, by the way.
                 This should be easy to generalize to 3D. A ﬁnal remark would be that any single system should
                 have ﬂuctuations, thus complexity could increase and decrease several times before reaching
                 the steady state. The law should be an averaged one, as for the usual entropy.
                 **Sorry for using so many times the word ‘measure’.
                 Terry Bollinger Says:
                 Comment #85 September 26th, 2011 at 7:36 pm
                 Scott #81:
                 OK, I’ve ﬁnally got the emphasis on the theme picture and the mixed-complexity domain down!
                 It’s  obvious,  I  know,  but  your  emphasis  helps  anyway.  I’ll  keep  an  eye  out  for  your  initial
                 framework as I read through the theory.
                 And yes, I was playing devil’s advocate by calling Maxwell a shnood. I knew very well that was
                 not what you intended, but I was curious as to your response (which was quite persuasive!) If
                 you look back at my original email, I tried to be very careful not to ask for super-simpliﬁcation,
                 but instead for some guidance on “here’s what I’m really hoping you will understand after you
                 read through all of this.”
                 Interestingly, I also did a very early review of ANKoS for a software magazine. I’m wondering
                 now if I read yours back then; your description sounds familiar. I’ll look it if I can.
                 Blog limit reached for the day (week?), real life calls.
                 Cheers,
                 Terry
                 Sunik Says:
                 Comment #86 September 26th, 2011 at 9:42 pm
                 I think it might have something to do with our universe’s rate of expansion. It’s fast(strong?)
                 enough to pull distance galaxies away from each other, yet it’s not quite strong enough to pull
                 particles  that  makes  up  milk  and  cof㘶ee  apart  from  each  other,  at  least  not  for  another
                 quintillion years. Imagine a universe with much higher rate of expansion, or one with much
                 weaker 4 forces of nature. It wouldn’t necessarily evolve “complex” structures we observe in
                 our universe. Having said that, the idea of “complexity” might very well be in the domain of
                 physics, modeled as a function of other cosmic parameters, and universal peak complexity
                 predicted.
                 Raoul Ohio Says:
                 Comment #87 September 27th, 2011 at 12:09 am
                 As usual, my DAH (Devil’s Advocate Hat) is on. This is convenient, because it allows you to
                 comment on anything without doing the work to really understanding it. Thus I will proceed to
                 disparage the notion of using Kolmogorov Complexity (KC) for anything but entertainment.
                 Math is a subject where a couple of interesting deﬁnitions and a few theorems can launch a
                 subﬁeld such as KC. I have never studied KC are tried any calculations that might give me some
                 insight, but a brief reading of the subject suggests that it started as a joke, and today a lot of
                 people are not in on it.
                 My intuition starts with the question: “How could you ever actually compute anything?”.
                 Furthermore, the KC of things would change as knowledge in other ﬁelds progresses. For
                 example, what is the KC of
                 δ = 4.66920160910299067185320382…, and
                 α = 2.502907875095892822283902873218… ?
                 These  are  Feigenbaum’s  constants  (http://en.wikipedia.org/wiki/Feigenbaum_constants).  A
                 couple  of  decades  ago,  no  one  knew  anything  about  these  numbers.  With  the  concept  of
                 analyzing discrete dynamical systems by bifurcation diagrams in hand, these can be calculated
                 with a short program. So, did KC(δ) and KC(α) drop dramatically 20 odd years ago?
                 Finally, using KC reminds me of physics arguments that use the wave function for the universe.
                 Sure, there must be such a thing, but it is hard to say much about it.
                 On the other side of the coin, the theorems and proofs in basic KC are rather similar to those in
                 many ﬁelds of TCS, and many SO readers might not think of these as a joke.
                 BTW, V.I. Arnol’d, who made major contributions to many areas of math (and died just last
                 year) was a student of Kolmogorov. Does anyone know if he discussed KC?
                 Raoul Ohio Says:
                 Comment #88 September 27th, 2011 at 12:40 am
                 (continuation) I failed to include a key point:
                 Given that there is a recently discovered short program for δ and α, now take any arbitrary
                 string S, and calculate KC(S) and suppose this is a large number. Who is to say that tomorrow a
                 new theory, perhaps arising from algorithms to gamble on FarmVille, won’t provide a short
                 program for S, in which case KC(S) is now a small number? How could you know if this might
                 happen?
                 My intuition is that the entire concept of KC is “ill-posed”, to borrow a term from PDE.
                 In the interest of “full disclosure”, I must mention that often in the past I have thought some
                 topic was a bunch of hooey until I understood it, after which I thought is was profound, just like
                 listening to Lenard Cohen.
                 Scott Says:
                 Comment #89 September 27th, 2011 at 4:05 am
                 Raoul: This is indeed one of those cases where if you understood more, you’d see why your
                 dismissal  was  wrong.  And  unlike  with  (say)  art,  music,  or  religion,  the  reasons why your
                 dismissal is wrong can be articulated in words.
                 Contrary to what you say, K(x) is not undeﬁnable: I’ll deﬁne it right now, as the length of the
                 shortest preﬁx-free program (in some ﬁxed universal programming language) that prints x and
                 then halts! K(x) is uncomputable, but that’s a completely dif㘶erent issue, and something that’s
                 been understood since the 1960s.
                 Basically, what K(x) lets you do is give a clear, observer-independent meaning to the loose
                 notion  of  there  “not  existing  any  patterns”  in  a  string.  Already  from  that  statement,  it’s
                 completely obvious that K(x) is going to be hard to compute—for as you correctly point out,
                 detecting the existence or nonexistence of patterns is hard!
                 (Though contrary to what you say, K(Feigenbaum’s constant) didn’t suddenly become small
                 when Feigenbaum deﬁned the constant, any more than 42038542390523059230 suddenly
                 became composite when I wrote it down, probably for the ﬁrst time in human history! Please
                 don’t tell me that you’re unable to distinguish between mathematical truths and our knowledge
                 of them.)
                 The key point is that, even without being able to compute K(x) for most x’s, you can still use
                 the  deﬁnition  of  K(x)  to  give  meaning  to  hundreds  of  intuitions  that  otherwise  would’ve
                 remained forever at a handwaving level. For example:
                 “The overwhelming majority of strings are patternless.”
                 “If  a  short computer program outputs a patternless string, then it can only be doing so by
                 generating the string randomly.”
                 And many, many less obvious statements—every one of which can be upgraded to a theorem
                 once you have a mathematical deﬁnition of “patternless.”
                 Furthermore,  the  idea  of  Kolmogorov  complexity  has  actually  inspired  some  important
                 experimental  work!  For  example,  if  you  could  compute  K,  then  you  could  compute  the
                 “similarity” between two DNA sequences D1 and D2 by comparing
                 K(D1)+K(D2) to K(D1,D2).
                 Of course you can’t compute K, but you can compute useful upper bounds on it. For example,
                 let G(x) be the number of bits in the gzip compression of the string x. Then comparing
                 G(D1)+G(D2) to G(D1,D2)
                 turns out to be a very useful way to measure similarity between DNA sequences.
                 It’s really no dif㘶erent from how, even though we can never tell whether a curve in the physical
                 world  is  continuous  or  not  (since  that  would  require  inﬁnitely  precise  measurements),  the
                 mathematical theories dealing with continuity (e.g., calculus, topology) can still be used in
                 physics in all sorts of ways.
                 Dr. Ajit R. Jadhav Says:
                 Comment #90 September 27th, 2011 at 4:32 am
                 Dear Scott:
                 I have now gone through your post. Here are my (obviously) layman’s comments. (I am acutely
                 aware of my ignorance of these topics, but, what the heck—this is just a blog-comment.)
                 1. First, look at the broad “structure” of the problem. With the progress of time, the entropy
                 increases  monotonically.  However,  in  order  to  capture  the  “interesting”  thing,  you  need  to
                 construct some other function that ﬁrst increases, slows down its growth, goes through a
                 hump, and then decreases.
                 Flip the graph vertically so that the hump becomes a “U”, and now it’s easier to see that this
                 can be taken as a typical engg. optimization problem: You have to imagine _two_ factors: one
                 that increases monotonically; another that decreases monotonically; then add the two together,
                 and the combined function gives you the required “U” shape. For instance, the total potential
                 energy of a mass-spring system acted upon by an applied force, or the total PE of a diatomic
                 molecule (with attractive and repulsive forces).
                 (Another way to look at the hump is as the function that describes the slope of the curve in the
                 logistic growth model, or of the S-growth curve of biology. However, the engg. optimization
                 version  brings  out  the  necessity  of  having  to  have  two  oppositely  acting  factors  more
                 explicitly/easily.)
                 2. As the spring-mass model shows, you can have one of the factors increasing/decreasing
                 linearly so long as the other factor compensates for it by decreasing/increasing more strongly
                 than linearly, so as to ultimately produce a hump (and not a straight-line).
                 Thus, a linear increase of computational entropy could also be OK.
                 However, it seems obvious that for the physical dif㘶usion process, the entropy should increase
                 logarithmically. (The decreasing factor in dif㘶usion is: the driving force (or the conc. gradient).)
                 3. Continuing with the hand-waving, methinks, in your idea, the algorithm that reconstructs x
                 given  the  sampling  oracle  seems  to  play  the  part  of  measuring  the  “driving  force”  of  the
                 dif㘶usion model.
                 4. As I said, I am quite ignorant of the actual CS theory with which this blog post of yours is
                 mainly concerned. However, since I have already said so much, let me venture some brain-
                 storming, with a goal of getting something concrete out of this reply.
                 Consider a game. Imagine a computer screen having its upper half ﬁlled with a meaningful text,
                 and  an  empty  bottom  half.  Recall  those  viruses  that  would  tear  away  the  on-screen
                 characters/pixel-colors and drop them at random. Instead of characters, suppose the program
                 drops down blocks of them (strings?) after chopping the text at random.
                 Main question: Does this game adequately bring the three cof㘶ee-cups problem into the realm
                 of the theoretical computer science?
                 Secondary  question:  How  does  one  deal  with  those  words  which,  when  chopped,  produce
                 word-fragments that still are meaningful by themselves? Or does one simply wish the problem
                 away  by  simply  redeﬁning  the  problem  to  refer  only  to  the  original  set  of  words  i.e.  by
                 restricting the reference dictionary?
                 5.  Finally,  a  couple  of  asides  (about  which  I  happen  to  be  much  more  conﬁdent):  (i)  The
                 objection you raised concerning the relativistic interconversion of mass and energy is irrelevant
                 to  the  deﬁnitions  of  the  closed  and  isolated  thermodynamic  systems.  Closed  systems  are
                 closed  to  the  ﬂow/exchange  of  matter,  not  of  mass/energy.  (ii)  In  the  three  cof㘶ee-cups
                 problem, inasmuch as you don’t care for any other interaction of the glass-contents with the
                 rest of the universe (and in fact exclusively focus only on the spatial rearrangements of the two
                 constituents (including formation of tendrils, droplets, etc.)), it does qualify to be abstractly
                 described as an isolated system—there is no exchange of anything with the surroundings to be
                 considered here. Think about it this way: The glass-contents ef㘶ectively make for the entirety of
                 your (abstract) universe. Now, recall that the universe as a whole always forms an isolated
                 system.
                 Best,
                 –Ajit.
                 [E&OE]
                 Juan Ramón González Álvarez Says:
                 Comment #91 September 27th, 2011 at 10:20 am
                 By the left picture, I am taking an initial state at t = 0 just when the mixing starts (it seems that
                 you also think the same when write “It’s going to start to mix”). It is rather evident that this is
                 the case when the macroscopic ﬂows are zero and a trivial computation gives dS = 0, which is
                 the condition that DEFINES thermodynamic equilibrium. Indeed, any textbook writting dS >= 0
                 that I know emphasizes that the equality holds for equilibrium states. Evidently for 0 < t = t_rel
                 the system is again in an equilibrium state (t_rel being the characteristic time needed to achieve
                 equilibrium).
                 It  seems  evident  to  me  that  you  are  confounding  equilibrium  with  homogeneous  (an
                 homogeneous system is an equilibrium system but the inverse is not needly true). All the
                 processes  studied  in  equilibrium  thermodynamics  textbooks  belong  to  the  kind  “Initial
                 equilibrium state” –> “Final equilibrium state” and those textbooks contains chapters devoted
                 to the study of mixing proccesses. Those chapters explain how to obtain enthalpy of mixing,
                 the entropy of mixing, etc. as the dif㘶erence between ﬁnal and initial equilibrium states. There
                 is very little more than I can say to, except maybe to draw an entropy vs time plot for your
                 milk-cof㘶ee system :-). Note: your diagram for the evolution of the entropy of Universe is
                 wrong as well.
                 The same about the distinction between “closed” and “isolated” systems. This distinction is
                 standard and appears in any textbook of thermodynamics or chemical thermodynamics that I
                 know (a list can be given under request). Several online resources discuss this also (including
                 the  Wikipedia).  The  fact,  this  is  the  FIRST  time  that  you  hear  about  the  existence  of  this
                 distinction and of the correct form of the second law says a lot of… Your claim of that the
                 Second Law says that “the entropy of any closed system tends to increase with time” is just
                 plain wrong. Fortunately, the same Wikipedia article that you link does not make your notorious
                 mistake, altough some readers could get the false impression that Wikipedia is suporting you.
                 No it is not.
                 Of course, the laws of thermodynamics also work in a relativistic context. Well-informed people
                 as Misner, Thorne, and Wheeler correctly note that the relativistic law (dS/dtau)>=0 is only
                 valid when there is not heat ﬂow in a system at constant composition. I.e. when the system is
                 ISOLATED. In a closed system (dS/dtau) can be positive, negative, or zero (J_S = J_Q/T).
                 Your last remark is also vacuous. First because when studying the thermodynamics of the
                 Universe we study it locally (by technical reasons that I will omit here) and saying that local
                 entropy increases for closed systems is plain wrong. Second, because nobody has proved that
                 Universe as a whole is an isolated system. In fact there are cosmological models where the
                 Universe is  considered  an  open  thermodynamic system and matter born obtained from an
                 initial unstability in a quantum spacetime without singularity involving an intermediate de Sitter
                 regime.
                 Regarding  the  deﬁnition  of  complexity,  your  informational  theory  approach,  even  if  ﬁnally
                 satisfactory  cannot  provide  a  measure  of  the  complexity  of  the  dinamical  laws  nor  of  the
                 involved  time  scales  in  the  dif㘶erent  dinamical  regimes.  For  example,  your  approach  to
                 discretize the “cofee cup” to pick an adjacent cof㘶ee pixel and milk pixel uniformly at random
                 swaping both only works if we want a description of the process for time scales much larger
                 than a tipical memory scale t_mem and if we work with not too large gradients. Better tools are
                 already at your hands.
                 Regards.
                 Pete Says:
                 Comment #92 September 27th, 2011 at 11:19 am
                 There is some cool work on this kind of thing where you try to evaluate something similar to
                 the complextropy of a string x by looking for the smallest probabilistic ﬁnite state machine that
                 can generate x.
                 I  suppose this boils down to roughly the same thing as the paragraph beginning “As a ﬁrst
                 step, let’s use Kolmogorov complexity to deﬁne entropy…”?
                 Dave Doty Says:
                 Comment #93 September 27th, 2011 at 11:30 am
                 Regarding Bennett’s logical depth versus (Moshe Koppel’s original notion of) sophistication:
                 under certain formalizations of the notions, these are actually equivalent. Of course, Koppel’s
                 notion of sophistication looks much dif㘶erent than Antunes and Fortnow. It’s been a while since
                 I’ve read the Antunes/Fortnow paper, so I forget the nuances of the connection to Koppel’s
                 work.
                 In particular, studying inﬁnite sequences instead of ﬁnite strings, one can qualitatively deﬁne
                 some sequences as deep or sophisticated, and the rest as not. There are two ways that Bennett
                 deﬁned depth and two ways that Koppel deﬁned sophistication for inﬁnite sequences. One of
                 these ways results in depth being equivalent to sophistication. The key dif㘶erence is whether we
                 allow short programs for preﬁxes of an inﬁnite sequence S to be any string, or whether we
                 require  that  they  themselves  all  are  preﬁxes  of  a  single  inﬁnite  sequence  X  that  is  a
                 “compressed representation” of S (despite being inﬁnite itself).
                 The  ﬁrst  notion,  strong  depth,  is  this:  an  inﬁnite  sequence  S  is  strongly  deep  if,  for  all
                 constants c, all computable time bounds t:N –> N, and all but ﬁnitely many n, every program
                 that prints S[1..n] in time t(n) is at least c bits larger than K(S[1..n]). Surprisingly, this is shown
                 by  Bennett  (and  later  more  formally  by  Juedes,  Lathrop  and  Lutz,  using  Levin’s  Coding
                 Theorem) to be equivalent to requiring that all t(n)-fast programs for S[1..n] are themselves
                 compressible by at least c bits (i.e., K(p) {0,1}*, and all but ﬁnitely many n, if U(p,x) = S[1..n],
                 then |x| > K(S[1..n]) + c.
                 It  turns  out  that  the  strongly  deep  sequences  coincide  with  the  strongly  sophisticated
                 sequences.
                 The connection between these notions is that each deﬁnes the complexity of S in terms of the
                 Kolmogorov  complexity  of  preﬁxes  of  S.  When  considering  programs  that  deﬁne  the
                 Kolmogorov complexity of various dif㘶erent preﬁxes of S, these programs need not have any
                 relation  to  one  another,  even  though  their  outputs  are  all  preﬁxes  of  the  same  inﬁnite
                 sequence. We could instead require that the short programs we consider are preﬁxes of one
                 another; i.e., we demand that there be a single inﬁnite sequence, the preﬁxes of which can be
                 used to compute preﬁxes of S. This leads to “weak depth” and “weak sophistication”.
                 A sequence is weakly deep if for all constants c, all computable time bounds t:N –> N, and all
                 inﬁnite sequences X, for all but ﬁnitely many n, if U^t(X[1..m]) = S[1..n], then m > K(S[1..n]) +
                 c. This turns out to be equivalent to stating that S is not truth-table reducible to any Martin-
                 Loef random sequence.
                 A sequence is weakly sophisticated if for all constants c, all total programs p, and all inﬁnite
                 sequences X, for all but ﬁnitely many n, if U(p,X[1..m]) = S[1..n], then m > K(S[1..n]). Again, this
                 looks like strong sophistication as deﬁned above, but requiring that all the candidate fast, short
                 programs for preﬁxes of S themselves be preﬁxes of a single inﬁnite sequence X.
                 Philippe Moser and I tried unsuccessfully for a while (speciﬁcally, for the length of one bus ride
                 from Siena to Rome) to prove that weak depth is equivalent to weak sophistication, without
                 success.
                 I’m  not  sure  if  any  of  this  translates  to  meaningful  statements  about  the  depth  or
                 sophistication  of  ﬁnite  strings.  I  know  the  inﬁnite  sequences  versions  of  depth  and
                 sophistication inspired Antunes and Fortnow’s work on these concepts for ﬁnite strings, but
                 they may have made enough changes that the concepts no longer coincide signiﬁcantly.
                 Dave Doty Says:
                 Comment #94 September 27th, 2011 at 11:33 am
                 For some reason the following paragraph was chopped from my comment. Mentally insert it
                 before “It turns out that…” in my post above:
                 What I will call strong sophistication is this (letting U denote a universal TM taking two input
                 strings, p a program and an input for p): an inﬁnite sequence S is strongly sophisticated if, for
                 all constants c, all programs p representing a total computable function p:{0,1}* –> {0,1}*, and
                 all but ﬁnitely many n, if U(p,x) = S[1..n], then |x| > K(S[1..n]) + c.
                 Scott Says:
                 Comment #95 September 27th, 2011 at 11:41 am
                 Juan: The philosopher David Chalmers recently wrote a paper proposing that, when two people
                 are locked in a dispute that they suspect is purely over words, what they should do is ban the
                 problematic words from the conversation and then see whether any substantive disagreement
                 remains. With your kind indulgence, I’d like to use this dispute as a test case for his method.
                 With “closed” vs. “isolated,” we don’t even have to use his method. It’s obvious that I was using
                 the word “closed” to mean exactly the same thing you mean by “isolated,” and that indeed, the
                 weaker property that you mean by “closed” is something that’s completely irrelevant to this
                 discussion  and  that  I  couldn’t  possibly  have  meant.  So  I’m  happy  to  follow  you  and  the
                 Wikipedia entry in using “isolated.”
                 Regarding “equilibrium,” I have to say you’re using that word in a way that I don’t recognize
                 from any discussion of probability  or  ergodic  theory  I’ve  ever  seen,  but  that  you  claim  is
                 standard. Fine. So then let’s use the term “max-entropy” for the type of equilibrium that you
                 don’t get out of once you’re in it: i.e., the type of equilibrium that cof㘶ee is in after it’s stirred
                 but not before, and that the universe will be in after it degenerates into radiation, but was not
                 in at the moment of the big bang. Then anywhere I use the word “equilibrium” in the post, you
                 can substitute “max-entropy.”
                 Now what are the actual, substantive disagreements that remain?
                 Ryan Says:
                 Comment #96 September 27th, 2011 at 2:35 pm
                 Pete:
                 “I suppose this boils down to roughly the same thing as the paragraph beginning “As a ﬁrst
                 step, let’s use Kolmogorov complexity to deﬁne entropy…”?”
                 Not really, since the work you’re referring to (Crutchﬁeld’s epsilon-machines) is only deﬁned
                 for  ensembles whereas Scott’s post here is about individual objects. However, Gell-Mann’s
                 “ef㘶ective complexity”, being the Kolmogorov complexity of the distribution an ensemble is
                 drawn from, has apparently been shown to be essentially equivalent to Crutchﬁeld’s statistical
                 complexity measure (the entropy of the state-occupation probabilities of the epsilon-machine)
                 by Karoline Wiesner in a talk at ECCS’11.
                 Karoline has also recently co-authored a paper on measures of complexity, discussing their
                 pros                                        and                                        cons:
                 http://www.maths.bris.ac.uk/~enxkw/Publications_ﬁles/Ladyman_Complex_2011.pdf .
                 However, like Grassberger ( http://www.springerlink.com/content/l25007637531552j/ ) and
                 many others before, the paper takes the stance that measures of complexity must be for
                 ensembles, not individual objects, and so does not directly address the questions raised in this
                 blog post.
                 Dan Carney Says:
                 Comment #97 September 27th, 2011 at 3:43 pm
                 A genuine confusion: how does this “complexity is highest in the middle” argument work with
                 cyclic systems?
                 (Eg.                 a                  clock                  reaction                  like
                 http://en.wikipedia.org/wiki/Belousov%E2%80%93Zhabotinsky_reaction)
                 Ilja Says:
                 Comment #98 September 28th, 2011 at 1:23 am
                 “And I don’t understand how one could have tachyonic neutrinos without getting CTCs as well
                 —would anyone who accepts that possibility be kind enough to explain it to me?”
                 The straightforward and simple solution is a preferred frame. It is hidden as long as Lorentz
                 symmetry is exact, but becomes observable if there are small violations of Lorentz symmetry.
                 There are independent arguments in favour of a preferred frame anyway, see my homepage.
                 Pieter Adriaans Says:
                 Comment #99 September 28th, 2011 at 6:53 am
                 Peter van Emde Boas drew my attention to this blog. On monday October 2nd Amos Golan and I
                 organize a one day conference on Philosophy of Information at the the Info-Metrics Institute in
                 Washington DC exactly on this issue.
                 See:        http://www.american.edu/cas/economics/info-metrics/workshop/program-2011-
                 october.cfm
                 On background information regarding Kolmogorov complexity and processes.
                 See:
                 P.W. Adriaans , Between Order and Chaos: The Quest for Meaningful Information, Theory of
                 Computing Systems, Volume 45 , Issue 4 (July 2009), Special Issue: Computation and Logic in
                 the Real World; Guest Editors: S. Barry Cooper, Elvira Mayordomo and Andrea Sorbi Pages 650-
                 674, 2009
                 and
                 P.Adriaans  and  P.  Van  Emdo  Boas,  Computation,  Information,  and  the  Arrow  of  Time,  In
