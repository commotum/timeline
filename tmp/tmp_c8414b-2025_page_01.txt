                                                       Product of Experts with LLMs:
                                  Boosting Performance on ARC Is a Matter of Perspective
                                               Daniel Franzen*12 Jan Disselhoff*12 David Hartmann*32
                                         Abstract
                     The Abstraction and Reasoning Corpus (ARC-
                     AGI) poses a significant challenge for large lan-
                     guage models (LLMs), exposing limitations in
                     their abstract reasoning abilities. In this work,
                     we leverage task-specific data augmentations
                     throughout the training, generation, and scoring
                     phases, and employ a depth-first search algo-
                     rithm to generate diverse, high-probability can-
                     didate solutions.    Furthermore, we utilize the
                     LLMnotonlyasageneratorbutalsoasascorer,
                     using its output probabilities to select the most
                     promising solutions.     Our method achieves a
                     score of 71.6% (286.5/400 solved tasks) on the
                     public ARC-AGI evaluation set, demonstrating
                     state-of-the-art performance among publicly avail-
                     able approaches. While concurrent closed-source
                     workhasreported higher scores, our method dis-
                     tinguishes itself through its transparency, repro-                     Figure 1. Example of a typical ARC-AGI task.
                     ducibility, and remarkably low inference cost, av-            illustrated in Figure 1) may appear straightforward to hu-
                     eraging only around 2ct per task on readily avail-            mans, both traditional algorithmic approaches (Wind, 2020)
                                     1
                     able hardware.                                                and contemporary neural architectures (Li et al., 2024) have
                                                                                   struggled to achieve significant success on ARC-AGI, high-
               1. Introduction                                                     lighting potential limitations in current artificial reasoning
                                                                                   methods.
               Large Language Models (LLMs) have demonstrated ex-                   Although scaling up models has undoubtedly yielded sub-
               traordinary capabilities across diverse tasks, from natural         stantial performance gains on many tasks, size alone does
               languageprocessingtocodegeneration. Evenso, evaluating              not fully address the core limitations evident in challenges
               the extent to which these systems possess abstract reasoning        like ARC-AGI. Indeed, the rapid evolution of open-source
         arXiv:2505.07859v2  [cs.CL]  11 Jun 2025abilities continues to pose a major challenge in the artificialsystems – such as LLaMA-3.2-3B (Dubey et al., 2024) and
               intelligence community. The Abstraction and Reasoning               NvidiaNeMo-Minitron-8B(Sreenivasetal.,2024)–demon-
               Corpus (ARC-AGI), introduced by Chollet (2019) and de-              strates that significant capabilities can emerge even at more
               signedtoassesscoreknowledgeandtheabilitytogeneralize                modest scales. This aligns with mounting evidence that
               in AI, exemplifies this difficulty. Although these tasks (as        many perceived shortcomings in large language models
                  *Equal contribution 1Johannes Gutenberg University Mainz         stem from implementation details or suboptimal data repre-
               2Members of “the ARChitects” Kaggle team. 3Lambda, Inc..            sentations rather than from fundamental reasoning deficits
               Correspondence to: Daniel Franzen <dfranzen.it@gmail.com>,          (Singh & Strouse, 2024; Bostrom & Durrett, 2020; Sun
               Jan Disselhoff <JanDissel.it@gmail.com>, David Hartmann             et al., 2023). For instance, Allen-Zhu & Li (2025) observe
               <davidh@lambda.ai>.                                                 that models may be aware of their mistakes without being
               Proceedings of the 42nd International Conference on Machine         able to correct them, while Allen-Zhu & Li (2024) highlight
               Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025         howsubtle data modeling choices can impede fine-tuning
               bythe author(s).                                                    progress. Collectively, these insights suggest that models of-
                   1Weassumeapriceof36ct/hour for a Nvidia 4090 GPU                ten possess the latent capacities needed to tackle ARC-AGI;
                                                                                1
