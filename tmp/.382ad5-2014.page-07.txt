                                Figure 3: Digits obtained by linearly interpolating between coordinates in z space of the full model.
                                               Deepdirected          Deepundirected       Generative            Adversarial models
                                               graphical models      graphical models     autoencoders
                                                                     Inference needed     Enforced tradeoff
                                                                     during training.     between mixing        Synchronizing the
                                 Training      Inference needed      MCMCneededto         and power of          discriminator with
                                               during training.      approximate          reconstruction        the generator.
                                                                     partition function   generation            Helvetica.
                                                                     gradient.
                                               Learned               Variational          MCMC-based            Learned
                                 Inference     approximate           inference            inference             approximate
                                               inference                                                        inference
                                 Sampling      Nodifﬁculties         Requires Markov      Requires Markov       Nodifﬁculties
                                                                     chain                chain
                                                                                          Not explicitly        Not explicitly
                                               Intractable, may be   Intractable, may be  represented, may be   represented, may be
                              Evaluating p(x)  approximated with     approximated with    approximated with     approximated with
                                               AIS                   AIS                  Parzen density        Parzen density
                                                                                          estimation            estimation
                                               Nearly all models     Careful design       Anydifferentiable     Anydifferentiable
                               Modeldesign     incur extreme         needed to ensure     function is           function is
                                               difﬁculty             multiple properties  theoretically         theoretically
                                                                                          permitted             permitted
                            Table2: Challengesingenerativemodeling: asummaryofthedifﬁcultiesencounteredbydifferentapproaches
                            to deep generative modeling for each of the major operations involving a model.
                            6    Advantages and disadvantages
                            Thisnewframeworkcomeswithadvantagesanddisadvantagesrelativetopreviousmodelingframe-
                            works. The disadvantages are primarily that there is no explicit representation of p (x), and that D
                                                                                                                g
                            must be synchronized well with G during training (in particular, G must not be trained too much
                            withoutupdatingD,inordertoavoid“theHelveticascenario”inwhichGcollapsestoomanyvalues
                            of z to the same value of x to have enough diversity to model p   ), much as the negative chains of a
                                                                                           data
                            Boltzmannmachinemustbekeptuptodatebetweenlearningsteps. TheadvantagesarethatMarkov
                            chains are never needed, only backprop is used to obtain gradients, no inference is needed during
                            learning, and a wide variety of functions can be incorporated into the model. Table 2 summarizes
                            the comparison of generative adversarial nets with other generative modeling approaches.
                            The aforementioned advantages are primarily computational. Adversarial models may also gain
                            some statistical advantage from the generator network not being updated directly with data exam-
                            ples, but only with gradients ﬂowing through the discriminator. This means that components of the
                            input are not copied directly into the generator’s parameters. Another advantage of adversarial net-
                            works is that they can represent very sharp, even degenerate distributions, while methods based on
                            Markov chains require that the distribution be somewhat blurry in order for the chains to be able to
                            mixbetweenmodes.
                            7    Conclusions and future work
                            This framework admits many straightforward extensions:
                            1. A conditional generative model p(x | c) can be obtained by adding c as input to both G and D.
                            2. Learned approximate inference can be performed by training an auxiliary network to predict z
                               given x. This is similar to the inference net trained by the wake-sleep algorithm [15] but with
                               the advantage that the inference net may be trained for a ﬁxed generator net after the generator
                               net has ﬁnished training.
                                                                              7
