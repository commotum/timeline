                                           12        Wen. et al.
                                                    Table 2. Evaluation for semantic segmentation on HOI4D dataset [29]
                                             Method            FramesTableGroundMetopeLockerPliersLaptop Safe Pillow Hand mIoU
                                                                                                                      Deposit        and Arm
                                             PSTNet [11]           3    57.45 63.38     83.80 44.69 13.71 35.03        51.55 76.30     40.39    51.81
                                             P4Transformer [10]    1    60.84 71.98     86.69 53.89 34.00 65.89        55.87 52.19     55.10    59.61
                                             P4Transformer [10]    3    63.58 66.60     87.17 58.39 32.29 72.03        65.87 57.41     54.36    61.97
                                             PPTr(ours)            1    67.49 74.92     87.92 62.12 40.06 69.00        71.39 77.18     62.50    68.07
                                             PPTr(ours)            3    66.78 72.76     88.21 60.83 41.22 72.04        73.10 80.64     61.27    68.54
                                             PPTr(ours)           30    67.76 79.55 90.67 59.43 39.43 72.67 73.29 84.13 64.26 70.13
                                           5.2     3D Action Recognition on MAR-Action3D
                                                 Setup. To demonstrate the effect of PPTr, we first conduct experiments
                                           on the 3D Action Recognition task. Followed by P4Transformer, we use the
                                           MAR-Action3D dataset which consists of 567 human body point cloud videos,
                                           including 20 action categories. Our test/train split follows previous work. Each
                                           frame is sampled with 2,048 points. As inputs, point cloud videos are split into
                                           multiple clips. Video-level labels are used as clip-level labels during training. In
                                           order to estimate the video-level probability, we take the mean of all clip-level
                                           probability predictions. We fit the human body point cloud into 4 primitives.
                                           Duetothe small scale of human point cloud videos, we can load the entire point
                                           cloud videos at one time, so we can avoid maintaining the long-term memory
                                           pool in this case. We use the video classification accuracy as the evaluation
                                           metric. We compare our method with the latest 4D backbone for point cloud
                                           video including MeteorNet, PSTNet and P4Transformer.
                                                 Result. As reported in Table 3, when the number of point cloud frames
                                           increases, the classification accuracy can be gradually improved. Our method
                                           outperforms all the state-of-the-art methods, demonstrating that our methods
                                           can better integrate spacial-temporal information.
                                           5.3     Ablation Study and Discussion
                                                 In this section, we first provide an ablation study to verify each component.
                                           Then, we provide more analysis to provide an in-depth understanding of our
                                           framework.
                                           EfÏcacy of intra/inter-primitive Transformer. We run ablation studies
                                           with and without intra/inter-primitive Transformer to quantify its efÏcacy. We
                                           find that PPTr without intra/inter-primitive Transformer results in a 16.73/1.39
                                           accuracy drop on the MSR-Action3D action recognition task. This shows that
                                           the intra-primitive transformer is essential in this task. It not only simplifies
                                           the optimization difÏculty but also aligns similar points, providing good fea-
                                           tures for the subsequent use of the inter-primitive transformer. Inter-primitive
                                           Transformer integrates spatio-temporal information from the entire video, using
