Let’s Verify Step by Step

Hunter Lightman” Vineet Kosaraju® Yura Burda* —_—- Harri Edwards
Bowen Baker Teddy Lee = Jan Leike © John Schulman _Ilya Sutskever

Karl Cobbe*

OpenAI

Abstract

In recent years, large language models have greatly improved in their
ability to perform complex multi-step reasoning. However, even state-
of-the-art models still regularly produce logical mistakes. To train more
reliable models, we can turn either to outcome supervision, which provides
feedback for a final result, or process supervision, which provides feedback
for each intermediate reasoning step. Given the importance of training
reliable models, and given the high cost of human feedback, it is impor-
tant to carefully compare the both methods. Recent work has already
begun this comparison, but many questions still remain. We conduct our
own investigation, finding that process supervision significantly outper-
forms outcome supervision for training models to solve problems from the
challenging MATH dataset. Our process-supervised model solves 78% of
problems from a representative subset of the MATH test set. Additionally,
we show that active learning significantly improves the efficacy of process
supervision. To support related research, we also release PRM800K, the
complete dataset of 800,000 step-level human feedback labels used to train
our best reward model.

2305.20050v1 [cs.LG] 31 May 2023

1 Introduction

Large language models are capable of solving tasks that require complex multi-
step reasoning by generating solutions in a step-by-step chain-of-thought format
(Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022). However, even state-
of-the-art models are prone to producing falsehoods — they exhibit a tendency
to invent facts in moments of uncertainty (Bubeck et al., 2023). These hallu-
cinations (Maynez et al., 2020) are particularly problematic in domains that
require multi-step reasoning, since a single logical error is enough to derail a
much larger solution. Detecting and mitigating hallucinations is essential to
improve reasoning capabilities.

arXiv

*Primary authors. Correspondence to: Karl Cobbe <karl@openai.com>
