                  However, training a DiT using token sequences naively generated from each feature plane of HexPlane could
                  not guarantee high generation quality, mainly due to the absence of modeling spatial and temporal relations
                  among the tokens.
                  PaddedRolloutOperation. Given that the feature planes of HexPlane may share spatial or temporal dimensions,
                  we employ the Padded Rollout Operation (PRO) to systematically arrange all six planes into a unified square
                  feature map, incorporating zero paddings in the uncovered corner areas. As shown in Fig. 4, the dimension of
                  the 2D square feature map is ( X + Z + T ), which minimizes the area for padding, where d , d , and d
                                                  dX    dZ    dT                                                    X Z           T
                  represent the downsampling rates along the X, Z, and T axes, respectively.
                  Subsequently, we follow DiT to first “patchify” the constructed 2D feature map, converting it into a sequence
                  of N = (( X + Z + T )/p)2 tokens, where p is the patch size, chosen so each token holds information from
                            d     d     d
                             X     Z     T
                  one feature plane. Following patchification, we apply the frequency-based positional embeddings to all tokens
                  similar to DiT. Note that tokens corresponding to padding areas are excluded from the diffusion process.
                  Consequently, the proposed PRO offers an efÏcient method for modeling spatial and temporal relationships
                  within the token sequence.
                  Conditional Generation. DiT enables conditional generation through the use of Classifier-Free Guidance
                 (CFG) [12]. To incorporate conditions into the generation process, we designed two branches for condition
                  insertion (see Fig. 5). For any condition c, we use the adaLN-Zero technique from DiT, generating scale
                  and shift parameters from c and injecting them before and after the attention and feed-forward layers. To
                  handle the complexity of image-based conditions, we add a cross-attention block to better integrate the image
                  condition into the DiT block.
                  4.3 DownstreamApplications
                  Beyond unconditional 4D scene generation, we explore novel applications of DynamicCity through conditional
                  generation and HexPlane manipulation.
                  First, we showcase versatile uses of image conditions in the conditional generation pipeline: 1) HexPlane:
                  By autoregressively generating the HexPlane, we extend scene duration beyond temporal constraints. 2)
                  Layout: We control vehicle placement and dynamics in 4D scenes using conditions learned from bird’s-eye
                  view sketches.
                  To manage ego vehicle motion, we introduce two numerical conditioning methods: 3)Command: Controls
                  general ego vehicle motion via instructions. 4) Trajectory: Enables fine-grained control through specific
                  trajectory inputs.
                  Inspired by SemCity [17], we also manipulate the HexPlane during sampling to: 5)Inpaint: Edit 4D scenes
                  by masking HexPlane regions and guiding sampling with the masked areas. For more applications and
                  implementation details, kindly refer to Sec. 7.5 in the Appendix.
                  5 Experiments
                  5.1   ExperimentalDetails
                  Datasets. We train the proposed model on the 1Occ3D-Waymo, 2Occ3D-nuScenes, and 3CarlaSC datasets.
                  The former two from Occ3D [40] are derived from Waymo [37] and nuScenes [6], where LiDAR point clouds
                  have been completed and voxelized to form occupancy data. Each occupancy scene has a resolution of
                  200×200×16, covering a region centered on the ego vehicle, extending 40 meters in all directions and 6.4
                  meters vertically. The CarlaSC dataset [42] is a synthetic occupancy dataset, with a scene resolution of
                  128×128×8, covering a region 25.6 meters around the ego vehicle, with a height of 3 meters.
                  ImplementationDetails. Our experiments are conducted using eight NVIDIA A100-80G GPUs. The global
                  batch size used for training the VAE is 8, while the global batch size for training the DiT is 128. Our latent
                  HexPlane H is compressed to half the size of the input Q in each dimension, with the latent channels C = 16.
                  The weights for the Lovász-softmax and KL terms are set to 1 and 0.005, respectively. The learning rate for
                                −3                                             −4
                  the VAE is 10    , while the learning rate for the DiT is 10   .
                                                                          7
