            APreprint.
                BeyondBruteForce: ANeuro-SymbolicArchitecturefor
                          Compositional Reasoning in ARC-AGI-2
                               1                2              2,3             3
                    AnugyanDas    OmkarGhugarkar    Vishvesh Bhat  Julian McAuley
                                             1IIT Madras
                                            2CoreThink AI
                                    3University of California, San Diego
                                           November4,2025
                                               Abstract
                  This paper investigates the potential of compositional reasoning approaches toward achieving Artificial General
                Intelligence (AGI) over purely test-time scaled solutions, using the Abstraction and Reasoning Corpus (ARC) as a
                testbed for evaluation.
                  Weposit that neither purely connectionist neural architectures nor strictly symbolic reasoning systems are suffi-
                cient to meet the requirements of ARC-AGI-2. Instead, a neuro-symbolic framework integrating subsymbolic per-
                ception with explicit reasoning provides a more comprehensive computational model.
                  In this work, we demonstrate how our neuro-symbolic reasoning framework augments existing large language
                models (LLMs) to tackle ARC-AGI-2 tasks far more effectively. Our system outperforms comparable baselines,
                achieving a score of 24.4%, a relative improvement of approximately +50% over Grok-4 on the ARC-AGI-2 public
                evaluation set. Furthermore, by ensembling our approach with the ARC Lang Solver approach (26.6%) via a meta-
                classifier, we reached a final state-of-the-art score of 30.8%. This reasoning layer not only boosts generalization on
                ARCtasks, solving a broader range of unseen tasks, but also improves interpretability, as each transformation is per-
                ceived and represented symbolically. We further identify key challenges encountered during our research and outline
                promisingdirections where neuro-symbolic reasoning can further advance ARC. Collectively, these innovations chart
                a compelling path forward for advancing neuro-symbolic AGI in ARC and beyond.
            1   Introduction
            Since its release in 2019, the Abstraction and Reasoning Corpus (ARC) has emerged as a premier benchmark for
            evaluating fluid intelligence—the human-like ability to infer abstract rules from a handful of examples and apply them
            in novel situations. The first iteration, ARC-AGI-1, challenged AI systems to generalize beyond pattern recognition,
            demanding symbolic abstraction from minimal examples. While early large language models like GPT-3 performed
            near 0%, and GPT-4 hovered between 5–21%, the field saw a breakthrough when OpenAI’s o3[11] reached nearly
            88%onARC-AGI-1underhigh-computesettings—aremarkableleapfrompreviousbaselines.
               Despite this progress, ARC-AGI-1 exhibited vulnerabilities: some tasks were solvable via brute-force search or
            memorization of visual templates. To address these weaknesses, the ARC Prize released ARC-AGI-2 in March 2025.
            This revamped benchmark maintains the same input–output grid format and core-knowledge constraints but intro-
            duces hundreds of novel, harder puzzles explicitly curated to require multi-step compositional reasoning, context-
            sensitive rule application, and rich symbolic interpretation. Extensive human testing confirmed that every task re-
            mains solvable by at least two people in two attempts or less, with humans achieving near 100% accuracy, while AI
            systems—including high-compute o3—struggle to reach even double-digit success rates on ARC-AGI-2.
               In essence, ARC-AGI-2 refines the evaluation methodology to reduce reliance on brute-force, increases task com-
            plexity, and places stricter demands on generalization and abstraction. The result is a benchmark that remains acces-
            sible to humans but continues to expose the gaps in AI reasoning architectures—a critical challenge for any system
            claiming progress toward AGI.
               Toclosethisgap,weproposeaneuro-symbolicarchitecturepurpose-builtforARC-AGI-2. Ourframeworkmarries
            neural perception with symbolic reasoning, aiming to deliver:
               APreprint.
                    • Fluid abstraction: interpretable transformation from raw grids to symbolic constructs.
                    • Compositional generalization: combining multiple reasoning steps and context-dependent rules.
                    • Robust test-time adaptation: across novel, noisy tasks.
                  Ourcontributions include:
                    • Layeredneuro-symbolicpipelineforvisualreasoning: Weproposeamodulararchitecturecombiningneu-
                      ral object extraction, transformer-based relational inference, and symbolic rule synthesis in a unified reason-
                      ing loop. This design bridges low-level perceptual recognition with high-level program induction, enabling
                      transparent and verifiable task solutions.
                    • Compositional solver for ARC-AGI-2–level complexity: Our system natively handles multi-rule, multi-
                      step, and context-sensitive transformations by dynamically composing symbolic operators rather than enu-
                      merating discrete hypotheses. This allows coherent reasoning across disjoint object groups and conditional
                      contexts within a single task instance.
                    • Preciseruleinductionviasymbolicsearchwithneuralpriors: Weintroduceahybridinferencelayerwhere
                      neural embeddings guide a domain-specific language (DSL)–driven symbolic search, drastically reducing the
                      combinatorial explosion of candidate programs while retaining interpretability and correctness guarantees.
                    • Empirical gains and cross-task generalization: On the ARC-AGI-2 benchmark, our approach achieves
                      over 50%relative improvement over state-of-the-art LLM-guided solvers. We further demonstrate transfer-
                      ability to unseen transformation types, validating its capacity for systematic generalization beyond training
                      distributions.
                    • State-of-the-art empirical gains: On the ARC-AGI-2 benchmark, our standalone approach achieves over
                      50%relative improvement over state-of-the-art LLM-guided solvers. Furthermore, by ensembling our sys-
                      tem with the ARC Lang Solver approach (26.6%) via a meta-classifier, we achieve a final score of 30.83%,
                      establishing a new state-of-the-art. We also demonstrate transferability to unseen transformation types, vali-
                      dating our capacity for systematic generalization.
               2   Related Work
               2.1  ARC-AGI-1
               TheAbstractionandReasoningCorpusforArtificialGeneralIntelligence(ARC-AGI-1)wasintroducedbyChollet[1]
               as a benchmark designed to measure an AI system’s ability to efficiently acquire new skills and solve novel problems.
               Unlike traditional benchmarks that test memorization or pattern matching on large datasets, ARC-AGI-1 presents a
               collection of visual reasoning tasks that require few-shot learning and abstraction capabilities. Each task consists of a
               smallnumberofinput-outputgridexamples,wherethesystemmustinfertheunderlyingtransformationruleandapply
               it to new test inputs. The benchmark explicitly targets core knowledge priors such as objectness, goal-directedness,
               and basic physics and geometry, making it particularly challenging for current deep learning approaches that rely
               heavily on extensive training data. The relatively poor performance of state-of-the-art neural networks on ARC-AGI-1
               has highlighted fundamental limitations in achieving human-like fluid intelligence and has motivated research into
               moresample-efficient and generalizable AI architectures.
               2.2  ARC-AGI-2: AdvancingFluidIntelligence
               Building upon the foundation of ARC-AGI-1, the ARC-AGI-2 benchmark extends and refines the evaluation frame-
               work for abstract reasoning capabilities [8]. ARC-AGI-2 introduces additional complexity dimensions and addresses
               some limitations identified in the original benchmark, including expanded task diversity, more nuanced difficulty
               gradations, and improved evaluation protocols. The updated benchmark places greater emphasis on compositional
               reasoning, hierarchical abstraction, and the ability to combine multiple primitive operations in novel ways. ARC-AGI-
               2 also incorporates lessons learned from the community’s attempts to solve the original ARC challenges, introducing
                   APreprint.
                   tasks that are specifically designed to resist brute-force search methods and solution memorization strategies. This
                   evolution of the benchmark continues to serve as a critical testbed for measuring progress toward artificial general
                   intelligence, particularly in assessing whether systems can demonstrate genuine understanding rather than statistical
                   pattern matching.
                      ApproachestriedonARC-AGIhavelargelyinvolveddiscreteprogramsearchandtheuseoflargelanguagemodels
                   (LLMs)fine-tunedspecificallyonARC-AGItasks,oftenaugmentedwithsyntheticallygenerateddatatoovercomethe
                   limited number of official tasks. Active inference techniques, where the LLM is fine-tuned in real-time on demonstra-
                   tion examples, have unlocked significant performance improvements, as demonstrated by some of the top individual
                   submissions scoring up to 76% [10]. However, progress remains challenging due to the difficulty of tasks designed to
                   resist brute force methods, and the next promising direction is to combine discrete program search with deep learning-
                   driven intuition to better capture general reasoning abilities.
                      Building on the challenges posed by ARC-AGI-2, a variety of methodological approaches can be employed to
                   tackle the benchmark [12]. These include hybrid neuro-symbolic systems that combine perceptual learning with
                   explicit rule-based reasoning, enabling the discovery of underlying abstract transformations from minimal examples.
                   Generative models, particularly transformer-based architectures, can be leveraged to predict output grids by learning
                   patterns across input-output pairs, while attention mechanisms allow the models to focus on relevant substructures
                   and relationships. Reinforcement learning approaches may also be applied, framing task completion as a sequential
                   decision problem where actions correspond to manipulations on the grid. Furthermore, combinatorial search strategies
                   guided by domain-specific heuristics can systematically explore possible transformations while avoiding brute-force
                   enumeration, aligning with ARC-AGI-2’s emphasis on compositionality and hierarchical abstraction. Importantly,
                   successful strategies often integrate multiple paradigms—neural, symbolic, and search-based—to capture both the
                   low-level visual regularities and the high-level abstract rules that characterize ARC-AGI-2 tasks, ensuring solutions
                   generalize beyond simple pattern memorization [12].
                   2.2.1  Description Of The Test Cases
                   ARC-AGI-2, introduced in mid-2025, builds on the original by enhancing task depth and preserving the “easy for
                   humans, hard for AI” philosophy. It comprises:
                         • 1,000 public training tasks providing core knowledge priors.
                         • 120 public evaluation tasks solvable by at least two humans under identical constraints.
                         • Private test sets curated for blind benchmarking in competitions.
                      Humanparticipantsaverage 2.3minutespertask,achievingnear-perfectperformance,whileAIsystems—including
                   frontier LLMs—score in the single-digit percentages.
                   2.2.2  KeyImprovementsOverARC-AGI-1
                         • Greater rule complexity: tasks require symbolic interpretation and multi-step reasoning.
                         • Higher granularity: tasks span a broader range of difficulty and human–AI performance gaps.
                         • Human-validated quality: all tasks are empirically verified as solvable by humans.
                   2.2.3  Challenges of Grid Scale
                   Gridsupto20×20cellsincreasecombinatorialsearchspaceandprecisiondemands. Brute-forceenumerationbecomes
                   infeasible, requiring object-based symbolic abstractions.
                   2.3   Neurosymbolic Systems
                   Neurosymbolic AI represents a paradigm that seeks to combine the strengths of neural networks and symbolic rea-
                   soning systems [2, 4]. This approach acknowledges that while deep learning excels at perception, pattern recognition,
                   and learning from raw data, symbolic systems provide interpretability, logical reasoning, and the ability to work with
                   explicit knowledge representations. Neurosymbolic architectures integrate these complementary capabilities through
                   APreprint.
                   Figure 1: ARC-AGI Neuro-Symbolic Flow: A multi-stage reasoning framework combining object detection, unit
                   pattern discovery, and pattern intersection across examples to enable self-consistent direct prompting
                   various mechanisms, including using neural networks to ground symbols in perception, employing symbolic modules
                   to guide neural learning, or developing hybrid representations that blend continuous and discrete elements [5]. Recent
                   work in this area has explored differentiable reasoning modules, neural theorem provers, and systems that can learn
                   symbolic programs from examples. The neurosymbolic approach has shown particular promise in domains requiring
                   both perception and reasoning, such as visual question answering, robotic planning, and scientific discovery, where
                   pure neural or pure symbolic approaches struggle to achieve human-level performance [3].
                   2.4   CoreThinkGeneralSymbolics
                   CoreThink introduces a novel approach to symbolic reasoning by constructing complex behaviors from a minimal set
                   of fundamental operations. This method emphasizes that intelligence can emerge from a few core primitives system-
                   atically combined. Unlike traditional neural models, CoreThink maintains explicit representations of its reasoning
                   processes, offering transparency and ease of debugging. The system excels in tasks requiring hierarchical problem de-
                   composition, analogical solution transfer across domains, and incremental refinement of symbolic structures through
                   interaction. By preserving detailed traces of its reasoning, CoreThink facilitates introspection and explanation gener-
                   ation, addressing the limitations inherent in black-box machine learning models. This approach aligns with cognitive
                   science theories suggesting that human intelligence relies on a small set of flexible cognitive primitives. CoreThink
                   has demonstrated effectiveness in tasks necessitating systematic generalization and compositional reasoning, areas
                   where end-to-end neural models often fall short.[13]
                   2.5   ARCLangSolver
                   ARCLangSolver[7]isanasynchronouspipelinefortackling Abstraction and Reasoning Corpus (ARC) puzzles with
                   language models. It iteratively prompts models to write instructions, tests those instructions on the training grids,
                   revises the best ideas, and finally applies the strongest instructions to produce two candidate outputs for each test grid.
                   2.5.1  Howthesystemworks
                   Thesystemoperates through several distinct stages:
                       • Datasetloading–Thesrc/run.pyscriptparsesARCchallengeJSONfiles(seedata/arc-prize-20XX/).
                         Challenges are processed in batches with a monitored semaphore so multiple tasks can run in parallel without
          APreprint.
          Figure 2: Hierarchy of 22 unique visual reasoning patterns used in compositional task solving. Each pattern is repre-
          sented as a block containing its description and parameters, with the block width reflecting the number of parameters.
          Patterns are organized under the top-level category ‘Visual Reasoning Patterns’ to highlight their structure and rela-
          tionships.
             exceeding API limits.
            • Instruction generation – For each Step in a RunConfig, get instruction scores prompts an LLM
             (defined by step.instruction model) with the training grids via src/main.py. Each response is
             scored by leave-one-out cross validation, which is another LLM call that follows the instructions.
            • Scoring – The score instructions on challenge function records per-example results, calculates a
             simple cell-wise similarity score, writes attempts to Postgres if NEON DSN is set, and keeps the top instructions
             in memory.
            • Revision and pooling – StepRevision asks the model to repair its own instructions using a rich feedback
             prompt that highlights wrong outputs. StepRevisionPool synthesizes a new plan from the best previous
             instructions and their scores. Both feed back into the scoring loop.
            • Finalpredictions–return answerreplaysthestrongestinstructionswithfinal follow modeltogen-
             erate multiple outputs per test grid. If ground-truth solutions are supplied, evaluate solutions computes
             accuracy; otherwise the guesses are ready for competition submission.
          3  OurApproach: ARC-AGICompositionalReasoning
          Weproposeafour-stageneuro-symbolic(NS)pipelinethatintegrates deterministic symbolic perception with neurally
          guidedreasoningandgeneration. Ourapproachisfoundedontheprincipleofdecomposition: weseparatetheproblem
          of ”what” is in the grid (perception) from ”how” it transforms (reasoning).
            This pipeline, depicted in Figure 1, systematically processes each ARC task:
            1. Symbolic Object Detection: Converts the raw N × M pixel grids into a structured scene graph of symbolic
             objects and their properties.
            2. Neural-Guided Hypothesis Generation: Uses a small and efficient neural model (o4-mini) to scan training
             pairs and propose a set of matching transformations, complete with parameters, from a 22-pattern library.
            3. Symbolic Rule Intersection: Apply logical intersection and synthesis to the hypotheses to find a single, con-
             sistent rule program that explains all training examples.
            4. ConstrainedLLMSolving: Feedstheoriginaltaskplusthesynthesizedrule(asa”SymbolicHint”)intoGrok-
             4, using self-consistency to generate a robust final answer.
            This decomposition enables both interpretable intermediate representations and robust final predictions. It is im-
          portant to note that this solution is highly specialized for the ARC-AGI benchmark and is not intended as a general-
          purpose reasoning system; its design choices, such as the 22-pattern library, are specifically tailored to succeed in the
          ARC-AGIcompetition.
                 APreprint.
                 3.1    Stage 1: Symbolic Object Detection
                 Thefirststageconvertstherawpixelgridintoastructuredsymbolicscenegraph. Thisprocessisentirelydeterministic,
                 ensuringthatidenticalperceptualinputsalwaysproduceidenticalsymbolicoutputs,whichiscriticalforruleinduction.
                     Wefirstprofilethegridtoidentifythebackgroundcolor,whichisdefinedasthemostfrequentpixelcolor(typically
                 ’0’ or black). Then, a Breadth-First Search (BFS) algorithm iterates over all non-background pixels, grouping them
                 into connected components based on 8-way adjacency.
                     For each detected object, we compute a comprehensive feature vector:
                       • Spatial Properties: Coordinates of the boundary box (y    , x   , y   , x    ), centroid (mean x,y) and
                                                                                min   min max max
                         pixel count (area).
                       • Geometric Properties: A pixel-geometry hash (for fast identity comparison) and a list of relative pixel
                         coordinates.
                       • Color Properties: A histogram of colors present within the object.
                       • Topological Properties: We perform cavity detection by running a secondary BFS on the background color
                         within an object’s bounding box. This allows us to distinguish solid shapes from hollow frames, a critical
                         feature in many ARC tasks.
                     TheaveragedimensionofthegridintheARC-AGI-2subsetthatweusedwas20. Thisstagetransformsa20×20
                 grid of 400 pixels into a structured list of, for example, ”3 objects,” each with a precise set of symbolic attributes.
                 This deterministic extraction, which averages ∼15ms per grid, provides a stable symbolic foundation for subsequent
                 reasoning stages, avoiding the stochasticity and brittleness of learned feature extractors.
                 3.2    Stage 2: Neural-Guided Hypothesis Generation
                 Thesecondstageidentifieswhichtransformationsmightexplainthechangesbetweenaninputandoutputpair. Instead
                 of a computationally intractable brute-force search, we use o4-mini as a fast, neurally-guided hypothesis generator.
                     Wedefinealibraryof22compositional”UnitPatterns”thatformourDomain-SpecificLanguage(DSL)forvisual
                 reasoning(seeFigure2andAppendixA).Thesepatternsaremorecomplexthansimpleatomicoperationsanddescribe
                 commonARCreasoningtasks. Examplesinclude:
                       • Filling Operations: Horizontal Fill, Vertical Fill,Diagonal Fill,Cavity Fill.
                       • Pattern/Repetition: Creating Patterns based on starting Objects,Alternating Pattern
                         Filling,Pattern Matching Fill / Remove.
                       • ObjectManipulation: Connecting Bridges,Object Translation Based on Goal,Falling
                         Down (Gravity-Effect).
                       • LogicalOperations: Find Objects...           and Color Them,Remove Objects...               in a Particular
                         Sequence,Symmetry-Based Pattern.
                     For each individual training example (or ”sub-question”) (I ,O ), we use a self-consistency (SC) mechanism to
                                                                              i  i
                 robustly identify candidate patterns. We query o4-mini N = 10 times with the same prompt, which includes the
                 symbolic object lists from Stage 1 and the full 22-pattern library (see Appendix B). This prompt specifically asks
                 the model to return a structured JSON object and a ‘reason‘ for its detection. We found this ”chain-of-thought” style
                 requirement, forcing the model to justify its choice, significantly improves accuracy. On our internal validation subset,
                 this method achieved 70% accuracy in identifying the correct primary Unit Pattern.
                     Each of the 10 queries returns a JSON detection, such as:
                 {
                       "pattern_name": "Falling Down (Gravity-Effect)",
                       "pattern_detected": true,
                       "params": {
                             "gravity_direction": "downward",
               APreprint.
                         "collision_map": "horizontal bar"
                         }
                    }
                  After collecting 10 such outputs, we perform an intersection on the detected patterns to find the most stable
               hypotheses. This produces a ranked list of the top 3 most consistently detected patterns for that single sub-question.
               3.3  Stage 3: Pattern Intersection and Constraint Formation
               Thethirdstagesynthesizesasingle,coherentrulesetfromthelistsofhypothesesgeneratedinStage2. Akeychallenge
               in ARC is that a rule must apply to all sub-questions within a task. This stage finds the ”program” that consistently
               explains every sub-question.
                  Thisprocessextendstheintersectionapproachhierarchically. Stage 2 provides a list of the top 3 candidate patterns
               for each individual sub-question (using SC=10). In Stage 3, we perform a final set intersection across these top-3
               lists to find the patterns common to all sub-questions in the task.
                  For example, for a task with k = 3 sub-questions:
                    • Sub-Question1Top3(fromSC=10): {"Cavity Fill","Symmetry-Based Pattern","Horizontal
                      Fill"}
                    • Sub-Question2Top3(fromSC=10): {"Cavity Fill","Symmetry-Based Pattern","Remove
                      Objects..."}
                    • Sub-Question3Top3(fromSC=10): {"Cavity Fill","Symmetry-Based Pattern","Vertical
                      Fill"}
                    • Task-Level Intersection: {"Cavity Fill", "Symmetry-Based Pattern"}
                  This hierarchical intersection robustly identifies a small set of high-probability rules that are valid across the entire
               task. From this final set (here, {"Cavity Fill", "Symmetry-Based Pattern"}), we select the top 3 (or
               fewer) patterns to form the final hint. This ensures that the generated hint captures the complete logic, even if it is
               compositional.
                  The final output is a compact, human-readable ”Symbolic Hint” string. This hint is the critical piece of prior
               knowledge for the final solving stage.
               3.4  Stage 4: LLMSolving with Self-Consistency
               In the final stage, we leverage Grok-4, not as a pure end-to-end reasoner, but as a constrained generative model guided
               byoursymbolicpipeline.
                  Weconstruct a detailed prompt containing:
                 1. All training input/output grid pairs.
                 2. The full test input grid.
                 3. The Symbolic Hint string generated in Stage 3.
                  Thepromptexplicitly instructs Grok-4 to: ”Use the provided symbolic hint to formulate a step-by-step plan. This
               hint is believed to be the correct logic. Apply this plan to the test input grid. Output only the final solved grid in the
               specified format.”
                  This approach anchors the LLM’s reasoning, preventing it from hallucinating incorrect patterns or getting dis-
               tractedbysuperficialfeatures. Ratherthanrelyingonasingle,greedy-decodedoutput,weimplementaself-consistency
               mechanism to improve robustness. We query the model N = 5 times to generate a small ensemble of candidate solu-
               tions.
                  Wethenapplyapixel-wisemajority vote across the 5 candidate grids. For each cell (i,j), we tally the 5 proposed
               colors and select the most frequent one. In our analysis of solved tasks, we found that typically 3-4 of the 5 samples
               werepixel-identical, withthemajorityvoteeffectivelyfilteringoutminorgenerativenoisefromtheremainingsamples.
               This ensemble approach combines Grok-4’s powerful pattern application capabilities with the stabilizing effect of
               democratic aggregation, yielding a final prediction far more reliable than any single model invocation.
                   APreprint.
                   4    Meta-Classifier for Candidate Solution Selection
                   To generate a pool of potential solutions, we first employed two distinct methodologies. We utilized CoreThink’s
                   ARC-AGICompositional Reasoning framework to generate two candidate solutions. Concurrently, we used the ARC
                   Lang Solvertoproduceanadditionaltwocandidatesolutions.
                       With a total of four candidates, we then developed a meta-classifier using Grok 4. The purpose of this classifier
                   is to select the optimal two solutions from the set of four. This aligns with the ARC-AGI-2 evaluation metric, which
                   is pass@2.
                       To train the meta-classifier, we provided it with examples to demonstrate the characteristics of a correct solution.
                   Akeyconstraint for the classifier’s output is that the return format must be enclosed in double asterisks (   ... )
                                                                                                                              **     **
                   to allow for easy location and extraction of the final selected solutions.
                   5    Results
                   Weevaluate our neuro-symbolic flow on the ARC-AGI-2 benchmark and compare its performance against both pure
                   neural approaches and competing hybrid systems. Table 1 presents results on the official leaderboard as of October
                   26, 2025, positioning our method relative to state-of-the-art language models and other neuro-symbolic architectures.
                                                     Team/AISystem                          Score (%)
                                                     HumanPanel                                100.0
                                                     CoreThink Meta-Classifier                  30.8
                                                     J. Berman                                  29.4
                                                     NVARC                                      27.6
                                                     CoreThink Compositional Reasoner           24.4
                                                     GPT-5-Pro                                  18.3
                                                     Grok-4 (Thinking)                          16.0
                                                     Claude Opus 4 (16K)                        8.6
                                                     o3(High)                                   6.5
                                                     o4-mini (High)                             6.1
                                                     Claude Sonnet 4 (16K)                      5.9
                                                     o3-Pro (High)                              4.9
                                                     Gemini 2.5 Pro (32K)                       4.9
                   Table1: ARC-AGI-2leaderboardcomparisonasofNovember03,2025. Ourneuro-symbolicflowachievesthehighest
                   score among all systems, demonstrating the effectiveness of combining symbolic reasoning with neural generation.
                       Our Compositional Reasoner achieves a score of 24.4% on the ARC-AGI-2 benchmark, placing us in the top 3
                   submissions using proprietary models, and our Meta-Classifier achieves as score of 30.8%, establishing a new state-
                   of-the-art performance and surpassing all competing approaches. This represents an 8 percentage point improvement
                   over Grok-4 with extended thinking. Notably, our approach substantially outperforms all frontier language models in
                   their standard configurations, including Claude Opus 4[6], OpenAI’s o3 series, and Gemini 2.5 Pro[9], despite these
                   models having significantly larger parameter counts and longer context windows.
                       The performance gap between our method and pure LLM approaches highlights the fundamental limitations of
                   end-to-end neural reasoning on tasks requiring systematic generalization. While models like Grok-4 and Claude Opus
                   4 possess strong pattern recognition capabilities, they struggle to consistently apply compositional transformation
                   rules across diverse visual reasoning scenarios. Our symbolic intermediate representations enable more reliable rule
                   extraction and application, reducing the brittleness observed in purely neural solutions.
                       Comparedtootherneuro-symboliccompetitors, our four-stage pipeline’s explicit separation of perception, pattern
                   detection, constraint synthesis, and ensemble generation appears to provide stronger inductive biases. The determinis-
                   tic object detection stage eliminates perceptual ambiguity early in the pipeline, while the atomic transformation library
                   constrains the hypothesis space to compositionally interpretable operations. The self-consistency mechanism in our
                   final stage further improves robustness by aggregating multiple solution attempts, mitigating the stochastic errors
                   inherent in LLM generation.
                APreprint.
                   Despite achieving state-of-the-art performance among AI systems, our approach still falls considerably short of
                human-level accuracy at 100%. This substantial gap underscores that abstract visual reasoning remains an open chal-
                lenge, requiring further innovations in both symbolic abstraction mechanisms and neural-symbolic integration strate-
                gies. Future work may explore richer transformation primitives, more sophisticated constraint solvers, and adaptive
                refinement of symbolic representations based on feedback from generation failures.
                6    Ablation Studies
                6.1   Analysis of the Compositional Reasoner framework
                Toquantify the individual contributions of the core components within our Compositional Reasoner (based on Core-
                Think’sframework),weconductedaseriesofablationstudiesontheARC-AGI-2publicevaluationset. Thisreasoner’s
                primary components are: (1) the Symbolic Hints generated by our pattern detection and intersection pipeline (Stages
                1–3), and (2) the Self-Consistency (SC) ensemble voting mechanism used in the final LLM solving stage (Stage 4).
                   Weevaluate four configurations to isolate the impact of these components, tracking not only their effect on accu-
                racy but also on computational cost and latency.
                      • FullModel(Reasoner): Ourcompletereasoner,whereGrok-4receivessymbolichintsandusesself-consistency
                        voting.
                      • w/oSymbolicHints: Weremovethesymbolichints(Stages1–3). Grok-4ispromptedonlywiththerawtask
                        examples, but still utilizes the self-consistency voting mechanism.
                      • w/oSelf-Consistency: Weusethefullpipeline, providing Grok-4 with symbolic hints, but take only a single,
                        greedy-decoded sample as the final answer.
                      • Baseline (LLM Only): We remove both our contributions. Grok-4 is prompted with only the raw task
                        examples and uses a single, greedy-decoded sample.
                 Compositional Reasoner Configuration   Score (%)      Est. Relative Latency        Est. Relative Cost
                 Full Model (Hints + SC)                   24.4     T    +N×T (Highest) C            +N×C (Highest)
                                                                     sym         llm             sym          llm
                 w/oSelf-Consistency (Hints)               20.5       T    +T (Medium)              C    +C (Low)
                                                                        sym    llm                   sym     llm
                 w/oSymbolicHints(SC)                      17.5          N×Tllm(High)                N×Cllm(High)
                 Baseline (LLM Only, greedy)               15.0            T   (Fastest)              C    (Lowest)
                                                                            llm                         llm
                Table2: AblationstudyoftheCompositionalReasonerontheARC-AGI-2publicevaluationset. T     /C    represent
                                                                                                     sym  sym
                the latency/cost of our symbolic hint pipeline. Tllm/Cllm represent the latency/cost of a single Grok-4 inference. N is
                the number of samples for self-consistency.
                   Theresults, presented in Table 2, clearly demonstrate the independent value of each component and the associated
                trade-offs.
                6.2   Analysis of Meta-Classifier Ensemble
                OurfinalsolutionisanensemblemodelthatcombinestheoutputsoftwodistinctsolversusingaMeta-Classifier. The
                scores included in this run come from the most recent Meta-Classifier Ensemble run, and the score variations
                are due to differences in Grok-4’s outputs.
                   Thecomponentsofthis final system are:
                      • Compositional Reasoner: CoreThink’s Compositional Reasoner which scores 20.4% in the latest run. This
                        solver generates 2 candidate solutions.
                      • ARCLangSolver: Aseparate, independent solver that also generates 2 candidate solutions, scoring 26.6%
                        in the latest run.
                   APreprint.
                          • Full Model (Meta-Classifier): A Grok 4 model trained to select the best two solutions from the combined
                            pool of four candidates (two from each solver), scoring 30.8% in the latest run.
                       This ensemble strategy is designed to leverage the different strengths of each solver. The Meta-Classifier is trained
                   with examples to demonstrate the characteristics of a correct solution. A key constraint for the classifier’s output is
                   that the return format must be enclosed in double asterisks (   ... )toallowforeasylocationandextraction.
                                                                                **      **
                                                Solver Configuration                               Score (%)
                                                Full Model (Meta-Classifier Ensemble)                 30.8
                                                ARCLangSolver(Alone)                                  26.6
                                                Compositional Reasoner (Alone, from Table 1)          20.4
                   Table 3: Performance comparison of the final ensemble model against its individual solver components on the ARC-
                   AGI-2public evaluation set (pass@2). Scores for the Full Model and ARC Lang Solver are illustrative placeholders.
                       AsshowninTable3,theMeta-Classifierensemble(illustrativescoreof30.8%)outperformseitheroftheindividual
                   solvers, demonstrating the effectiveness of combining their distinct approaches. This aligns with the ARC-AGI-2
                   evaluation metric, which is pass@2.
                   6.3    ImpactonPerformance
                   SymbolicHints: Themostsignificant performance drop occurs when the symbolic hints are removed. The score falls
                   from24.4%to17.5%(a7.0percentagepointdecrease),evenwhentherobustself-consistency mechanismis retained.
                   This degradation highlights that the LLM, when relying solely on its own internal reasoning over the raw pixel grids,
                   struggles to identify the correct underlying transformations. The explicit symbolic constraints provided by our pipeline
                   are therefore crucial for guiding the LLM toward the correct solution space and are the primary driver of our system’s
                   performance.
                       Self-Consistency: Removing the self-consistency voting mechanism also results in a notable performance de-
                   crease, with the score dropping from 24.4% to 20.5% (a 4.0 percentage point decrease). This finding indicates that
                   even when provided with strong symbolic hints, the generative process of the LLM remains stochastic. A single,
                   greedy-decoded output may contain errors or fail to perfectly adhere to the provided constraints. The pixel-wise ma-
                   jority voting acts as a vital stabilization layer, filtering out noise and averaging over minor generative errors to produce
                   a more reliable and accurate final grid.
                   6.4    Computational Trade-offs
                   Theperformancegainsofourfullmodelcomeatthehighestcomputationalcost. AsshowninTable2,thetwoprimary
                   cost drivers are the symbolic pipeline and the self-consistency mechanism.
                       SymbolicHintsCost(T          , C    ): This represents a fixed, upfront computational cost for each task. It includes
                                                sym    sym
                   the deterministic object detection (BFS, etc.) and the inference calls to o4-mini to detect the 22 atomic patterns.
                   While not negligible, this cost is additive and significantly smaller than the cost of the main LLM.
                       Self-Consistency Cost (N ×T        , N ×C      ): This is the dominant factor in both latency and cost. By sampling
                                                       llm        llm
                   N times from Grok-4, we multiply the most expensive part of our pipeline. This makes any configuration with self-
                   consistency (the Full Model and the ”w/o Symbolic Hints” model) an order of magnitude slower and more expensive
                   than configurations using a single greedy decode.
                       Collectively, these ablations underscore that our system’s state-of-the-art performance is a result of this high-cost,
                   high-accuracy configuration. However, the w/o Self-Consistency model (Grok-4 + Hints) presents a highly efficient
                   alternative. It achieves a score of 20.5%—a 5.5 percentage point improvement over the baseline—while incurring
                   only a modest, additive computational cost from the symbolic pipeline. This configuration represents an excellent
                   trade-off, securing the majority of the reasoning gains from our symbolic hints without the multiplicative expense of
                   self-consistency.
                   APreprint.
                   7     Conclusion
                   The ARC benchmark continues to reveal the limitations of purely neural or symbolic systems. Neuro-symbolic AI,
                   combining perception and logical reasoning, offers a promising avenue toward AGI-level abstraction and composi-
                   tional generalization. Our results on ARC-AGI-2 show that structured hybrid architectures can significantly advance
                   the state of reasoning systems, pointing toward more interpretable and adaptable forms of general intelligence.
                   8     CodeAvailability
                   Thesource code for this work is available at: https://github.com/CoreThink-AI/arc-agi-2-reasoner
                   9     Acknowledgment
                   The authors would like to thank Ram Shanmugam, Bryan Landers, and Chandra Khatri for their helpful discussions
                   and contributions to this work. This research was supported by CoreThink AI.
                   References
                    [1]   Franc¸ois Chollet. On the Measure of Intelligence. 2019. arXiv: 1911.01547 [cs.AI]. URL: https:
                          //arxiv.org/abs/1911.01547.
                    [2]   Artur d’Avila Garcez et al. Neural-Symbolic Computing: An Effective Methodology for Principled Integration
                          of Machine Learning and Reasoning. 2019. arXiv: 1905.06088 [cs.AI]. URL: https://arxiv.org/
                          abs/1905.06088.
                    [3]   Kexin Yi et al. Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding.
                          2019. arXiv: 1810.02338 [cs.AI]. URL: https://arxiv.org/abs/1810.02338.
                    [4]   Henry Kautz. “The Third AI Summer: AAAI Robert S. Engelmore Memorial Lecture”. In: AI Magazine
                          43.1 (Mar. 2022), pp. 105–125. DOI: 10.1002/aaai.12036. URL: https://ojs.aaai.org/
                          aimagazine/index.php/aimagazine/article/view/19122.
                    [5]   ZenanLietal.SoftenedSymbolGroundingforNeuro-symbolicSystems.2024.arXiv:2403.00323[cs.AI].
                          URL:https://arxiv.org/abs/2403.00323.
                    [6]   Anthropic. System Card: Claude Opus 4 & Claude Sonnet 4. Accessed: 2025-10-10. 2025. URL: https:
                          //www-cdn.anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf.
                    [7]   JeremyBerman.ARCLangSolver.2025. URL:https://jeremyberman.substack.com/p/how-i-
                          got-the-highest-score-on-arc-agi-again.
                    [8]   Francois Chollet et al. ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems. 2025. arXiv: 2505.
                          11831[cs.AI].URL:https://arxiv.org/abs/2505.11831.
                    [9]   G. Comanici et al. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Con-
                          text, and Next Generation Agentic Capabilities. Accessed: 2025-10-10. 2025. URL: https://storage.
                          googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf.
                   [10]   Daniel Franzen, Jan Disselhoff, and David Hartmann. Product of Experts with LLMs: Boosting Performance
                          on ARC Is a Matter of Perspective. 2025. arXiv: 2505.07859 [cs.CL]. URL: https://arxiv.org/
                          abs/2505.07859.
                   [11]   OpenAI. OpenAI o3 and o4-mini System Card. Accessed: 2025-10-06. Apr. 2025. URL: https://cdn.
                          openai.com/pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-
                          system-card.pdf.
                   [12]   ARCPrize.ARCPrizeGuide.Accessed:2025-10-10.2025. URL: https://arcprize.org/guide.
                   [13]   Jay Vaghasiya et al. CoreThink: A Symbolic Reasoning Layer to reason over Long Horizon Tasks with LLMs.
                          2025. arXiv: 2509.00971 [cs.AI]. URL: https://arxiv.org/abs/2509.00971.
        APreprint.
        A AtomicTransformationLibrary(UnitPatterns)
        Our symbolic reasoning engine is built upon a curated library of 22 atomic transformations, or ”Unit Patterns,” that
        serveastheDomain-SpecificLanguage(DSL)forARC.Thesepatterns(visualizedinFigure2)wereidentifiedthrough
        manual analysis of the ARC-AGI-1 and ARC-AGI-2 training sets. They are designed to be a ”core knowledge” set,
        covering the vast majority of primitive and compositional operations seen in the corpus.
         Unlike simple primitives (e.g., Rotate90), these Unit Patterns are more complex, parameterized operations that
        describe a common reasoning process found in ARC tasks. They are the target operations our o4-mini hypothesis
        generator (Stage 2) attempts to detect.
         The22patterns are as follows:
        • Horizontal Fill
          – Description: Extend or fill an object horizontally across contiguous empty or target cells.
          – Parameters:
             * source object: ["line","square","rectangle","cavity"]
             * column index: ["left of an object","right of an object"]
             * fill color: ["based on source","based on some different objects"]
             * sequence: ["based on source width","based on source height"]
             * stop condition: ["another object","grid boundary","specific color"]
             * overlaps: ["keep the latest","no overlaps possible"]
        • Vertical Fill
          – Description: Extend or fill an object vertically across contiguous empty or target cells.
          – Parameters:
             * source object: ["line","square","rectangle","cavity"]
             * row index: ["top of an object","below an object"]
             * fill color: ["based on source color"]
             * sequence: ["based on source width","based on source height"]
             * stop condition: ["another object","grid boundary","specific color"]
        • Connecting Bridges
          – Description: Draw a “bridge” (line/shape) between two objects in a specified color order.
          – Parameters:
             * source object: ["line","square","rectangle","cavity"]
             * target object: ["line","square","rectangle","cavity"]
             * bridge color: ["based on bridge starting point","based on bridge ending
              point","based on cavity inside an object"]
             * connection shape: ["line","triangle","rectangle","circle"]
             * path direction: ["orthogonal","diagonal","based on color sequence"]
             * thickness: ["based on width of cavity","based on width of starting object"]
        • Boundary Attachment Fill
          – Description: Close holes or voids inside an object’s boundary bounding area.
          – Parameters:
             * objects with holes: ["horizontally laid", "vertically laid", "diagonally
              laid"]
             * attachment direction: ["left","right","top","bottom"]
             * fill logic: ["fits in space to form rectangle","gets laid on the object"]
        APreprint.
             * object filled: ["irregular","triangle","rectangle","square"]
        • Diagonal Fill
          – Description: Propagate color or object along a diagonal axis.
          – Parameters:
             * source point or corner: ["L-shaped","rectangle"]
             * direction: ["bottom-right","top-left","top-right","bottom-left"]
             * fill color: ["same as source","complementary to source","change on bounce"]
             * stop condition: ["object obstruction","hit grid boundary"]
        • Pattern Matching Fill / Remove
          – Description: Identify a repeating subpattern and either color it in or erase it.
          – Parameters:
             * template pattern: ["alternate objects", "similar objects", "symmetry via
              some axis"]
             * operation: ["remove cells to match pattern","fill cells to match pattern"]
             * fill color: ["boundary color","pattern color"]
             * tolerance: ["no tolerance","edges are exceptions"]
             * target regions: ["inside a cavity","outside an object"]
        • Creating Patterns based on starting Objects
          – Description: Generate a larger or repeated pattern seeded from one or more “starter” objects.
          – Parameters:
             * seed objects: ["colored cell","rectangle","diagonal"]
             * transformation sequence: ["circular", "straight", "fill all", "towards an
              object"]
             * inter object spacing: ["none","single","multiple","variable"]
             * repeat: ["till filling the cavity","only once"]
             * stopping condition:["reached an object","reached boundary","filled object
              completely"]
        • Find Objects in the Input Image and Color Them
          – Description: Detects all instances of a certain object class and applies a new color.
          – Parameters:
             * object type: ["plus","rectangle","irregular","circle","cell","horizontal
              bar"]
             * new color: ["complements the original color","constant throughout","alternating
              pattern"]
             * detection method: ["exact match","fuzzy","at some location"]
             * overlap policy: ["all unique","overlaps allowed"]
        • Remove Objects from the Output in a Particular Sequence
          – Description: Systematically delete objects one at a time in a defined order.
          – Parameters:
             * object list ordered: ["all in the row","all in a column","same shape"]
             * removal method: ["erase and color","replace with background"]
        APreprint.
             * trigger condition:["based on an object","leftmost","rightmost","topmost",
              "overlaps"]
        • Rearrange the Objects in the Output in a Particular Sequence/Pattern
          – Description: From a set of objects, only retain those in a given order, rearrange the rest.
          – Parameters:
             * keep sequence: ["ascending order of height","descending order of height"]
             * color of object: ["same as in-place object","original color"]
             * pattern: ["to a particular part of another object","to a particular region"]
        • Alternating Pattern Filling
          – Description: Fill cells with two (or more) colors/objects in an alternating rhythm (checkerboard, stripes).
          – Parameters:
             * colors: [["A", "B"],["A", "A", "B"]]
             * pattern type: ["checkerboard","stripe vertical","stripe horizontal"]
             * internal sequence spacing: ["none","singular"]
        • Object Translation Based on Environment Colors
          – Description: Move an object to a place based on the colors surrounding them.
          – Parameters:
             * moving object shape: ["plus","square","rectangle","all cells"]
             * target environment color:["same as moving object","complementary color"]
             * translation vector:["centroid of the environment colors","on top of environment
              color"]
             * step size: ["arbitrary","fixed size"]
        • Cavity Fill
          – Description: Fill the cavities inside bigger objects.
          – Parameters:
             * object outline: ["U shaped","V shaped","rectangular","triangle","square"]
             * max indent depth: ["based on available filling material", "till complete
              object"]
             * fill color: ["arbitrary","based on material already present"]
        • Add/Replace an Object
          – Description: Swap out one object for another, preserving position or properties.
          – Parameters:
             * source object: ["horizontal bar","vertical bar","rectangle","square","circle",
              "triangle","irregular"]
             * add replacement object:["horizontal bar","vertical bar","rectangle","square",
              "circle","triangle","cell"]
             * inherit properties:["same midpoint","same centroid","at some location"]
             * additional change: ["add a boundary to new object","do nothing"]
        • Falling Down (Gravity-Effect)
          – Description: Let objects “drop” vertically until they hit another object or the floor.
          – Parameters:
        APreprint.
             * object list: ["cell","square","rectangle"]
             * gravity direction: ["downward"]
             * collision map: ["horizontal bar","vertical bar"]
        • Get Attached to Similar Object
          – Description: Move or grow an object until it contacts another of the same type.
          – Parameters:
             * moving object: ["plus","U shaped","V shaped","square","rectangle","irregular"]
             * target object type: ["rectangle","square","irregular"]
             * attachment rule: ["head on with common color side","fit into cavity"]
             * movement path: ["fixed numeric steps","reach goal"]
        • Object Translation Based on Goal
          – Description: Move objects toward a specified “goal” region or object.
          – Parameters:
             * source object: ["square","rectangle","irregular"]
             * goal location or object: ["square","matching pattern"]
             * pathfinding method: ["straight-line","fixed path"]
             * step count or speed: ["stop on obstacle","stop on goal","fixed"]
        • Object Dismantles
          – Description: Break an object into constituent parts or pixels.
          – Parameters:
             * source object: ["irregular","rectangular","square"]
             * fragment shape: ["individual cells","smaller tiles","break at hit"]
             * dismantle sequence:["outer-to-inner","when hit by other object","symmetric"]
             * dispersion pattern:["momentum conserved","toward hit object","away from
              hit object"]
        • Symmetry-Based Pattern
          – Description: Reflect or rotate objects/patterns around an axis or point.
          – Parameters:
             * symmetry type: ["horizontal","vertical","rotational"]
             * axis or center point: ["horizontal bar","vertical bar","single cell"]
             * object group: ["individual cells","square"]
             * copy mode: ["duplicate","mirror"]
        • Ray-Cast / Ray-Trace Pattern
          – Description: Project a “ray” from a source until it hits a wall or object, marking its path in a shape.
          – Parameters:
             * ray source: ["starting cell","object"]
             * direction: ["horizontal","vertical","diagonal","change on hit"]
             * shape: ["line","triangle","circle","rectangle"]
             * stop condition: ["object","boundary"]
             * mark color: ["same as starting point", "alternating pattern", "change on
              hit","based on other objects"]
        APreprint.
        • Scattering Pattern
          – Description: Project a scatter-like pattern which is triangular in shape with staircase-like edges, and fills all
           the cells in its path.
          – Parameters:
             * source: ["starting cell","object"]
             * direction: ["horizontal","vertical","diagonal","radially outwards"]
             * shape: ["triangle"]
             * stop condition: ["object","boundary"]
             * mark color: ["same as starting point", "alternating pattern", "change on
              hit","based on other objects"]
             * boundary: ["single cell thickness of different color than the pattern",
              "multi cell thickness of different color than the pattern"]
             * edge pattern: ["staircase with a width ’w’ and height ’h’, where ’w’ and
              ’h’ are number of cells"]
        • Patterns formed using small objects
          – Description: Spatial patterns and color scheme formed by smaller objects.
          – Parameters:
             * small object type: ["small adjacent objects","parts of a bigger object"]
             * small pattern type: ["spatial pattern and/or color scheme pattern formed
              by smaller distinct objects","coloring scheme pattern formed inside a
              object"]
             APreprint.
             B PromptTemplates
             B.1  Solver Prompt Template
             This prompt is used to guide the model in performing step-by-step reasoning and transformation inference based on
             provided Input–Output grid examples and structured hints.
             Example {i}:
             Input:
             {input_viz}
             Output:
             {output_viz}
             """
             solver_prompt_template = """Consider the following examples:
             {examples}
             Based on the pattern in the examples, what would the output for the following
             test input be?
             Test input:
             {test_input_viz}
             Test output:
             (Hint: {hint})
             """
             new_solver_prompt = """
             You are given a set of Input{Output Examples and a detailed list of
             Transformation Steps (the Hint).
             ## Your Task
             1. Examine the examples to understand exactly how the transformations
             are applied.
             2. Follow the hint steps in order, exactly as described, with
             no additions or omissions.
             3. Use the examples to resolve any remaining ambiguities in the hint.
             4. Apply the same sequence of transformations to the Test Input.
             > Note: The Test question is slightly more challenging than the training
             examples|it may involve rotation invariance and color invariance.
             ---
             ## Inputs Provided
              Examples: ‘{examples}‘
              Hint (Transformation Steps): ‘{hint}‘
              Test Input Visualization: ‘{test_input_viz}‘
             ---
             APreprint.
             ## Output Instructions
             1. Present your full reasoning, detailing how each step from the hint maps
             to the transformation operations you perform.
             2. Do not invent any new rules or skip any hint steps. Ensure the Test Output
             reflects the same behavior demonstrated by the examples.
             3. Embed the Final output in ‘‘‘ ‘‘‘ and use \n for new row and | as column
             separator.
             4. I want you to solve this puzzle step by step. First, restate the problem.
             Then outline your plan. Then execute each step, numbering them, and finally give
             your answer.
             Test Output:
             B.2  Pattern Detection Prompt
             This prompt is designed for object-level pattern recognition, enabling the model to analyze Input and Output grids and
             return structured JSON detections for each candidate pattern.
             Coordinate System:
              Top-left cell is (0, 0). x increases down (rows), y increases right (columns).
             Given:
              Input Grid:
             ‘{}‘
              Output Grid:
             ‘{}‘
              Input Objects: ‘{}‘
              Output Objects: ‘{}‘
              Pattern Specifications: ‘{}‘
             Task: For each pattern in the given list:
             1. Compare Input vs Output objects to identify moves, removals, additions,
             rotations, shifts, duplications, or color changes.
             2. Do note that some objects might combine to form a multi color bigger object.
             3. Decide if the pattern applies.
             4. Provide a concise reason for your decision even if ‘pattern_detected‘
             is ‘false‘. Include the precise reasons for object movements, additions, removals,
             retention. There exist some logic, your task is to find it using the help of patterns
             and params.
             5. List only the matched parameter values under ‘params‘ (use an empty object if none).
             Output: Return only this JSON array (no extra text):
             ‘‘‘json
             [
               {
                 "reason": "<detailed explanation>",
                 "pattern_detected": <true|false>,
                 "pattern_name": "<Pattern Name>",
                 "pattern_description": "<Pattern Specification.description>",
                 "params": { / matched values or {} / }
                 APreprint.
                    },
                    ...
                 ]
                 B.3    Meta-Classifier Prompt
                 This prompt is used by the final ensemble model (Section 2). It instructs Grok-4 to analyze the training task, the test
                 task, and a list of 4 candidate solutions (2 from the Compositional Reasoner, 2 from the ARC Lang Solver) and select
                 the single most likely solution. The pass@2 score is achieved by running this classifier twice.
                 """You are given a some input-output examples, and a test task. You are also
                 given a list of possible solutions.
                 Your job is to figure out the pattern in the input-output examples
                 and select the most likely solution from the list
                 of possible solutions. Output only the solution ID.
                 Input-output examples:
                 {train_tasks_prompt}
                 Test task:
                 {test_task_viz}
                 Possible solutions:
                 {answers_viz}
                 Enter your answer in this format:
                 ***Solution ID***
                 """
                 B.4    Object Detection Prompts
                 These prompts are used in the initial object detection and feature extraction stages (Stage 1) to build a symbolic
                 representation of the grid. They are called by the ‘Grid‘ and ‘BaseObject‘ classes defined in our system to perform
                 fine-grained feature analysis.
                 B.4.1   BackgroundColorDetectionPrompt
                 This prompt is used by the ‘Grid.find-background‘ method to identify the most likely background color, which is
                 essential for segmenting foreground objects.
                 You are given a 2D grid of integers ranging from 0 to 9.
                 Each integer represents a color. The goal is to determine the most likely
                 background color in the grid.
                 The background color is typically:
                 - The color that appears most frequently overall in the grid.
                 - The color that touches the edges of the grid (top, bottom, left, or right).
                 - The color that is not part of compact or enclosed clusters (i.e., likely
                    not part of foreground objects).
                 Use the following decision strategy:
                 1. Start by identifying the most frequent color in the grid.
                 2. If that color also touches one or more edges of the grid, assume it is
                     the background.
                 3. If multiple colors meet these criteria or the result is ambiguous, return -1.
                 APreprint.
                 Only return a JSON object in this exact format:
                 {"background_color": <integer from 0 to 9 or -1>}
                 Do not explain your reasoning before the JSON. Only output the JSON on
                 the first line.
                 Here is the grid:
                 {grid_str}
                 B.4.2   Object Shape Analysis Prompt
                 This prompt is used by the ‘BaseObject.-analyze-shape‘ method to generate a natural language description of an
                 object’s shape, which can be used in downstream reasoning.
                 You are given a rectangular 2D grid of shape {grid_shape}.
                 Each pixel is represented by an integer between 0 and 9, where 0 means black
                 (background), and other values are colors. The pixels in colors represent
                 an object
                 Here is the full grid:
                 {grid_str}
                 The object is defined by the following coordinates:
                 {coord_str}
                 Your task is to analyse the shape of this object, remember that the object
                 can also be irregular and have cavities inside it.
                 Describe the shape of this object in a single statement, try to include as
                 much detail as possible
                 B.4.3   Cavity Detection Prompt
                 This prompt is used by the ‘BaseObject.get-cavities‘ method. It uses the shape analysis (if available) to identify and
                 extract the coordinates of any enclosed cavities (holes) within an object.
                 You are given a rectangular 2D grid of shape {grid_shape}.
                 Each pixel is represented by an integer between 0 and 9, where 0 means black
                 (background), and other values are colors. The pixels in colors represent
                 an object
                 Here is the full grid:
                 {grid_str}
                 The object is defined by the following coordinates:
                 {coord_str}
                 Here is the analysis of the shape of the object: {shape_analysis}
                 Your task is to find out the cavities inside this object and return the
                 coordinates that constitute the object.
                 Return the output in this format:
                 Cavity 1 : <List of coordinates of Cavity 1>
                 Cavity 2 : <List of coordinates of Cavity 2 >
                 .....................
                 Cavity n : <List of coordinates of Cavity n>
                 where ’n’ is the number of cavities you detected. Return the coordinates
                 only, don’t give any explanation.
        APreprint.
        Use only plain ASCII characters, standard spaces, parentheses, and commas
        exactly as shown.
        Do not include any special Unicode spaces or extra formatting.
