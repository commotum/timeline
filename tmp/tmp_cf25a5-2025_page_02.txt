                                                        Test-Time Learning for Large Language Models
                             Table 1. Characteristics of problem settings for adapting trained models to potentially shifted test domains.
                Setting                                           Knowledge Source Data Target Data Training Loss Testing Loss Learning Type
                                                                                              t   t         t   t
                Fine-tuning                                           %           %          x ,y       L(x ,y )          –         Supervised
                                                                                                t
                Retrieval-Augmented Generation (Fan et al., 2024)     !           %            x            –             –              –
                                                                                                t                           t
                Test-Time Adaptation (Wang et al., 2021)              %           %            x            %           L(x )      Unsupervised
                                                                                  s  s          t                       t   s  s
                Test-Time Training (Hardt & Sun, 2024)                !          x ,y          x            %       L(x ;x ,y )          –
                                                                                                t                           t
                Test-Time Learning (Ours)                             %           %            x            %           L(x )     Self-supervised
               methods rely on entropy minimization as the optimization            (LoRA) (Hu et al., 2022) is more effective at mitigating
               objective, which overlooks the autoregressive dependencies          catastrophic forgetting compared to full parameter updates
               within LLMs, limiting its effectiveness in improving perfor-        (see Observation 3). Based on this, we utilize LoRA for
               manceondynamictasks(seeFigure1a). Test-TimeTraining                 TTLparameterupdates, enabling lightweight training and
                                               ¨
               (TTT) (Hardt & Sun, 2024; Hubotter et al., 2024) retrieves          effectively mitigating catastrophic forgetting, in contrast to
               data relevant to the input from the training set or knowledge       updating the full parameters of LLMs. Lastly, we construct
               base during inference to fine-tune the model, improving its         a comprehensive benchmark named AdaptEval for TTL.
               performance in dynamic scenarios. However, these methods            Wesummarizeourmaincontributions as follows:
               assume that the model’s training data or knowledge data is
               accessible, which is often not the case in practice, and they          • Empirical Insights on Input Perplexity Minimiza-
               also incur additional retrieval overhead.                                tion: We empirically demonstrate that output perplex-
               Although recent methods address distributional shifts in test            ity can be reduced by minimizing input perplexity.
               data, they still face the following limitations: 1) Difficulty           Based on this insight, we adopt input perplexity mini-
               in acquiring labeled data: High-quality labeled data for                 mization as the optimization objective, enabling LLMs
               SFTofLLMs,especially for domain-specific tasks, is time-                 to adapt effectively to target domains during test time.
               consuming, and becomes more difficult in online model up-
               dates. 2) Neglecting autoregressive dependencies: Many                 • Sample Efficient Learning Strategy with High-
               existing methods, such as TTA, overlook the autoregressive               Perplexity Focus: We propose a Sample Efficient
               nature of LLMs, leading to potential harm when using en-                 Learning Strategy using a perplexity-based weighting
               tropy minimization for parameter updates. 3) High training               scheme to select high-perplexity test samples for up-
               overhead and catastrophic forgetting: Many methods re-                   date, ensuring efficient utilization of computational re-
               quire substantial computational resources to update model                sources. Moreover, we use LoRAtoenablelightweight
               parameters and may suffer from catastrophic forgetting.                  training and mitigate catastrophic forgetting.
               To address these limitations, we propose a Test-Time                   • Benchmark and Experimental Validation of Test-
               Learning (TTL) method for Large Language Models,                         TimeLearning: Weestablish the AdaptEval bench-
               namely TLM,whichdynamicallyadapts LLMsusingonly                          mark for TTL and demonstrate through experiments
               unlabeled test data. Specifically, we provide empirical evi-             that our TLM improves performance by at least 20%
               dence and theoretical insights to reveal that more accurate              over original LLMs on domain knowledge adaptation.
               autoregressive predictions from LLMs can be achieved by
               minimizing the input perplexity of the unlabeled test data          2. Related Work
               (see Observation 1). Based on this insight, we formulate
               the TTL process of LLMs as input perplexity minimization,           Theadaptability of deep learning models to dynamic and di-
               enabling self-supervised enhancement of LLM performance.            verse real-world environments has emerged as a prominent
               Furthermore, we observe that high-perplexity test samples           focus in recent research. Various methods have been pro-
               contribute more significantly to model updates compared to          posed to enhance model performance under distributional
               low-perplexity samples (see Observation 2). Building on             shifts, as summarized in Table 1.
               this observation, we propose a Sample Efficient Learning            Fine-Tuning adapts pre-trained models to specific tasks
               Strategy that employs a perplexity-based weighting scheme           or domains by updating their parameters, such as LoRA,
               toactivelyselectandemphasizehigh-perplexitytestsamples              with labeled data (Hu et al., 2022; Thirunavukarasu et al.,
               for backpropagation, thereby facilitating efficient parameter       2023; Chen et al., 2024c; Wang et al., 2025b). This ap-
               updates during Test-Time Learning. Moreover, we observe             proach allows models to specialize in domain-specific tasks
               that during the Test-Time Learning, Low-Rank Adaptation             by leveraging transfer learning to refine their capabilities.
                                                                                2
