                           Published as a conference paper at ICLR 2024
                                                                                     ∗
                             Year      Total   25%     Claude 2    GPT-3.5    GPT-4      SWE-Llama13b        SWE-Llama7b
                             2023       244     61       4.51        1.56       0.00           4.07               3.50
                             2022       395     117      4.05        0.85       3.42           2.80               2.46
                             2021       383     102      4.18        0.00       2.94           4.45               2.56
                             2020       427     109      5.15        0.71       0.00           3.96               3.43
                             2019       437     112      5.72        1.49       1.79           4.55               2.21
                             2018       165     37       5.45        0.00       0.00           3.57               2.94
                             <2018       89     36       4.49        0.00       2.78           3.37               1.09
                           Table 21: We present an extended temporal analysis in this table, showing the % resolved for task
                           instances across models in the “Oracle” retrieval setting, separated by different cutoff dates. The
                           Year column refers to the subset of tasks that were created during the specified calendar year. In
                           the Total column, we list the number of tasks that fall within the given year. The 25% column is
                           the same information for the subset that GPT-4 was evaluated on. The remaining model-specific
                           columns contain the % resolved metric.
                                                                              # F2P Tests Pass
                                        # P2P Tests Pass           All                 Partial           None
                                        All                     Resolved         Partially Resolved     No-Op
                                        Partial            Breaking Resolved      WorkinProgress      Regression
                                        None               Breaking Resolved      WorkinProgress      Regression
                           Table 22: We present the 6 possible outcomes for a patch generation that is applied successfully and
                           then executed. The outcomes are distinguished by the number of F2P and P2P tests that pass.
                           successfully. Across all metrics, we find that patch generations across models are much closer in
                           size to the characteristics of average gold edits. While some models still generate fewer lines relative
                           to the corresponding Gold edit (e.g., Claude-2, ChatGPT-3.5, GPT-4), the SWE-Llama models edits
                           are on average longer in most respects.. When considering both Table 8 and Table 24, it becomes
                           clear that models struggle with generating longer output sequences to be correctly formatted patches.
                           Further inspection of such occurrences, as shown in our case studies in Section F, indicate that
                           hallucinations, abiding to existing code style/structure, and referencing long range dependencies
                           correctly are common errors that surface more frequently in longer generations.
                           C.7    SOFTWARE ENGINEERING METRICS
                           Weperformpreliminary evaluations that explore using software engineering metrics to evaluate the
                           efficiency and complexity of large code blocks integrated within a complex codebase. Unlike se-
                           mantic similarity scoring functions for evaluating fluency and surface form likeness that are popular
                           withtraditional NLPbenchmarksandhavebeenadoptedforcodegeneration,metricssuchasCyclo-
                           matic complexity McCabe (1976) and Halstead complexity measures Halstead (1977) are founded
                           upon logical abstractions (e.g., Abstract Syntax Trees) and software principles to quantify the com-
                           plexity, efficiency, and readability of code as a scalar value. The patch generations and SWE-bench
                           evaluation logs are rich sources of information that software engineering metrics and static analyz-
                           ers can readily be applied to. Unlike small, code contest benchmarks where the insights of soft-
                           ware engineering metrics are not meaningful due to the minuscule scope of the target functionality,
                           SWE-bench’s task is complex enough that practitioners can use these tools to gain well-structured,
                           rigorous, and wide-ranging feedback signals on the complexity of a patch generation’s change and
                           its effect on the rest of the codebase.
                           Weinclude our exploratory work here that demonstrates how software engineering metrics can re-
                           liably capture characteristics of code quality, and how comparing these statistics across two patches
                           can provide automatic observations about model capabilities. We use the Radon package, a library
                           for computing different software engineering metrics directly from source code.
                               RadonDocumentation, open-source codebase
                                                                           27
