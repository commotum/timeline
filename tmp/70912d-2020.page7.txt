                                                          Generative Pretraining from Pixels
                                                                             Table 4. Comparing performance on low-data CIFAR-10. By lever-
                                                                             aging many unlabeled ImageNet images, iGPT-L is able to outper-
                                                                             form methods such as Mean Teacher (Tarvainen & Valpola, 2017)
                                                                             and MixMatch (Berthelot et al., 2019) but still underperforms the
                                                                             state of the art methods (Xie et al., 2019; Sohn et al., 2020). Our
                                                                             approach to semi-supervised learning is very simple since we only
                                                                             ﬁt a logistic regression classiﬁer on iGPT-L’s features without any
                                                                             data augmentationorﬁne-tuning-asigniﬁcantdifferencefromspe-
                                                                             cially designed semi-supervised approaches. Other results reported
                                                                             from FixMatch (Sohn et al., 2020).
                                                                                Model             40labels     250 labels  4000 labels
              Figure 4. Comparison of auto-regressive pre-training with BERT    MeanTeacher                   32.3 ± 2.3    9.2 ± 0.2
                                                              2                 MixMatch         47.5 ± 11.5  11.0 ± 0.9    6.4 ± 0.1
              pre-training using iGPT-L at an input resolution of 32 × 3. Blue  iGPT-L           26.8 ± 1.5   12.4 ± 0.6    5.7 ± 0.1
              bars display linear probe accuracy and orange bars display ﬁne-   UDA              29.0 ± 5.9    8.8 ± 1.1    4.9 ± 0.2
              tune accuracy. Bold colors show the performance boost from        FixMatch RA      13.8 ± 3.4    5.1 ± 0.7    4.3 ± 0.1
              ensembling BERT masks. We see that auto-regressive models         FixMatch CTA     11.4 ± 3.4    5.1 ± 0.3    4.3 ± 0.2
              produce much better features than BERT models after pre-training,
              but BERTmodelscatchupafter ﬁne-tuning.
                                                                             Asisstandard in the low-data setting, we sample 5 random
                                                                             subsets and report mean and standard deviation accuracies
                                                                             (Table 4). OnCIFAR-10,weﬁndthatwith4labelsperclass,
              accuracy, only 0.4% behind its auto-regressive counterpart,    weachieve 73.2% accuracy outperforming MixMatch with
              while a fully ﬁne-tuned ImageNet model achieves 66.5%,         muchlowervariance between runs and with 25 labels per
              slightly surpassing auto-regressive performance.               class, we achieve 87.6% accuracy, though still signiﬁcantly
              Finally, because inputs to the BERT model are masked at        lower than the state of the art, FixMatch.
              training time, we must also mask them at evaluation time to    Although we have established that large models are neces-
              keep inputs in-distribution. This masking corruption may       sary for producing good representations, large models are
              hinder the BERT model’s ability to correctly predict image     also difﬁcult to ﬁne-tune in the ultra-low data regime. In-
              classes. Therefore, we also try an evaluation scheme where     deed, we ﬁnd that iGPT-L quickly memorizes a 40-example
              wesample5independentmasksforeachinputandtakethe                training set and fails to generalize well, achieving only
              modal prediction, breaking ties at random. In this setting,    42.1% accuracy. We expect adapting recent approaches
              CIFAR-10results are largely unchanged, but on ImageNet,        to semi-supervised learning will help in this setting.
              wegainalmost1%onourlinearprobesandﬁne-tunes.
              4.7. Low-Data CIFAR-10 Classiﬁcation                           5. Related Work
              Evaluations of unsupervised representations often reuse su-    Many generative models have been developed and evalu-
              pervised learning datasets which have thousands to millions    ated for their representation learning capabilities. Notably,
              of labeled examples. However, a representation which has       GANs(Goodfellowetal., 2014; Radford et al., 2015; Don-
              robustly encoded a semantic concept should be exceedingly      ahue et al., 2016) and VAEs (Kingma & Welling, 2013;
              data efﬁcient. As inspiration, we note that humans are able    Kingmaetal., 2014; Higgins et al., 2017) have been well-
              to reliably recognize even novel concepts with a single ex-    studied.
              ample (Carey and Bartlett 1978). This motivates evaluating     As of yet, most generative model based approaches have
              performance in a low-data regime as well. It is also a more    not been competitive with supervised and self-supervised
              realistic evaluation setting for the potential practical use-  methods in the image domain. A notable exception is Big-
              fulness of an approach since it better matches the common      BiGAN(Donahue&Simonyan,2019)whichﬁrstdemon-
              real-world scenario of an abundance of raw data but a lack     strated that sufﬁciently high ﬁdelity generative models learn
              of labels.                                                     imagerepresentations which are competitive with other self-
              In contrast with recent approaches for low-data classiﬁca-     supervised methods.
              tion, we do not make use of pseudo-labeling or data aug-       Manyself-supervised approaches focus on designing aux-
              mentation. Instead, we work directly on a subset of the raw    iliary objectives which support the learning of useful rep-
              supervised dataset, extracting features using our pre-trained  resentations without attempting to directly model the input
              model, and training a linear classiﬁer on those features.      data. Examples include surrogate classiﬁcation (Dosovit-
