                          Question: <image 1> The Grange, represented by the                         Question: Label the five stages of a bacteriophage infection in the figure.
                          standing figure in the cartoon, received its strongest                     Options:
                          support from which of the following groups?                                (A) A- Attachment. B- Penetration. C- Biosynthesis. D- Maturation.E- Lysis
                          Options:                                                                   (B) A- Penetration. B- Attachment. C- Biosynthesis. D- Maturation.E- Lysis
                          (A)Westernfarmers     (C) Wealthy industrialists                           (C) A- Biosynthesis. B- Penetration. C- Attachment.D- Maturation.E- Lysis
                          (B) Urban immigrants (D) Middle-class managers                             (D) A- Attachment. B- Maturation. C- Biosynthesis.D- Penetration.E- Lysis
                                                                                                                                               Llama3-70B-Instruct: I do not
                          Llama3-70B-Instruct: The Grange, or the Patrons of Husbandry, was an                                                 see the image, but the correct
                          organization founded in the late 19th century to support and advocate for                                            sequence based on the standard
                          the interests of farmers, particularly in the Western United States. So the                                          steps involved in bacteriophage
                          answeris(A)Westernfarmers.                                                                                           infection is likely to be (A).
                        Figure 2: Two MMMUquestions that are answered correctly by a text-only LLM Llama-3-70B Instruct. The model
                        finds shortcuts or correlations in the text question and the candidate options.
                        four models answer correctly across the majority                               standing of models, we introduce a vision-only
                        of trials. We randomly sample 1800 questions from                              input setting in MMMU-Pro. In this setting, the
                        the remaining pool, evenly distributed across 30                               model is presented with a question embedded
                        subjects (60 questions per subject).                                           within a screenshot or photo, without any text ex-
                        AugmentingCandidateOptions: Despite the fil-                                   plicitly fed into the model. To implement this set-
                        tering, some questions can still be answered by text-                          ting, we ask the human annotators to manually
                        only LLMs, often exploiting subtle hints within the                            capture photos and screenshots over a simulated
                        candidate options. To counteract this, we increase                             display environment. This process involves vary-
                        the number of candidate options from four to ten,                              ing the backgrounds, font styles, and font sizes to
                        makingit more challenging for models to rely on                                replicate the diversity of real-world conditions. By
                        guessing. This augmentation is done by human                                   using different combinations of these elements, we
                        experts with the help of GPT-4o, with additional                               create a broad range of visual contexts, ensuring
                        validation steps to ensure the quality and diver-                              that the models are not only challenged by the inte-
                        sity of the options. Specifically, GPT-4o generates                            gration of text and images but also by the variability
                        and Claude 3.5 filters the options, followed by two                            in how this content is presented. Examples of the
                        rounds of human review to refine and verify the                                vision-only input setting are shown in Figure 4.
                        augmented options. This augmentation is done by                                    Themotivation for this setting comes from real-
                        human experts with the help of GPT-4o. During                                  world usage and human cognition. Users often
                        this process, experts also review the original an-                             capture screenshots of questions with both text
                        notated questions to ensure their relevance to the                             and images instead of inputting text separately, re-
                        images and to eliminate any questions that lack a                              flecting a natural tendency to process information
                        clear connection or coherence. This step filters out                           holistically. Humans excel at understanding inte-
                        70questions, and we obtain 1730 questions in total.                            grated visual-textual content, and this setting en-
                                                                                                       couragesmodelstodevelopsimilarcomprehension.
                                                                                                       Bymimickingthis behavior, the vision-only input
                                        Original   w/ Filtering   w/ Option Augmentation
                          50                                                                           setting enhances realism and prepares models for
                          40                                                                           real-world multimodal tasks. Ultimately, we ob-
                          30                                                                           tain 3,460 questions—1,730 in standard format and
                          20                                                                           1,730 as screenshots or photos.
                          10
                                                                                                       3 Experiments
                           0
                                 Llama-3-70B-Ins. Qwen2-72B-Ins. Yi-1.5-34B-Chat Mixtral-8x22B-Ins.
                                                      Text Only LLMs                                   3.1     Experimental Setups
                        Figure 3: Accuracy of text-only LLMs in different sets                         Baselines. To establish a comprehensive under-
                        of MMMUquestions.                                                              standing of MMMU-Pro’s difficulty and to provide
                                                                                                       reference points for future research, we evaluate a
                            As illustrated in Figure 3, these two steps sig-                           diverse set of state-of-the-art multimodal models
                        nificantly reduce the accuracy of text-only models                             as baselines. These models represent a range of
                        attempting to guess the answers.                                               training approaches and capabilities in the field of
                        Enhancing Evaluation with a Vision-Only Set-                                   multimodal AI. Our baseline models include:
                        ting: To further challenge the multimodal under-                                   Proprietary Models: GPT-4o (0513) (OpenAI,
                                                                                                15136
