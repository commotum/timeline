                                                         Journal of Physiology - Paris 100 (2006) 70–87
                                                                                                                            www.elsevier.com/locate/jphysparis
                                              Afree energy principle for the brain
                                                                       *
                                                 Karl Friston , James Kilner, Lee Harrison
                         The Wellcome Department of Imaging Neuroscience, Institute of Neurology, University College London, 12 Queen Square,
                                                                   London WC1N 3B, United Kingdom
          Abstract
             By formulating Helmholtz’s ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference
          andlearningthatcanexplainaremarkablerangeofneurobiologicalfacts:usingconstructsfromstatistical physics, the problems of infer-
          ring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles.
          Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and
          hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a
          dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organisation and
          responses.
             In this paper, we show these perceptual processes are just one aspect of emergent behaviours of systems that conform to a free energy
          principle. The free energy considered here measures the diﬀerence between the probability distribution of environmental quantities that
          act on the system and an arbitrary distribution encoded by its conﬁguration. The system can minimise free energy by changing its con-
          ﬁguration to aﬀect the way it samples the environment or change the distribution it encodes. These changes correspond to action and
          perception respectively and lead to an adaptive exchange with the environment that is characteristic of biological systems. This treatment
          assumes that the system’s state and structure encode an implicit and probabilistic model of the environment. We will look at the models
          entailed by the brain and how minimisation of its free energy can explain its dynamics and structure.
          2006 Published by Elsevier Ltd.
          Keywords: Variational Bayes; Free energy; Inference; Perception; Action; Learning; Attention; Selection; Hierarchical
          1. Introduction                                                              Kording and Wolpert, 2004; Kersten et al., 2004; Friston,
                                                                                         ¨
                                                                                       2005). In everyday life, these rules are applied to informa-
             Our capacity to construct conceptual and mathematical                     tion obtained by sampling the world with our senses. Over
          models is central to scientiﬁc explanations of the world                     the past years, we have pursued this perspective in a Bayes-
          aroundus.Neuroscienceisuniquebecauseitentailsmodels                          ian framework to suggest that the brain employs hierarchi-
          of this model making procedure itself. There is something                    cal or empirical Bayes to infer the causes of its sensations
          quite remarkable about the fact that our inferences about                    (Friston, 2005). The hierarchical aspect is important
          the world, both perceptual and scientiﬁc, can be applied                     because it allows the brain to learn its own priors and,
          to the very process of making those inferences: Many peo-                    implicitly, the intrinsic causal structure generating sensory
          ple now regard the brain as an inference machine that con-                   data. This model of brain function can explain a wide
          forms to the same principles that govern the interrogation                   range of anatomical and physiological aspects of brain sys-
          of scientiﬁc data (MacKay, 1956; Neisser, 1967; Ballard                      tems; for example, the hierarchical deployment of cortical
          et al., 1983; Mumford, 1992; Kawato et al., 1993; Rao                        areas, recurrent architectures using forward and backward
          and Ballard, 1998; Dayan et al., 1995; Friston, 2003;                        connections and functional asymmetries in these connec-
                                                                                       tions (Angelucci et al., 2002a; Friston, 2003). In terms of
           * Corresponding author. Tel.: +44 207 833 7488; fax: +44 207 813 1445.      synaptic physiology, it predicts associative plasticity and,
             E-mail address: k.friston@ﬁl.ion.ucl.ac.uk (K. Friston).                  for dynamic models, spike-timing-dependent plasticity. In
          0928-4257/$ - see front matter  2006 Published by Elsevier Ltd.
          doi:10.1016/j.jphysparis.2006.10.001
                                                         K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87                                          71
             terms of electrophysiology it accounts for classical and                        starting from a selectionist standpoint and ending with
             extra-classical receptive ﬁeld eﬀects and long-latency or                       the implications of the free energy principle in neurobio-
             endogenous components of evoked cortical responses                              logical and cognitive terms. The second section (Sections
             (Rao and Ballard, 1998; Friston, 2005). It predicts the                         8–10) addresses the implementation of free energy minimi-
             attenuation of responses encoding prediction error with                         sation in hierarchical neuronal architectures and provides a
             perceptual learning and explains many phenomena like                            simple simulation of sensory evoked responses. This illus-
             repetition suppression, mismatch negativity and the P300                        trates some of the key behaviours of brain-like systems that
             in electroencephalography. In psychophysical terms, it                          self-organise in accord with the free energy principle. A key
             accounts for the behavioural correlates of these physiolog-                     phenomenon; namely, suppression of prediction error by
             ical phenomena, e.g., priming, and global precedence (see                       top-down predictions from higher cortical areas, is exam-
             Friston, 2005 for an overview).                                                 ined in the third section. In this ﬁnal section (Section 11),
                 It is fairly easy to show that both perceptual inference                    we focus on one example of how neurobiological studies
             andlearning rest on a minimisation of free energy (Friston,                     are being used to address the free energy principle. In this
             2003) or suppression of prediction error (Rao and Ballard,                      example, we use functional magnetic resonance imaging
             1998). The notion of free energy derives from statistical                       (fMRI) of human subjects to examine visually evoked
             physics and is used widely in machine learning to convert                       responses to predictable and unpredictable stimuli.
             diﬃcult integration problems, inherent in inference, into
             easier optimisation problems. This optimisation or free                         3. Theory
             energy minimisation can, in principle, be implemented
             using relatively simple neuronal infrastructures.                                  In this section, we develop a series of heuristics that lead
                 The purpose of this paper is to suggest that inference is                   to a variational free energy principle for biological systems
             just one emergent aspect of free energy minimisation and                        and, in particular, the brain. We start with evolutionary or
             that a free energy principle for the brain can explain the                      selectionist considerations that transform diﬃcult ques-
             intimate relationship between perception and action. Fur-                       tions about how biological systems operate into simpler
             thermore, the processes entailed by the free energy princi-                     questions about the constraints on their behaviour. These
             ple cover not just inference about the current state of the                     constraints lead us to the important notion of an ensemble
             world but a dynamic encoding of context that bears the                          density that is encoded by the state of the system. This den-
             hallmarks of attention and related mechanisms.                                  sity is used to construct a free energy for any system that is
                 The free energy principle states that systems change to                     in exchange with its environment. We then consider the
             decrease their free energy. The concept of free-energy arises                   implications of minimising this free energy with regard to
             in many contexts, especially physics and statistics. In ther-                   quantities that determine the systems (i.e., brains) state
             modynamics, free energy is a measure of the amount of                           and, critically, its action upon the environment. We will
             work that can be extracted from a system, and is useful                         see that this minimisation leads naturally to perceptual
             in engineering applications. It is the diﬀerence between                        inference about the world, encoding of perceptual context
             the energy and the entropy of a system. Free-energy also                        (i.e., attention), perceptual learning about the causal struc-
             plays a central role in statistics, where, borrowing from sta-                  ture of the environment and, ﬁnally, a principled exchange
             tistical thermodynamics; approximate inference by varia-                        with, or sampling of, that environment.
             tional     free    energy     minimization        (also     known as               Under the free energy principle (i.e., the brain changes
             variational Bayes, or ensemble learning) has maximum                            to minimise its free energy), the free energy becomes a
             likelihood and maximum a posteriori methods as special                          Lyapunov function for the brain. A Lyapunov function is
             cases. It is this sort of free energy, which is a measure of                    a scalar function of a systems state that decreases with
             statistical   probability distributions; we apply to the                        time; it is also referred to colloquially as a Harmony func-
             exchange of biological systems with the world. The impli-                       tion in the neural network literature (Prince and Smolen-
             cation is that these systems make implicit inferences about                     sky, 1997). There are many examples of related energy
             their surroundings. Previous treatments of free energy in                       functionals in the time-dependent partial diﬀerential equa-
             inference (e.g., predictive coding) have been framed as                         tions literature (e.g., Kloucek, 1998). Usually, one tries to
             explanations or mechanistic descriptions. In this work,                         infer the Lyapunov function given a systems structure
             we try to go a step further by suggesting that free energy                      and behaviour. In what follows we address the converse
             minimisation is mandatory in biological systems and there-                      problem; given the Lyapunov function, what would sys-
             fore has a more fundamental status. We try to do this by                        tems that minimise free energy look like?
             presenting a series of heuristics that draw from theoretical
             biology and statistical thermodynamics.                                         4. The nature of biological systems
             2. Overview                                                                        Biological systems are thermodynamically open, in the
                                                                                             sense that they exchange energy and entropy with the envi-
                 This paper has three sections. In the ﬁrst (Sections 3–7),                  ronment. Furthermore, they operate far-from-equilibrium
             we lay out the theory behind the free energy principle,                         and are dissipative, showing self-organising behaviour
        72                                  K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87
        (Ashby, 1947; Nicolis and Prigogine, 1977; Haken, 1983;          ment and maintain thermodynamic homeostasis. Similar
        Kauﬀman, 1993). However, biological systems are more             mechanisms can be envisaged easily in an evolutionary set-
        than simply dissipative self-organising systems. They can        ting, wherein systems that avoid phase-transitions will be
        negotiate a changing or non-stationary environment in a          selected above those that cannot (cf., the selection of che-
        way that allows them to endure over substantial periods          motaxis in single-cell organisms or the phototropic behav-
        of time. This endurance means that they avoid phase tran-        iour of plants). By considering the nature of biological
        sitions that would otherwise change their physical structure     systems in terms of selective pressure one can replace diﬃ-
        (interesting exceptions are phase-transitions in develop-        cult questions about how biological systems emerge with
        mental trajectories; e.g., in metamorphic insects). A key        questions about what behaviours they must exhibit to exist.
        aspect of biological systems is that they act upon the envi-     In other words, selection explains how biological systems
        ronment to change their position within it, or relation to it,   arise; the only outstanding issue is what characterises they
        in a way that precludes extremes of temperature, pressure        must possess. The snowﬂake example suggests biological
        and other external ﬁelds. By sampling or navigating the          systems act upon the environment to preclude phase-tran-
        environment selectively they restrict their exchange with        sitions. It is therefore suﬃcient to deﬁne a principle that
        it within bounds that preserve their physical integrity and      ensures this sort of exchange with the environment. We will
        allow them to last longer. A fanciful example is provided        see that free energy minimisation in one such principle.
        in Fig. 1: Here, we have taken a paradigm example of a
        non-biological self-organising system, namely a snowﬂake         4.1. The ensemble density
        andendoweditwithwingsthatenableitto act on the envi-
        ronment. A normal snowﬂake will fall and encounter a                Todevelop these arguments formally, we need to deﬁne
        phase-boundary, at which the environments temperature            some quantities that describe the environment, the system
        will cause it to melt. Conversely, snowﬂakes that can main-      and their interactions. Let # parameterise environmental
        tain their altitude, and regulate their temperature, survive     forces or ﬁelds that act upon the system and k be quantities
        indeﬁnitely with a qualitatively recognisable form. The          that describe the systems physical state. We will unpack
        key diﬀerence between the normal and adaptive snowﬂake           these quantities later. At the moment, we will simply note
        is the ability to change their relationship with the environ-    that they can be very high dimensional and time-varying.
        Fig. 1. Schematic highlighting the diﬀerence between dissipative, self-organising systems like snowﬂakes and adaptive systems that can change their
        relationship to the environment. By occupying a particular environmental niche, biological systems can restrict themselves to a domain of parameter space
        that is far from phase-boundaries. The phase-boundary depicted here is a temperature phase-boundary that would cause the snowﬂake to melt (i.e., induce
        a phase-transition). In this fanciful example, we have assumed that snowﬂakes have been given the ability to ﬂy and maintain their altitude (and
        temperature) and avoid being turned into raindrops.
                                                                         K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87                                                                         73
                 To link these two quantities, we will invoke an arbitrary                                             the ensemble density. Note that the free energy is deﬁned
                 function q(#;k), which we will refer to as an ensemble den-                                           by two densities; the ensemble density q(#;k) and some-
                 sity. This is some arbitrary density function on the environ-                                                                                                             ~
                                                                                                                       thing we will call the generative density pðy;#Þ, from which
                 ments parameters that is speciﬁed or encoded by the                                                   one can generate sensory samples and their causes. The
                 systems parameters. For example, k could be the mean                                                  generative density factorises into a likelihood and prior
                 and variance of a Gaussian distribution on the environ-                                                               ~
                 ments temperature, #. The reason q(#;k) is called an                                                  density pðyj#Þpð#Þ, which specify a generative model. This
                                                                                                                       means the free energy induces a generative model for any
                 ensemble density is that it can be regarded as the probabil-                                          system and an ensemble density over the causes or param-
                 ity density that a speciﬁc environmental state # would be                                             eters of that model. The functional form of these densities
                 selected from an inﬁnite ensemble of environments given                                               is needed to evaluate the free energy. We will consider func-
                 the systems state k. Notice that q(#;k) is not a conditional                                          tional forms that may be employed by the brain in the next
                 density because k is treated as ﬁxed and known (as opposed                                            section. At the moment, we will just note that these func-
                 to a random variable).                                                                                tional forms enable the free energy to be deﬁned as a func-
                     Invoking the ensemble density links the state of the sys-                                                     ~
                                                                                                                       tion Fðy;kÞ of the systems sensory input and state.
                 tem to the environment and allows us to interpret the sys-                                                 The free energy principle states that all the quantities
                 tem as a probabilistic model of the environment. The                                                  that can change; i.e., that are owned by the system, will
                 ensemble density plays a central role in the free energy for-                                         change to minimise free energy. These quantities are the
                 mulation described below. Before describing this formula-                                             system parameters k and the action parameters a. This
                 tion, we need to consider two other sets of variables that                                            principle, as we will see below, is suﬃcient to account for
                 describe, respectively, the eﬀect of the environment on                                               adaptive exchange with the environment which precludes
                 the system and the eﬀect of the system on the environment.                                            phase-transitions. We will show this by considering the
                                                           ~                                   1 ~
                 We will denote these as y and a, respectively. y can be                                               implications of minimising the free energy with respect to
                 thought of as system states that are acted upon by the envi-                                          k and a, respectively.
                 ronment; for example the state of sensory receptors. This
                                    ~
                 means that y can be regarded as sensory input. The action                                             5.1. Perception: optimising k
                 variables a represent the force exerted by eﬀectors that act
                 on the environment to change sensory samples. We will                                                      It is fairly easy to show that optimizing the systems
                 represent this dependency by making the sensory samples                                               parameters with respect to free energy renders the ensemble
                 ~
                 yðaÞ a functional of action. Sometimes, this dependency                                               density the posterior or conditional density of the environ-
                 can be quite simple: For example, the activity of stretch                                             mental causes, given the sensory data. This can be seen by
                 receptors in muscle spindles is aﬀected directly by muscular                                          rearranging Eq. (1) to show the dependence of the free
                 forces causing that spindle to contract. In other cases, the                                          energy on k3
                 dependency can be more complicated: For example, the
                 oculomotor system, controlling eye position, can inﬂuence                                                                ~                              ~
                 the activity of every photoreceptor in the retina. Fig. 2                                             F ¼lnpðyÞþDðqð#;kÞkpð#jyÞÞ                                                                ð2Þ
                 shows a schematic of these variables and how they relate                                              Only the second term is a function of k; this is a Kullback–
                 to each other. With these quantities in place we can now                                              Leibler cross-entropy or divergence term that measures the
                 formulate an expression for the systems free energy.                                                  diﬀerence between the ensemble density and the condi-
                                                                                                                       tional density of the causes. Because this measure is always
                 5. The free energy principle                                                                          positive, minimising the free energy corresponds to making
                                                                                                                       the ensemble density the same as the conditional density. In
                     Thefree energy is a scalar function of the ensemble den-                                          other words, the ensemble density encoded by the systems
                                                                                                               2       state becomes an approximation to the posterior probabil-
                 sity and the current sensory input. It comprises two terms
                            Z                  ~                                                                       ity of the causes of its sensory input. This means the system
                 F ¼ qð#Þlnpðy;#Þd#                                                                                   implicitly infers or represents the causes of its sensory sam-
                                              qð#Þ                                                                     ples. Clearly, this approximation depends upon the physi-
                                    ~
                    ¼hilnpðy;#Þ þhilnqð#Þ                                                                  ð1Þ        cal structure of the system and the implicit form of the
                                            q                   q
                 The ﬁrst is the energy of this system expected under the                                              ensemble density; and how closely this matches the causal
                 ensemble density. The energy is simply the surprise or                                                structure of the environment. Again, invoking selectionist
                 information about the joint occurrence of the sensory input                                           arguments; those systems that match their internal struc-
                 andits causes #. The second term is the negative entropy of                                           ture to the external causal structure of the environment
                                                                                                                       in which they are immersed will be able to minimise their
                                                                                                                       free energy more eﬀectively.
                   1 Tilde denotes variables in generalised coordinates that cover high-
                 order motion; i.e., ~y ¼ y;y0;y00;...This is important when considering the
                                                                                        ~
                 free energy of dynamic systems and enables a to aﬀect y through its high-
                 order temporal derivatives.                                                                             3 We have used the deﬁnition of Kullback–Leibler or relative entropy
                   2                                                                                                                       R      q
                     hÆi  means the expectation under the density q.                                                   here DðqkpÞ¼ qln d#.
                        q                                                                                                                         p
        74                                   K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87
        Fig. 2. Schematic detailing the quantities that deﬁne the free energy. These quantities refer to the internal conﬁguration of the brain and quantities that
                                                                                             ~
        determine how a system is inﬂuenced by the environment. This inﬂuence is encoded by the variables yðaÞ that could correspond to sensory input or any
        other changes in the system state due to external environmental forces or ﬁelds. The parameters a correspond to physical states of the system that change
        the way the external forces act upon it or, more simply, change the way the environment is sampled. A simple example of these would be the state of ocular
                                                     ~
        motorsystemscontrolling the direction of eye gaze. pðyðaÞ;#Þ is the joint probability of sensory input and its causes, #. q(#;k) is called an ensemble density
        and is encoded by the systems parameters, k. These parameters (e.g., mean or expectation) change to minimise free energy, F and, in so doing, make the
        ensemble density an approximate conditional density on the causes of sensory input.
        5.2. Action: optimising a                                         tal forces, because each system is its own existence proof.
                                                                          In short, low free energy systems will look like they are
           Changing the conﬁguration of the system to move or re-         responding adaptively to changes in the external or internal
        sample the environment by minimising the free energy with         milieu, to maintain a homeostatic exchange with the
        respect to the action variables enforces a sampling of the        environment.
        environment that is consistent with the ensemble density.            Systemswhichfail to minimise free energy will have sub-
        This can be seen with a second rearrangement of Eq. (1)           optimal structures for representing the ensemble density or
        that shows how the free energy depends upon a                     inappropriate eﬀectors for sampling the environment.
                     ~                                                    These systems will not restrict themselves to speciﬁc
        F ¼hilnpðyðaÞj#Þ     þDðqð#Þkpð#ÞÞ                        ð3Þ
                             q                                            domains of their milieu and will ultimately experience a
        In this instance, only the ﬁrst term is a function of action.     phase transition.
        Minimising this term corresponds to maximising the log               In summary, the free energy principle can be motivated,
        probability of the sensory input, expected under the ensem-       quite simply, by noting that any system that does not min-
        ble density. In other words, the system will reconﬁgure it-       imise its free energy cannot respond to environmental
        self to sample sensory inputs that are the most likely            changes and cannot have the attribute ‘biological’. It fol-
        under the ensemble density. However, as we have just seen,        lows that minimisation of free energy may be a necessary,
        the ensemble density approximates the conditional distri-         if not suﬃcient, biological characteristic. The mechanism
        bution of the causes given sensory inputs. The inherent cir-      that causes biological systems to minimise their free energy
        cularity obliges the system to fulﬁl its own expectations. In     can be ascribed to selective pressure; operating at somatic
        other words, the system will expose itself selectively to         (i.e., the life time of the organism) or evolutionary time-
        causes in the environment that it expects to encounter.           scales (Edelman, 1993). Before turning to minimisation of
        However, these expectations are limited to the repertoire         free energy in the brain, we now need to unpack the quan-
        of physical states the system can occupy, which specify           tities describing the biological system and relate their
        the ensemble density. Therefore, systems with a low free          dynamics to processes in neuroscience.
        energy can only sample parts of the environment they
        can encode with their repertoire of physical states. Because      5.3. The mean-ﬁeld approximation
        the free energy is low, the inferred causes approximate the
        real environmental conditions. This means the systems                Hitherto, we have treated the quantities describing the
        physical state must be sustainable under these environmen-        environment together. Clearly, these quantities are enor-
                                                 K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87                         75
           mousin number and variety. A key diﬀerence among them               dominate under this approximation, the free energy in
           is the timescales over which they change. We will use this          Eq. (1) is also known as the variational free energy and
           distinction to partition the parameters into three sets             ki are called variational parameters. The mean-ﬁeld factori-
           #=#,#,# that change quickly, slowly and very slowly;                sation means that the mean-ﬁeld approximation cannot
                 u   c  h
           and factorise the ensemble density                                  cover the eﬀect of random ﬂuctuations in one partition,
                   Y                                                           ontheﬂuctuationsinanother. However, this is not a severe
           qð#Þ¼       qð#;kÞ¼qð# ;k Þqð# ;k Þqð# ;k Þð4Þ
                           i i        u  u     c  c     h  h                   limitation because these eﬀects are modelled through mean-
                     i                                                         ﬁeld eﬀects (i.e., through the means or dispersions of ran-
           This also induces a partitioning of the systems parameters          dom ﬂuctuations). This approximation is particularly easy
           into k = ku,kc,kh that encode time-varying partitions of the        to motivate in the present framework because random ﬂuc-
           ensemble density. The ﬁrst set ku, are system quantities that       tuations at fast timescales are unlikely to have a direct
           change rapidly. These could correspond to neuronal activ-           eﬀect at slower timescales.
           ity or electromagnetic states of the brain and change with a           Using variational calculus it is simple to show (see Fris-
           timescale of milliseconds. The causes # they encode, corre-         ton et al., in press) that, under the mean-ﬁeld approxima-
                                                     u
           spond to rapidly changing environmental states, for exam-           tion above, the ensemble density has the following form:
           ple, changes in the environment caused by structural                qð#Þ/expðIð#ÞÞ
           instabilities or other organisms.                                       i            i                                          ð5Þ
                                                                                             ~
                                                                               Ið# Þ¼ lnpðy;#Þ
              The second set kc change more slowly, over a time scale              i    hi
                                                                                                   qni
           ofseconds.Thesecouldcorrespondtothekineticsofmolec-
                                                                               where I(#) is simply the log-probability of # and the data
           ular signalling in neurons; for example calcium-dependent                     i                                     i
           mechanisms underlying short-term changes in synaptic eﬃ-            expected under the ensemble density of the other parti-
           cacyandclassical neural modulatory eﬀects. The equivalent           tions, qni. We will call this the variational energy. From
           partition of causes in the environment may be contextual in         Eq. (5) it is evident that the mode of the ensemble density
           nature, such as the level of radiant illumination or the inﬂu-      maximises the variational energy. The mode is an impor-
           enceofslowlyvaryingexternalﬁeldsthatsetthecontextfor                tant variational parameter. For example, if we assume
           more rapid ﬂuctuations in its state.                                q(#i) is Gaussian, then it is parameterised by two varia-
              Finally, kh represent system quantities that change              tional parameters ki = li,Ri encoding the mode and covari-
           slowly; for example long-term changes in synaptic connec-           ance,   respectively.   This   is  known as the Laplace
           tions during experience-dependent plasticity, or the deploy-        approximation and will be used later. In what follows, we
                                                                               will focus on minimising the free energy by optimizing l;
           ment of axons that change on a neurodevelopmental                                                                                 i
           timescale. The homologous quantities in the environment             noting that there may be other variational parameters
           are invariances in the causal architecture. These could cor-        describing higher moments of the ensemble density, for
           respond to physical laws and other structural regularities          each partition. Fortunately, under the Laplace approxima-
           that shape our interaction with the world.                          tion, the only other variational parameter we require is the
              The factorization in Eq. (4) is, in statistical physics,         covariance. This has a simple form, which is an analytic
           knownasamean-ﬁeldapproximation.Clearlyourapproxi-                   function of the mode and therefore does not need to be rep-
           mation with three partitions is a little arbitrary, but it helps    resented explicitly (see Friston et al., in press and Appendix
           organise the functional correlates of their respective optimi-      A). We now look at the optimisation of the variational
           sationinthenervoussystem.Othertimescaleswouldbenec-                 modes li and the neurobiological and cognitive processes
           essary for other systems like plants. The mean-ﬁeld                 this optimisation entails:
           approximation greatly ﬁnesses the minimisation of free
           energywhenconsideringparticularschemes.Theseschemes                 6.1. Perceptual inference: optimising l
                                                                                                                        u
           usually    employ     variational   techniques.    Variational
           approacheswereintroducedbyFeynman(1972),inthecon-                      Minimising the free energy with respect to neuronal
           text of quantum mechanics using the path integral formula-          states lu means maximising I(#u)
           tion.Theyhavebeenadoptedwidelybythemachinelearning                  l ¼maxIð# Þ
           community (e.g., Hinton and von Cramp, 1993; MacKay,                 u            u
                                                                                             ~                             ~             ~
                                                                               Ið# Þ¼ lnpðyj#Þþlnpð#Þ            ¼ lnpð#jyÞ       þlnpðyÞ
           1995). Established statistical methods like expectation max-            u    hihi
                                                                                                             q q              q q
                                                                                                              c h              c h
           imisation and restricted maximum likelihood (Dempster                                                                           ð6Þ
           etal.,1977;Harville,1977)canbeformulatedintermsoffree
           energy (Neal and Hinton, 1998; Friston et al., in press).           This means that the free energy principle is served when the
                                                                               variational mode of the states (i.e., neuronal activity)
           6. Optimising variational modes                                     changes to maximize its log-posterior, expected under the
                                                                               ensemble density of causes that change more slowly. This
              We now revisit optimisation of the systems parameters            can be achieved, without knowing the true posterior, by
           that underlie perception in more detail, using the mean-            maximisingthe expected log-likelihood and prior that spec-
           ﬁeld approximation. Because variational techniques pre-             ify a probabilistic generative model (second line of Eq. (6)).
                 76                                                                           K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87
                 As mentioned above, this optimisation requires the func-                                                                                   proceed as above; however, we can assume that the context
                 tional form of the generative model. In the next section,                                                                                  changes suﬃciently slowly that we can make the approxi-
                 we will look at hierarchical forms that are commensurate                                                                                   mation l0 ¼ 0. This gives the simple gradient ascent
                                                                                                                                                                                c
                 with the structure of the brain. For now, it is suﬃcient to                                                                                  _
                                                                                                                                                             l ¼joIð# Þ=o#
                 note that the free energy principle means that brain states                                                                                    c                    c           c                                                                                     ð8Þ
                                                                                                                                                                                          ~
                                                                                                                                                             Ið# Þ¼ lnpðy;#Þ
                 will come to encode the most likely state of the environ-                                                                                          c          hi
                                                                                                                                                                                                      q q
                                                                                                                                                                                                        u h
                 ment that is causing sensory input.
                                                                                                                                                            Notethattheexpectationisoverthegeneralisedcoordinates
                 6.2. Generalised coordinates                                                                                                               ofthestatesand,implicitly, an extended period of time over
                                                                                                                                                            whichthestatetrajectoryevolves.4Wewillseebelowthatthe
                        Because states are time-varying quantities, it is impor-                                                                            conditional mode lc encoding context might correspond to
                 tant to consider what their ensemble density covers. This                                                                                  the strength of lateral or horizontal interactions between
                 can cover not just the states at one moment in time but                                                                                    neurons in the brain. These lateral interactions control the
                 their higher-order motion. In other words, a particular                                                                                    relative eﬀects of top-down and bottom-up inﬂuences on
                 state of the environment and its probabilistic encoding in                                                                                 the expected states and therefore control the balance be-
                 the brain can embody dynamics by representing the trajec-                                                                                  tween empirical priors and sensory information, in making
                 tories of states in generalised coordinates. Generalised                                                                                   perceptual inferences. This suggests that attention could be
                 coordinates are a common device in physics and normally                                                                                    thought of in terms of optimizing contextual parameters of
                 cover position and momentum. In the present context, a                                                                                     thissort.Itisimportanttonotethat,inEq.(8),thedynamics
                 generalised state includes the current state, and its general-                                                                             of lc are determined by the expectation under the ensemble
                 ised motion # =u,u0,u00,... with corresponding varia-                                                                                      densityoftheperceptualstates.Thismeansthatitispossible
                                                   u                                                                                                        for the system to adjust its internal representation of proba-
                 tional modes lu;l0;l00;... It is fairly simple to show
                                                             u      u                                                                                       bilistic contingencies in a way that is sensitive to the states
                 (Friston, in preparation) that the extremisation in Eq. (6)                                                                                and their history. A simple example of this, in psychology,
                 can be achieved with a rapid gradient descent, while cou-                                                                                  would be the Posner paradigm, where a perceptual state;
                 pling higher to lower-order motion via mean-ﬁeld terms                                                                                     namelyanorientingcue,directsvisual attention to a partic-
                   _                                           0
                  lu ¼ joIð#uÞ=ouþlu                                                                                                                        ular part of visual space in which a target cue will be pre-
                   _ 0  ¼joIð# Þ=ou0 þl00                                                                                                                   sented. In terms of the current formulation, this would
                  lu                      u                     u                                                                           ð7Þ
                   _ 00 ¼ joIð# Þ=ou00 þ l000                                                                                                               correspond to a state-dependent change in the variational
                  lu                       u                     u                                                                                          parameters encoding context that bias perceptual inference
                   _ 000
                  lu ¼                                                                                                                                   towards the cued part of the sensorium (we will model this
                               _                                                                                                                            in subsequent communication).
                 Here lu mean the rate of change of l and j is some suit-
                                                                                                     u                                                            The key point here is that the mean-ﬁeld approximation
                 able rate constant. The simulations in the next section use
                 this descent scheme, which can be implemented using rela-                                                                                  allows for inferences about rapidly changing perceptual
                 tively simple neural networks. Note, when the conditional                                                                                  states and more slowly changing context to inﬂuence each
                 mode has found the maximum of I(# ), its gradient is zero                                                                                  other through mean-ﬁeld eﬀects (i.e., the expectations in
                                                                                                    u                                                       Eqs. (6) and (8)). This can proceed without explicitly repre-
                 and the motion of the mode becomes the mode of the mo-
                                       _             0                                                                                                      senting the joint distribution in an ensemble density over
                 tion; i.e., lu ¼ lu. However, it is perfectly possible, in gen-                                                                            state and context explicitly (cf., Rao, 2005). Another
                 eralised coordinates, for these quantities to diﬀer, unless
                 there are special constraints. At the level of perception,                                                                                 important interaction between variational parameters
                 psychophysical phenomena, like the motion after-eﬀect,                                                                                     relates to the encoding of uncertainly. Under the Laplace
                 suggest the brain uses generalised coordinates; for example,                                                                               assumption this is encoded by the conditional covariances.
                 onstopping, after a period of looking at the scenery from a                                                                                Critically the conditional covariance of one ensemble is a
                 moving train, the world is perceived as moving but without                                                                                 function of the conditional mode of the others (see Eq.
                 changing its position. The impression that visual objects                                                                                  (A.2) in Appendix A). In the present context, the inﬂuence
                 change their position in accord with their motion is some-                                                                                 of context on perceptual inference can be cast in terms of
                 thing that our brains have learned about the causal struc-                                                                                 encoding uncertainty. We will look at neuronal implemen-
                 ture of the world. It is also something that can be                                                                                        tations of this in the next section.
                 unlearned, temporarily (e.g., perceptual after-eﬀects). We
                 now turn to how these causal regularities are learned.                                                                                     6.4. Perceptual learning: optimising lh
                 6.3. Perceptual context and attention: optimising lc                                                                                             Optimizing the variational mode encoding # corre-
                                                                                                                                                                                                                                                                            h
                                                                                                                                                            sponds to inferring and learning structural regularities in
                        If we call the causes that change on an intermediate
                 timescale, #c contextual, then optimizing lc corresponds
                 to encoding the probabilistic contingencies in which the                                                                                      4 In the simulations below, we take the expectation over peristimulus
                 fast dynamics of the states evolve. This optimization can                                                                                  time.
                                                 K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87                         77
           the causal architecture of the environment. As above, this             Anevolutionary perceptive on this considers the log-evi-
           learning can be implemented as a gradient ascent on                 dence as a lower-bound on free energy,5 which is deﬁned
                                                                                                                                     ~
           I(# ), which represents an expectation under the ensemble           for any systems exchange with the environment yðaÞ and
              h
           density encoding the generalised states and context                 is independent of the systems parameters k. An adaptive
                                                                               system will keep this exchange within bounds that ensure
            _
            l ¼joIð# Þ=o#
             h         h     h                                         ð9Þ     its physical integrity. All this requires is the selection of
                          ~                                                    an appropriate model that renders the log-evidence con-
            Ið# Þ¼ lnpðy;#Þ
               h    hi
                                q q
                                 u c                                           cave within these bounds and processes that minimise its
           In the brain, this descent can be formulated as changes in          free energy (see Fig. 2). Selecting models with the lowest
           connections that are a function of pre-synaptic prediction          free energy will select models that are best able to model
           and post-synaptic prediction error (see Friston, 2003,              their environmental niche and therefore remain within it.
           2005). The ensuing learning rule conforms to simple asso-           Notice that this hierarchical selection rests upon interplay
           ciative plasticity or, in dynamic models, plasticity that           between optimising the parameters of a particular model
           looks like spike-timing-dependent plasticity. In the sense          (to minimise the free energy) and optimising the model
           that optimizing the variational parameters that correspond          per se (using the minimised free energy). Optimisation at
           to connection strengths in the brain encodes causal struc-          both levels is prescribed by the free energy principle. In
           ture in the environment; this instance of free energy mini-         the theory of genetic algorithms, this is called hierarchical
           misation corresponds to learning. The implicit change in            coevolution (e.g., Maniadakis and Trahanias, 2006). A
           the brains connectivity endows it with a memory of past             similar relationship is found in Bayesian inference, where
           interactions with the environment that aﬀects the free en-          model selection is based on a free energy approximation
           ergy dynamics underlying perception and attention. This             to the model evidence that is furnished by optimising the
           is through the mean-ﬁeld eﬀects in Eqs. (6) and (8). Put            parameters of each model to minimise free energy. In short,
           simply, sustained exposure to environmental inputs causes           free energy may be a useful surrogate for adaptive ﬁtness in
           the internal structure of the brain to recapitulate the causal      an evolutionary setting and the marginal likelihood in
           structure of those inputs. In turn, this enables eﬃcient per-       model selection. We introduce model selection because it
           ceptual inference. This formulation provides a transparent          is linked to value learning (Fig. 3).
           account of perceptual learning and categorization, which
           enables the system to remember associations and contin-
           gencies among causal states and context. The extension              7.1. Value-learning: optimising mi
           of these ideas into episodic memory remains an outstand-
           ing challenge.                                                         Value-learning here denotes the ability of a system to
                                                                               learn valuable or adaptive responses. It refers to re-enforce-
           7. Model optimisation                                               ment or emotional learning in the psychological literature
                                                                               and is closely related to dynamic programming (e.g., tem-
              Hitherto, we have only considered the quantitative opti-         poral diﬀerence models) in the engineering and neurosci-
           misation of variational parameters given a particular sys-          ence literature (e.g., Montague et al., 1995; Suri and
           tem and its implicit generative model. Exactly the same             Schultz, 2001). In an early formulation of value-learning
           free energy principle can be applied to optimise the model          (Friston et al., 1994) we introduced the distinction between
           itself. Diﬀerent models can come from populations of sys-           innate and acquired value. Innate value is an attribute of
           tems or from qualitative changes in one system over time.           stimuli or sensory input that releases genetically or epige-
           A model here corresponds to a particular conﬁguration               netically speciﬁed responses that confer ﬁtness. Acquired
           that can be enumerated with the same set of variational             value is an attribute of stimuli that comes to evoke behav-
           parameters. Removing a part of the system or adding,                iours, which ultimately disclose stimuli or cues with innate
           for example, another set of connections, changes the model          value. Acquired value is therefore learnt during neurode-
           and the variational parameters in a qualitative or categor-         velopment and exposure to the environment.
           ical fashion.                                                          The free energy principle explains adaptive behaviour
              Model optimisation involves maximising the marginal              without invoking notions of acquired value or re-enforce-
           likelihood of the model itself. In statistics and machine           ment: From the point of view of the organism, it is simply
           learning this is equivalent to Bayesian model selection,            sampling the environment so that its sensory input con-
           where the free energy can be used to approximate the mar-           forms to its expectations. From its perspective, the envi-
                                ~                                              ronment is a stable and accommodating place. However,
           ginal likelihood, pðyjmiÞ or evidence for a particular model        for someone observing this system, it will appear to
           m. This approximation can be motivated easily using Eq.
             i
           (2): If the system has minimised its free energy and the            respond adaptively to environmental changes and avoid
           divergence term is near zero, then the free energy                  adverse conditions. In other words, it will seem as if certain
           approaches the negative log-evidence for that model.
           Therefore, the model with the smallest free energy has               5 In machine learning, one usually regards the free energy as an upper
           the highest marginal likelihood.                                    bound on the log-evidence.
           78                                           K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87
                                                                                                 In summary, within an organism’s lifetime its parame-
                                                                                             ters minimise free energy, given the model implicit in its
                                                                                             phenotype. At a supraordinate level, the models themselves
                                                                                             may be selected, enabling the population to explore model
                                                                                             space and ﬁnd optimal models. This exploration depends
                                                                                             upon the heritability of key model components, which
                                                                                             include priors about the environmental niche, in which
                                                                                             the organism can operate.
                                                                                                 In this section, we have developed a free energy principle
                                                                                             for the evolution of an organism’s state and structure and
                                                                                             have touched upon minimisation of free energy at the pop-
                                                                                             ulation level, through hierarchical selection. Minimising
                                                                                             free energy corresponds to optimising the organism’s con-
                                                                                             ﬁguration, which parameterises an ensemble density on
                                                                                             the causes of sensory input and optimising the model itself
                                                                                             in somatic or evolutionary time. Factorization of the
           Fig. 3. A schematic showing how the free energy of a system could change          ensemble density to cover quantities that change with dif-
           over time as a function of changes in its exchange with the environment.          ferent timescales provides an ontology of processes that
           This free energy is bounded below by the log-evidence or marginal                 map nicely onto perceptual inference, attention and learn-
           likelihood of the model used to specify the free energy. In this illustration     ing. Clearly, we have only touched upon these issues in a
           we have, somewhat artiﬁcially, broken down the action-perception
           dynamics into two steps: First, the model responds to change its sensory          somewhat superﬁcial way; each deserves a full treatment.
                        *                                                      *             In this paper, we will focus on perceptual inference. In
           input; a ! a and then it adjusts its variational parameters k ! k to infer
           the new input. Both these changes undo the increase in free energy caused         the next section, we consider how the brain might instanti-
                                                 
                                           ~    ~
           by an change in sensory input y ! y . The upper grey line shows the log-          ate the free energy principle with a special focus on the like-
           evidence of a suboptimal model as a function of input. Input could be the         lihood models entailed by its structure.
           state of chemo-receptors and action could correspond to movement along
           the concentration gradients of chemical attractants.
                                                                                             8. Generative models in the brain
           stimulus-response links are selectively re-enforced to ensure                         In this section, we will look at how the rather abstract
           the homeostasis of its internal milieu. However, this rein-                       principles of the previous section might be applied to the
           forcement emerges spontaneously in the larger context of                          brain. We have already introduced the idea that a biologi-
           action and perception under the free energy principle. A                          cal structure encodes a model of the environment in which
           simple example might be an insect that ‘prefers’ the dark;                        it is immersed. We now look at the form of these models
           imagine an insect that has evolved to expect the world is                         implied by the structure of the brain and try to understand
           dark. It will therefore move into shadows to ensure it                            how evoked responses and associative plasticity emerge
           always samples a dark environment. From the point of                              naturally as a minimisation of free energy. In the current
           view of an observer, this adaptive behaviour may be                               formulation, every attribute or quantity describing the
           [mis]construed as light-avoiding behaviour that has been                          brain parameterises an ensemble density on environmental
           reinforced by the value of ‘shadows’.                                             causes. To evaluate the free energy of this density we need
              The above arguments suggest biological systems sample                          to specify the functional form of the ensemble and genera-
           their environment to fulﬁl expectations that are generated                        tive densities. We will assume a Gaussian form for the
           bythemodelimplicit in their structure. The likelihood part                        ensemble densities (i.e., the Laplace approximation), which
           of its model is learnt on exposure to the environment.                            is parameterised by its mode or expectation and covari-
           However, its priors may be inherited. It is these priors that                     ance. The generative density is speciﬁed by its likelihood
           correspond to innate value and are part of the model m                            and priors. Together these constitute a generative model.
                                                                                       i
           per se. Value-learning is often framed in terms of maximis-                       If this model is speciﬁed properly, we should be able to pre-
           ing expected reward or value. However, this is not neces-                         dict, using the free energy principle, how the brain behaves
           sary in the free energy formulation; all that is required is                      in diﬀerent contexts. In a series of previous papers (e.g.,
           that the organism maximises its expectations. Selection will                      Friston and Price, 2001; Friston, 2005) we have described
           ensure these expectations are valuable; through selective                         the form of hierarchical generative models that might be
           pressure on innately valuable priors. Anthropomorphi-                             employed by the brain. In this section, we will cover brieﬂy
           cally, we may not interact with the world to maximise                             the main points again.
           our reward but simply to ensure it behaves as we think it
           should. Only phenotypes with good models and a priori,                            8.1. Hierarchical dynamic models in the brain
           models of a good world will survive. Those who have
           bad models or model, a priori, a bad world will become                                Akeyarchitectural principle of the brain is its hierarchi-
           extinct.                                                                          cal organisation (Zeki and Shipp, 1988; Felleman and Van
                                                         K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87                                          79
             Essen, 1991; Mesulam, 1998; Hochstein and Ahissar, 2002)                        put to any level is the output of the level above. This means
             This organisation has been studied most thoroughly in the                       casual states v(i) link hierarchical levels and dynamic states
             visual system, where cortical areas can be regarded as                          x(i) generate dynamics that are intrinsic to each level. The
             forming a hierarchy; with lower areas being closer to pri-                      random ﬂuctuations can be assumed to be Gaussian with
             mary sensory input and higher areas adopting a multi-                           a covariance encoded by the hyper-parameters #ðiÞ. The
                                                                                                                                                       ðiÞ    c
             modal or associational role. The notion of a hierarchy                          functions at each level are parameterised by #h . This form
             rests upon the distinction between forward and backward                         of hierarchical dynamical model is extremely generic and
             connections (Rockland and Pandya, 1979; Murphy and                              subsumes most models found in statistics and machine
             Sillito, 1987; Felleman and Van Essen, 1991; Sherman                            learning as special cases.
             andGuillery, 1998; Angelucci et al., 2002a). The distinction                       This model speciﬁes the functional form of the genera-
             betweenforwardandbackwardconnectionsisbasedonthe                                tive density in generalised coordinates of motion (see
             speciﬁcity of the cortical layers that are the predominant                      Appendix B) and induces an ensemble density on the gen-
                                                                                                                 ðiÞ    ðiÞ  ðiÞ
                                                                                                                       ~   ~
             sources and origins of extrinsic connections in the brain.                      eralised states #u ¼ x ;v . If we assume neuronal activity
                                                                                                                          ~ðiÞ    ~ðiÞ ~ðiÞ
             Forward connections arise largely in superﬁcial pyramidal                       is the variational mode l         ¼l ;l ofthesestatesandthe
                                                                                                                            u      v     x          ðiÞ        ðiÞ
             cells, in supra-granular layers and terminate in spiny stel-                    variational mode of the model parameters #c and #h cor-
             late cells of layer four or the granular layer of a higher cor-                 responds to synaptic eﬃcacy or connection strengths; we
             tical area (Felleman and Van Essen, 1991; DeFelipe et al.,                      canwritedownthevariationalenergyasafunctionofthese
                                                                                                                                     ð0Þ
             2002). Conversely, backward connections arise largely                           modes using Eq. (5); with y ¼ l
                                                                                                                                     v
             from deep pyramidal cells in infra-granular layers and tar-                                  1 X ðiÞT      ðiÞ ðiÞ
                                                                                               ~                 ~        ~
             get cells in the infra and supra granular layers of lower cor-                  IðluÞ¼             e   P e
             tical areas. Intrinsic connections are both intra and inter                                  2 i
                                                                                                               2                          3
                                                                                                    "#                       
             laminar and mediate lateral interactions between neurons                                             ~ði1Þ    ~ ~ðiÞ    ðiÞ
                                                                                                       ðiÞ        lv     g lu ;lh
                                                                                                      ~
                                                                                              ðiÞ     ev       6                          7
             that are a few millimetres away. Due to convergence and                        ~ ¼                             
                                                                                             e         ðiÞ  ¼4 0ðiÞ             ðiÞ  ðiÞ  5                       ð11Þ
                                                                                                      ~
                                                                                                      ex           ~       ~ ~
             divergence of extrinsic forward and backward connections,                                             lx f lu ;lh
             receptive ﬁelds of higher areas are generally larger than                                   "#
                                                                                                           PðiÞ
             lower areas (Zeki and Shipp, 1988). There is a key                              PðlðiÞÞ¼         z
             functional distinction between forward and backward con-                             c                PðiÞ
                                                                                                                     w
             nections that renders backward connections more modula-                                 ðiÞ
                                                                                                    ~
             tory or non-linear in their eﬀects on neuronal responses                        Here e     is a generalised prediction error for the states at
             (e.g., Sherman and Guillery, 1998). This is consistent with                     the ith level. The generalised predictions of the casual states
                                                                                                                                              ~ðiÞ       ~ðiÞ
             the deployment of voltage sensitive and non-linear NMDA                         and motion of the dynamic states are g                and f , respec-
                                                                                                                                   ~0ðiÞ     0ðiÞ  00ðiÞ 000ðiÞ
             receptors in the supra-granular layers that are targeted by                     tively (see Appendix B). Here, l           ¼lx ;lx ;lx ;...rep-
                                                                                                                         ~ðiÞ       ðiÞ
                                                                                             resents the motion of l . Pðl Þ are the precisions of the
             backward connections. Typically, the synaptic dynamics                                                        x        c
             of backward connections have slower time constants. This                        random ﬂuctuations that control their amplitude and
             has led to the notion that forward connections are driving                      smoothness. For simplicity, we have omitted terms that de-
             and illicit an obligatory response in higher levels, whereas                    pend on the conditional covariance of the parameters; this
             backward connections have both driving and modulatory                           is the same approximation used by expectation maximisa-
             eﬀects and operate over greater spatial and temporal scales.                    tion (Dempster et al., 1977).
                 The hierarchical structure of the brain speaks to hierar-                   8.2. The dynamics and architecture of perceptual inference
             chical models of sensory input. For example
              y ¼ gðxð1Þ;vð1ÞÞþzð1Þ                                                             As mentioned above, we will focus on the optimization
              _ð1Þ        ð1Þ  ð1Þ      ð1Þ                                                  of the ensemble density covering the states, implicit in per-
              x   ¼fðx ;v Þþw                                                                ception or perceptual inference. From Eq. (7) we obtain an
                 .
                 .                                                                           expression that describes the dynamics of neuronal activity
                 .
               ði1Þ       ðiÞ  ðiÞ     ðiÞ                                       ð10Þ       under the free energy principle
              v     ¼gðx ;v Þþz                                                              _ ðiÞ      ðiÞ  ðiþ1Þ
                                                                                             ~         ~   ~
               ðiÞ       ðiÞ  ðiÞ      ðiÞ                                                   l ¼hðe ;e           Þ
              _                                                                               u
              x   ¼fðx ;v Þþw                                                                                    ðiÞT                ðiþ1ÞT
                                                                                                                ~                   ~
                 .                                                                                    0ðiÞ    oe       ðiÞ ðiÞ     oe         ðiþ1Þ ðiþ1Þ
                                                                                                    ~                    ~                        ~
                 .                                                                               ¼lu j ðiÞ P e j                     ðiÞ P e                    ð12Þ
                 .                                                                                              ~                     ~
                                                                                                               olu                  olu
             In this model sensory states y are caused by a non-linear                       These dynamics describe how neuronal states self-organise
                                           (1)  (1)                                 (1)
             function of states, g(x ,v ) plus a random eﬀect z .                            when the brain is exposed to sensory input. The form of
             Thedynamicstates x(1) have memory and evolve according                          Eq.(12)isquite revealing; it is principally a function of pre-
             to equations of motion prescribed by the non-linear func-                       diction error, namely the mismatch between the expected
             tion f(x(1),v(1)). These dynamics are subject to random ﬂuc-                    state of the world, at any level, and that predicted on the
                          (1)
             tuations w       and perturbations from higher levels that are                  basis of the expected state in the level above. Critically,
             generated in exactly the same way. In other words, the in-                      inference only requires the prediction error from the lower
         80                                     K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87
         Fig. 4. Schematic detailing the neuronal architectures that encode an ensemble density on the states and parameters of hierarchical models. The upper
         panel shows the deployment of neurons within three cortical areas (or macro-columns). Within each area the cells are shown in relation to the laminar
         structure of the cortex that includes supra-granular (SG) granular (L4) and infra-granular (IG) layers. The lower panel shows an enlargement of a
         particular area and the speculative cells of origin of forward driving connections that convey prediction error from a lower area to a higher area and the
         backward connections that carry predictions. These predictions try to explain away input from lower areas by suppressing the mismatch or prediction
         error. In this scheme, the source of forward connections is the superﬁcial pyramidal cell population and the source of backward connections is the deep
         pyramidal cell population. The diﬀerential equations relate to the free energy minimisation scheme detailed in the main text.
                ðiÞ                              ðiþ1Þ                                             "#
               ~                                ~                                  ðiþ1ÞT              ðiþ1Þ
         level e   and the level in question e        . This drives condi-        ~
                                 ðiÞ                                             oe      Pðiþ1Þ ¼    Pv       0                                ð14Þ
                               ~
         tional expectations lu to provide a better prediction, con-                ~ðiÞ               00
         veyed by backward connections, to explain away the                       olu
         prediction error. This is the essence of the recurrent
         dynamics that self-organise to suppress free energy or pre-             and reduce to the precisions of the causes at that level. We
         diction error; i.e., recognition dynamics.                              will look at the biological substrate of these interactions
            Critically, the motion of the expected states is a linear            below.
         function of the bottom-up prediction error. This is exactly                The form of Eq. (12) allows us to ascribe the source of
         what is observed physiologically, in the sense that bottom-             prediction error to superﬁcial pyramidal cells and we can
         updriving inputs elicit obligatory responses in higher levels           posit these as encoding prediction error. This is because
         that do not depend on other bottom-up inputs. In fact, the              the only quantity that is passed forward from one level in
         forward connections in Eq. (12) have a simple form6                     the hierarchy to the next is prediction error and superﬁcial
                                                                                 pyramidal cells are the source of forward aﬀerents in the
                       "#
            ðiÞT         I gðiÞ       I gðiÞ                                 brain. This is useful because it is these cells that are primar-
           ~
          oe      ðiÞ            v              x       ðiÞ                      ily responsible for the genesis of electroencephalographic
           ~ðiÞ P    IfðiÞ DðIfðiÞÞ P                             ð13Þ
          olu                   v                x                               (EEG) signals that can be measured non-invasively. The
                                                                                 prediction error itself is formed by predictions conveyed
                                                                                 by backward connections and dynamics intrinsic to the
         This comprises block diagonal repeats of the derivatives                level in question. These inﬂuences embody the non-lineari-
         gx = og/ox (similarly for the other derivatives). D is an                                 ~ðiÞ      ~ðiÞ
                                                                                 ties implicit in g     and f ; see Eq. (11). Again, this is
         block matrix with identity matrices in its ﬁrst diagonal that           entirely consistent with the non-linear or modulatory role
         ensure the internal consistency of generalised motion. The              of backward connections that, in this context, model inter-
         connections are modulated by the precisions encoded by                  actions among inferred states to predict lower level infer-
         lðiÞ. The lateral interactions within each level have an even           ences. See Fig. 4 for a schematic of the implicit neuronal
           c
         simpler form                                                            architecture.
                                                                                    In short, the dynamics of the conditional modes are dri-
           6  is the Kronecker tensor product.                                  ven by three terms. The ﬁrst links generalised coordinates
                                                         K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87                                          81
             to ensure the motion of the mode approximates the mode                          Onecould think of lc as describing the short-term changes
             of the motion. This ensures the representation of causal                        in synaptic eﬃcacy, in lateral or intrinsic connections that
             dynamics is internally consistent. The second is a bottom-                      depend upon classical neuromodulatory inputs and other
             up eﬀect that depends upon prediction error from the level                      slower synaptic dynamics (e.g., after-hyperpolarisation
             below. This can be thought of as a likelihood term. The                         potentials and molecular signalling). The physiological
             third term, corresponding to an empirical prior, is medi-                       aspects of these intermediate dynamics provide an interest-
             ated by prediction error at the current level. This is con-                     ing substrate for attentional mechanisms in the brain (see
             structed using top-down predictions. An important aspect                        Schroeder et al., 2001 for review) and are not unrelated
             of hierarchical models is that they can construct their                         to the ideas in Yu and Dayan (2005). These authors posit
             ownempirical priors. In the statistics literature these mod-                    a role for acetylcholine (an ascending modulatory neuro-
             els are known as parametric empirical Bayes models (Efron                       transmitter) mediating expected uncertainty. Neural modu-
             and Morris, 1973) and rely on the conditional indepen-                          latory neurotransmitters have, characteristically, much
             dence of random ﬂuctuation at each level (Kass and Stef-                        slower time constants, in terms of their synaptic eﬀects,
             fey, 1989). In summary, the dynamics of perceptual                              than glutaminergic neurotransmission that is employed
             inference at any level in the brain are moderated by top-                       by forward and backward extrinsic connections.
             down priors from the level above. This is recapitulated at                         In conclusion, we have seen how a fairly generic hierar-
             all  levels, enabling self-organisation through recurrent                       chical and dynamical model of environmental inputs can
             interactions to minimise free energy by suppressing predic-                     be transcribed onto neuronal quantities to specify the free
             tion error throughout the hierarchy. In this way, higher lev-                   energy and its minimisation. This minimisation corre-
             els provide guidance to lower levels and ensure an internal                     sponds, under some simplifying assumptions, to a suppres-
             consistency of the inferred causes of sensory input at multi-                   sion of prediction error at all levels in a cortical hierarchy.
             ple levels of description.                                                      This suppression rests upon a balance between bottom-up
                                                                                             (likelihood) inﬂuences and top-down (prior) inﬂuences that
             9. Perceptual attention and learning                                            are balanced by representations of uncertainty. In turn,
                                                                                             these representations may be mediated by classical neural
                 The dynamics above describe the optimization of condi-                      modulatory eﬀects or slow post-synaptic cellular processes
             tional or variational modes describing the most likely cause                    that are driven by overall levels of prediction error. Over-
             of sensory inputs. This is perceptual inference and corre-                      all, this enables Bayesian inversion of a hierarchical model
             sponds to Bayesian inversion of the hierarchical generative                     of sensory input that is context-sensitive and conforms to
             model described in Eq. (10). In this simpliﬁed scheme, in                       the free energy principle. We will next illustrate the sorts
             which conditional covariances have been ignored, minimis-                       of dynamics and behaviours one might expect to see in
             ing the free energy is equivalent to suppressing hierarchical                   the brain, using a simple simulation.
             prediction error. Exactly the same treatment can be applied
             to changes in extrinsic and intrinsic connectivity encoding                     10. Simulations
             the conditional modes l and l . As above, the changes
                                             c         h
             in these modes or synaptic eﬃcacies are relatively simple                       10.1. Generative and recognition models
             functions of prediction error and lead to forms that are rec-
             ognisable as associative plasticity. Examples of these deri-                       Here, we describe a very simple simulation of a two-
             vations, for static systems are provided in Friston (2005).                     layer neuronal hierarchy to show the key features of its
             The contextual variables are interesting because of their                       self-organised dynamics, when presented with a stimulus.
             role in moderating perceptual inference. Eq. (12) shows                         The system is shown in Fig. 5. On the left, is the system
             that the inﬂuence of prediction error from the level below                      used to generate sensory input and on the right the neuro-
             and the current level is scaled by the precision matrices                       nal architecture used to invert this generation; i.e., to rec-
                  ðiÞ             ðiþ1Þ                                                      ognise or disclose the underlying cause. The generative
             Pðl Þ and Pðl            Þ that are functions of lc. This means
                  c               c
             that the relative inﬂuence of the bottom-up likelihood term                     system used a single input (a Gaussian bump function) that
             and top-down prior is controlled by modulatory inﬂuences                        excites a damped oscillatory transient in two reciprocally
             encoded by l . This selective modulation of aﬀerents is                         connected dynamic units. The output of these units is then
                               c
             exactly the same as gain control mechanisms that have                           passed through a linear mapping to four sensory channels.
             been invoked for attention (e.g., Treue and Maunsell,                           Note that the form of the neuronal or recognition model is
             1996; Martinez-Trujillo and Treue, 2004). It is fairly simple                   exactly the same as the generative model: The only diﬀer-
             to formulate neuronal architectures in which this gain is                       ence is that the causal states are driven by prediction errors
             controlled by lateral interactions that are intrinsic to each                   which invoke the need for forward connections (depicted in
             cortical level (see Fig. 4).                                                    red). The inferred causes, with conditional uncertainty
                 As noted in the previous section changes in lc are sup-                     (shown as 95% conﬁdence intervals) concur reasonably
             posed to occur at a timescale that is intermediate between                      with the real causes. The input pattern is shown as a func-
             the fast dynamics of the states and slow associative changes                    tion of time and in image format at the top of the ﬁgure.
             in extrinsic connections mediating the likelihood model.                        This can be thought of as either a changing visual stimulus,
                82                                                                  K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87
                Fig. 5. Diagram showing the generative model (left) and corresponding recognition; i.e., neuronal model (right) used in the simulations. Left panel: this is
                                                                                 (1)                                   ð1Þ   ð1Þ
                the generative model using a single cause v                          , two dynamic states x               ; x     andfouroutputsy ,...,y . The lines denote the dependencies of the variables on
                                                                                                                       1     2                                 1         4
                each other, summarised by the equation on top (in this example both the equations were simple linear mappings). This is eﬀectively a linear convolution
                model, mapping one cause to four outputs, which form the inputs to the recognition model (solid arrow). The architecture of the corresponding
                                                                                                                                                                                                              ðiÞ
                                                                                                                                                                                                             ~
                recognition model is shown on the right. This has a similar architecture, apart from the inclusion of prediction error units; e . The combination of forward
                                                                                                                                                                                                              u
                                                                                                                                                                                                                                   _ ðiÞ         ðiÞ  ðiþ1Þ
                                                                                                                                                                                                                                   ~           ~     ~
                (red lines) and backward inﬂuences (black lines) enables recurrent dynamics that self-organise (according to the recognition equation; lu ¼ hðe ;e                                                                                          ÞÞ
                to suppress and hopefully eliminate prediction error, at which point the inferred causes and real causes should correspond. (For interpretation of the
                references in colour in this ﬁgure legend, the reader is referred to the web version of this article.)
                impinging on four photo-receptor channels or, perhaps, a
                formant over time–frequency in an acoustic setting.
                     This simulation can be regarded as reproducing sensory
                evoked transients and corresponds to Bayesian inversion of
                the generative model shown on the left hand side of the ﬁg-
                ure. In this context, because we used a dynamical genera-
                tive      model, the inversion corresponds to an online
                deconvolution. If we allow the connection strengths in
                the recognition model to minimise free energy, we are also
                implicitly estimating the parameters of the corresponding
                generative model. In machine learning and signal process-
                ing this is known as blind deconvolution. Examples of this
                are shown in Fig. 6. Here, we presented the same stimulus
                eight times and recorded the prediction error in the input
                or lowest level, summed over all peristimulus time. The ini-
                tial values of the parameters were the same as in the gener-
                ative model (those used in Fig. 5). The upper panels show
                the stimulus and predicted input, in image format, for the                                                                 Fig. 6. Results of repeated presentations to the simulated neural network
                ﬁrst and last trial. It can be seen that both the ﬁrst and                                                                 shown in the previous ﬁgure. Left panels: the four channel sensory data
                eighth predictions are almost identical to the real input.                                                                 used to evoke responses and the predictions from these evoked responses
                This is because the connection strengths, i.e., conditional                                                                for the ﬁrst and last of eight trials are shown on top, in image format. The
                                                                                                                                           corresponding prediction error (summed over the entire trial period after
                modesoftheparameters(intherecognitionmodel),started                                                                        rectiﬁcation) is shown below. As expected, there is a progressive reduction
                with the same values used by the generative model. Despite                                                                 in prediction error as the system learns the most eﬃcient causal
                this, minimising the free energy of the ensemble density on                                                                architecture underlying the generation of sensory inputs. Right panels:
                the parameters enables the recognition model to encode                                                                     exactly the same as above but now using an unpredictable or unfamiliar
                this stimulus more eﬃciently, with a progressive suppres-                                                                  stimulus that was created using a slightly diﬀerent generative model. Here,
                                                                                                                                           learning the causal architecture of this new stimulus occurs progressively
                sion of prediction error with repeated exposure. This eﬀect                                                                over repeated presentations, leading to profound reduction in prediction
                is much more marked if we use a stimulus that the recogni-                                                                 error and repetition suppression.
                                                         K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87                                          83
             tion model has not seen before. We produced this stimulus                          The phenomenon that we will focus on is the diﬀerence
             byadding a small random number to the parameters of the                         betweentheprediction errors elicited by the familiar or pre-
             generative model. At the ﬁrst presentation, the recognition                     dictable stimulus, relative to that elicited by the unpredict-
             model tries to perceive the input in terms of what it already                   able stimulus. A strong prediction of the free energy
             knows and has experienced. In this case a prolonged ver-                        formulation is that unpredictable or incoherent stimuli will
             sion of the expected stimulus. This produces a large predic-                    evoke a much greater prediction error than familiar or
             tion error. By the eighth presentation, changes in the                          coherent stimuli. Furthermore, this relative suppression
             parameters enable it to recognise and predict the input                         will be mediated by backward connections in the brain that
             almost exactly, with a profound suppression of prediction                       convey the predictions. In the ﬁnal section, we present an
             error with each repetition of the input. Note that the sup-                     empirical test of this hypothesis, using an fMRI study of
             pression of prediction error is more dramatic for the unpre-                    visually evoked responses using predictable and unpredict-
             dicted stimulus; this is because more is learned during                         able stimuli (Harrison et al., in press).
             repeated exposure.
                                                                                             11. Suppressing free energy in the human brain
             10.2. Repetition suppression                                                       There are clearly a vast number of predictions and
                 This simple simulation shows a ubiquitous and generic                       experiments that follow from the free energy treatment of
             aspect of free energy minimisation schemes and indeed real                      the previous sections. We have reviewed many of these
             brain responses; namely repetition suppression. This phe-                       from the neurophysiological, electrophysiological, psycho-
             nomenon describes the reduction or suppression in evoked                        physical and imaging neuroscience literature in other
             responses on repeated presentation of stimuli. This can be                      papers (e.g., Friston and Price, 2001; Friston, 2003,
             seen in many contexts, ranging from the mismatch negativ-                       2005). In this paper, we focus on a simple but quite enlight-
             ity in EEG research (Naatanen, 2003) to fMRI examples of                        ening study that was designed to address the role of back-
                                          ¨ ¨ ¨                                              ward connections in suppressing prediction error, using
             face processing (see Henson et al., 2000 and Fig. 7).
                                                                                             predictable and unpredictable visual stimuli.
                                                                                             11.1. Experimental design and methods
                                                                                                At its simplest, this experiment can be conceived on
                                                                                             measuring visually evoked responses to predictable and
                                                                                             unpredictable stimuli, where we hypothesized that the
                                                                                             evoked responses in early (lower) visual areas would be
                                                                                             reduced for predictable, relative to unpredictable stimuli.
                                                                                             The stimuli comprised a sparse grid of visual dots that
                                                                                             moved either in a coherent (predictable) or incoherent
                                                                                             (unpredictable) fashion. However, simply showing reduced
                                                                                             responses to predictable stimuli does not allow us to infer
                                                                                             that this reduction is mediated by backward connections.
                                                                                             To do this we exploit known anatomical characteristics
                                                                                             of connectivity in the visual system to ensure that any eﬀect
                                                                                             of coherent motion is mediated by backward connections.
                                                                                             Wedid this by using sparse stimuli that excited retinotop-
                                                                                             ically mapped responses beyond the range of horizontal
                                                                                             connections in striate cortex or V1. The classical receptive
                                                                                             ﬁeld of V1 units is about one degree of visual angle (see
                                                                                             Fig. 8). On the basis of anatomical studies, horizontal con-
                                                                                             nections in V1 cover about two degrees of visual angle
                                                                                             (Angelucci et al., 2002a,b). The separation of the stimuli
                                                                                             weemployedwasaboutthreedegrees.Therefore, any com-
                                                                                             ponent motion of a single dot, that could be predicted by
             Fig. 7. A summary of the results of an fMRI experiment reported in              other dots, can only be ‘seen’ by higher visual areas with
             Hensonetal.,2000.Theupperpanelshowsresponsestovisuallypresented                 larger receptive ﬁelds (i.e., V2 or higher). This means that
             faces for the ﬁrst presentation (blue) and the second presentation (red).       diﬀerences, due to predictability, in V1 responses must be
             This is a nice example of repetition suppression as measured using fMRI.        mediated by backward connections from V2 or higher.
             The inserts show voxels that were signiﬁcantly activated by all faces (red)     Incoherent and globally coherent sparse stimuli were pre-
             and those that showed signiﬁcant repetition suppression in the fusiform
             cortex (blue). (For interpretation of the references in colour in this ﬁgure    sented to normal human subjects every second or so while
             legend, the reader is referred to the web version of this article.)             hemodynamic responses were measured using functional
     84                K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87
     Fig. 8. Schematic showing the extent of the classical receptive ﬁeld, horizontal connections in V1 and the inﬂuence of backward connections from V2 and
     V3. The sparse stimulus arrays, used to excite visual responses, are shown as superimposed dots for incoherent or unpredictable motion (upper) and
     coherent or predictable motion (lower). The schematic on the right is meant to indicate that coherent motion can only aﬀect V1 responses through
     backward inﬂuences from V2 that has suﬃciently large receptive ﬁelds. A neuron receives input directly from the stimulus (forward) and additional
     information from backward connections. The extra-classical receptive ﬁeld (ECRF) comprises a proximal and a distal surround ﬁeld whose spatial extent
     is consistent with the deployment of horizontal and backward connections, respectively (Angelucci et al., 2002a). When using a sparse stimulus, only one
     dot can fall within the proximal surround ﬁeld of a V1 neuron. Abbreviations: V1 = striate cortex, (E)CRF = (extra)-classical receptive ﬁeld.
     Fig. 9. Results of an fMRI study of twelve subjects comparing visually evoked responses to coherent, relative to incoherent stimuli. Signiﬁcant decreases
     are shown on the left in blue and increases on the right in red. The upper panel shows the time course of responses for blocks of incoherent, stationary and
     coherent stimuli for V1, V2 and V5. These responses were summarized as the ﬁrst principal eigenvariate of a local sphere placed upon the maxima of the
     statistical parametric maps shown below. The middle bar charts represent the diﬀerences in activity or responses as modelled by a canonical hemodynamic
     response to coherent relative to incoherent motion. These have arbitrary units. The bars represent 95% conﬁdence intervals. V1; early visual cortex. V5;
     motion sensitive extra striate cortex. V2; secondary extrastriate cortex. PCG; posterior cingulate cortex. (For interpretation of the references in colour in
     this ﬁgure legend, the reader is referred to the web version of this article.)
                                                         K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87                                          85
             magnetic resonance imaging. The data were analysed using                        energy minimisation, in which both the behaviour of the
             conventional statistical parametric mapping. This involved                      organism and its internal conﬁguration minimise its free
             modelling evoked responses with a stimulus function                             energy. This free energy is a function of the ensemble den-
             encoding the occurrence of coherent or incoherent stimuli                       sity encoded by the organism’s conﬁguration and the sen-
             and convolving these with a hemodynamic response func-                          sory data to which it is exposed. Minimisation of free
             tion to form regressors for a general linear model. Infer-                      energy occurs through action-dependent changes in sen-
             ences about diﬀerential responses between coherent and                          sory input and the ensemble density implied by internal
             incoherent stimuli were assessed using a single-sample t-test                   changes. Systems that fail to maintain a low free energy will
             over subjects, on appropriate contrasts from each subject.                      encounter phase-transitions as their relationship to the
             The results of this random eﬀects analysis are shown in                         environment changes. It is therefore necessary, if not suﬃ-
             Fig. 9.                                                                         cient, for biological systems to minimise their free energy.
                                                                                                This free energy is not a thermodynamic free energy but
             11.2. Results                                                                   a free energy formulated in terms of information theoretic
                                                                                             quantities. The free energy principle discussed here is not a
                 Aspredicted, there were profound reductions in visually                     consequence of thermodynamics but arises from popula-
             evoked responses to predictable, relative to incoherent,                        tion dynamics and selection. Put simply, systems with a
             visual stimuli in V1. Interestingly, these decreases were also                  low free energy will be selected over systems with a higher
             seen in V5, bilaterally. The time course of hemodynamic                         free energy. The free energy rests on a speciﬁcation of a
             activity for a single subject is shown in the upper panel                       generative model, which is entailed by the organism’s struc-
             for V1, V2 and V5. This graphic also shows the estimated                        ture. Identifying this model enables one to predict how a
             responses to control stimuli that did not move. Again, as                       system will change if it conforms to the free energy princi-
             predicted, enhanced responses to unpredictable stimuli                          ple. For the brain, a plausible model is a hierarchical
             was seen at the ﬁrst level that the receptive ﬁelds could                       dynamic system in which neural activity encodes the condi-
             encompass more than one dot. This was in area V2. This                          tional modes of environmental states and its connectivity
             may reﬂect the activity of deep pyramidal cells encoding                        encodes the causal context in which these states evolve.
             global motion subtended by multiple dots. It is interesting                     Bayesian inversion of this model, to infer the causes of sen-
             to note that V5 showed a reduced prediction error, despite                      sory input, is a natural consequence of minimising free
             the fact that this area is generally thought to be hierarchi-                   energy or, under simplifying assumptions, the suppression
             cally higher in the visual cortex than V2. However, extra-                      of prediction error. We concluded with a simple but com-
             geniculate pathways can bypass V1 and V2 and provide                            pelling experiment that showed the relative suppression
             information directly to V5 which, in some circumstances,                        of prediction error, in the context of predictable stimuli,
             may make it behave like a hierarchically low area. This is                      is indeed mediated by backward connections in the brain
             consistent with the short-latency responses of V5, in rela-                     as predicted by a free energy descent scheme.
             tion to V1 (see Nowak and Bullier, 1997).                                          The ideas presented in this paper have a deep history;
                 In summary, this fMRI study conﬁrms our predictions                         starting with the notions of neuronal energy described by
             from the theoretical analysis that evoked responses are                         Helmholtz (1860) and covering ideas like analysis by syn-
             smaller for predictable, relative to unpredictable stimuli.                     thesis (Neisser, 1967) and more recent formulations like
             This is consistent with measured responses reﬂecting, in                        Bayesian inversion and predictive coding (e.g., Ballard
             large part, prediction error evoked as the sensory cortex                       et al., 1983; Mumford, 1992; Dayan et al., 1995; Rao and
             self-organises to infer the causes of its geniculate input.                     Ballard, 1998). The speciﬁc contribution of this paper is
             Furthermore, by careful design of the stimuli to preclude                       to provide a general formulation of the free energy princi-
             horizontal interactions among V1 units, we are able to infer                    ple to cover both action and perception. Furthermore, this
             that this suppression of prediction error has to be mediated                    formulation can be used to connect constructs from
             by backward connections from higher cortical areas. This                        machine learning and statistical physics with selectionist
             is consistent with the recurrent dynamics entailed by the                       ideas from theoretical biology.
             hierarchical formulation of generative models in the brain
             and the inversion of these models in accord with the free
             energy principle.                                                               Acknowledgements
             12. Conclusion                                                                     TheWellcomeTrustfundedthiswork.Wewouldliketo
                                                                                             express our greatest thanks to Marcia Bennett for prepar-
                 In this paper, we have considered the characteristics of                    ing this manuscript.
             biological systems, in relation to non-biological self-orga-
             nizing and dissipative systems. Biological systems act on                       Appendix A. The conditional covariances
             the environment and can sample it selectively to avoid
             phase-transitions that will irreversibly alter their structure.                    Under the Laplace approximation, the variational den-
             This adaptive exchange can be formalised in terms of free                       sity assumes a Gaussian form q = N(l,R) with variational
                                                                                                                                    i        i   i
               86                                                               K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87
               parameters l and R, corresponding to the conditional                                                                   g ¼ gðx;vÞ                         f ¼ fðx;vÞ
                                        i               i
               mode and covariance of the ith parameter set. The advan-                                                                  0           0           0         0           0           0
                                                                                                                                      g ¼g x þg v                        f ¼f x þf v
                                                                                                                                                  x           v                     x           v
               tage of this approximation is that the conditional covari-                                                                00           00           00      00           00          00                                   ðA:5Þ
                                                                                                                                      g ¼g x þg v                        f ¼f x þf v
               ance can be evaluated very simply: Under the Laplace                                                                               x            v                     x           v
                                                                                                                                           .                                 .
               approximation the free energy is                                                                                            .                                 .
                                                                                                                                           .                                 .
                                      1 X                                                                                            P(# ) is the precision (i.e., inverse covariance) of ~z that
                F ¼LðlÞþ                        ðU þlnjRjþp ln2peÞ                                                                          c z
                                      2              i              i         i                                                      controls its amplitude and roughness. Similarly, Gaussian
                                            i
                U ¼trðRo2LðlÞ=o#o#Þ                                                                                                  assumptions about ﬂuctuations in the states allow us to ex-
                   i              i                   i    i                                                                                                                        ~~ ~
                                                                                                                   ðA:1Þ             press the prior pð#uÞ¼pðxjvÞpðvÞ in terms of predicted mo-
                                      ~                                                                                                                                                  1
                Lð#Þ¼lnpðy;#Þ                                                                                                                                  ~~                ~
                                                                                                                                     tion, where pðxjvÞ¼Nðf;P Þ and P(# ) is the precision
                                                       X                                                                                                                                 w                      c w
                                                    1                                                                                      ~
                Ið# Þ¼Lð#;l Þþ                                U                                                                      of w. We now have now the functional form of the likeli-
                      i              i    ni        2             j                                                                  hood and priors and implicitly the generative model.
                                                        j6¼i                                                                              Adding hierarchical levels, with y = v(0) gives Eq. (10)
               pi is the number of parameters in the ith set. The condi-                                                              vði1Þ ¼ gðxðiÞ;vðiÞÞþzðiÞ
               tional covariances obtain as an analytic function of the                                                                                                                                                                  ðA:6Þ
                                                                                                                                      _ðiÞ             ðiÞ    ðiÞ          ðiÞ
               modes by diﬀerentiating the free energy and solving for                                                                x     ¼fðx ;v Þþw
               zero
                                                                                                                                     These induce empirical priors on the states that lend the
                                  1 2                               1 1                                                             generative density a Markov form, through independence
               oF=oR ¼ o LðlÞ=o#o# þ R ¼0
                           i      2                     i     i     2 i                                                              assumptions (Kass and Steﬀey, 1989) about the random
                                      1             2                                                                               ﬂuctuations in diﬀerent levels
                              )R ¼oLðlÞ=o#o#                                                                      ðA:2Þ
                                      i                               i     i
                                                                                                                                                                 ð1Þ         ð1Þ     ð2Þ              ðmÞ
                                                                                                                                         ~                 ~                                        ~
               This solution for the conditional covariances does not de-                                                             pðy;#Þ¼pðyj# Þpð# j# Þpðv Þ
                                                                                                                                                                 u           u       u
                                                                                                                                        
               pend on the mean-ﬁeld approximation but only on the La-                                                                      ði1Þ      ðiÞ              ði1Þ     ði1Þ          ði1Þ     ðiÞ
                                                                                                                                                                      ~         ~              ~
                                                                                                                                      p #          j#        ¼p x              jv          p v          j#
                                                                                                                                            u          u                                                   u                             ðA:7Þ
               place approximation. See Friston et al. (in press) for more                                                              
                                                                                                                                            ðiÞ   ðiÞ             ~ðiÞ        ðiÞ1
                                                                                                                                          ~ ~
               details.                                                                                                               p x jv            ¼N f ;Pw
                                                                                                                                        
                                                                                                                                            ði1Þ     ðiÞ                ðiÞ      ðiÞ1
                                                                                                                                          ~                            ~
                                                                                                                                      p v         j#u        ¼N g ;Pz
               Appendix B. Dynamic models                                                                                                                         ~ ðiÞ         ðiÞ
                                                                                                                                     The prediction gð# ;# Þ plays the role of a prior expecta-
                                                                                                                                                     ði1Þ              u       h
                                                                                                                                                   ~
                    Here we consider the functional form of the generative                                                           tion on v               andits prior precision is estimated empirically
                                                                                                                                                   ðiÞ
                                                                                                                                     as Pð# Þ ; hence empirical Bayes (Efron and Morris,
               density for hierarchical dynamic models of the sort descried                                                                        c    v
               in Eq. (10). To simplify things, we will deal with a single                                                           1973); similarly for the hidden states. In short, a hierarchi-
               level and generalise to multiple levels later                                                                         cal form endows a model with the ability to construct its
                                                                                                                                     own priors. This feature is central to many inference and
                y ¼ gðx;vÞþz                                                                                       ðA:3Þ             estimation procedures ranging from mixed-eﬀects analyses
                _                                                                                                                    in classical covariance component analysis to automatic
                x ¼ fðx;vÞþw                                                                                                         relevance determination. See Friston et al., in press for a
               The continuous non-linear functions f(x,v)andg(x,v)of                                                                 fuller discussion of static models.
               states are parameterised by #h. Stochastic innovations z(t)
               are assumed to be analytic such that the covariance of
                            0   00
               ~z ¼ z;z ;z ;... is well deﬁned in generalised coordinates;                                                           References
                                                                                                      ~
               similarly for random ﬂuctuations in the states, w. Under lo-
                                                                                                            ~                        Angelucci, A., Levitt, J.B., Walton, E.J., Hupe, J.M., Bullier, J., Lund,
               cal linearity assumptions, the generalised motion y is given
               by                                                                                                                         J.S., 2002a. Circuits for local and global signal integration in primary
                                                                                                                                          visual cortex. J. Neurosci. 22, 8633–8646.
                                                           0
                y ¼ gðx;vÞþzx
                                                              ¼fðx;vÞþw                                                              Angelucci, A., Levitt, J.B., Lund, J.S., 2002b. Anatomical origins of the
                 _            0           0       0        00           0           0        0                                            classical receptive ﬁeld and modulatory surround ﬁeld of single
                y ¼ g x þg v þz                           x ¼f x þf v þw
                          x           v                              x           v                                                        neurons in macaque visual cortical area V1. Prog. Brain Res. 136,
                €             00           00      00      000           00          00        00                  ðA:4Þ
                y ¼ g x þg v þz                           x ¼fx þfv þw                                                                    373–388.
                          x            v                              x           v
                     .                                        .                                                                      Ashby, W.R., 1947. Principles of the self-organising dynamic system. J.
                     .                                        .                                                                           Gen. Psychol. 37, 125–128.
                     .                                        .
                                                                                                                                     Ballard, D.H., Hinton, G.E., Sejnowski, T.J., 1983. Parallel visual
               This model induces a variational density q(# ,t) on the gen-
                                                                                                 u                                        computation. Nature 306, 21–26.
                                                      ~ ~                                                                            Dayan, P., Hinton, G.E., Neal, R.M., 1995. The Helmholtz machine.
               eralised causes #u ¼ x;v. The second set of equations pre-                                                                 Neural Comput. 7, 889–904.
               scribes dynamics by coupling low and high-order motion
               of x(t), which confers memory on the system. Gaussian                                                                 DeFelipe, J., Alonso-Nanclares, L., Arellano, J.I., 2002. Microstructure of
               assumptions about the ﬂuctuations furnish the functional                                                                   the neocortex: comparative aspects. J. Neurocytol. 31, 299–316.
                                                                                         1                                ~         Dempster, A.P., Laird, N.M., Rubin, D.B., 1977. Maximum likelihood
                                                               ~                 ~                            ~
               form of the likelihood pðyj#Þ¼Nðg;Pz Þ, where g and f                                                                      from incomplete data via the EM algorithm. J. Roy. Stat. Soc. Ser. B
               comprise the generalised predictions                                                                                       39, 1–38.
                                                         K. Friston et al. / Journal of Physiology - Paris 100 (2006) 70–87                                          87
             Edelman, G.M., 1993. Neural Darwinism: selection and reentrant                  MacKay, D.M., 1956. The epistemological problem for automata. In:
                 signaling in higher brain function. Neuron 10, 115–125.                        Shannon, C.E., McCarthy, J. (Eds.), Automata Studies. Princeton
             Efron, B., Morris, C., 1973. Stein’s estimation rule and its competitors –         University Press, Princeton, NJ, pp. 235–251.
                 an empirical Bayes approach. J. Am. Stat. Assoc. 68, 117–130.               MacKay, D.J.C., 1995. Free energy minimisation algorithm for decoding
             Felleman, D.J., Van Essen, D.C., 1991. Distributed hierarchical process-           and cryptoanalysis. Electron. Lett. 31, 445–447.
                 ing in the primate cerebral cortex. Cerebral Cortex 1, 1–47.                Maniadakis, M., Trahanias, P., 2006. Modelling brain emergent behav-
             Feynman, R.P., 1972. Statistical Mechanics. Benjamin, Reading MA,                  iours through coevolution of neural agents. Neural Networks 19, 705–
                 USA.                                                                           720.
             Friston, K.J., 2003. Learning and inference in the brain. Neural Networks       Martinez-Trujillo, J.C., Treue, S., 2004. Feature-based attention increases
                 16, 1325–1352.                                                                 the selectivity of population responses in primate visual cortex. Curr.
             Friston, K.J., 2005. A theory of cortical responses. Philos. Trans. R Soc.         Biol. 14, 744–751.
                 Lond. B Biol. Sci. 360, 815–836.                                            Mesulam,M.M.,1998.Fromsensationtocognition.Brain121,1013–1052.
             Friston, K.J., in preparation. DEM: a variational treatment of dynamic          Montague, P.R., Dayan, P., Person, C., Sejnowski, T.J., 1995. Bee
                 systems.                                                                       foraging in uncertain environments using predictive Hebbian learning.
             Friston, K.J., Price, C.J., 2001. Dynamic representations and generative           Nature 377, 725–728.
                 models of brain function. Brain Res. Bull. 54, 275–285.                     Mumford, D., 1992. On the computational architecture of the neocortex.
             Friston, K.J., Tononi, G., Reeke Jr., G.N., Sporns, O., Edelman, G.M.,             II. The role of cortico-cortical loops. Biol. Cybern. 66, 241–251.
                 1994. Value-dependent selection in the brain: simulation in a synthetic     Murphy, P.C., Sillito, A.M., 1987. Corticofugal feedback inﬂuences the
                 neural model. Neuroscience 59, 229–243.                                        generation of length tuning in the visual pathway. Nature 329, 727–
             Friston, K., Mattout, J., Trujillo-Barreto, N., Ashburner, J., Penny, W.,          729.
                 in press. Variational free energy and the Laplace approximation.            Naatanen, R., 2003. Mismatch negativity: clinical research and possible
                                                                                               ¨¨ ¨
                 NeuroImage.                                                                    applications. Int. J. Psychophysiol. 48, 179–188.
             Haken, H., 1983. Synergistics: An introduction. Non-equilibrium Phase           Neal, R.M., Hinton, G.E., 1998. A view of the EM algorithm that justiﬁes
                 Transition and Self-organisation in Physics, Chemistry and Biology,            incremental sparse and other variants. In: Jordan, M.I. (Ed.), Learning
                 third ed. Springer Verlag.                                                     in Graphical Models. Kulver Academic Press.
             Harrison, L.M., Stephan, K.E., Rees, G., Friston, K.J., in press. Extra-        Neisser, U., 1967. Cognitive Psychology. Appleton-Century-Crofts, New
                 classical receptive ﬁeld eﬀects measured in striate cortex with fMRI.          York.
                 NeuroImage.                                                                 Nicolis, G., Prigogine, I., 1977. Self-organisation in Non-equilibrium
             Harville, D.A., 1977. Maximum likelihood approaches to variance                    Systems. John Wiley, New York, USA, p. 24.
                 component estimation and to related problems. J. Am. Stat. Assoc.           Nowak, L.G., Bullier, J., 1997. The timing of information transfer in the
                 72, 320–338.                                                                   visual system. Cereb. Cortex. 12, 205–241.
             Helmholtz, H., 1860. Handbuch der physiologischen optik, vol. 3. Dover,         Prince, A., Smolensky, P., 1997. Optimality: from neural networks to
                 NewYork (English trans., Southall JPC, Ed.).                                   universal grammar. Science 275, 1604–1610.
             Henson, R., Shallice, T., Dolan, R., 2000. Neuroimaging evidence for            Rao, R.P., 2005. Bayesian inference and attentional modulation in the
                 dissociable forms of repetition priming. Science 287, 1269–1272.               visual cortex. Neuro Rep. 16, 1843–1848.
             Hinton, G.E., von Cramp, D., 1993. Keeping neural networks simple by            Rao, R.P., Ballard, D.H., 1998. Predictive coding in the visual cortex: a
                 minimising the description length of weights. In: Proceedings of               functional interpretation of some extra-classical receptive ﬁeld eﬀects.
                 COLT-93, pp. 5–13.                                                             Nature Neurosci. 2, 79–87.
             Hochstein, S., Ahissar, M., 2002. View from the top: hierarchies and            Rockland, K.S., Pandya, D.N., 1979. Laminar origins and terminations of
                 reverse hierarchies in the visual system. Neuron 36, 791–804.                  cortical connections of the occipital lobe in the rhesus monkey. Brain
             Kass, R.E., Steﬀey, D., 1989. Approximate Bayesian inference in                    Res. 179, 3–20.
                 conditionally independent hierarchical models (parametric empirical         Sherman, S.M., Guillery, R.W., 1998. On the actions that one nerve cell
                 Bayes models). J. Am. Stat. Assoc. 407, 717–726.                               can have on another: distinguishing ‘‘drivers’’ from modulators. Proc.
             Kauﬀman, S., 1993. Self-organisation on Selection in Evolution. Oxford             Natl. Acad. Sci. USA 95, 7121–7126.
                 University Press, Oxford, UK.                                               Schroeder, C.E., Mehta, A.D., Foxe, J.J., 2001. Determinants and
             Kawato, M., Hayakawa, H., Inui, T., 1993. A forward-inverse optics                 mechanisms of attentional modulation of neural processing. Front
                 model of reciprocal connections between visual cortical areas.                 Biosci. 6, D672–D684.
                 Network 4, 415–422.                                                         Suri, R.E., Schultz, W., 2001. Temporal diﬀerence model reproduces
             Kersten, D., Mamassian, P., Yuille, A., 2004. Object perception as                 anticipatory neural activity. Neural Comput. 13, 841–862.
                 Bayesian inference. Annu. Rev. Psychol. 55, 271–304.                        Treue, S., Maunsell, H.R., 1996. Attentional modulation of visual motion
             Kording, K.P., Wolpert, D.M., 2004. Bayesian integration in sensorimo-             processing in cortical areas MT and MST. Nature 382, 539–541.
               ¨
                 tor learning. Nature 427, 244–247.                                          Yu, A.J., Dayan, P., 2005. Uncertainty, neuromodulation and attention.
             Kloucek, P., 1998. The computational modeling of nonequilibrium                    Neuron 46, 681–692.
                 thermodynamics of the martensitic transformations. Comput. Mech.            Zeki, S., Shipp, S., 1988. The functional logic of cortical connections.
                 23, 239–254.                                                                   Nature 335, 311–317.
