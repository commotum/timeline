Along the Frontiers = 469

The second approach is to modulate learning by processes of
strengthening and weakening. The viewpoint is that adding perma-
ent structure is serious business, not only because it will be
around for a long time but because whether it is in the organism's
interest to have it or not cannot be determined quickly. Therefore,
commitment should occur slowly and reflect the accumulation of
experience. Weights associated with each production provide a nat-
ural mechanism for making this commitment. The weight governs
the probability that the production will fire if satisfied. With every
positive experience with the production the weight is increased;
with every negative experience it is decreased. No structure is
actually permanent, for its weight can be decreased so much that
the production will never be evoked again. This solution has a lot to
recommend it. It has a venerable history going all the way back
to linear-operator stochastic learning theory (Bush & Mosteller,
1955). It is the solution that Act* adopts (Anderson, 1983). How-
ever, it does have some problems. It commits to slow acquisition,
whereas fast acquisition may be what the situation demands. In
effect, it treats learning as a distinct long-term process with a lim-
ited role to play in performance. It can also pose credit-assignment
problems.

The third approach is to build a discrimination mechanism so that
the bad structure is never (or hardly ever) evoked again. If the
system never goes down that way again, then the productions never
get a chance to fire. It always works in an environment where only
the successful learning exists. It avoids the built-in slow adaptation
of the strengthening approach, because once the discrimination
mechanisms are in place, the bad past behavior in effect does not
exist any more and hence doesnâ€™t take up any time or effort.

There is a hitch. This solution requires that there be a way to get
upstream of the bad structure. If the system waits until it encoun-
ters the bad learning again, then it is already in error-recovery mode
and wading through the decision-time override. It needs to avoid
the bad learning, not recover from it. Thus, once it has been discov-
ered that some productions are wrong in a certain situation, what
the right actions are, and even how to discriminate the good from
the bad, the discrimination mechanism must still be installed so the
preferred situation is encountered before that contaminated situa-
tion. That is a problem, since we are not talking about getting up-
stream in some existing static structure but in the stream of be-
havior.

