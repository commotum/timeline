                           Journal of Machine Learning Research 6 (2005) 503–556                             Submitted 11/03; Revised 4/04; Published 4/05
                                              Tree-Based Batch Mode Reinforcement Learning
                           DamienErnst                                                                         ERNST@MONTEFIORE.ULG.AC.BE
                           Pierre Geurts                                                                     GEURTS@MONTEFIORE.ULG.AC.BE
                           Louis Wehenkel                                                                        LWH@MONTEFIORE.ULG.AC.BE
                           Department of Electrical Engineering and Computer Science
                           Institut Monteﬁore, University of Liege
                                                                    `
                           Sart-Tilman B28
                           B4000Liege, Belgium
                                      `
                           Editor: Michael L. Littman
                                                                                   Abstract
                                 Reinforcementlearningaimstodetermineanoptimalcontrolpolicyfrominteractionwithasystem
                                or from observations gathered from a system. In batch mode, it can be achieved by approximating
                                the so-called Q-function based on a set of four-tuples (x ,u ,r ,x                ) where x denotes the sys-
                                                                                                    t   t  t  t+1            t
                                tem state at time t, u the control action taken, r the instantaneous reward obtained and x                     the
                                                         t                               t                                                t+1
                                successor state of the system, and by determining the control policy from this Q-function. The
                                Q-function approximation may be obtained from the limit of a sequence of (batch mode) super-
                                vised learning problems. Within this framework we describe the use of several classical tree-based
                                supervised learning methods (CART, Kd-tree, tree bagging) and two newly proposed ensemble al-
                                gorithms, namely extremely and totally randomized trees. We study their performances on several
                                examples and ﬁnd that the ensemble methods based on regression trees perform well in extracting
                                relevant information about the optimal control policy from sets of four-tuples. In particular, the to-
                                tally randomized trees give good results while ensuring the convergence of the sequence, whereas
                                by relaxing the convergence constraint even better accuracy results are provided by the extremely
                                randomized trees.
                                Keywords: batch mode reinforcement learning, regression trees, ensemble methods, supervised
                                learning, ﬁtted value iteration, optimal control
                           1. Introduction
                           Researchinreinforcementlearning(RL)aimsatdesigningalgorithmsbywhichautonomousagents
                           can learn to behave in some appropriate fashion in some environment, from their interaction with
                           this environment or from observations gathered from the environment (see e.g. Kaelbling et al.
                           (1996) or Sutton and Barto (1998) for a broad overview). The standard RL protocol considers a
                           performanceagentoperatingindiscrete time, observing at timet the environment state x , taking an
                                                                                                                                          t
                           action u , and receiving back information from the environment (the next state x                           and the instan-
                                     t                                                                                           t+1
                           taneous reward r ). After some ﬁnite time, the experience the agent has gathered from interaction
                                                t
                           with the environment may thus be represented by a set of four-tuples (x ,u ,r ,x                         ).
                                                                                                                     t   t  t   t+1
                                In on-line learning the performance agent is also the learning agent which at each time step can
                           revise its control policy with the objective of converging as quickly as possible to an optimal control
                           policy. In this paper we consider batch mode learning, where the learning agent is in principle not
                           directly interacting with the system but receives only a set of four-tuples and is asked to determine
                           c
                           2005DamienErnst,PierreGeurtsandLouisWehenkel.
                                                                                                          ERNST, GEURTS AND WEHENKEL
                                         from this set a control policy which is as close as possible to an optimal policy. Inspired by the
                                         on-line Q-learning paradigm (Watkins, 1989), we will approach this batch mode learning problem
                                         by computing from the set of four-tuples an approximation of the so-called Q-function deﬁned on
                                         the state-action space and by deriving from this latter function the control policy.
                                                 Whenthestateandactionspacesareﬁniteandsmallenough,theQ-functioncanberepresented
                                         in tabular form, and its approximation (in batch and in on-line mode) as well as the control policy
                                         derivation are straightforward. However, when dealing with continuous or very large discrete state
                                         and/or action spaces, the Q-function cannot be represented anymore by a table with one entry for
                                         each state-action pair. Moreover, in the context of reinforcement learning an approximation of the
                                         Q-function all over the state-action space must be determined from ﬁnite and generally very sparse
                                         sets of four-tuples.
                                                 Toovercomethisgeneralization problem, a particularly attractive framework is the one used by
                                         Ormoneit and Sen (2002) which applies the idea of ﬁtted value iteration (Gordon, 1999) to kernel-
                                         basedreinforcementlearning,andreformulatestheQ-functiondeterminationproblemasasequence
                                         of kernel-based regression problems. Actually, this framework makes it possible to take full advan-
                                         tage in the context of reinforcement learning of the generalization capabilities of any regression
                                         algorithm, and this contrary to stochastic approximation algorithms (Sutton, 1988; Tsitsiklis, 1994)
                                         whichcanonlyuseparametricfunction approximators (for example, linear combinations of feature
                                         vectors or neural networks). In the rest of this paper we will call this framework the ﬁtted Q iteration
                                         algorithm so as to stress the fact that it allows to ﬁt (using a set of four-tuples) any (parametric or
                                         non-parametric) approximation architecture to the Q-function.
                                                 The ﬁtted Q iteration algorithm is a batch mode reinforcement learning algorithm which yields
                                         an approximation of the Q-function corresponding to an inﬁnite horizon optimal control problem
                                         with discounted rewards, by iteratively extending the optimization horizon (Ernst et al., 2003):
                                                 • At the ﬁrst iteration it produces an approximation of a Q -function corresponding to a 1-step
                                                                                                                                                                     1
                                                      optimization. Since the true Q -function is the conditional expectation of the instantaneous
                                                                                                                   1
                                                      reward given the state-action pair (i.e., Q (x,u) = E[r |x = x,u = u]), an approximation of
                                                                                                                                        1                        t    t             t
                                                      it can be constructed by applying a (batch mode) regression algorithm to a training set whose
                                                      inputs are the pairs (x ,u ) and whose target output values are the instantaneous rewards r
                                                                                                  t     t                                                                                                                                  t
                                                      (i.e., q         =r).
                                                                  1,t         t
                                                 • The Nth iteration derives (using a batch mode regression algorithm) an approximation of a
                                                      Q -function corresponding to an N-step optimization horizon. The training set at this step
                                                         N
                                                      is obtained by merely refreshing the output values of the training set of the previous step by
                                                      using the “value iteration” based on the approximate Q -function returned at the previous
                                                                                                              ˆ                                                       N
                                                      step (i.e., q              =r +γmax Q                             (x        ,u), where γ ∈ [0,1) is the discount factor).
                                                                           N,t          t                 u     N−1 t+1
                                         Ormoneit and Sen (2002) have studied the theoretical convergence and consistency properties of
                                         this algorithm when combined with kernel-based regressors. In this paper, we study within this
                                         framework the empirical properties and performances of several tree-based regression algorithms
                                         on several applications. Just like kernel-based methods, tree-based methods are non-parametric
                                         and offer a great modeling ﬂexibility, which is a paramount characteristic for the framework to be
                                         successful since the regression algorithm must be able to model any Q -function of the sequence,
                                                                                                                                                                                      N
                                         functions which are a priori totally unpredictable in shape. But, from a practical point of view these
                                         tree-based methods have a priori some additional advantages, such as their high computational
                                                                                                                                        504
                                   TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                  efﬁciency and scalability to high-dimensional spaces, their fully autonomous character, and their
                  recognized robustness to irrelevant variables, outliers, and noise.
                     In addition to good accuracy when trained with ﬁnite sets of four-tuples, one desirable feature of
                  the regression method used in the context of the ﬁtted Q iteration algorithm is to ensure convergence
                  of the sequence. We will analyze under which conditions the tree-based methods share this property
                  and also what is the relation between convergence and quality of approximation. In particular, we
                  will see that ensemblesoftotallyrandomizedtrees(i.e.,treesbuiltbyselectingtheirsplitsrandomly)
                  can be adapted to ensure the convergence of the sequence while leading to good approximation
                  performances. On the other hand, another tree-based algorithm named extremely randomized trees
                  (Geurts et al., 2004), will be found to perform consistently better than totally randomized trees even
                  though it does not strictly ensure the convergence of the sequence of Q-function approximations.
                     The remainder of this paper is organized as follows. In Section 2, we formalize the reinforce-
                  mentlearningproblemconsideredhereandrecallsomeclassicalresultsfromoptimalcontroltheory
                  upon which the approach is based. In Section 3 we present the ﬁtted Q iteration algorithm and in
                  Section 4 we describe the different tree-based regression methods considered in our empirical tests.
                  Section 5 is dedicated to the experiments where we apply the ﬁtted Q iteration algorithm used with
                  tree-based methods to several control problems with continuous state spaces and evaluate its perfor-
                  mancesinawiderangeofconditions. Section6concludesandalsoprovidesourmaindirectionsfor
                  further research. Three appendices collect relevant details about algorithms, mathematical proofs
                  and benchmark control problems.
                  2. Problem Formulation and Dynamic Programming
                  Weconsider a time-invariant stochastic system in discrete time for which a closed loop stationary
                  control policy1 must be chosen in order to maximize an expected discounted return over an inﬁnite
                  timehorizon. Weformulatehereafterthebatchmodereinforcementlearningprobleminthiscontext
                  and we restate some classical results stemming from Bellman’s dynamic programming approach to
                  optimalcontroltheory(introducedinBellman,1957)andfromwhichtheﬁttedQiterationalgorithm
                  takes its roots.
                  2.1 Batch Mode Reinforcement Learning Problem Formulation
                  Let us consider a system having a discrete-time dynamics described by
                                           x   =f(x,u,w) t=0,1,···,                              (1)
                                            t+1     t t  t
                  whereforallt, the state x is an element of the state space X, the action u is an element of the action
                                      t                                      t
                  spaceU and the random disturbance w an element of the disturbance spaceW. The disturbance w
                                                  t                                 2              t
                  is generated by the time-invariant conditional probability distribution P (w|x,u).
                                                                            w
                     To the transition from t to t +1 is associated an instantaneous reward signal r = r(x ,u ,w )
                                                                                       t     t t  t
                  where r(x,u,w) is the reward function supposed to be bounded by some constant Br.
                     Let µ(·) : X → U denote a stationary control policy and Jµ denote the expected return ob-
                                                                       ∞
                  tained over an inﬁnite time horizon when the system is controlled using this policy (i.e., when
                   1. Indeed, in terms of optimality this restricted family of control policies is as good as the broader set of all non-
                    anticipating (and possibly time-variant) control policies.
                   2. In other words, the probability P(w = w|x = x,u = u) of occurrence of w = w given that the current state x and
                                             t    t    t                t                       t
                    the current control u are x and u respectively, is equal to P (w|x,u),∀t = 0,1,···.
                                  t                         w
                                                         505
                                                      ERNST, GEURTS AND WEHENKEL
                     u =µ(x ),∀t). For a given initial condition x = x, Jµ is deﬁned by
                      t      t                                    0      ∞
                                             µ                   N−1 t
                                           J (x)= lim      E    [    γ r(x ,µ(x ),w )|x = x],                        (2)
                                            ∞       N→∞    w      ∑       t    t   t   0
                                                             t   t=0
                                                        t=0,1,···,N−1
                     where γ is a discount factor (0 ≤ γ < 1) that weights short-term rewards more than long-term ones,
                     and where the conditional expectation is taken over all trajectories starting with the initial condi-
                                                                                          ∗
                     tion x0 = x. Our objective is to ﬁnd an optimal stationary policy µ , i.e. a stationary policy that
                     maximizes Jµ for all x.
                                  ∞
                         The existence of an optimal stationary closed loop policy is a classical result from dynamic
                     programming theory. It could be determined in principle by solving the Bellman equation (see
                     below,Eqn(6))giventheknowledgeofthesystemdynamicsandrewardfunction. However,thesole
                     information that we assume available to solve the problem is the one obtained from the observation
                     of a certain number of one-step system transitions (from t to t+1). Each system transition provides
                     the knowledge of a new four-tuple (x ,u ,r ,x     ) of information. Since, except for very special
                                                           t  t  t  t+1
                     conditions, it is not possible to determine exactly an optimal control policy from a ﬁnite sample of
                                                                                         ∗
                     such transitions, we aim at computing an approximation of such a µ from a set
                                                             l  l l  l
                                                    F ={(x ,u ,r ,x     ),l = 1,··· ,#F }
                                                             t  t t  t+1
                     of such four-tuples.
                         Wedonot make any particular assumptions on the way the set of four-tuples is generated. It
                     could be generated by gathering the four-tuples corresponding to one single trajectory (or episode)
                     as well as by considering several independently generated one or multi-step episodes.
                         Wecall this problem the batch mode reinforcement learning problem because the algorithm is
                     allowed to use a set of transitions of arbitrary size to produce its control policy in a single step. In
                     contrast, an on-line algorithm would produce a sequence of policies corresponding to a sequence of
                     four-tuples.
                     2.2 Results from Dynamic Programming Theory
                     For a temporal horizon of N steps, let us denote by
                                                 π (t,x) ∈U,t ∈{0,···,N−1};x∈X
                                                   N
                     a (possibly time-varying) N-step control policy (i.e., u = π (t,x ) ), and by
                                                                           t    N    t
                                             π                N−1 t
                                            J N(x) =    E    [    γ r(x ,π (t,x ),w )|x = x]                         (3)
                                             N          w      ∑       t  N    t   t   0
                                                          t   t=0
                                                     t=0,1,···,N−1
                                                                                 ∗
                     its expected return over N steps. An N-step optimal policy π   is a policy which among all possible
                                               π                                 N
                                                N                                                          ´
                     suchpoliciesmaximizesJ       for anyx. Noticethatundermildconditions(seee.g.Hernandez-Lerma
                                               N
                     and Lasserre, 1996, for the detailed conditions) such a policy always does indeed exist although it
                     is not necessarily unique.
                         Ouralgorithm exploits the following classical results from dynamic programming theory (Bell-
                     man, 1957):
                                                                     506
                                                 TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                            1. The sequence of Q -functions deﬁned on X ×U by
                                                     N
                                                              Q (x,u) ≡ 0                                                              (4)
                                                                0
                                                             Q (x,u) = (HQ               )(x,u),   ∀N>0,                               (5)
                                                                N                   N−1
                                converges(ininﬁnitynorm)totheQ-function,deﬁnedasthe(unique)solutionoftheBellman
                                equation:
                                                                      Q(x,u)=(HQ)(x,u)                                                 (6)
                                                                                                                                  3
                                where H is an operator mapping any function K : X ×U → R and deﬁned as follows:
                                                      (HK)(x,u) = E[r(x,u,w)+γmaxK(f(x,u,w),u0)].                                      (7)
                                                                          w                 u0∈U
                                Uniqueness of solution of Eqn (6) as well as convergence of the sequence of Q -functions
                                                                                                                             N
                                to this solution are direct consequences of the ﬁxed point theorem and of the fact that H is a
                                contraction mapping.
                            2. The sequence of policies deﬁned by the two conditions4
                                                           ∗                                0
                                                         π (0,x) = argmaxQ (x,u),∀N >0                                                 (8)
                                                           N                         N
                                                                            u0∈U
                                                       ∗                    ∗
                                                     π (t+1,x) = π              (t,x),∀N >1,t ∈{0,...,N−2}                             (9)
                                                       N                    N−1
                                are N-step optimal policies, and their expected returns over N steps are given by
                                                                        ∗
                                                                       π
                                                                     J N(x) =maxQ (x,u).
                                                                      N          u∈U   N
                                            ∗
                            3. A policy µ that satisﬁes
                                                                         ∗
                                                                        µ (x) =argmaxQ(x,u)                                           (10)
                                                                                    u∈U
                                                                                                                                   ∗     .
                                is an optimal stationary policy for the inﬁnite horizon case and the expected return of µ (x)=
                                                                                                                                   N
                                 ∗                                                  ∗
                                π (0,x) converges to the expected return of µ :
                                 N
                                                                          µ∗         µ∗
                                                                    lim J N(x) = J (x)        ∀x∈X.                                   (11)
                                                                   N→∞ ∞             ∞
                                                            ∗         ∗
                                                          π          µ
                                WehavealsolimN→∞J N(x)=J (x) ∀x∈X.
                                                          N          ∞
                        Equation (5) deﬁnes the so-called value iteration algorithm5 providing a way to determine iter-
                        atively a sequence of functions converging to the Q-function and hence of policies whose return
                        converges to that of an optimal stationary policy, assuming that the system dynamics, the reward
                        function and the noise distribution are known. As we will see in the next section, it suggests also a
                        waytodetermine approximations of these Q -functions and policies from a sample F .
                                                                            N
                          3. The expectation is computed by using P(w) = P (w|x,u).
                                                                          w
                          4. Actually this deﬁnition does not necessarily yield a unique policy, but any policy which satisﬁes these conditions is
                            appropriate.
                                                                                                           ∗
                          5. Strictly, the term “value iteration” refers to the computation of the value function Jµ and corresponds to the iteration
                                                                                                          ∞
                              ∗                        ∗
                             π                        π
                            J N =maxE[r(x,u,w)+γJ N−1(f(x,u,w))],∀N >0 rather than Eqn (5).
                             N     u∈U w              N−1
                                                                                507
                                                                                                          ERNST, GEURTS AND WEHENKEL
                                         3. Fitted Q Iteration Algorithm
                                         In this section, we introduce the ﬁtted Q iteration algorithm which computes from a set of four-
                                         tuples an approximation of the optimal stationary policy.
                                         3.1 The Algorithm
                                         Atabular version of the ﬁtted Q iteration algorithm is given in Figure 1. At each step this algorithm
                                         mayusethefullsetoffour-tuplesgatheredfromobservationofthesystemtogetherwiththefunction
                                         computedattheprevioussteptodetermineanewtrainingsetwhichisusedbyasupervisedlearning
                                                                                                                                                                                                                                      ˆ
                                         (regression) method to compute the next function of the sequence. It produces a sequence of Q -
                                                                                                                                                                                                                                        N
                                         functions, approximations of the Q -functions deﬁned by Eqn (5).
                                                                                                              N
                                         Inputs: a set of four-tuples F and a regression algorithm.
                                         Initialization:
                                         Set N to 0 .
                                                   ˆ
                                         Let Q beafunction equal to zero everywhere on X ×U.
                                                     N
                                         Iterations:
                                         Repeat until stopping conditions are reached
                                                      - N ←N+1.
                                                                                                                      l     l                                                                                        ˆ
                                                      - Build the training set T S = {(i ,o ), l = 1,··· ,#F } based on the the function Q                                                                                     and on
                                                      the full set of four-tuples F :                                                                                                                                  N−1
                                                                                                                   l                 l     l
                                                                                                                  i      = (x,u),                                                                                                    (12)
                                                                                                                                     t     t
                                                                                                                   l               l                  ˆ             l
                                                                                                                 o       = r +γmaxQ                            (x         ,u).                                                       (13)
                                                                                                                                   t         u∈U        N−1 t+1
                                                                                                                                                                                    ˆ
                                                      - Use the regression algorithm to induce from T S the function Q (x,u).
                                                                                                                                                                                      N
                                                                                                      Figure 1: Fitted Q iteration algorithm
                                                 Notice that at the ﬁrst iteration the ﬁtted Q iteration algorithm is used in order to produce an
                                         approximation of the expected reward Q (x,u) = E [r(x,u,w)]. Therefore, the considered training
                                                                                                                         1                     w
                                                                                                                  l     l
                                         set uses input/outputpairs(denoted(i ,o ))wheretheinputsarethestate-actionpairsandtheoutputs
                                         the observed rewards. In the subsequent iterations, only the output values of these input/output pairs
                                                                                                                                               ˆ
                                         are updated using the value iteration based on the Q -function produced at the preceding step and
                                                                                                                                                 N
                                         information about the reward and the successor state reached in each tuple.
                                                 It is important to realize that the successive calls to the supervised learning algorithm are totally
                                         independent. Hence, at each step it is possible to adapt the resolution (or complexity) of the learned
                                         modelsoastoreachthebestbias/variance tradeoff at this step, given the available sample.
                                         3.2 Algorithm Motivation
                                         To motivate the algorithm, let us ﬁrst consider the deterministic case. In this case the system dy-
                                         namics and the reward signal depend only on the state and action at time t. In other words we have
                                                                                                                                        508
                                                                                 TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                        x        =f(x,u)andr =r(x,u)andEqn(5)mayberewritten
                                          t+1                t    t              t            t     t
                                                                                                                                                                             0
                                                                                              Q (x,u)=r(x,u)+γmaxQ                                        (f(x,u),u ).                                                         (14)
                                                                                                 N                                       0         N−1
                                                                                                                                       u ∈U
                                        If we suppose that the function Q                                         is known, we can use this latter equation and the set of four-
                                                                                                         N−1                                                                                l    l
                                        tuples F in order to determine the value of Q                                                  for the state-action pairs (x ,u ),l = 1,2,··· ,#F .
                                                                                                                                  N                                                         t    t
                                                                                  l     l          l                              l         0                  l                  l    l              l            l     l
                                        WehaveindeedQ (x ,u )=r +γmaxQ                                                        (x        ,u ), since x                 =f(x,u)andr =r(x,u).
                                                                           N t t                   t        u0∈U       N−1 t+1                                 t+1                t    t              t            t     t
                                                                                                                                 l     l              l     l
                                                WecanthusbuildatrainingsetT S ={((x ,u ),Q (x ,u )),l =1,···,#F }andusearegression
                                                                                                                                 t     t       N t t
                                        algorithm in order to generalize this information to any unseen state-action pair or, stated in another
                                                                                                                                                                                                            ˆ
                                        way,toﬁtafunctionapproximatortothistrainingsetinordertogetanapproximationQ ofQ over
                                                                                                                                   ˆ                                                                           N           N
                                        the whole state-action space. If we substitute Q                                                 for Q we can, by applying the same reasoning,
                                                                                   ˆ            ˆ                                    N              N
                                        determine iteratively Q                             , Q          , etc.
                                                                                     N+1          N+2
                                                In the stochastic case, the evaluation of the right hand side of Eqn (14) for some four-tuples
                                        (x ,u ,r ,x               ) is no longer equal to Q (x ,u ) but rather is the realization of a random variable
                                            t     t    t    t+1                                                     N t t                                                                                  6
                                        whose expectation is Q (x ,u ). Nevertheless, since a regression algorithm usually seeks an ap-
                                                                                      N t t
                                        proximation of the conditional expectation of the output variable given the inputs, its application
                                        to the training set T S will still provide an approximation of Q (x,u) over the whole state-action
                                                                                                                                                                   N
                                        space.
                                        3.3 Stopping Conditions
                                        The stopping conditions are required to decide at which iteration (i.e., for which value of N) the
                                        process can be stopped. A simple way to stop the process is to deﬁne a priori a maximum number
                                                                                                                                                                                                                             ∗
                                        of iterations. This can be done for example by noting that for a sequence of optimal policies µ , an
                                                                                                                                                                                                                             N
                                        error boundonthesub-optimalityintermsofnumberofiterationsisgivenbythefollowingequation
                                                                                                              µ∗          µ∗                  γNBr
                                                                                                          kJ N −J k ≤2                                     .                                                                   (15)
                                                                                                              ∞           ∞ ∞                            2
                                                                                                                                           (1−γ)
                                        Given the value of Br and a desired level of accuracy, one can then ﬁx the maximum number of
                                        iterations by computing the minimum value of N such that the right hand side of this equation is
                                                                                                      7
                                        smaller than the tolerance ﬁxed.
                                                                                                                                                                                                                         ˆ
                                                Another possibility would be to stop the iterative process when the distance between Q                                                                                          and
                                          ˆ                                                                                                                                                                                 N
                                        Q           drops below a certain value. Unfortunately, for some supervised learning algorithms there is
                                            N−1                                                              ˆ
                                        no guarantee that the sequence of Q -functions actually converges and hence this kind of conver-
                                                                                                               N
                                        gence criterion does not necessarily make sense in practice.
                                        3.4 Control Policy Derivation
                                        Whenthestopping conditions - whatever they are - are reached, the ﬁnal control policy, seen as an
                                        approximation of the optimal stationary closed loop control policy is derived by
                                                                                                             ∗                              ˆ
                                                                                                          µˆ    (x) =argmaxQ (x,u).                                                                                            (16)
                                                                                                             N                                N
                                                                                                                               u∈U
                                           6. This is true in the case of least squares regression, i.e. in the vast majority of regression methods.
                                                                                                                                                     ∗                        ∗
                                           7. Equation (15) gives an upper bound on the suboptimality of µ                                               and not of µˆ . By exploiting this upper bound
                                                                                                                                                     N                        N
                                                                                                                                                                         ∗                                                  ∗
                                               to determine a maximum number of iterations, we assume implicitly that µˆ                                                     is a good approximation of µ                       (that
                                                    ∗         ∗                                                                                                          N                                                  N
                                                   µˆ       µ
                                               kJ N −J Nk is small).
                                                  ∞         ∞ ∞
                                                                                                                                    509
                                                                                                          ERNST, GEURTS AND WEHENKEL
                                                                                                                                                                                               ˆ
                                                 When the action space is discrete, it is possible to compute the value Q (x,u) for each value
                                                                                                                                                                                                 N
                                         of u and then ﬁnd the maximum. Nevertheless, in our experiments we have sometimes adopted
                                         a different approach to handle discrete action spaces. It consists of splitting the training samples
                                                                                                                                                                          ˆ
                                         according to the value of u and of building the approximation Q (x,u) by separately calling for
                                                                                                                                                                             N
                                         each value of u ∈U the regression method on the corresponding subsample. In other words, each
                                         such model is induced from the subset of four-tuples whose value of the action is u, i.e.
                                                                                                        F ={(x,u,r,x                            ) ∈ F |u =u}.
                                                                                                           u              t     t    t    t+1                   t
                                         At the end, the action at some point x of the state space is computed by applying to this state each
                                                        ˆ
                                         modelQ (x,u),u∈U andlookingforthevalueofuyielding the highest value.
                                                          N
                                                 When the action space is continuous, it may be difﬁcult to compute the maximum especially
                                         because we can not make any a priori assumption about the shape of the Q-function (e.g. convex-
                                         ity). However, taking into account particularities of the models learned by a particular supervised
                                         learning method, it may be more or less easy to compute this value (see Section 4.5 for the case of
                                         tree-based models).
                                         3.5 Convergence of the Fitted Q Iteration Algorithm
                                                                                                                                                                                                     ˆ
                                         The ﬁtted Q iteration algorithm is said to converge if there exists a function Q : X ×U → R such
                                         that ∀ε > 0 there exists a n ∈ N such that:
                                                                                                                ˆ           ˆ
                                                                                                            kQ −Qk <ε ∀N>n.
                                                                                                                  N             ∞
                                         Convergence may be ensured if we use a supervised learning method which given a sample T S =
                                               1     1                #T S        #T S
                                         {(i ,o ),...,(i                      ,o         )} produces at each call the model (proof in Appendix B):
                                                                                                                            #T S               l            l
                                                                                                               f (i) = ∑ kTS(i ,i)∗o ,                                                                                               (17)
                                                                                                                             l=1
                                                                                     l
                                         with the kernel k                       (i ,i) being the same from one call to the other within the ﬁtted Q iteration
                                                            8               TS
                                         algorithm and satisfying the normalizing condition:
                                                                                                                #T S                l
                                                                                                                 ∑|kTS(i ,i)|=1,∀i.                                                                                                  (18)
                                                                                                                 l=1
                                         Supervised learning methods satisfying these conditions are for example the k-nearest-neighbors
                                         method, partition and multi-partition methods, locally weighted averaging, linear, and multi-linear
                                         interpolation. They are collectively referred to as kernel-based methods (see Gordon, 1999; Or-
                                         moneit and Sen, 2002).
                                         3.6 Related Work
                                         As stated in the Introduction, the idea of trying to approximate the Q-function from a set of four-
                                         tuples by solving a sequence of supervised learning problems mayalreadybefoundinOrmoneitand
                                            8. This is true when the kernel does not depend on the output values of the training sample and when the supervised
                                                learning method is deterministic.
                                                                                                                                        510
                                                  TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                         Sen(2002). Thisworkhoweverfocusesonkernel-basedmethodsforwhichitprovidesconvergence
                         and consistency proofs, as well as a bias-variance characterization. While in our formulation state
                         and action spaces are handled in a symmetric way and may both be continuous or discrete, in their
                         workOrmoneitandSenconsideronlydiscreteactionspacesanduseaseparatekernelforeachvalue
                         of the action.
                             TheworkofOrmoneitandSenisrelatedtoearlierworkaimedtosolvelarge-scaledynamicpro-
                         grammingproblems(seeforexampleBellmanetal.,1973;Gordon,1995b;TsitsiklisandVanRoy,
                         1996; Rust, 1997). The main difference is that in these works the various elements that compose
                         the optimal control problem are supposed to be known. We gave the name ﬁtted Q iteration to our
                         algorithm given in Figure 1 to emphasize that it is a reinforcement learning version of the ﬁtted
                         value iteration algorithm whose description may be found in Gordon (1999). Both algorithms are
                         quite similar except that Gordon supposes that a complete generative model is available,9 which is
                         a rather strong restriction with respect to the assumptions of the present paper.
                             Inhiswork,Gordoncharacterizesaclassofsupervisedlearningmethodsreferredtoasaveragers
                         that lead to convergence of his algorithm. These averagers are in fact a particular family of kernels
                         as considered by Ormoneit and Sen. In Boyan and Moore (1995), serious convergence problems
                         that may plague the ﬁtted value iteration algorithm when used with polynomial regression, back-
                         propagation, or locally weighted regression are shown and these also apply to the reinforcement
                         learning context. In their paper, Boyan and Moore propose also a way to overcome this problem
                         by relying on some kind of Monte-Carlo simulations. In Gordon (1995a) and Singh et al. (1995)
                         on-line versions of the ﬁtted value iteration algorithm used with averagers are presented.
                             In Moore and Atkeson (1993) and Ernst (2003), several reinforcement learning algorithms
                         closely related to the ﬁtted Q iteration algorithm are given. These algorithms, known as model-
                         based algorithms, build explicitly from the set of observations a ﬁnite Markov Decision Process
                         (MDP)whosesolution is then used to adjust the parameters of the approximation architecture used
                         to represent the Q-function. When the states of the MDP correspond to a ﬁnite partition of the
                         original state space, it can be shown that these methods are strictly equivalent to using the ﬁtted Q
                         iteration algorithm with a regression method which consists of simply averaging the output values
                         of the training samples belonging to a given cell of the partition.
                             In Boyan (2002), the Least-Squares Temporal-Difference (LSTD) algorithm is proposed. This
                         algorithm uses linear approximation architectures and learns the expected return of a policy. It is
                         similar to the ﬁtted Q iteration algorithm combined with linear regression techniques on problems
                         for which the action space is composed of a single element. Lagoudakis and Parr (2003a) intro-
                         duce the Least-Squares Policy Iteration (LSPI) which is an extension of LSTD to control problems.
                         The model-based algorithms in Ernst (2003) that consider representative states as approximation
                         architecture may equally be seen as an extension of LSTD to control problems.
                             Finally, we would like to mention some recent works based on the idea of reductions of rein-
                         forcement learning to supervised learning (classiﬁcation or regression) with various assumptions
                         concerning the available a priori knowledge (see e.g. Kakade and Langford, 2002; Langford and
                         Zadrozny, 2004, and the references therein). For example, assuming that a generative model is
                         available,10 an approach to solve the optimal control problem by reformulating it as a sequence of
                          9. Gordon supposes that the functions f(·,·,·), r(·,·,·), and P (·|·,·) are known and considers training sets composed of
                                                                                    w
                                                                        ∗
                                                                       π
                                                                       ˆ N−1
                             elements of the type (x,maxE[r(x,u,w)+γJ      (f(x,u,w))]).
                                                    u∈U w              N−1
                         10. Agenerativemodelallowssimulatingtheeffectofanyactiononthesystematanystartingpoint;thisislessrestrictive
                             than the complete generative model assumption of Gordon (footnote 9, page 511).
                                                                                511
                                                                 ERNST, GEURTS AND WEHENKEL
                          standard supervised classiﬁcation problems has been developed (see Lagoudakis and Parr, 2003b;
                          Bagnell et al., 2003), taking its roots from the policy iteration algorithm, another classical dynamic
                          programming algorithm. Within this “reductionist” framework, the ﬁtted Q iteration algorithm can
                          beconsideredasareductionofreinforcementlearningtoasequenceofregressiontasks,inspiredby
                          the value iteration algorithm and usable in the rather broad context where the available information
                          is given in the form of a set of four-tuples. This batch mode context incorporates indeed both the
                          on-line context (since one can always store data gathered on-line, at least for a ﬁnite time interval) as
                          well as the generative context (since one can always use the generative model to generate a sample
                          of four-tuples) as particular cases.
                          4. Tree-Based Methods
                          Wewill consider in our experiments ﬁve different tree-based methods all based on the same top-
                          down approach as in the classical tree induction algorithm. Some of these methods will produce
                          from the training set a model composed of one single regression tree while the others build an en-
                          sembleofregressiontrees. Wecharacterizeﬁrstthemodelsthatwillbeproducedbythesetree-based
                          methods and then explain how the different tree-based methods generate these models. Finally, we
                          will consider some speciﬁc aspects related to the use of tree-based methods with the ﬁtted Q itera-
                          tion algorithm.
                          4.1 Characterization of the Models Produced
                          Aregressiontreepartitions the input space into several regions and determines a constant prediction
                          in each region of the partition by averaging the output values of the elements of the training set T S
                          whichbelongtothisregion. LetS(i)bethefunctionthatassignstoaninputi(i.e.,astate-actionpair)
                          the region of the partition it belongs to. A regression tree produces a model that can be described
                          byEqn(17)withthekerneldeﬁnedbytheexpression:
                                                                                            l
                                                                                      I   (i )
                                                                        l              S(i)
                                                                 kTS(i ,i) = ∑              I   (a)                                         (19)
                                                                                  (a,b)∈T S S(i)
                          where I (·) denotes the characteristic function of the region B (I (i) = 1 if i ∈ B and 0 otherwise).
                                   B                                                                   B
                              When a tree-based method builds an ensemble of regression trees, the model it produces av-
                          erages the predictions of the different regression trees to make a ﬁnal prediction. Suppose that a
                          tree-based ensemble method produces p regression trees and gets as input a training set T S. Let
                          TSm11 bethetraining set used to build the mth regression tree (and therefore the mth partition) and
                          S (i) be the function that assigns to each i the region of the mth partition it belongs to. The model
                           m
                          produced by the tree-based method may also be described by Eqn (17) with the kernel deﬁned now
                          bythe expression:
                                                                               p                l
                                                                           1            I     (i )
                                                                                         S (i)
                                                                  l                       m
                                                            kTS(i ,i) = p ∑ ∑                  I     (a).                                   (20)
                                                                                    (a,b)∈T S   S (i)
                                                                             m=1             m m
                              It should also be noticed that kernels (19) and (20) satisfy the normalizing condition (18).
                          11. These subsets may be obtained in different ways from the original training set, e.g. by sampling with or without
                              replacement, but we can assume that each element of T Sm is also an element of T S.
                                                                                   512
                                            TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                      4.2 The Different Tree-Based Algorithms
                      All the tree induction algorithms that we consider are top-down in the sense that they create their
                      partition by starting with a single subset and progressively reﬁning it by splitting its subsets into
                      pieces. The tree-based algorithms that we consider differ by the number of regression trees they
                      build (one or an ensemble), the way they grow a tree from a training set (i.e., the way the different
                      tests inside the tree are chosen) and, in the case of methods that produce an ensemble of regression
                      trees, also the way they derive from the original training set T S the training set T Sm they use to
                      build a particular tree. They all consider binary splits of the type [ij < t], i.e. “if ij smaller than t go
                      left else go right” where ij represents the jth input (or jth attribute) of the input vector i. In what
                      follows the split variables t and ij are referred to as the cut-point and the cut-direction (or attribute)
                      of the split (or test) [ij < t].
                          Wenowdescribethetree-based regression algorithms used in this paper.
                      4.2.1 KD-TREE
                      In this method the regression tree is built from the training set by choosing the cut-point at the local
                      median of the cut-direction so that the tree partitions the local training set into two subsets of the
                      same cardinality. The cut-directions alternate from one node to the other: if the direction of cut is
                      ij for the parent node, it is equal to ij+1 for the two children nodes if j+1 < n with n the number
                      of possible cut-directions and i1 otherwise. A node is a leaf (i.e., is not partitioned) if the training
                      sample corresponding to this node contains less than n      tuples. In this method the tree structure is
                                                                              min
                      independent of the output values of the training sample, i.e. it does not change from one iteration to
                      another of the ﬁtted Q iteration algorithm.
                      4.2.2 PRUNED CART TREE
                      The classical CART algorithm is used to grow completely the tree from the training set (Breiman
                      et al., 1984). This algorithm selects at a node the test (i.e., the cut-direction and cut-point) that
                      maximizestheaveragevariancereductionoftheoutputvariable (see Eqn (25) in Appendix A). The
                      tree is pruned according to the cost-complexity pruning algorithm with error estimate by ten-fold
                      cross validation. Because of the score maximization and the post-pruning, the tree structure depends
                      onthe output values of the training sample; hence, it may change from one iteration to another.
                      4.2.3 TREE BAGGING
                      Werefer here to the standard algorithm published by Breiman (1996). An ensemble of M trees is
                      built. Each tree of the ensemble is grown from a training set by ﬁrst creating a bootstrap replica
                      (random sampling with replacement of the same number of elements) of the training set and then
                      building an unpruned CART tree using that replica. Compared to the Pruned CART Tree algorithm,
                      Tree Bagging often improves dramatically the accuracy of the model produced by reducing its
                      variance but increases the computing times signiﬁcantly. Note that during the tree building we also
                      stop splitting a node if the number of training samples in this node is less than n   . This algorithm
                                                                                                         min
                      has therefore two parameters, the number M of trees to build and the value of nmin.
                                                                        513
                                                                 ERNST, GEURTS AND WEHENKEL
                                                                            Onesingle                    Anensembleof
                                                                      regression tree is built       regression trees is built
                              Tests do depend on the output                    CART                       Tree Bagging
                               values (o) of the (i,o) ∈ T S                                               Extra-Trees
                            Tests do not depend on the output                 Kd-Tree              Totally Randomized Trees
                               values (o) of the (i,o) ∈ T S
                             Table 1: Main characteristics of the different tree-based algorithms used in the experiments.
                         4.2.4 EXTRA-TREES
                         Besides Tree Bagging, several other methods to build tree ensembles have been proposed that often
                         improve the accuracy with respect to Tree Bagging (e.g. Random Forests, Breiman, 2001). In
                         this paper, we evaluate our recently developed algorithm that we call “Extra-Trees”, for extremely
                         randomizedtrees (Geurts et al., 2004). Like Tree Bagging, this algorithm works by building several
                         (M) trees. However, contrary to Tree Bagging which uses the standard CART algorithm to derive
                         the trees from a bootstrap sample, in the case of Extra-Trees, each tree is built from the complete
                         original training set. To determine a test at a node, this algorithm selects K cut-directions at random
                         andforeachcut-direction,acut-pointatrandom. ItthencomputesascoreforeachoftheK testsand
                         chooses among these K tests the one that maximizes the score. Again, the algorithm stops splitting
                         anodewhenthenumberofelementsinthisnodeislessthanaparametern                                 . Three parameters are
                                                                                                                  min
                         associated to this algorithm: the number M of trees to build, the number K of candidate tests at each
                         node and the minimal leaf size nmin. The detailed tree building procedure is given in Appendix A.
                         4.2.5 TOTALLY RANDOMIZED TREES
                         Totally Randomized Trees corresponds to the case of Extra-Trees when the parameter K is chosen
                         equal to one. Indeed, in this case the tests at the different nodes are chosen totally randomly and
                         independently from the output values of the elements of the training set. Actually, this algorithm is
                         equivalent to an algorithm that would build the tree structure totally at random without even looking
                         at the training set and then use the training set only to remove the tests that lead to empty branches
                         and decide when to stop the development of a branch (Geurts et al., 2004). This algorithm can
                         therefore be degenerated in the context of the usage that we make of it in this paper by freezing the
                         tree structure after the ﬁrst iteration, just as the Kd-Trees.
                         4.2.6 DISCUSSION
                         Table 1 classiﬁes the different tree-based algorithms considered according to two criteria: whether
                         they build one single or an ensemble of regression trees and whether the tests computed in the trees
                         depend on the output values of the elements of the training set. We will see in the experiments that
                         these two criteria often characterize the results obtained.
                              Concerning the value of parameter M (the number of trees to be built) we will use the same
                         value for Tree Bagging, Extra-Trees and Totally Randomized Trees and set it equal to 50 (except in
                         Section 5.3.6 where we will assess its inﬂuence on the solution computed).
                                                                                  514
                                         TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                        FortheExtra-Trees,experimentsinGeurtsetal.(2004)haveshownthatagooddefaultvaluefor
                    the parameter K in regression is actually the dimension of the input space. In all our experiments,
                    Kwillbesettothis default value.
                        While pruning generally improves signiﬁcantly the accuracy of single regression trees, in the
                    context of ensemble methods it is commonly admitted that unpruned trees are better. This is sug-
                    gested from the bias/variance tradeoff, more speciﬁcally because pruning reduces variance but in-
                    creases bias and since ensemble methods reduce very much the variance without increasing too
                    muchbias,thereisoftennoneedforpruningtreesinthecontextofensemblemethods. However,in
                    high-noise conditions, pruning may be useful even with ensemble methods. Therefore, we will use
                    a cross-validation approach to automatically determine the value of nmin in the context of ensemble
                    methods. In this case, pruning is carried out by selecting at random two thirds of the elements of
                    TS,usingthe particular ensemble method with this smaller training set and determining for which
                    value of n    the ensemble minimizes the square error over the last third of the elements. Then,
                               min
                    the ensemble method is run again on the whole training set using this value of n  to produce the
                                                                                                   min
                    ﬁnal model. In our experiments, the resulting algorithm will have the same name as the original
                    ensemble method preceded by the term Pruned (e.g. Pruned Tree Bagging). The same approach
                    will also be used to prune Kd-Trees.
                    4.3 Convergence of the Fitted Q Iteration Algorithm
                    Sincethemodelsproducedbythetree-basedmethodsmaybedescribedbyanexpressionofthetype
                                              l
                    (17) with the kernel k  (i ,i) satisfying the normalizing condition (18), convergence of the ﬁtted Q
                                         TS
                                                                        l
                    iteration algorithm can be ensured if the kernel kT S(i ,i) remains the same from one iteration to the
                    other. This latter condition is satisﬁed when the tree structures remain unchanged throughout the
                    different iterations.
                        FortheKd-Treealgorithmwhichselectstestsindependentlyoftheoutputvaluesoftheelements
                    of the training set, it can be readily seen that it will produce at each iteration the same tree structure
                    if the minimum number of elements to split a leaf (nmin) is kept constant. This also implies that the
                    tree structure has just to be built at the ﬁrst iteration and that in the subsequent iterations, only the
                    values of the terminal leaves have to be refreshed. Refreshment may be done by propagating all the
                    elements of the new training set in the tree structure and associating to a terminal leaf the average
                    output value of the elements having reached this leaf.
                        For the totally randomized trees, the tests do not depend either on the output values of the
                    elements of the training set but the algorithm being non-deterministic, it will not produce the same
                    tree structures at each call even if the training set and the minimum number of elements (n  ) to
                                                                                                              min
                    split a leaf are kept constant. However, since the tree structures are independent from the output, it
                    is not necessary to refresh them from one iteration to the other. Hence, in our experiments, we will
                    build the set of totally randomized trees only at the ﬁrst iteration and then only refresh predictions
                    at terminal nodes at subsequent iterations. The tree structures are therefore kept constant from one
                    iteration to the other and this will ensure convergence.
                    4.4 No Divergence to Inﬁnity
                                                          ˆ                              ˆ
                    Wesaythatthesequenceoffunctions Q diverges to inﬁnity if limkQ k →∞.
                                                           N                      N→∞     N ∞
                        With the tree-based methods considered in this paper, such divergence to inﬁnity is impossible
                    since we can guarantee that, even for the tree-based methods for which the tests chosen in the tree
                                                                   515
                                             ERNST, GEURTS AND WEHENKEL
                                                                                         ˆ
                  depend on the output values (o) of the input-output pairs ((i,o)), the sequence of Q -functions
                                                                                          N
                  remainsbounded. Indeed, the prediction value of a leaf being the average value of the outputs of the
                                                                       ˆ                 ˆ
                  elementsofthetrainingsetthatcorrespondtothisleaf,wehavekQ (x,u)k ≤B +γkQ   (x,u)k
                                                              ˆ         N      ∞    r     N−1     ∞
                  where B is the bound of the rewards. And, since Q (x,u) = 0 everywhere, we therefore have
                         r                                     0
                   ˆ           Br
                  kQ (x,u)k ≤     ∀N∈N.
                    N      ∞   1−γ
                     However, we have observed in our experiments that for some other supervised learning meth-
                  ods, divergence to inﬁnity problems were plaguing the ﬁtted Q iteration algorithm (Section 5.3.3);
                  such problems have already been highlighted in the context of approximate dynamic programming
                  (Boyan and Moore, 1995).
                                           ˆ
                  4.5 Computation of maxu∈UQN(x,u) when u Continuous
                                                  ˆ
                  In the case of a single regression tree, Q (x,u) is a piecewise-constant function of its argument u,
                                                   N           ˆ
                  whenﬁxingthe state value x. Thus, to determine maxQ (x,u), it is sufﬁcient to compute the value
                                                           u∈U  N
                    ˆ
                  of Q (x,u) for a ﬁnite number of values of U, one in each hyperrectangle delimited by the values
                     N
                  of discretization thresholds found in the tree.
                     Thesameargumentcanbeextendedtoensemblesofregressiontrees. However,inthiscase,the
                  numberofdiscretizationthresholdsmightbemuchhigherandthisresolutionschememightbecome
                  computationally inefﬁcient.
                  5. Experiments
                  Before discussing our simulation results, we ﬁrst give an overview of our test problems, of the
                  type of experiments carried out and of the different metrics used to assess the performances of the
                  algorithms.
                  5.1 Overview
                  Weconsider ﬁve different problems, and for each of them we use the ﬁtted Q iteration algorithm
                  with the tree-based methods described in Section 4 and assess their ability to extract from different
                  sets of four-tuples information about the optimal control policy.
                  5.1.1 TEST PROBLEMS
                  The ﬁrst problem, referred to as the “Left or Right” control problem, has a one-dimensional state
                  space and a stochastic dynamics. Performances of tree-based methods are illustrated and compared
                  with grid-based methods.
                     Next we consider the “Car on the Hill” test problem. Here we compare our algorithms in
                  depth with other methods (k-nearest-neighbors, grid-based methods, a gradient version of the on-
                  line Q-learning algorithm) in terms of accuracy and convergence properties. We also discuss CPU
                  considerations, analyze the inﬂuence of the number of trees built on the solution, and the effect of
                  irrelevant state variables and continuous action spaces.
                     Thethird problem is the “Acrobot Swing Up” control problem. It is a four-dimensional and de-
                  terministic control problem. While in the ﬁrst two problems the four-tuples are generated randomly
                  prior to learning, here we consider the case where the estimate of µ∗ deduced from the available
                  four-tuples is used to generate new four-tuples.
                                                         516
                                         TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                        The two last problems (“Bicycle Balancing” and “Bicycle Balancing and Riding”) are treated
                     together since they differ only in their reward function. They have a stochastic dynamics, a seven-
                     dimensional state space and a two-dimensional control space. Here we look at the capability of our
                     methodtohandle rather challenging problems.
                     5.1.2 METRICS TO ASSESS PERFORMANCES OF THE ALGORITHMS
                     In our experiments, we will use the ﬁtted Q iteration algorithm with several types of supervised
                     learning methods as well as other algorithms like Q-learning or Watkin’s Q(λ) with various ap-
                     proximation architectures. To rank performances of the various algorithms, we need to deﬁne some
                     metrics to measure the quality of the solution they produce. Hereafter we review the different met-
                     rics considered in this paper.
                     Expectedreturnofapolicy. Tomeasurethequalityofasolutiongiven by a RL algorithm, we can
                     use the stationary policy it produces, compute the expected return of this stationary policy and say
                     that the higher this expected return is, the better the RL algorithm performs. Rather than computing
                     the expected return for one single initial state, we deﬁne in our examples a set of initial states named
                     Xi, chosen independently from the set of four-tuples F , and compute the average expected return of
                     the stationary policy over this set of initial states. This metric is referred to as the score of a policy
                     and is the most frequently used one in the examples. If µ is the policy, its score is deﬁned by:
                                                                   ∑x∈XiJµ(x)
                                                       score of µ =       ∞                                      (21)
                                                                       #Xi
                     To evaluate this expression, we estimate, for every initial state x ∈ Xi, Jµ(x) by Monte-Carlo sim-
                                                                                           ∞              µ
                     ulations. If the control problem is deterministic, one simulation is enough to estimate J (x). If the
                                                                                                          ∞
                     control problem is stochastic, several simulations are carried out. For the “Left or Right” control
                     problem, 100,000 simulations are considered. For the “Bicycle Balancing” and “Bicycle Balancing
                     and Riding” problems, whose dynamics is less stochastic and Monte-Carlo simulations computa-
                     tionally more demanding, 10 simulations are done. For the sake of compactness, the score of µ is
                     represented in the ﬁgures by Jµ.
                                                  ∞
                     Fulﬁllment of a speciﬁc task. The score of a policy assesses the quality of a policy through its
                     expected return. In the “Bicycle Balancing” control problem, we also assess the quality of a policy
                     through its ability to avoid crashing the bicycle during a certain period of time. Similarly, for the
                     “Bicycle Balancing and Riding” control problem, we consider a criterion of the type “How often
                     does the policy manage to drive the bicycle, within a certain period of time, to a goal ?”.
                     Bellman residual. While the two previous metrics were relying on the policy produced by the
                     RL algorithm, the metric described here relies on the approximate Q-function computed by the
                                                         ˆ
                     RLalgorithm. For a given function Q and a given state-action pair (x,u), the Bellman residual is
                     deﬁned to be the difference between the two sides of the Bellman equation (Baird, 1995), the Q-
                     function being the only function leading to a zero Bellman residual for every state-action pair. In
                                                                        ˆ
                     our simulation, to estimate the quality of a function Q, we exploit the Bellman residual concept by
                                    ˆ                                                        i
                     associating to Q the mean square of the Bellman residual over the set X ×U, value that will be
                                                          ˆ
                     referred to as the Bellman residual of Q. We have
                                                                          ˆ            ˆ       2
                                                              ∑      i   (Q(x,u)−(HQ)(x,u))
                                                         ˆ     (x,u)∈X ×U
                                     Bellman residual of Q =              #(Xi×U)               .                (22)
                                                                   517
                                                       ERNST, GEURTS AND WEHENKEL
                                                               u +w
                                               Reward            t    t               Reward
                                                 =50          x         x              =100
                                                         0     t         t+1    10
                                                Figure 2: The “Left or Right” control problem.
                      This metric is only used in the “Left or Right” control problem to compare the quality of the solu-
                      tions obtained. A metric relying on the score is not discriminant enough for this control problem,
                      since all the algorithms considered can easily learn a good approximation of the optimal stationary
                                                                                   ˆ
                      policy. Furthermore, for this control problem, the term (HQ)(x,u) in the right side of Eqn (22) is
                      estimated by drawing independently and for each (x,u) ∈ Xi×U, 100,000 values of w according to
                      P (.|x,u) (see Eqn (7)).
                       w                                      ˆ                     ˆ    ˆ
                      In the ﬁgures, the Bellman residual of Q is represented by d(Q,HQ).
                      5.2 The “Left or Right” Control Problem
                      Weconsider here the “Left or Right” optimal control problem whose precise deﬁnition is given in
                      Appendix C.1.
                         The main characteristics of the control problem are represented on Figure 2. A point travels in
                      the interval [0,10]. Two control actions are possible. One tends to drive the point to the right (u = 2)
                      while the other to the left (u = −2). As long as the point stays inside the interval, only zero rewards
                                                                                        12
                      are observed. When the point leaves the interval, a terminal state  is reached. If the point goes out
                      onthe right side then a reward of 100 is obtained while it is twice less if it goes out on the left.
                                                                                             ∗
                         Evenifgoingoutontherightmayﬁnallyleadtoabetterreward,µ isnotnecessarilyequalto2
                      everywhere since the importance of the reward signal obtained after t steps is weighted by a factor
                      γ(t−1) = 0.75(t−1).
                      5.2.1 FOUR-TUPLES GENERATION
                      Tocollect the four-tuples we observe 300 episodes of the system. Each episode starts from an initial
                      state chosen at random in [0,10] and ﬁnishes when a terminal state is reached. During the episodes,
                      the action u selected at time t is chosen at random with equal probability among its two possible
                                  t
                      values u = −2 and u = 2. The resulting set F is composed of 2010 four-tuples.
                      5.2.2 SOME BASIC RESULTS
                      Toillustrate the ﬁtted Q iteration algorithm behavior we ﬁrst use “Pruned CART Tree” as supervised
                                                                                 ˆ
                      learning method. Elements of the sequence of functions Q      obtained are represented on Figure 3.
                                                                                  N
                      While the ﬁrst functions of the sequence differ a lot, they gain in similarities when N increases
                                                                                                ˆ        ˆ
                      which is conﬁrmed by computing the distance on F between functions Q and Q              (Figure 4a).
                                                                                                 N        N−1
                      Weobservethatthedistancerapidly decreases but, due to the fact that the tree structure is refreshed
                      at each iteration, never vanishes.
                      12. A terminal state can be seen as a regular state in which the system is stuck and for which all the future rewards
                         obtained in the aftermath are zero. Note that the value of Q (terminal state,u) is equal to 0 ∀N ∈ N and ∀u ∈U.
                                                                           N
                                                                      518
                                                                                       TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                              ˆ                                                                  ˆ                                                                 ˆ
                                             Q                                                                  Q                                                                 Q
                                               1                                                                  2                                                                  3
                                               100.                                                               100.                                                              100.
                                                                            ˆ                                                                ˆ                                                                 ˆ
                                                                           Q (x,2)                                                          Q (x,2)                                                           Q (x,2)
                                                75.                          1                                     75.                        2                                      75.                         3
                                                                                                                           ˆ                                                                 ˆ
                                                                                                                          Q (x,−2)                                                          Q (x,−2)
                                                        ˆ                                                                   2                                                                 3
                                                       Q (x,−2)
                                                50.      1                                                         50.                                                               50.
                                                25.                                                                25.                                                               25.
                                                0.0                                                                0.0                                                               0.0
                                                      0.0         2.5        5.          7.5        10. x                 0.0        2.5         5.         7.5         10. x               0.0        2.5         5.         7.5         10. x
                                             ˆ
                                            Q
                                              4                                                                  ˆ                                                                 ˆ
                                                                                                                Q                                                                 Q
                                              100.                                                                5                                                                  10
                                                                                                                  100.                                                              100.
                                                                          ˆ
                                                                         Q (x,2)
                                                                           4                                                                 ˆ                                                              ˆ
                                                                                                                                            Q (x,2)                                                        Q (x,2)
                                               75.                                                                 75.                        5                                      75.                      10
                                                       ˆ
                                                      Q (x,−2)                                                             ˆ                                                                 ˆ
                                                        4                                                                 Q (x,−2)                                                          Q (x,−2)
                                                                                                                            5                                                                  10
                                               50.                                                                 50.                                                               50.
                                               25.                                                                 25.                                                               25.
                                               0.0                                                                 0.0                                                               0.0
                                                      0.0         2.5        5.          7.5         10. x                0.0        2.5         5.         7.5         10. x               0.0        2.5         5.         7.5         10. x
                                                                                                    ˆ
                                           Figure 3: Representation of Q for different values of N. The set F is composed of 2010 elements and the
                                                                                                      N
                                                                supervised learning method used is Pruned CART Tree.
                                                                                                                      µˆ∗
                                                ˆ                                                                      N                                                                   ˆ
                                            d(Q ,                                                                   J                                                                 d(Q ,
                                             ˆ   N                                                                   ∞                                                                    ˆ N
                                            Q      )                                                                    64.                                                           HQ )
                                              N−1 35.                                                                                                                                      N
                                                     30.                                                                                                                                       12.5
                                                                                                                        62.
                                                     25.                                                                                                                                        10.
                                                     20.                                                                60.
                                                                                                                                                                                                7.5
                                                     15.
                                                     10.                                                                                                                                         5.
                                                                                                                        58.
                                                       5.                                                                                                                                       2.5
                                                     0.0                                                                                                                                        0.0
                                                         2     3     4     5      6     7     8     9     10 N              1    2     3    4    5     6    7    8     9    10 N                    1     2    3    4     5    6    7     8    9    10 N
                                                                                                                                                       µˆ∗
                                                                            ˆ       ˆ                                                                    N                                                             ˆ           ˆ
                                                           (a)        d(Q ,Q                 ) =                                          (b)        J       =                                        (c)        d(Q ,HQ )=
                                                                              N       N−1                                                              ∞                                                                 N           N
                                                           #F                                                                                         ∗                                                        ˆ                  ˆ            2
                                                                  ˆ     l   l      ˆ         l   l   2                                              µˆ                                             ∑         (Q (x,u)−(HQ )(x,u))
                                                        ∑ (Q (x ,u )−Q                    (x ,u ))                                                    N                                                i         N                  N
                                                           l=1     N t t            N−1 t t                                                  ∑ J (x)                                                 X ×U
                                                                             #F                                                                Xi ∞                                                                #(Xi×U)
                                                                                                                                                  #Xi
                                                                                                                                                   ˆ               ˆ
                                           Figure 4: Figure (a) represents the distance between Q                                                        and Q              . Figure (b) provides the average return
                                                                                                              ∗                                      N               N−1            i
                                                                obtained by the policy µˆ                        while starting from an element of X . Figure (c) represents the Bellman
                                                                                       ˆ                      N
                                                                residual of Q .
                                                                                         N
                                                                                                                                             519
                                                         ERNST, GEURTS AND WEHENKEL
                                              ˆ                                                         ˆ           ˆ
                          FromthefunctionQ wecandeterminethepolicyµˆ . StatesxforwhichQ (x,2)≥Q (x,−2)
                                               N                                 N                       N           N
                                                                                   ˆ            ˆ                           ∗
                      correspond to a value of µˆ (x) = 2 while µˆ (x) = −2 if Q (x,2) < Q (x,−2). For example, µˆ
                                                  N                 N                N           N                          10
                      consists of choosing u = −2 on the interval [0,2.7[ and u = 2 on [2.7,10]. To associate a score to
                                    ∗                              i                               µˆN
                      each policy µˆ , we deﬁne a set of states X = {0,1,2,··· ,10}, evaluate J      (x) for each element of
                                    N                                                              ∞
                                                                                                ∗
                      this set and average the values obtained. The evolution of the score of µˆ   with N is drawn on Figure
                                                                                                N
                      4b. We observe that the score ﬁrst increases rapidly to become ﬁnally almost constant for values of
                      Ngreater than 5.
                                                                             ˆ
                          In order to assess the quality of the functions Q     computed, we have computed the Bellman
                                         ˆ                                    N
                      residual of these Q -functions. We observe in Figure 4c that even if the Bellman residual tends to
                                          N
                      decrease when N increases, it does not vanish even for large values of N. By observing Table 2, one
                      can however see that by using 6251 four-tuples (1000 episodes) rather than 2010 (300 episodes),
                      the Bellman residual further decreases.
                      5.2.3 INFLUENCE OF THE TREE-BASED METHOD
                      Whendealingwithsuchasystemforwhichthedynamicsishighlystochastic,pruningisnecessary,
                      evenfortree-basedmethodsproducinganensembleofregressiontrees. Figure5thusrepresentsthe
                       ˆ
                      Q -functions for different values of N with the pruned version of the Extra-Trees. By comparing
                        N
                      this ﬁgure with Figure 3, we observe that the averaging of several trees produces smoother functions
                      than single regression trees.
                          Bywayofillustration, we have also used the Extra-Trees algorithm with fully developed trees
                                                          ˆ
                      (i.e., n  =2)andcomputedtheQ -functionwiththeﬁttedQiterationusingthesamesetoffour-
                             min                           10
                      tuples as in the previous section. This function is represented in Figure 6. As fully grown trees are
                      able to match perfectly the output in the training set, they also catch the noise and this explains the
                      chaotic nature of the resulting approximation.
                                                                      ˆ
                          Table 2 gathers the Bellman residuals of Q      obtained when using different tree-based methods
                                                                       10
                      and this for different sets of four-tuples. Tree-based ensemble methods produce smaller Bellman
                      residuals and among these methods, Extra-Trees behaves the best. We can also observe that for any
                      of the tree-based methods used, the Bellman residual decreases with the size of F .
                          Note that here, the policies produced by the different tree-based algorithms offer quite similar
                      scores. For example, the score is 64.30 when Pruned CART Tree is applied to the 2010 four-tuple
                      set and it does not differ from more than one percent with any of the other methods. We will see
                      that the main reason behind this, is the simplicity of the optimal control problem considered and the
                      small dimensionality of the state space.
                      5.2.4 FITTED Q ITERATION AND BASIS FUNCTION METHODS
                      WenowassessperformancesoftheﬁttedQiteration algorithm when combined with basis function
                      methods. Basis function methods suppose a relation of the type
                                                                      nbBasis
                                                                  o= ∑ cjφj(i)                                            (23)
                                                                       j=1
                      between the input and the output where cj ∈ R and where the basis functions φj(i) are deﬁned on
                      the input space and take their values on R. These basis functions form the approximation architec-
                                                                        520
                                                                                       TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                             ˆ                                                                 ˆ                                                                 ˆ
                                            Q                                                                 Q                                                                 Q
                                              1                                                                 2                                                                 3
                                              100.                                                              100.                                                              100.
                                                                                                                                          ˆ                                                                 ˆ
                                                                                                                                          Q (x,2)                                                          Q (x,2)
                                                                           ˆ                                                                2                                                                3
                                                                          Q (x,2)
                                               75.                          1                                    75.                                                               75.
                                                                                                                        ˆ                                                                  ˆ
                                                                                                                                                                                          Q (x,−2)
                                                      ˆ                                                                Q (x,−2)                                                              3
                                                     Q (x,−2)                                                             2
                                               50.     1                                                         50.                                                               50.
                                               25.                                                               25.                                                               25.
                                               0.0                                                               0.0                                                               0.0
                                                     0.0         2.5        5.          7.5        10. x               0.0         2.5        5.          7.5        10. x               0.0         2.5        5.          7.5        10. x
                                             ˆ                                                                 ˆ                                                                 ˆ
                                            Q                                                                 Q                                                                 Q
                                              4                                                                 5                                                                 10
                                              100.                                                              100.                                                              100.
                                                                        ˆ                                                                 ˆ                                                              ˆ
                                                                       Q (x,2)                                                           Q (x,2)                                                        Q (x,2)
                                               75.                        4                                      75.                       5                                       75.                     10
                                                      ˆ                                                                  ˆ                                                                 ˆ
                                                     Q (x,−2)                                                           Q (x,−2)                                                          Q (x,−2)
                                                       4                                                                  5                                                                 10
                                               50.                                                               50.                                                               50.
                                               25.                                                               25.                                                               25.
                                               0.0                                                               0.0                                                               0.0
                                                     0.0         2.5        5.          7.5        10. x               0.0         2.5        5.          7.5        10. x               0.0         2.5        5.          7.5        10. x
                                                                                                    ˆ
                                           Figure 5: Representation of Q for different values of N. The set F is composed of 2010 elements and the
                                                                                                      N
                                                                supervised learning method used is the Pruned Extra-Trees.
                                                                                                                                                            Tree-based                                              #F
                                                                                                                                                               method                               720           2010           6251
                                                          ˆ                                                                                          Pruned CARTTree                                2.62          1.96           1.29
                                                         Q
                                                           10
                                                           100.            ˆ                                                                            Pruned Kd-Tree                              1.94          1.31           0.76
                                                                          Q (x,2)
                                                                             10                                                                    Pruned Tree Bagging                              1.61          0.79           0.67
                                                            75.
                                                                   ˆ                                                                                 Pruned Extra-Trees                             1.29          0.60           0.49
                                                                  Q (x,−2)
                                                                    10
                                                            50.                                                                                 Pruned Tot. Rand. Trees                             1.55          0.72           0.59
                                                            25.                                                                                                                                          ˆ
                                                                                                                                             Table 2: Bellman residual of Q . Three different
                                                                                                                                                                                                           10
                                                            0.0                                                                                                 sets of four-tuples are used. These sets
                                                                  0.0         2.5        5.          7.5        10. x                                           have been generated by considering 100,
                                                                                                                                                                300 and 1000 episodes and are composed
                                                                                                      ˆ
                                           Figure 6: Representation of Q                                       when Extra-                                      respectively of 720, 2010 and 6251 four-
                                                                                                        10
                                                                Trees is used with no pruning                                                                   tuples.
                                           ture. The training set is used to determine the values of the different cj by solving the following
                                           minimization problem:13
                                                                                                                                                                                                                                    l             T
                                           13. This minimization problem can be solved by building the (#T S×nbBasis) Y matrix with Y                                                                                   =φj(i ). If Y Y
                                                                                                                                                                                                                    l j
                                                  is invertible, then the minimization problem has a unique solution c = (c ,c ,··· ,c                                                                       ) given by the following
                                                                                   T     −1 T                            #T S                               l                   1     2           nbBasis
                                                  expression: c = (Y Y)                       Y bwith b∈R                        such that bl = o . In order to overcome the possible problem of non-
                                                  invertibility ofYTY that occurs when solution of (24) is not unique, we have added toYTY the strictly deﬁnite positive
                                                  matrix δI, where δ is a small positive constant, before inverting it. The value of c used in our experiments as solution
                                                                                                      T              −1 T
                                                  of (24) is therefore equal to (Y Y +δI)                                Y bwhereδhasbeenchosenequalto0.001.
                                                                                                                                             521
                                                                                                                   ERNST, GEURTS AND WEHENKEL
                                                 ˆ
                                             d(Q ,
                                                   10                                                                  ˆ                                                                      ˆ
                                                 ˆ                                                                    Q                                                                      Q
                                             HQ )            piecewise-constant grid                                    10                                                                      10
                                                  10                                                                    100.                                                                    100.
                                                       2.5                                                                                       ˆ                                                                       ˆ
                                                                                                                                                Q (x,2)                                                                 Q (x,2)
                                                                                                                         75.                      10                                             75.                      10
                                                        2.                                                                         ˆ                                                                      ˆ
                                                                               piecewise-linear grid                              Q (x,−2)                                                               Q (x,−2)
                                                                                                                                    10                                                                      10
                                                       1.5                                                               50.                                                                     50.
                                                        1.
                                                                                                                         25.                                                                     25.
                                                       0.5
                                                                          Extra-Trees
                                                       0.0                                                               0.0                                                                     0.0
                                                                 10         20          30         40        Grid
                                                                                                             size               0.0          2.5         5.           7.5         10. x                 0.0         2.5          5.          7.5          10. x
                                                                                                                                                ˆ
                                                                                                                                        (b) Q          computed
                                                                                                                                                  10                                                             ˆ
                                                                                                                                                                                                         (c) Q           computed when
                                                                                                      ˆ                                  whenusinga28                                                               10
                                                      (a) Bellman residual of Q                                                                                                                        using a 7 piecewise-linear
                                                                                                        10                            piecewise-constant
                                                                                                                                     grid as approx. arch.                                                  grid as approx. arch.
                                            Figure 7: FittedQiterationwithbasisfunctionmethods. Twodifferenttypesofapproximationarchitectures
                                                                   are considered: piecewise-constant and piecewise-linear grids. 300 episodes are used to generate
                                                                   F.
                                                                                                                                                  #T S nbBasis                    l          l  2
                                                                                                                      argmin nbBasis ∑( ∑ cjφj(i )−o ) .                                                                                               (24)
                                                                                                        (c1,c2,···,cnbBasis)∈R                    l=1         j=1
                                                     Weconsider two different sets of basis functions φj. The ﬁrst set is deﬁned by partitioning the
                                            state space into a grid and by considering one basis function for each grid cell, equal to the indicator
                                                                                                                                                                     ˆ
                                            function of this cell. This leads to piecewise constant Q-functions. The other type is deﬁned by
                                            partitioning the state space into a grid, triangulating every element of the grid and considering that
                                              ˆ                                                               ˆ
                                            Q(x,u)=∑                                         W(x,v)Q(v,u) whereVertices(x) is the set of vertices of the hypertriangle x
                                                                      v∈Vertices(x)
                                            belongs to andW(x,v) is the barycentric coordinate of x that corresponds to v. This leads to a set of
                                            overlapping piecewise linear basis functions, and yields a piecewise linear and continuous model.
                                            In this paper, these approximation architectures are respectively referred to as piecewise-constant
                                            grid and piecewise-linear grid. The reader can refer to Ernst (2003) for more information.
                                                     To assess performances of ﬁtted Q iteration combined with piecewise-constant and piecewise-
                                            linear grids as approximation architectures, we have used several grid resolutions to partition the
                                            interval [0,10] (a 5 grid, a 6 grid, ···, a 50 grid). For each grid, we have used ﬁtted Q iteration
                                                                                                                                                                                                            ˆ
                                            with each of the two types of approximation architectures and computed Q . The Bellman resid-
                                                                                                                ˆ                                                                                             10
                                            uals obtained by the different Q -functions are represented on Figure 7a. We can see that basis
                                                                                                                  10
                                            function methods with piecewise-constant grids perform systematically worse than Extra-Trees, the
                                            tree-based method that produces the lowest Bellman residual. This type of approximation archi-
                                                                                                                                                                                                                                ˆ
                                            tecture leads to the lowest Bellman residual for a 28 grid and the corresponding Q -function is
                                                                                                                                                                                                                                  10
                                            sketched in Figure 7b. Basis function methods with piecewise-linear grids reach their lowest Bell-
                                            man residual for a 7 grid, Bellman residual that is smaller than the one obtained by Extra-Trees.
                                                                                                             ˆ
                                            Thecorresponding smoother Q -function is drawn on Figure 7b.
                                                                                                               10
                                                     Evenif piecewise-linear grids were able to produce on this example better results than the tree-
                                            based methods, it should however be noted that it has been achieved by tuning the grid resolution
                                            andthat this resolution strongly inﬂuences the quality of the solution. We will see below that, as the
                                                                                                                                                  522
                                                  TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                         state space dimensionality increases, piecewise-constant or piecewise-linear grids do not compete
                         anymore with tree-based methods. Furthermore, we will also observe that piecewise-linear grids
                         mayleadtodivergence to inﬁnity of the ﬁtted Q iteration algorithm (see Section 5.3.3).
                         5.3 The “Car on the Hill” Control Problem
                         Weconsiderherethe“CarontheHill”optimalcontrolproblemwhoseprecisedeﬁnitionisgivenin
                         Appendix C.2.
                             Acar modeled by a point mass is traveling on a hill (the shape of which is given by the func-
                         tion Hill(p) of Figure 8b). The action u acts directly on the acceleration of the car (Eqn (31),
                         AppendixC)andcanonlyassumetwoextremevalues(fullacceleration(u=4)orfulldeceleration
                         (u = −4)). The control problem objective is roughly to bring the car in a minimum time to the top
                         of the hill (p = 1 in Figure 8b) while preventing the position p of the car to become smaller than
                         −1anditsspeedstogooutsidetheinterval [−3,3]. This problem has a (continuous) state space of
                         dimension two (the position p and the speed s of the car) represented on Figure 8a.
                             Note that by exploiting the particular structure of the system dynamics and the reward function
                         of this optimal control problem, it is possible to determine with a reasonable amount of computation
                                               µ∗                                                    14
                         the exact value of J     (Q) for any state x (state-action pair (x,u)).
                                               ∞
                              s
                               3.
                               2.                                                            Hill(p)
                                                                                                  0.4 Resistance
                               1.
                              0.0                                                                 0.2         u
                              −1.                                                                       mg
                              −2.                                                     −1.    −.5     0.0    0.5     1. p
                              −3.                                                                −0.2
                                  −1.    −.5     0.0    0.5     1. p
                                   (a) X \{terminal state}            (b)   Representation of Hill(p) (shape of the hill) and
                                                                                 of the different forces applied to the car.
                                                      Figure 8: The “Car on the Hill” control problem.
                         5.3.1 SOME BASIC RESULTS
                         To generate the four-tuples we consider episodes starting from the same initial state corresponding
                         to the car stopped at the bottom of the hill (i.e., (p,s) = (−0.5,0) ) and stopping when the car leaves
                         the region represented on Figure 8a (i.e., when a terminal state is reached). In each episode, the
                         action u at each time step is chosen with equal probability among its two possible values u = −4
                                  t
                         andu=4. Weconsider1000episodes. ThecorrespondingsetF iscomposedof58090four-tuples.
                         Note that during these 1000 episodes the reward r(x ,u ,w ) = 1 (corresponding to an arrival of the
                                                                                     t   t   t
                         car at the top of the hill with a speed comprised in [−3,3]) has been observed only 18 times.
                                           ∗
                         14. To compute Jµ (x), we determine by successive trials the smallest value of k for which one of the two following
                                          ∞
                             conditions is satisﬁed (i) at least one sequence of actions of length k leads to a reward equal to 1 when x = x (ii) all
                                                                                                                                 0
                                                                                523
                                                              ERNST, GEURTS AND WEHENKEL
                           s                                        s                                        s
                            3.                                      3.                                       3.
                            2.                                      2.                                       2.
                            1.                                      1.                                       1.
                           0.0                                      0.0                                      0.0
                           −1.                                     −1.                                      −1.
                           −2.                                     −2.                                      −2.
                           −3.                                     −3.                                      −3.
                              −1.     −.5    0.0    0.5    1. p        −1.    −.5    0.0    0.5     1. p        −1.    −.5    0.0    0.5    1. p
                                               ˆ                                        ˆ                                       ˆ
                                  (a) argmaxQ (x,u)                        (b) argmaxQ (x,u)                       (c) argmaxQ (x,u)
                                                1                                        5                                       10
                           s            u∈U                         s            u∈U                         s           u∈U
                            3.                                      3.                                       3.
                            2.                                      2.                                       2.
                            1.                                      1.                                       1.
                           0.0                                      0.0                                      0.0
                           −1.                                     −1.                                      −1.
                           −2.                                     −2.                                      −2.
                           −3.                                     −3.                                      −3.
                              −1.     −.5    0.0    0.5    1. p        −1.    −.5    0.0    0.5     1. p        −1.    −.5    0.0    0.5    1. p
                                              ˆ                                        ˆ
                                  (d) argmaxQ (x,u)                        (e) argmaxQ (x,u)                           (f) Trajectory
                                               20                                       50
                                        u∈U                                     u∈U
                                                                 ∗
                        Figure 9: (a)-(e): Representation of µˆ    for different values of N. (f): Trajectory when x = (−0.5,0) and
                                                                 N                                                    0
                                                      ∗
                                    whenthepolicy µˆ     is used to control the system.
                                                      50
                                                                               524
                                                                               TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                            ˆ                                                                                                                               ˆ
                                        d(Q ,                                                                                                                           d(Q ,.)
                                         ˆ   N                                                                µˆ∗                                                            N
                                        Q      )                                                               N
                                          N−1                                                               J
                                                                                                              ∞
                                               0.006                                                            0.3                                                              0.25
                                                                                                                0.2
                                               0.005                                                                                                                              0.2                             ˆ
                                                                                                                0.1                                                                                           d(Q ,Q)
                                                                                                                                                                                                                   N
                                               0.004                                                            0.0                                                              0.15
                                               0.003                                                          −0.1         10       20       30       40       50 N
                                                                                                                                                                                  0.1
                                               0.002                                                          −0.2                                                                                            ˆ      ˆ
                                                                                                                                                                                 0.05                     d(Q ,HQ )
                                               0.001                                                          −0.3                                                                                             N      N
                                                 0.0                                                          −0.4                                                                0.0
                                                             10       20        30       40        50 N                                                                                       10       20        30       40       50 N
                                                                                                                                              ∗                                                           ˆ
                                                                                                                                            µˆ                                             (c)       d(Q ,F)=
                                                                      ˆ       ˆ                                                 (b)       J N =                                                             N
                                                       (a)       d(Q ,Q               ) =                                                   ∞                                                                             2
                                                                        N       N−1                                                                                                                 ˆ
                                                                                                                                                                                         ∑        (Q (x,u)−F(x,u))
                                                       #F                                                                                  ∗                                               Xi×U      N
                                                             ˆ     l   l     ˆ        l   l  2                                            µˆ
                                                     ∑ (Q (x ,u )−Q                (x ,u ))                                                N                                                             i
                                                       l=1     N t t          N−1 t t                                             ∑ J (x)                                                           #(X ×U)
                                                                       #F                                                            Xi ∞
                                                                                                                                       #Xi                                                                        ˆ
                                                                                                                                                                                              F =QorHQ
                                                                                                                                                                                                                    N
                                                                                                                                        ˆ             ˆ
                                       Figure 10: Figure (a) represents the distance between Q                                               and Q            . Figure (b) provides the average return
                                                                                                      ∗                                   N             N−1           i
                                                             obtainedbythepolicyµˆ whilestartingfromanelementofX . Figure(c)representsthedistance
                                                                              ˆ                       N                                                      ˆ
                                                             between Q and Q as well as the Bellman residual of Q as a function of N (distance between
                                                                                N                                                                             N
                                                              ˆ                 ˆ
                                                             Q andHQ ).
                                                                N                 N
                                               WeﬁrstuseTreeBaggingasthesupervised learning method. As the action space is binary, we
                                                                                          ˆ                            ˆ
                                       again model the functions Q (x,−4) and Q (x,4) by two ensembles of 50 trees each, and n                                                                                            =2.
                                                                                            N                            N                                                                                         min
                                       The policy µˆ∗ so obtained is represented on Figure 9a. Black bullets represent states for which
                                                                1
                                         ˆ                       ˆ                                                                        ˆ                        ˆ
                                       Q (x,−4)>Q (x,4),whitebulletsstatesforwhichQ (x,−4)<Q (x,4)andgreybulletsstatesfor
                                           1                       1                                                                        1                        1
                                                     ˆ                        ˆ                                                       ∗
                                       which Q (x,−4)=Q (x,4). Successive policies µˆ                                                     for increasing N are given on Figures 9b-9e.
                                                       1                        1                                                     N
                                               OnFigure9f,wehaverepresentedthetrajectory obtained when starting from (s,p)=(−0.5,0)
                                                                                 ∗                                                                                                                                            µ∗
                                       andusingthepolicy µˆ                          to control the system. Since, for this particular state the computation of J
                                                                                 50                                                                                                                                           ∞
                                                                                       µˆ∗
                                       gives the same value as J 50, the trajectory drawn is actually an optimal one.
                                                                                      ∞
                                                                                                                                                    ˆ              ˆ
                                               Figure 10a shows the evolution of distance between Q                                                       and Q              with N. Notice that while a
                                                                                                                                                      N              N−1
                                       monotonic decrease of the distance was observed with the “Left or Right” control problem (Figure
                                       4a), it is not the case anymore here. The distance ﬁrst decreases and then from N = 5 suddenly
                                       increasestoreachamaximumforN=19andtoﬁnallyredecreasetoanalmostzerovalue. Actually,
                                       this apparently strange behavior is due to the way the distance is evaluated and to the nature of
                                       the control problem. Indeed, we have chosen to use in the distance computation the state-action
                                       pairs (xl,ul) l = 1,··· ,#F from the set of four-tuples. Since most of the states xl are located
                                                      t     t                                                                                                                                            t
                                       around the initial state (p,s) = (−0.5,0) (see Figure 11), the distance is mostly determined by
                                                                              ˆ                ˆ
                                       variations between Q                          and Q               in this latter region. This remark combined with the fact that
                                                                                N                N−1                                                                                          ˆ
                                       the algorithm needs a certain number of iterations before obtaining values of Q                                                                              around (p,s) =
                                                                                                                                                                                   15           N
                                       (−0.5,0) different from zero explains this sudden increase of the distance.
                                                                                                                                              i       i                                     t
                                               To compute policy scores, we consider the set X : X∗= {(p,s) ∈ X \{x }|∃i, j ∈ Z|(p,s) =
                                       (0.125∗i,0.375∗ j)} and evaluate the average value of JµˆN(x) over this set. The evolution of the
                                                                                                                                                    ∞
                                              the sequences of actions of length k lead to a reward equal to −1 when x = x. Let k                                                       be this smallest value of k.
                                                                                                                                                               0                  min
                                                          ∗
                                                         µ                          k    −1                                                                              k   −1
                                              ThenJ (x)isequaltoγ min                         if condition (i) is satisﬁed when k = k                         and −γ min          otherwise.
                                                        ∞              ˆ                                                                                min
                                        15. The reason for Q                being equal to zero around (p,s) = (−0.5,0) for small values of N is that when the system starts
                                                                        N
                                              from (−0.5,0) several steps are needed to observe non zero rewards whatever the policy used.
                                                                                                                                 525
                                                                                                                                      ERNST, GEURTS AND WEHENKEL
                                                                                                                                 0.04
                                                                                                                                0.035
                                                                                                                                 0.03
                                                                                                                                0.025
                                                                                                                                 0.02
                                                                                                                                0.015
                                                                                                                                 0.01
                                                                                                                               Frequency of visit of a tile
                                                                                                                                0.005
                                                                                                                                    0
                                                                                                                                    3
                                                                                                                                        2
                                                                                                                                             1
                                                                                                                                                  0                                                                            1
                                                                                                                                                     −1                                                          0.5
                                                                                                                                                          −2                                      0
                                                                                                                                                 s             −3                  −0.5
                                                                                                                                                                    −1
                                                                                                                                                                                            p
                                                    Figure 11: Estimation of the x distribution while using episodes starting from (−0.5,0) and choosing ac-
                                                                                                                            t
                                                                                 tions at random.
                                                                                        1                                                                                            1
                                                                                      0.5                                                                                          0.5
                                                                                        0                                                                                            0
                                                                                     −0.5                                                                                         −0.5
                                                                                       −1                                                                                           −1
                                                                                        3                                                                                            3
                                                                                            2                                                                                            2
                                                                                                 1                                                                                            1
                                                                                                      0                                                                   1                        0                                                                   1
                                                                                                          −1                                                  0.5                                      −1                                                  0.5
                                                                                                                                                 0                                                                                            0
                                                                                                              −2                     −0.5                                                                  −2                     −0.5
                                                                                                                   −3   −1                                                                                      −3   −1
                                                                                                                   (a) Q(.,−4)                                                                                    (b) Q(.,4)
                                                                                        1                                                                                            1
                                                                                      0.5                                                                                          0.5
                                                                                        0                                                                                            0
                                                                                     −0.5                                                                                         −0.5
                                                                                       −1                                                                                           −1
                                                                                        3                                                                                            3
                                                                                            2                                                                                            2
                                                                                                 1                                                                                            1
                                                                                                      0                                                                   1                        0                                                                   1
                                                                                                          −1                                                  0.5                                      −1                                                  0.5
                                                                                                                                                 0                                                                                            0
                                                                                                              −2                     −0.5                                                                  −2                     −0.5
                                                                                                                   −3   −1                                                                                      −3   −1
                                                                                                                          ˆ                                                                                              ˆ
                                                                                                                 (c) Q (.,−4)                                                                                   (d) Q (.,4)
                                                                                                                            50                                                                                              50
                                                                                                                                                                             ˆ           ˆ
                                                    Figure 12: RepresentationoftheQ-functionandofQ . Q                                                                                           is computedbyusingﬁttedQiterationtogether
                                                                                                                                                                               50          50
                                                                                 with Tree Bagging.
                                                    score for increasing values of N is represented in Figure 10b. We see that the score rapidly increases
                                                                                                                                                                                                                                        ∗
                                                    to ﬁnally oscillate slightly around a value close to 0.295. The score of µ being equal to 0.360, we
                                                                                                        ∗                                                                                                                                         ˆ
                                                    seethatthepoliciesµˆ                                     are suboptimal. To get an idea of how different is the Q -function computed
                                                                                                        N                                                                                                                                            50
                                                    byﬁtted Q iteration from the true Q-function, we have represented both functions on Figure 12. As
                                                    wemayobserve,somesigniﬁcantdifferencesexistbetweenthem,especiallyinareaswereveryfew
                                                    information has been generated, like the state space area around x = (−1,3).
                                                                                                                                                                           526
                                                               TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                  µˆ∗                                                µˆ∗                                               µˆ∗
                                 J N                                                J N                                               J N
                                  ∞                     Extra-Trees                  ∞                                                 ∞                       k =2
                                    0.3                                               0.3         Totally Randomized Trees               0.3
                                    0.2                                               0.2                                                0.2
                                    0.1                        Tree Bagging           0.1                                                0.1                       k =5
                                                                                               Kd-Tree with best n    (4)
                                    0.0                                               0.0                         min                    0.0
                                             10     20     30     40     50 N                   10     20     30     40     50 N                  10     20     30     40     50 N
                                   −0.1                                              −0.1                                               −0.1
                                   −0.2                                              −0.2                                               −0.2  k =1
                                   −0.3 Pruned CARTTree                              −0.3    Kd-Tree with n    =2                       −0.3
                                                                                                            min
                                   −0.4                                              −0.4                                               −0.4
                                       (a) Output values (o) used                         (b) Output values (o) not                                     (c) kNN
                                       to choose tree tests                               used to choose tree tests
                               Figure 13: Inﬂuence of the supervised learning method on the solution. For each supervised learning
                                                             ˆ                    ˆ
                                                 method Q (x,−4) and Q (x,4) are modeled separately. For Tree Bagging, Extra-Trees, and
                                                              N                     N
                                                 Totally Randomized Trees, the trees are developed completely (n                              =2). The distance used in
                                                                                                                                          min
                                                 the nearest neighbors computation is the Euclidean distance.
                               5.3.2 INFLUENCE OF THE TREE-BASED METHOD AND COMPARISON WITH kNN.
                                                                                                                   ∗
                               Figure 13a sketches the scores obtained by the policies µˆ                             whenusingdifferent tree-based methods
                                                                                                                   N
                               which use the output values (o) of the input-output pair ((i,o)) of the training set to compute the
                               tests.     It is clear that Tree Bagging and Extra-Trees are signiﬁcantly superior to Pruned CART
                               Tree. Figure 13b compares the performances of tree-based methods for which the tests are chosen
                               independently of the output values. We observe that even when using the value of n                                                   leading to
                                                                                                                                                              min
                               the best score, Kd-Tree does not perform better than Totally Randomized Trees. On Figure 13c, we
                               have drawn the scores obtained with a k-nearest-neighbors (kNN) technique.
                                     Notice that the score curves corresponding to the k-nearest-neighbors, Totally Randomized
                               Trees, and Kd-Tree methods stabilize indeed after a certain number of iterations.
                                     To compare more systematically the performances of all these supervised learning algorithms,
                                                                                                                                                                   ∗
                               wehavecomputedforeachoneofthemandforseveralsetsoffour-tuples the score of µˆ . Results
                                                                                                                                                                   50
                               are gathered in Table 3. A ﬁrst remark suggested by this table and which holds for all the supervised
                               learning methods is that the more episodes are used to generate the four-tuples, the larger the score
                               of the induced policy. Compared to the other methods, performances of Tree Bagging and Extra-
                               Treesareexcellentonthetwolargestsets. Extra-Treesstillgivesgoodresultsonthesmallestsetbut
                               this is not true for Tree Bagging. The strong deterioration of Tree Bagging performances is mainly
                               duetothefact that when dealing with this set of four-tuples, information about the optimal solution
                               is really scarce (only two four-tuples correspond to a reward of 1) and, since a training instance
                               has 67% chance of being present in a bootstrap sample, Tree Bagging often discards some critical
                               information. On the other hand, Extra-Trees and Totally Randomized Trees which use the whole
                               training set to build each tree do not suffer from this problem. Hence, these two methods behave
                               particularly well compared to Tree Bagging on the smallest set.
                                     OneshouldalsoobservefromTable3thatevenwhenusedwiththevalueofkthatproducesthe
                               largest score, kNN is far from being able to reach for example the performances of the Extra-Trees.
                                                                                                      527
                                                         ERNST, GEURTS AND WEHENKEL
                                                   Supervised learning        Nbofepisodes used
                                                         method                  to generate F
                                                                             1000    300      100
                                                    Kd-Tree (Best n   )      0.17    0.16    -0.06
                                                                   min
                                                    Pruned CARTTree          0.23    0.13    -0.26
                                                       Tree Bagging          0.30    0.24    -0.09
                                                        Extra-Trees          0.29    0.25    0.12
                                                 Totally Randomized Trees    0.18    0.14    0.11
                                                       kNN(Bestk)            0.23    0.18    0.02
                                                   ∗
                                Table 3: Score of µˆ  for different set of four-tuples and supervised learning methods.
                                                   50
                      5.3.3 FITTED Q ITERATION AND BASIS FUNCTION METHODS
                      In Section 5.2.4, when dealing with the “Left or Right” control problem, basis function methods
                      with two types of approximation architectures, piecewise-constant or piecewise-linear grids, have
                      been used in combination with the ﬁtted Q iteration algorithm.
                          In this section, the same types of approximation architectures are also considered and, for each
                                                                        ∗
                      type of approximation architecture, the policy µˆ    has been computed for different grid resolutions
                                                                        50
                      (a 10×10 grid, a 11×11 grid, ···, a 50×50 grid). The score obtained by each policy is repre-
                      sented on Figure 14a. The horizontal line shows the score previously obtained on the same sample
                      of four-tuples by Tree Bagging. As we may see, whatever the grid considered, both approximation
                      architectures lead to worse results than Tree Bagging, the best performing tree-based method. The
                      highest score is obtained by a 18×18 grid for the piecewise-constant approximation architecture
                      andbya14×14gridforthepiecewise-linearapproximationarchitecture. These two highest scores
                      are respectively 0.21 and 0.25, while Tree Bagging was producing a score of 0.30. The two cor-
                      responding policies are sketched in Figures 14b and 14c. Black polygons represent areas where
                       ˆ           ˆ                                      ˆ           ˆ
                      Q(x,−4)>Q(x,4),whitepolygonsareaswhereQ(x,−4)<Q(x,4)andgreypolygonsareaswhere
                       ˆ            ˆ
                      Q(x,−4)=Q(x,4).
                          Whenlooking at the score curve corresponding to piecewise-linear grids as approximation ar-
                      chitectures, one maybesurprisedtonoteitsharshaspect. Forsomegrids,thistypeofapproximation
                      architectureleadstosomegoodresultswhilebyvaryingslightlythegridsize,thescoremaystrongly
                      deteriorate. This strong deterioration of the score is due to fact that for some grid sizes, the ﬁtted Q
                      iteration actually diverges to inﬁnity while it is not the case for other grid sizes. Divergence to in-
                      ﬁnity of the algorithm is illustrated on Figures 15a and 15c where we have drawn for a 12×12 grid
                                             ˆ        ˆ      ˆ                ˆ         ˆ
                      the distance between Q and Q         , Q   and Q, and Q and HQ . Remark that a logarithmic scale
                                               N       N−1    N                N         N
                      has been used for the y-axis. When using Tree Bagging in the inner loop of the ﬁtted Q iteration,
                      similar graphics have been drawn (Figure 10) and the reader may refer to them for comparison.
                      5.3.4 COMPARISON WITH Q-LEARNING
                      In this section we use a gradient descent version of the standard Q-learning algorithm to compute
                      the cj parameters of the approximation architectures of Section 5.3.3. The degree of correction α
                      used inside this algorithm is chosen equal to 0.1 and the estimate of the Q-function is initialized
                      to 0 everywhere. This latter being refreshed by this algorithm on a four-tuple by four-tuple basis,
                      wehavechosen to use each element of F only once to refresh the estimate of the Q-function. The
                                                                        528
                                                                 TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                    µˆ∗                                                 s                                                   s
                                  J 50
                                   ∞                                                     3.                                                 3.
                                                       Tree Bagging
                                     0.3                                                 2.                                                 2.
                                     0.2                     piecewise-linear            1.                                                 1.
                                                             grid
                                     0.1                                                0.0                                                 0.0
                                     0.0
                                    −0.1 10×10    20×20    30×30    40×40    Grid      −1.                                                 −1.
                                                                             size
                                    −0.2                                               −2.                                                 −2.
                                    −0.3            piecewise-constant grid            −3.                                                 −3.
                                                                                            −1.      −.5      0.0      0.5      1. p            −1.      −.5      0.0      0.5      1. p
                                                                                                             ˆ                                                   ˆ
                                                                                            (b) argmaxQ           computed                      (c) argmaxQ           computed
                                                                                                              50                                                  50
                                                                                                    u∈U                                                 u∈U
                                                                 ˆ                             whenusinga18×18                                     whenusinga14×14
                                               (a) Score of Q
                                                                   50                            piecewise-constant                                   piecewise-linear
                                                                                                grid as approx. arch.                              grid as approx. arch.
                                Figure 14: FittedQiterationwithbasisfunctionmethods. Twodifferenttypesofapproximationarchitecture
                                                  are considered: piecewise-constant and piecewise-linear grids. F is composed of the four-tuples
                                                  gathered during 1000 episodes.
                                 log (
                                    10                                                                                                  log (d
                                    ˆ                                                   µˆ∗                                                10
                                 d(Q ,                                                   N
                                     N                                                 J                                                  ˆ
                                                                                        ∞                                               (Q ,.))
                                 ˆ                                                                                                         N
                                 Q    ))
                                   N−1 17.5                                               0.3
                                                                                                                                                                      ˆ
                                                                                                                                               30.                 d(Q ,Q)
                                        15.                                               0.2                                                                          N
                                       12.5                                               0.1                                                  25.
                                        10.                                               0.0                                                  20.
                                                                                        −0.1       10     20      30     40     50 N
                                        7.5                                                                                                    15.
                                         5.                                             −0.2                                                   10.
                                                                                        −0.3                                                    5.                       ˆ     ˆ
                                                                                                                                                                      d(Q ,HQ )
                                        2.5                                                                                                                               N     N
                                        0.0                                             −0.4                                                   0.0
                                                 10      20     30      40     50 N                                                                      10     20      30     40     50 N
                                                                                                                  ∗                                                ˆ
                                                                                                                 µˆ                                   (c)     d(Q ,F)=
                                                         ˆ     ˆ                                       (b)     J N =                                                N
                                            (a)     d(Q ,Q            ) =                                        ∞                                                              2
                                                          N     N−1                                                                                           ˆ
                                                                                                                                                    ∑       (Q (x,u)−F(x,u))
                                            #F                                                                  ∗                                     Xi×U     N
                                                 ˆ    l  l    ˆ       l l  2                                   µˆ
                                          ∑ (Q (x ,u )−Q           (x ,u ))                                     N                                                 i
                                            l=1   N t t        N−1 t t                                   ∑ J (x)                                              #(X ×U)
                                                          #F                                               Xi ∞
                                                                                                             #Xi                                                         ˆ
                                                                                                                                                        F =QorHQ
                                                                                                                                                                           N
                                Figure 15: Fitted Q iteration algorithm with basis function methods. A 12×12 piecewise-linear grid is the
                                                                                                                                   ˆ
                                                  approximation architecture considered. The sequence of Q -functions diverges to inﬁnity.
                                                                                                                                     N
                                                                                                          529
                                                                                         ERNST, GEURTS AND WEHENKEL
                                     Jµˆ∗                                                      s                                                      s
                                      ∞                                                        3.                                                      3.
                                                           Tree Bagging
                                        0.3                                                    2.                                                      2.
                                        0.2                                                    1.                                                      1.
                                        0.1                                                   0.0                                                     0.0
                                        0.0
                                       −0.1 10×10    20×20     30×30     40×40    Grid        −1.                                                    −1.
                                                                                  size
                                       −0.2              piecewise-linear grid                −2.                                                    −2.
                                       −0.3                                                   −3.                                                    −3.
                                                      piecewise-constant grid                      −1.       −.5      0.0       0.5      1. p              −1.      −.5       0.0      0.5       1. p
                                                                                                    (b) Policy computed by                                  (c) Policy computed by
                                              (a) Score of the policy                                    Q-learning when                                         Q-learning when
                                                     computed by                                       used with a 13×13                                       used with a 19×19
                                                      Q-learning                                        piecewise-constant                                       piecewise-linear
                                                                                                       grid as approx. arch.                                  grid as approx. arch.
                                   Figure 16: Q-learning with piecewise-constant and piecewise-linear grids as approximation architectures.
                                                      EachelementofF isusedoncetorefreshtheestimateoftheQ-function. ThesetF iscomposed
                                                      of the four-tuples gathered during 1000 episodes.
                                   main motivation for this is to compare the performances of the ﬁtted Q iteration algorithm with an
                                                                                                                             16
                                   algorithm that does not require to store the four-tuples.
                                         The scores obtained by Q-learning for the two types of approximation architectures and for
                                   different grid sizes are reported on Figure 16a. Figure 16b (16c) represents the policies that have
                                   led, by screening different grid sizes, to the highest score when piecewise-constant grids (piecewise-
                                   linear grids) are the approximation architectures considered. By comparing Figure 16a with Figure
                                   14a, it is obvious that ﬁtted Q iteration exploits more effectively the set of four-tuples than Q-
                                   learning. In particular, the highest score is 0.21 for ﬁtted Q iteration while it is only of 0.04 for Q-
                                   learning. If we compare the score curves corresponding to piecewise-linear grids as approximation
                                   architectures, we observe also that the highest score produced by ﬁtted Q iteration (over the different
                                   grids), is higher than the highest score produced by Q-learning. However, when ﬁtted Q iteration is
                                   plagued with some divergence to inﬁnity problems, as illustrated on Figure 15, it may lead to worse
                                   results than Q-learning.
                                         Observe that even when considering 10,000 episodes with Q-learning, we still obtain worse
                                   scores than the one produced by Tree Bagging with 1000 episodes. Indeed, the highest score pro-
                                   16. Performances of the gradient descent version of the Q-learning algorithm could be improved by processing several
                                        times each four-tuple to refresh the estimate of the Q-function, for example by using the experience replay technique
                                        of Lin (1993). This however requires to store the four-tuples.
                                               It should also be noted that if a piecewise-constant grid is the approximation architecture considered, if each
                                        element of F is used an inﬁnite number of times to refresh the estimate of the Q-function and if the sequence of
                                        αs satisﬁes the stochastic approximation condition (i.e.,                        ∞ α →∞and ∞ α2<∞,α beingthe value of α
                                                                                                                       ∑         k               ∑                     k
                                                                                                                         k=1                       k=1 k
                                        the kth times the estimate of the Q-function is refreshed), then the Q-function estimated by the Q-learning algorithm
                                        wouldbethesameastheoneestimatedbyﬁttedQiterationusingthesamepiecewise-constantgridasapproximation
                                        architecture. This can be seen by noting that in such conditions, the Q-function estimated by Q-learning would be
                                        the same as the one estimated by a model-based algorithm using the same grid (see Ernst (2003), page 131 for the
                                        proof) which in turn can be shown to be equivalent to ﬁtted Q iteration (see Ernst et al., 2005).
                                                                                                                  530
                                                  TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                                             ∗
                                                            µˆ
                                                           J 50                         Tree Bagging
                                                            ∞
                                                              0.2     Pruned CARTTree          Extra-Trees
                                                              0.1
                                                              0.0
                                                                0.0      2.5     5.      7.5      Nbof
                                                                        Totally Rand. Trees       irrelevant
                                                             −0.1                                 variables
                                                             −0.2             Kd-Tree (Best nmin)
                                                             −0.3               kNN(k=2)
                                                 ∗
                         Figure 17: Score of µˆ     whenfour-tuples are gathered during 1000 episodes and some variables that do not
                                                 50
                                      contain any information about the position and the speed of the car are added to the state vector.
                         duced by Q-learning with 10,000 episodes and over the different grid sizes, is 0.23 if piecewise-
                         constant grids are considered as approximation architectures and 0.27 for piecewise-linear grids,
                         compared to a score of 0.30 for Tree Bagging with 1000 episodes.
                             At this point, one may wonder whether the poor performances of Q-learning are due to the fact
                         that it is used without eligibility traces. To answer this question, we have assessed the performances
                                                                                                                                     17
                         of Watkin’s Q(λ) algorithm (Watkins, 1989) that combines Q-learning with eligibility traces.                   The
                         degree of correction α is chosen, as previously, equal to 0.1 and the value of λ is set equal to
                         0.95. This algorithm has been combined with piecewise-constant grids and 1000 episodes have
                         been considered. The best score obtained over the different grids is equal to −0.05 while it was
                         slightly higher (0.04) for Q-learning.
                         5.3.5 ROBUSTNESS WITH RESPECT TO IRRELEVANT VARIABLES
                         In this section we compare the robustness of the tree-based regression methods and kNN with re-
                         spect to the addition of irrelevant variables. Indeed, in many practical applications the elementary
                         variables which compose the state vector are not necessarily all of the same importance in deter-
                         mining the optimal control action. Thus, some variables may be of paramount importance, while
                         someothers may inﬂuence only weakly or even sometimes not at all the optimal control.
                             OnFigure17,wehavedrawntheevolutionofthescorewhenusingfour-tuplesgatheredduring
                                                                                                                       18
                         1000 episodes and adding progressively irrelevant variables to the state-vector.                  It is clear that
                         not all the methods are equally robust to the introduction of irrelevant variables. In particular, we
                         observethatthethreemethodsforwhichtheapproximationarchitectureisindependentoftheoutput
                         variable are not robust: the kNN presents the fastest deterioration, followed by Kd-Tree and Totally
                         Randomized Trees. The latter is more robust because it averages out several trees, which gives the
                         relevant variables a better chance to be taken into account in the model.
                         17. In Watkin’s Q(λ), accumulating traces are considered and eligibility traces are cut when a non-greedy action is
                             chosen. Remark that by not cutting the eligibility traces when a non-greedy action is selected, we have obtained
                             worse results.
                         18. See Section C.2 for the description of the irrelevant variables dynamics.
                                                                                531
                                                            ERNST, GEURTS AND WEHENKEL
                            Ontheotherhand,themethodswhichtakeintoaccounttheoutputvariablesintheirapproxima-
                       tion architecture are all signiﬁcantly more robust than the former ones. Among them, Tree Bagging
                       and Extra-Trees which are based on the averaging of several trees are almost totally immune, since
                       even with 10 irrelevant variables (leading to the a 12-dimensional input space) their score decrease
                       is almost insigniﬁcant.
                            Thisexperimentshowsthattheregressiontreebasedensemblemethodswhichadapttheirkernel
                       to the output variable may have a strong advantage in terms of robustness over methods with a kernel
                       which is independent of the output, even if these latter have nicer convergence properties.
                        5.3.6 INFLUENCE OF THE NUMBER OF REGRESSION TREES IN AN ENSEMBLE
                       In this paper, we have chosen to build ensembles of regression trees composed of 50 trees (M = 50,
                       Section 4.2), a number of elements which, according to our simulations, is large enough to ensure
                       that accuracy of the models produced could not be improved signiﬁcantly by increasing it. In order
                       to highlight the inﬂuence of M on the quality of the solution obtained, we have drawn on Figure
                       18, for the different regression tree based ensemble methods, the quality of the solution obtained
                       as a function of M. We observe that the score grows rapidly with M, especially with Extra-Trees
                       and Tree Bagging in which cases a value of M = 10 would have been sufﬁcient to obtain a good
                       solution.
                            Note that since the CPU times required to compute the solution grow linearly with the number
                       of trees built, computational requirements of the regression tree based ensemble methods could be
                       adjusted by choosing a value of M.
                                                            ∗
                                                           µˆ
                                                          J 50       Extra-Trees     Tree Bagging
                                                           ∞
                                                             0.25
                                                             0.2
                                                             0.15
                                                             0.1
                                                             0.05   Totally Randomized Trees
                                                             0.0
                                                                     10     20    30     40 Nboftrees
                                                            −0.05                           built (M)
                                                            −0.1
                                                                ∗
                       Figure 18: Evolution of the score of µˆ     with the number of trees built. F is composed of the four-tuples
                                                                50
                                    gathered during 300 episodes.
                        5.3.7 CAR ON THE HILL WITH CONTINUOUS ACTION SPACE
                       To illustrate the use of the ﬁtted Q iteration algorithm with continuous action spaces we consider
                       hereU =[−4,4]rather than {−4,4}. We use one-step episodes with (x ,u ) drawn at random with
                                                                                                       0  0
                       uniformprobabilityinX×U togenerateasetF of50,000four-tuplesanduseTreeBaggingwith50
                       trees as supervised learning method. We have approximated the maximization over the continuous
                       action space needed during the training sample refreshment step (see Eqn (13), Figure 1) by an
                                                                                          ∗
                       exhaustive search over u ∈ {−4,−3,···,3,4}. The policy µˆ             thus obtained by our algorithm after
                                                                                          50
                                                                             532
                                                                          TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                            s
                                            3.                                                                                   ∗
                                                                                                                                µˆ
                                                                                                                              J 50        1modelbuilt
                                                                                                                                ∞
                                            2.                                                                                    0.35
                                                                                                                                                 2modelsbuilt (1 for each value of u)
                                            1.                                                                                     0.3
                                           0.0                                                                                    0.25                       Continuous action space
                                          −1.                                                                                      0.2   Discrete action space
                                          −2.                                                                                     0.15
                                          −3.                                                                                      0.1
                                                                                                                                                10000     20000     30000      40000   Nbof
                                                  −1.            −.5           0.0            0.5           1. p                                                                four-tuples
                                                                                                 ∗                                                                                                     ∗
                                                         (a) Representation of µˆ                                   (b) Inﬂuence of #F on the score of the policy µˆ
                                                                                                 50                                                                                                    50
                                     Figure 19: Car on the Hill with continuous action space. Tree Bagging is used on one-step episodes with
                                                         (x ,u ) drawn at random in X ×U are used to generate the four-tuples.
                                                            0     0
                                     50 iterations is represented on Figure 19a, where black bullets are used to represent states x for
                                                   ∗                                                               ∗
                                     which µˆ         (x) is negative, white ones when µˆ                             (x) is positive. The size of a bullet is proportional to
                                                   50                                                              50
                                     the absolute value of the control signal |µˆ∗ (x)|. We see that the control policy obtained is not far
                                     from a “bang-bang” policy.                                               50
                                           Tocomparethese results with those obtained in similar conditions with a discrete action space,
                                     wehavemadetwoadditionalexperiments, where the action space is restricted again to the extreme
                                     values, i.e. u ∈ {−4,4}. The two variants differ in the way the Q -functions are modeled. Namely,
                                                                                                                                                      N
                                     in the ﬁrst case one single model is learned where u is included in the input variables whereas in the
                                     second case one model is learned per possible value of u, i.e. one model for Q (x,−4) and one for
                                                                                                                                                                              N
                                     Q (x,4). All experiments are carried out for an increasing number of samples and a ﬁxed number
                                        N
                                     of iterations (N =50) and bagged trees (M =50). The three curves of Figure 19b show the resulting
                                                                                                                                                            ∗
                                     scores. The two upper curves correspond to the score of the policy µˆ                                                      obtained when considering a
                                                                                                                                                            50
                                     discrete action spaceU ={−4,4}. Weobservethatbothcurvesareclosetoeachotheranddominate
                                     the “Continuous action space” scores. Obviously the discrete approach is favored because of the
                                     “bang-bang” nature of the problem; nevertheless, the continuous action space approach is able to
                                                                                                       19
                                     provide results of comparable quality.
                                     5.3.8 COMPUTATIONAL COMPLEXITY AND CPU TIME CONSIDERATIONS
                                     Table 4 gathers the CPU times required by the ﬁtted Q iteration algorithm to carry out 50 iterations
                                                                    ˆ
                                     (i.e., to compute Q (x,u)) for different types of supervised learning methods and different sets F .
                                                                      50
                                     Wehave also given in the same table the repartition of the CPU times between the two tasks the
                                     algorithm has to perform, namely the task which consists of building the training sets (evaluation
                                     of Eqns (12) and (13) for all l ∈ {1,2,··· ,#F }) and the task which consists of building the models
                                     from the training sets. These two tasks are referred to hereafter respectively as the “Training Set
                                     19. The bang-bang nature was also observed in Smart and Kaelbling (2000), where continuous and a discrete action
                                           spaces are treated on the “Car on the Hill” problem, with qualitatively the same results.
                                                                                                                        533
                                                        ERNST, GEURTS AND WEHENKEL
                      Building” (TSB) task and the “Model Building” (MB) task. When Kd-Tree or Totally Randomized
                      Trees are used, each tree structure is frozen after the ﬁrst iteration and only the value of its terminal
                      nodes are refreshed. The supervised learning technique referred to in the table as “kNN smart” is a
                      smart implementation the ﬁtted Q iteration algorithm when used with kNN in the sense that the k
                      nearest neighbors of xl   are determined only once and not recomputed at each subsequent iteration
                      of the algorithm.     t+1
                           Supervised                   CPUtimesconsumedbytheModelsBuilding(MB)
                            learning                          and Training Sets Building (TSB) tasks
                            algorithm             #F =5000               #F =10000                  #F =20000
                                             MB TSB Total MB TSB Total                         MB       TSB      Total
                       Kd-Tree (nmin=4) 0.01        0.39     0.40    0.04    0.91     0.95    0.06      2.05      2.11
                       Pruned CARTTree 16.6          0.3     16.9    42.4     0.8     43.2    95.7      1.6       97.3
                          Tree Bagging       97.8   54.0    151.8 219.7 142.3        362.0    474.4    333.7     808.1
                           Extra-Trees       24.6   55.5     80.1    51.0   145.8    196.8 105.72 337.48         443.2
                       Totally Rand. Trees   0.4    67.8     68.2     0.8   165.3    166.2     1.7     407.5     409.2
                              kNN            0.0   1032.2 1032.2      0.0   4096.2 4096.2      0.0    16537.7 16537.7
                           kNNsmart          0.0    21.0     21.0     0.0    83.0     83.0     0.0     332.4     332.4
                                                                                                                ˆ
                      Table 4: CPU times (in seconds on a Pentium-IV, 2.4GHz, 1GB, Linux) required to compute Q . For each
                                                                 ˆ             ˆ                                 50
                               of the supervised learning method Q (x,−4) and Q (x,4) have been modeled separately. 50 trees
                                                                  N             N
                               are used with Tree Bagging, Extra-Trees and Totally Randomized Trees and the value of k for kNN
                               is 2.
                          Byanalyzing the table, the following remarks apply:
                       • CPU times required to build the training sets are non negligible with respect to CPU times for
                         building the models (except for Pruned CART Tree which produces only one single regression
                         tree). In the case of Extra-Trees, Totally Randomized Trees and kNN, training set update is even
                         the dominant task in terms of CPU times.
                       • Kd-Tree is (by far) the fastest method, even faster than Pruned CART Tree which produces also
                         one single tree. This is due to the fact that the MB task is really inexpensive. Indeed, it just
                         requires building one single tree structure at the ﬁrst iteration and refresh its terminal nodes in
                         the aftermath.
                       • Concerning Pruned CART Tree, it may be noticed that tree pruning by ten-fold cross validation
                         requires to build in total eleven trees which explains why the CPU times for building 50 trees
                         with Tree Bagging is about ﬁve times greater than the CPU times required for Pruned CART
                         Tree.
                       • TheMBtaskisaboutfourtimesfasterwithExtra-TreesthanwithTreeBagging,becauseExtra-
                         Trees only computes a small number (K) of test scores, while CART searches for an optimal
                         threshold for each input variable. Note that the trees produced by the Extra-Trees algorithm
                         are slightly more complex, which explains why the TSB task is slightly more time consuming.
                         On the two largest training sets, Extra-Trees leads to almost 50 % less CPU times than Tree
                         Bagging.
                                                                       534
                        TREE-BASED BATCH MODE REINFORCEMENT LEARNING
             • The MB task for Totally Randomized Trees is much faster than the MB task for Extra-Trees,
              mainly because the totally randomized tree structures are built only at the ﬁrst iteration. Note
              that, when totally randomized trees are built at the ﬁrst iteration, branch development is not
              stopped when the elements of the local training set have the same value, because it can not
              be assumed that these elements would still have the same value in subsequent iterations. This
              also implies that totally randomized trees are more complex than trees built by Extra-Trees and
              explains why the TSB task with Totally Randomized Trees is more time consuming.
             • Full kNN is the slowest method. However, its smart implementation is almost 50 times (the
              numberofiterations realized by the algorithm) faster than the naive one. In the present case, it is
              evenfasterthanthemethodsbasedonregressiontreesensembles. However,asitscomputational
              complexity (in both implementations) is quadratic with respect to the size of the training set
              whileitisonlyslightlysuper-linearfortree-basedmethods, itsadvantagequicklyvanisheswhen
              the training set size increases.
            5.4 The “Acrobot Swing Up” Control Problem
                                    L1
                                   θ
                                    1   u
                                  M1g    L2
                                        θ
                                         2
                                            Mg
                                             2
                            Figure 20: Representation of the Acrobot.
              Weconsider here the “Acrobot Swing Up” control problem whose precise deﬁnition is given in
            Appendix C.3.
              The Acrobot is a two-link underactuated robot, depicted in Figure 20. The second joint applies
            a torque (represented by u), while the ﬁrst joint does not. The system has four continuous state
                                                  ˙   ˙
            variables: two joint positions (θ1 and θ2) and two joint velocities (θ1 and θ2). This system has been
            extensively studied by control engineers (e.g. Spong, 1994) as well as machine learning researchers
            (e.g. Yoshimoto et al., 1999).
              We have stated this control problem so that the optimal stationary policy brings the Acrobot
            quickly into a speciﬁed neighborhood of its unstable inverted position, and ideally as close as pos-
            sible to this latter position. Thus, the reward signal is equal to zero except when this neighborhood
            is reached, in which case it is positive (see Eqn (44) in Appendix C.3). The torque u can only take
            two values: −5 and 5.
                                        535
                                                           ERNST, GEURTS AND WEHENKEL
                       5.4.1 FOUR-TUPLES GENERATION
                       Togeneratethefour-tupleswehaveconsidered2000episodesstartingfromaninitialstatechosenat
                                             ˙   ˙       4                              ˙     ˙
                       randomin{(θ1,θ2,θ1,θ2)∈R |θ1∈[−π+1,π−1],θ2=θ1=θ2=0}andﬁnishingwhent=100
                                                                           20
                       or earlier if the terminal state is reached before.
                           Twotypesofstrategies are used here to control the system, leading to two different sets of four-
                       tuples. The ﬁrst one is the same as in the previous examples: at each instant the system is controlled
                       by using a policy that selects actions fully at random. The second strategy however interleaves the
                       sequence of four-tuples generation with the computation of an approximate Q-function from the
                                                                                                ˆ
                       four-tuples already generated and uses a policy that exploits this Q-function to control the system
                       whilegeneratingadditional four-tuples. More precisely, it generates the four-tuples according to the
                                              21
                       following procedure:
                                         ˆ
                           • Initialize Q to zero everywhere and F to the empty set;
                           • Repeat 20 times:
                                                                    ˆ
                                 – useanε-greedypolicyfromQtogenerate100episodesandaddtheresultingfour-tuples
                                    to F ;
                                                                                                             ˆ                     ˆ
                                 – use the ﬁtted Q iteration algorithm to build a new approximation Q           from F and set Q
                                        ˆ                                                                     N
                                    to Q .
                                         N
                       whereε=0.1andwheretheﬁttedQiterationalgorithmiscombinedwiththeExtra-Trees(n                             =2,
                                                                                                                           min
                       K=5,M=50)algorithmanditerates100times.
                           The random policy strategy produces a set of four-tuples composed of 193,237 elements while
                       154,345 four-tuples compose the set corresponding to the ε-greedy policy.
                                                                ˙   ˙                                 ˙  ˙
                           Note that since the states (θ ,θ ,θ ,θ ) and (θ +2k π,θ +2k π,θ ,θ ) k ,k ∈ Z are equiv-
                                                          1   2  1   2         1      1    2      2    1  2    1  2
                       alent from a physical point of view, we have, before using the four-tuples as input of the ﬁtted Q
                       iteration algorithm, added or subtracted to the values of θ1 and θ2 a multiple of 2π to guarantee that
                       these values belong to the interval [−π,π]. A similar transformation is also carried out on each state
                                ˙  ˙                                             ∗
                       (θ1,θ2,θ1,θ2) before it is used as input of a policy µˆ (x).
                                                                                 N
                       5.4.2 SIMULATION RESULTS
                       First, we consider the set of four-tuples gathered when using the ε-greedy policy to control the
                       system. We have represented on Figure 21 the evolution of the Acrobot starting with zero speed in
                       a downwardpositionandbeingcontrolledbythepolicyµˆ                  whentheﬁttedQiterationalgorithmis
                                                                                      100
                       used with Extra-Trees. As we observe, the control policy computed manages to bring the Acrobot
                       close to its unstable equilibrium position.
                           In order to attribute a score to a policy µˆ , we deﬁne a set
                                                                       N
                                        i             ˙   ˙       4                               ˙     ˙
                                      X ={(θ1,θ2,θ1,θ2)∈R |θ1 ∈{−2,−1.9,···,2},θ2θ1 =θ2 =0},
                       evaluate JµˆN(x) for each element x of this set and average the values obtained. The evolution of
                                  ∞
                                      ∗
                       the score of µˆ   with N for different tree-based methods is drawn on Figure 22. Extra-Trees gives
                                      N
                       20. We say that a terminal state is reached when the Acrobot has reached the target neighborhood of the unstable equi-
                           librium set.
                       21. The ε-greedy policy chooses with probability 1 − ε the control action u at random in the set {u ∈ U|u =
                                      ˆ                                                          t
                           argmax     Q(x ,u)}) and with probability ε at random inU.
                                  u∈U    t
                                                                            536
                                                 TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                                                                                                     u=5
                                                                                                                     u=−5
                        Figure 21: A typical control sequence with a learned policy. The Acrobot starts with zero speed in a down-
                                     wardposition. Itspositionandtheappliedcontrolactionarerepresentedatsuccessivetimesteps.
                                     Thelast step corresponds to a terminal state.
                               ∗
                              µˆ
                             J N
                              ∞                 Extra-Trees
                               0.05
                               0.04                                                  Tree-based         Policy which generates F
                               0.03                                                    method           ε-greedy        Random
                                                                                 Pruned CARTTree         0.0006            0.
                               0.02                Tree Bagging                 Kd-Tree (Best n     )    0.0004            0.
                                                                                                 min
                               0.02        Totally Randomized Trees                 Tree Bagging         0.0417          0.0047
                                                                                     Extra-Trees         0.0447          0.0107
                                0.0                                              Totally Rand. Trees     0.0371          0.0071
                                     10     30     50    70     90  N
                                                                                                     ∗
                                                                               Table 5: Score of µˆ      for the two sets of four-
                                                                                                     100
                                                ∗
                        Figure 22: Score of µˆ     whenusing the set gen-                 tuples and different tree-based meth-
                                                N
                                     erated by the ε-greedy policy.                       ods.
                        the best score while the score of Tree Bagging seems to oscillate around the value of the score
                        corresponding to Totally Randomized Trees.
                                                         ∗
                             The score obtained by µˆ        for the different tree-based methods and the different sets of four-
                                                         100
                        tuples is represented in Table 5. One may observe, once again, that methods which build an en-
                        semble of regression trees perform much better. Surprisingly, Totally Randomized Trees behaves
                        well compared to Tree Bagging and to a lesser extent to Extra-Trees. On the other hand, the single
                                                                                                                              ∗
                        tree methods offer rather poor performances. Note that for Kd-Tree, we have computed µˆ                   and its
                                                                                                                              100
                        associated score for each value of n         ∈{2,3,4,5,10,20,···,100} and reported in the Table 5 the
                                                                 min
                        highest score thus obtained.
                             Wecanalso observe from this table that the scores obtained while using the set of four-tuples
                        corresponding to the totally random policy are much worse than those obtained when using an
                        ε-greedy policy. This is certainly because the use of a totally random policy leads to very little
                                                                               537
                                                           ERNST, GEURTS AND WEHENKEL
                       information along the optimal trajectories starting from elements of Xi. In particular, out of the
                       2000episodesusedtogeneratethesetoffour-tuples,only21managetoreachthegoalregion. Note
                       also that while Extra-Trees remains superior, Tree Bagging offers this time poorer performances
                       than Totally Randomized Trees.
                       5.5 The Bicycle
                       Weconsidertwocontrolproblemsrelatedtoabicyclewhichmovesatconstantspeedonahorizontal
                       plane (Figure 23). For the ﬁrst problem, the agent has to learn how to balance the bicycle. For the
                       second problem, he has not only to learn how to balance the bicycle but also how to drive it to a
                       speciﬁc goal. The exact deﬁnitions of the two optimal control problems related to these two tasks
                                                     22
                       are given in Appendix C.4.
                           These two optimal control problems have the same system dynamics and differ only by their
                       reward function. The system dynamics is composed of seven variables. Four are related to the
                       bicycle itself and three to the position of the bicycle on the plane. The state variables related to the
                                                                                    ˙
                       bicycle are ω (the angle from vertical to the bicycle), ω, θ (the angle the handlebars are displaced
                                            ˙
                       from normal) and θ. If |ω| becomes larger than 12 degrees, then the bicycle is supposed to have
                       fallen down and a terminal state is reached. The three state variables related to the position of
                       the bicycle on the plane are the coordinates (x ,y ) of the contact point of the back tire with the
                                                                           b   b
                       horizontal plane and the angle ψ formed by the bicycle frame and the x-axis. The actions are
                       the torque T applied to the handlebars (discretized to {−2,0,2}) and the displacement d of the
                       rider (discretized to {−0.02,0,0.02}). The noise in the system is a uniformly distributed term in
                       [−0.02,0.02] added to the displacement component action d.
                           As is usually the case when dealing with these bicycle control problems, we suppose that the
                       state variables xb and yb cannot be observed. Since these two state variables do not intervene in the
                       dynamics of the other state variables nor in the reward functions considered, they may be taken as
                       irrelevant variables for the optimal control problems and, therefore, their lack of observability does
                       not make the control problem partially observable.
                           The reward function for the “Bicycle Balancing” control problem (Eqn (56), page 553) is such
                       that zero rewards are always observed, except when the bicycle has fallen down, in which case
                       the reward is equal to -1. For the “Bicycle Balancing and Riding” control problem, a reward of
                       −1is also observed when the bicycle has fallen down. However, this time, non-zero rewards are
                       also observed when the bicycle is riding (Eqn (57), page 553). Indeed, the reward r when the
                                                                                                                        t
                       bicycle is supposed not to have fallen down, is now equal to c            (d     (ψ )−d        (ψ    )) with
                                                                                           reward  angle   t     angle   t+1
                       22. Several other papers treat the problems of balancing and/or balancing and riding a bicycle (e.g. Randløv and Alstrøm,
                           1998; Ng and Jordan, 1999; Lagoudakis and Parr, 2003b,a). The reader can refer to them in order to put the perfor-
                           mances of ﬁtted Q iteration in comparison with some other RL algorithms. In particular, he could refer to Randløv
                           and Alstrøm (1998) to get an idea of the performances of SARSA(λ), an on-line algorithm, on these bicycle control
                           problems and to Lagoudakis and Parr (2003b) to see how the Least-Square Policy Iteration (LSPI), a batch mode
                           RLalgorithm, performs. If his reading of these papers and of the simulation results reported in Sections 5.5.1 and
                           5.5.2 is similar to ours, he will conclude that ﬁtted Q iteration combined with Extra-Trees performs much better
                           than SARSA(λ) in terms of ability to extract from the information acquired from interaction with the system, a good
                           control policy. He will also conclude that LSPI and ﬁtted Q iteration combined with Extra-Trees are both able to
                           produce good policies with approximately the same number of episodes. Moreover, the reader will certainly notice
                           the obvious strong dependence of performances of LSPI and SARSA(λ) on the choice of the parametric approxi-
                           mation architecture these algorithms use to approximate the Q-function, which makes extremely difﬁcult any strict
                           comparison with them.
                                                                            538
                                               TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                                                                      front wheel         θ
                                                                                                             T
                                                                                 contact                 frame of the bike
                                                                        back wheel - ground
                                                                                               ψ
                               d+w F                                            (x ,y )       ψ
                            CM           cen                                       b  b        goal                   x-axis
                                        hϕ
                           Mg                                                                                                  goal
                                           ω
                                                                            center of goal (coord. = (x   ,y    ))
                                                                                                       goal goal
                                           (a)                                                   (b)
                        Figure 23: Figure (a) represents the bicycle seen from behind. The thick line represents the bicycle. CM
                                     is the center of mass of the bicycle and the cyclist. h represents the height of the CM over the
                                     ground. ω represents the angle from vertical to bicycle. The angle ϕ represents the total angle
                                     of tilt of the center of mass. Action d represents how much the agent decides to displace the
                                     center of mass from the bicycle’s plan and w is the noise laid on the choice of displacement, to
                                     simulate imperfect balance. Figure (b) represents the bicycle seen from above. θ is the angle
                                     the handlebars are displaced from normal, ψ the angle formed by the bicycle frame and the x-
                                     axis and ψ     the angle between the bicycle frame and the line joining the back - wheel ground
                                               goal
                                     contact and the center of the goal. T is the torque applied by the cyclist to the handlebars. (x ,y )
                                                                                                                                 b  b
                                     is the contact point of the backwheel with the ground.
                        c       =0.1 and d        (ψ) = min|ψ+2kπ| (d            (ψ) represents the “distance” between an angle
                         reward              angle        k∈Z               angle
                        ψandthe angle 0). Positive rewards are therefore observed when the bicycle frame gets closer to
                        the position ψ = 0 and negative rewards otherwise. With such a choice for the reward function, the
                                          ∗
                        optimal policy µ tends to control the bicycle so that it moves to the right with its frame parallel
                                                                                              ∗
                        to the x-axis. Such an optimal policy or a good approximate µˆ of it can then be used to drive the
                        bicycle to a speciﬁc goal. If ψgoal represents the angle between the bicycle frame and a line joining
                                                             t
                        the point (x ,y ) to the center of the goal (x       ,y    ) (Figure 23b), this is achieved by selecting at
                                     b  b                                goal  goal
                                             ∗     ˙      ˙                          ∗     ˙      ˙
                        time t the action µˆ (ω ,ω ,θ ,θ ,ψ        ), rather than µˆ (ω ,ω ,θ ,θ ,ψ ). In this way, we proceed
                                                 t   t  t  t   goal                      t   t  t  t   t
                                                                   t
                        as if the line joining (x ,y ) to (x     ,y     ) were the x-axis when selecting control actions, which
                                                  b  b        goal  goal 23
                        makesthebicyclemovingtowardsthegoal.               Notethatinoursimulations, (x           ,y    ) =(1000,0)
                                                                                                              goal  goal
                        and the goal is a ten meter radius circle centered on this point. Concerning the value of the decay
                        23. The reader may wonder why, contrary to the approach taken by other authors (Lagoudakis, Parr, Randløv, Alstrøm,
                           Ng, Jordan),
                            • wedidnotconsiderinthestatesignalavailableduringthefour-tuplesgenerationphaseψ         rather than ψ (which
                                                                      ˙    ˙                                    goal
                               would have amounted here to consider (ω,ω,θ,θ,ψ    ) as state signal when generating the four-tuples)
                                                                              goal
                                                                             539
                                                               ERNST, GEURTS AND WEHENKEL
                        factor γ, it has been chosen for both problems equal to 0.98. The inﬂuence of γ and creward on the
                        trajectories for the “Bicycle Balancing and Riding” control problem will be discussed later.
                             All episodes used to generate the four-tuples start from a state selected at random in
                                          ˙     ˙                 7                         ˙         ˙
                                    {(ω,ω,θ,θ,x ,y ,ψ)∈R |ψ∈[−π,π]andω=ω=θ=θ=x =y =0},
                                                   b   b                                                    b     b
                        andendwhenaterminalstateisreached,i.e. whenthebicycleissupposedtohavefallendown. The
                        policy considered during the four-tuples generation phase is a policy that selects at each instant an
                        action at random inU.
                             For both optimal control problems, the set Xi considered for the score computation (Section
                        5.1.2) is:
                                i         ˙    ˙                7              3π                   ˙        ˙
                               X ={(ω,ω,θ,θ,x ,y ,ψ)∈R |ψ∈{−π,−                   ,··· ,π}andω=ω=θ=θ=x =y =0}.
                                                  b   b                        4                                   b    b
                             Since ψ and ψ+2kπ(k∈Z)areequivalentfromaphysicalpointofview,inoursimulationswe
                        have modiﬁed each value of ψ observed by a factor 2kπ in order to guarantee that it always belongs
                        to [−π,π].
                         5.5.1 THE “BICYCLE BALANCING” CONTROL PROBLEM
                        To generate the four-tuples, we have considered 1000 episodes. The corresponding set F is com-
                        posed of 97,969 four-tuples. First, we discuss the results obtained by Extra-Trees (n                 =4,K=7,
                                  24                                                                                      min
                        M=50) andthenweassesstheperformancesoftheothertree-basedmethods.
                             Figure 24a represents the evolution of the score of the policies µˆ∗ with N when using Extra-
                                                                                                          N
                        Trees. To assess the quality of a policy µ, we use also another criterion than the score. For this
                        criterion, we simulate for each x0 ∈ Xi ten times the system with the policy µ, leading to a total of
                        90trajectories. If no terminal state has been reached before t = 50,000, that is if the policy was able
                        to avoid crashing the bicycle during 500 seconds (the discretization time step is 0.01 second), we say
                        that the trajectory has been successful. On Figure 24c we have represented for the different policies
                          ∗
                        µˆ   the number of successful trajectories among the 90 simulated. Remark that if from N = 60
                          N
                                                                                  ∗         ∗
                        the score remains really close to zero, polices µˆ           and µˆ    do not produce as yet any successful
                                                                                  60        70
                        trajectories, meaning that the bicycle crashes for large values of t even if these are smaller than
                             • the reward function for the bicycle balancing and riding control problem does not give directly information about
                                the direction to the goal (which would have led here to observe at t +1 the reward c       (d     (ψ    ) −
                                                                                                                      reward angle  goal
                                d    (ψ       ))).                                                                                      t
                                 angle  goal
                                           t+1
                            Wedid not choose to proceed like this because ψ        depends not only on ψ      and θ but also on x   and y .
                                                                             goal                        goal       t             b      b
                                                                                t+1                          t                     t      t
                            Therefore, since we suppose that the coordinates of the back tire cannot be observed, the optimal control problems
                            would have been partially observable if we had replaced ψ by ψ       in the state signal and the reward function.
                                                                                             goal
                            Although in our simulations this does not make much difference since ψ ' ψ      during the four-tuples generation
                                                                                                       goal
                            phases, we prefer to stick with fully observable systems in this paper.
                         24. When considering ensemble methods (Extra-Trees, Totally Randomized Trees, Tree Bagging) we always keep con-
                            stant the value of these parameters. Since we are not dealing with a highly stochastic system, as for the case of the
                            “Left or Right” control problem, we decided not to rely on the pruned version of these algorithms. However, we
                            found out that by developing the trees fully (nmin = 2), variance was still high. Therefore, we decided to use a larger
                            value for n   . This value is equal to 4 and was leading to a good bias-variance tradeoff. Concerning the value of
                                       min                                                                                ˙    ˙
                            K=7,itisequaltothe dimension of the input space, that is the dimension of the state signal (ω,ω,θ,θ,ψ) plus the
                            dimension of the action space.
                                                                                540
                                           TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                                        Tree-based method                               Control problem
                                                                                Bicycle            Bicycle
                                                                               Balancing    Balancing and Riding
                                             Pruned CARTTree                    -0.02736          -0.03022
                               Kd-Tree (Best n   ∈{2,3,4,5,10,20,···,100})      -0.02729          -0.10865
                                              min
                                      Tree Bagging (n    =4,M=50)                  0              0.00062
                                                     min
                                   Extra-Trees (n   =4,K=7,M=50)                   0              0.00157
                                                 min
                                Totally Randomized Trees (n    =4,M=50)         -0.00537          -0.01628
                                                           min
                                         ∗
                      Table 6: Score of µˆ   for the “Balancing” and “Balancing and Riding” problems. Different tree-based
                                         300
                               methods are considered, with a 1000 episode based set of four-tuples.
                      50,000. Figure 24b gives an idea of the trajectories of the bicycle on the horizontal plane when
                                                               i                          ∗
                      starting from the different elements of X and being controlled by µˆ   .
                                                                                          300
                         To assess the inﬂuence of the number of four-tuples on the quality of the policy computed, we
                      have drawn on Figure 24d the number of successful trajectories when different number of episodes
                      are used to generate F . As one may see, by using Extra-Trees, from 300 episodes (' 10,000
                      four-tuples) only successful trajectories are observed. Tree Bagging and Totally Randomized Trees
                      perform less well. It should be noted that Kd-Tree and Pruned CART Tree were not able to pro-
                      duce any successful trajectories, even for the largest set of four-tuples. Furthermore, the fact that
                      we obtained some successful trajectories with Totally Randomized Trees is only because we have
                      modiﬁed the algorithm to avoid selection of tests according to ψ, a state variable that plays for this
                      “Bicycle Balancing” control problem the role of an irrelevant variable (ψ does not intervene in the
                                                                                   ˙     ˙
                      reward function and does not inﬂuence the dynamics of ω, ω, θ, θ) (see also Section 5.3.5). Note
                                                   ∗
                      that the scores obtained by µˆ   for the different tree-based methods when considering the 97,969
                                                   300
                      four-tuples are reported in the second column of Table 6.
                      5.5.2 THE “BICYCLE BALANCING AND RIDING” CONTROL PROBLEM
                      To generate the four-tuples, we considered 1000 episodes that led to a set F composed of 97,241
                      elements. First, we study the performances of Extra-Trees (n      =4,K=7,M=50). Figure25a
                                                                                    min
                                                                ∗                                                   ∗
                      represents the evolution of the score of µˆ with N. The ﬁnal value of the score (score of µˆ     ) is
                      equal to 0.00157.                         N                                                   300
                         As mentioned earlier, with the reward function chosen, the policy computed by our algorithm
                      should be able to drive the bicycle to the right, parallel to the x-axis, provided that the policy is a
                      goodapproximationoftheoptimalpolicy. Toassessthisability,wehavesimulated,foreachx ∈Xi,
                                                                                                                    0
                                                  ∗
                      the system with the policy µˆ   and have represented on Figure 25b the different trajectories of the
                                                  300
                      back tire. As one may see, the policy tends indeed to drive the bicycle to the right, parallel to the
                      x-axis. The slight shift that exists between the trajectories and the x-axis (the shift is less than 10
                      degrees) could be reduced if more four-tuples were used as input of the ﬁtted Q iteration algorithm.
                                                               ∗                           ˙    ˙
                         Now,ifratherthanusingthepolicyµˆ300 withthestatesignal(ω,ω,θ,θ,ψ)weconsiderthestate
                                 ˙    ˙
                      signal (ω,ω,θ,θ,ψgoal), where ψgoal is the angle between the bicycle frame and the line joining
                      (x ,y ) with (x    ,y   ), we indeed observe that the trajectories converge to the goal (see Figure
                        b  b         goal  goal
                      25c). Under such conditions, by simulating from each x ∈ Xi ten times the system over 50,000
                                                                                 0
                      time steps, leading to a total of 90 trajectories, we observed that every trajectory managed to reach
                                                                      541
                                                                               ERNST, GEURTS AND WEHENKEL
                                         ∗
                                        µˆ
                                      J N
                                         0.0                                                                                    y
                                                                                                                                 b
                                       −0.01                                                                            π      1000
                                                                                                                  ψ0 = 2                      ψ0 = π
                                       −0.02                                                                                                       4
                                                                                                              ψ0 = 3π
                                                                                                                    4           500
                                       −0.03                                                                                                          ψ0 =0
                                       −0.04                                                                                      0
                                                                                                                   −1000   −500     0        500     1000  x
                                                                                                            ψ =π                                             b
                                       −0.05                                                                 0                 −500
                                                                                                                                         ψ0 =−π
                                       −0.06                                                                                                    4
                                                                                                                ψ0 =−π                       ψ0 =−π
                                                                                                                              −1000                 2
                                                   50     100     150     200    250                             ψ0 =−3π
                                                                                         N                               4
                                                             (a)                                                                (b)
                                                                                                  Nbof                              Tree Bagging
                                  Nbof                                                            succ.         Extra-Trees
                                  succ.                                                           traj.   90
                                  traj.   90                                                              80
                                          80                                                              70
                                          70                                                              60
                                          60                                                              50
                                          50                                                              40
                                          40                                                              30
                                          30                                                              20
                                          20                                                              10
                                          10                                                                                Totally Randomized Trees
                                                                                                           0                (attribute ψ never selected)
                                           0                                                                 100       300      500       700       900  Nbof
                                                   50     100     150     200    250     N                                                              episodes
                                                             (c)                                                                (d)
                                                                                                                                                             ∗
                               Figure 24: The “Bicycle Balancing” control problem. Figure (a) represents the score of µˆ                                        with Extra-
                                                                                                                                                             N
                                                Trees and 1000 episodes used to generate F . Figure (b) sketches trajectories of the bicycle on
                                                                                                     ∗
                                                the x −y plane when controlled by µˆ                     . Trajectories are drawn from t = 0 till t = 50,000.
                                                      b      b                                       300
                                                                                                                                                   ∗
                                                Figure (c) represents the number of times (out of 90 trials) the policy µˆ                            (Extra-Trees, 1000
                                                                                                                                                   N
                                                episodes) manages to balance the bicycle during 50,000 time steps, i.e. 500 s. Figure (d) gives
                                                                                                                                                                           ∗
                                                for different numbers of episodes and for different tree-based methods the number of times µˆ
                                                                                                                                                                           300
                                                leads to successful trajectories.
                                                                                                    542
                                             TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                      a 2 meter neighborhood of (x       ,y    ). Furthermore, every trajectory was able to reach the goal in
                                                     goal  goal                                                 10           −1
                      less that 47,000 time steps. Note that, since the bicycle rides at constant speed v = 3.6 ' 2.77ms
                      andsince the time discretization step is 0.01s, the bicycle does not have to cover a distance of more
                      that 1278m before reaching the goal while starting from any element of Xi.
                           It is clear that these good performances in terms of the policy ability to drive the bicycle to the
                      goal depend on the choice of the reward function. For example, if the same experience is repeated
                      with creward chosen equal to 1 rather than 0.1 in the reward function, the trajectories lead rapidly
                      to a terminal state. This can be explained by the fact that, in this case, the large positive rewards
                      obtained for moving the frame of the bicycle parallel to the x-axis lead to a control policy that
                      modiﬁestoorapidly the bicycle riding direction which tends to destabilize it. If now, the coefﬁcient
                      creward is taken smaller than 0.1, the bicycle tends to turn more slowly and to take more time to
                      reach the goal. This is illustrated on Figure 25d where a trajectory corresponding to creward = 0.01
                      is drawn together with a trajectory corresponding to creward = 0.1. On this ﬁgure, we may also
                      clearly observe that after leaving the goal, the control policies tend to drive again the bicycle to it.
                      It should be noticed that the policy corresponding to c           =0.1 manages at each loop to bring
                                                                                 reward
                      the bicycle back to the goal while it is not the case with the policy corresponding to creward = 0.01.
                      Notethat the coefﬁcient γ inﬂuences also the trajectories obtained. For example, by taking γ = 0.95
                      instead of γ = 0.98, the bicycle crashes rapidly. This is due to the fact that a smaller value of γ tends
                      to increase the importance of short-term rewards over long-term ones, which favors actions that turn
                      rapidly the bicycle frame, even if they may eventually lead to a fall of the bicycle.
                           Ratherthanrelyingonlyonthescoretoassesstheperformancesofapolicy,letusnowassociate
                      to a policy a value that depends on its ability to drive the bicycle to the goal within a certain time
                                           ˙    ˙
                      interval, when (ω,ω,θ,θ,ψgoal) is the state signal considered. To do so, we simulate from each
                      x0 ∈Xi tentimesthesystemover50,000timestepsandcountthenumberoftimesthegoalhasbeen
                      reached. Figure 25e represents the “number of successful trajectories” obtained by µˆ∗ for different
                                                                                                                N
                      values of N. Observe that 150 iterations of ﬁtted Q iteration are needed before starting to observe
                      some successful trajectories. Observe also that the “number of successful trajectories” sometimes
                      drops when N increases, contrary to intuition. These drops are however not observed on the score
                      values (e.g. for N = 230, all 90 trajectories are successful and the score is equal to 0.00156, while
                      for N = 240, the number of successful trajectories drops to 62 but the score increases to 0.00179).
                      Additional simulations have shown that these sudden drops tend to disappear when using more
                      four-tuples.
                           Figure25fillustratestheinﬂuenceofthesizeofF onthenumberofsuccessfultrajectorieswhen
                      ﬁtted Q iteration is combined with Extra-Trees. As expected, the number of successful trajectories
                      tends to increase with the number of episodes considered in the four-tuples generation process.
                      It should be noted that the other tree-based methods considered in this paper did not manage to
                      produce successful trajectories when only 1000 episodes are used to generate the four-tuples. The
                      different scores obtained by µˆ∗   when1000episodesareconsideredandforthedifferenttree-based
                                                      300
                      methods are gathered in Table 6, page 541. Using this score metric, Extra-Trees is the method
                      performing the best, which is in agreement with the “number of successful trajectories” metric,
                      followed successively by Tree Bagging, Totally Randomized Trees, Pruned CART Tree and Kd-
                      Tree.
                                                                         543
                                                                                   ERNST, GEURTS AND WEHENKEL
                                              ∗
                                             µˆ
                                           J N
                                              0.0                                                                   y
                                                                                                                     b
                                                                                                                    250                                        ψ0 =π
                                            −0.05                                                                                                                   ψ0 = 3π
                                                                                                                                                                          4
                                                                                                                    200                                               ψ0 = π
                                                                                                                                                                            2
                                                                                                                                                                       ψ0 = π
                                             −0.1                                                                   150                                                     4
                                                                                                                                                                       ψ0 =0
                                                                                                                    100                                                ψ0 =−π
                                                                                                                                                                              4
                                            −0.15                                                                    50                                               ψ0 =−π
                                                                                                                                                                             2
                                                                                                                                                                     ψ0 =−3π
                                            −0.20                                                                     0                                           ψ0 =−π    4
                                                                                                                        0       250     500     750     1000    1250 x
                                                                                                                    −50                                                b
                                            −0.25
                                                         50      100     150     200     250     N                −100
                                                                    (a)                                                                      (b)
                                               y            ψ0 =−π                                                  y                     creward = 0.01
                                                 b                  ψ = 3π                                            b
                                                                     0    4                                                               Trajectory reaches
                                                                           ψ0 = π                                                         goal at t = 48182 (ψ0 = π)
                                                                                2                                   150
                                                100                              ψ0 = π
                                                                                      4
                                                 50                                       goal                      100 creward = 0.1
                                                                                                                     50 Trajectory reaches
                                                  0                                                                      goal at t = 45276
                                                    0         250       500       750       1000                         (ψ0 =π)
                                                                                                 x                     0
                                               −50                                                 b                     0        250      500       750      1000   x
                                                                                                                    −50                                               b
                                                                     ψ0 =0
                                              −100              ψ0 =−π
                                                             ψ =−π 4                                               −100
                                                              0     2
                                              −150       ψ0 =π
                                                     ψ0 =−3π                                                       −150
                                                             4
                                                                    (c)                                      Nbof                            (d)
                                        Nbof                                                                 succ.
                                        succ.                                                                traj.   90
                                        traj.   90                                                                   80
                                                80                                                                   70
                                                70                                                                   60
                                                60                                                                   50
                                                50                                                                   40                           Extra-Trees
                                                40                                                                   30
                                                30                                                                   20
                                                20                                                                   10
                                                10                                                                    0
                                                 0                                                                      100        300       500       700        900  Nbof
                                                         50      100     150     200     250     N                                                                    episodes
                                                                    (e)                                                                      (f)
                                Figure 25: The “Bicycle Balancing and Riding” control problem.                                        Figure (a) represents the score of
                                                    ∗                                                                                                           ∗
                                                  µˆ    (Extra-Trees, 1000 episodes).                 Figure (b) sketches trajectories when µˆ                        (Extra-Trees,
                                                    N                                                                                                           300
                                                  1000 episodes) controls the bicycle (trajectories drawn from t = 0 till t = 50,000). Figure
                                                                                                   ∗
                                                  (c) represents trajectories when µˆ                    (Extra-Trees, 1000 episodes) controls the bicycle with
                                                                  ˙                                300
                                                         ˙
                                                  (ω ,ω ,θ ,θ ,ψ              ) used as input signal for the policy (trajectories drawn from t = 0 till
                                                      t    t   t   t    goal
                                                                            t
                                                                                                                                                                  ∗
                                                  t =50,000). Figure (d) represents the inﬂuence on c                                on the trajectories (µˆ          , Extra-Trees,
                                                                                                                            reward                                300
                                                  1000episodes and trajectories drawn from t = 0 till t = 100,000). Figure (e) lists the number of
                                                                           ∗
                                                  times the policy µˆ         managestobringthebicycletothegoalinlessthan50,000timesteps(high-
                                                                           N
                                                  est possible value for “Number of successful trajectories” is 90). Figure (f) gives for different
                                                                                                             ∗
                                                  numberofepisodes the number of times µˆ                         leads to successful trajectories.
                                                                                                             300
                                                                                                          544
                                          TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                     5.6 Conclusion of the Experiments
                     We discuss in this section the main conclusions that may be drawn from the simulation results
                     previously reported.
                     5.6.1 INFLUENCE OF THE TREE-BASED METHODS
                     Let us analyze the results of our experiments in the light of the classiﬁcation given in Table 1.
                     Single trees vs ensembles of trees.  Whateverthesetoffour-tuples used, the top score has always
                     been reached by a method building an ensemble of regression trees, and furthermore, the larger the
                     state space, the better these regression tree ensemble methods behave compared to methods building
                     only one single tree. These results are in agreement with previous work in reinforcement learning
                     which suggests that multi-partitioning of the state space is leading to better function approxima-
                                                 25
                     tors than single partitioning.  They are also in agreement with the evaluation of these ensemble
                     algorithms on many standard supervised learning problems (classiﬁcation and regression), where
                     tree-based ensemble methods typically signiﬁcantly outperform single trees (Geurts et al., 2004).
                         However, from the viewpoint of computational requirements, we found that ensemble methods
                     are clearly more demanding, both in terms of computing times and memory requirements for the
                     storage of models.
                     Kernel-based vs non kernel-based methods.         Among the single tree methods, Pruned CART
                     Tree, which adapts the tree structure to the output variable, offers typically the same performances
                     as Kd-Tree, except in the case of irrelevant variables where it is signiﬁcantly more robust. Among
                     the tree-based ensemble methods, Extra-Trees outperforms Totally Randomized Trees in all cases.
                     On the other hand, Tree Bagging is generally better than the Totally Randomized Trees, except
                     when dealing with very small numbers of samples, where the bootstrap resampling appears to be
                     penalizing. These experiments thus show that tree-based methods that adapt their structure to the
                     new output at each iteration usually provide better results than methods that do not (that we name
                     kernel-based). Furthermore, the non kernel-based tree-based algorithms are much more robust to
                     the presence of irrelevant variables thanks to their ability to ﬁlter out tests involving these variables.
                         Adrawbackofnonkernel-based methods is that they do not guarantee convergence. However,
                     with the Extra-Trees algorithm, even if the sequence was not converging, the policy quality was
                     oscillating only moderately around a stable value and even when at its lowest, it was still superior
                     to the one obtained by the kernel-based methods ensuring the convergence of the algorithm. Fur-
                     thermore, if really required, convergence to a stable approximation may always be provided in an
                     ad hoc fashion, for example by freezing the tree structures after a certain number of iterations and
                     then only refreshing predictions at terminal nodes.
                     5.6.2 PARAMETRIC VERSUS NON-PARAMETRIC SUPERVISED LEARNING METHOD
                     Fitted Q iteration has been used in our experiments with non-parametric supervised learning meth-
                     ods(kNN,tree-basedmethods)andparametricsupervisedlearningmethods(basisfunctionmethods
                     with piecewise-constant or piecewise-linear grids as approximation architectures).
                     25. See e.g. Sutton (1996); Sutton and Barto (1998), where the authors show that by overlaying several shifted tilings
                        of the state space (type of approximation architecture known as CMACs), good function approximators could be
                        obtained.
                                                                    545
                        ERNST, GEURTS AND WEHENKEL
           It has been shown that the parametric supervised learning methods, compared to the non-
          parametric ones, were not performing well. The main reason is the difﬁculty to select a priori the
          shape of the parametric approximation architecture that may lead to some good results. It should
          also be stressed that divergence to inﬁnity of the ﬁtted Q iteration has sometimes been observed
          whenpiecewise-linear grids were the approximation architectures considered.
          5.6.3 FITTED Q ITERATION VERSUS ON-LINE ALGORITHMS
          AnadvantageofﬁttedQiterationoveron-linealgorithmsisthatitcanbecombinedwithsomenon-
          parametric function approximators, shown to be really efﬁcient to generalize the information. We
          have also compared the performances of ﬁtted Q iteration and Q-learning for some a priori given
          parametric approximation architectures. In this context, we found out that when the approximation
          architecture used was chosen so as to avoid serious convergence problems of the ﬁtted Q iteration
          algorithm, thenthislatterwasalsoperformingmuchbetterthanQ-learningonthesamearchitecture.
          6. Conclusions and Future Work
          In this paper, we have considered a batch mode approach to reinforcement learning, which consists
          of reformulating the reinforcement learaning problem as a sequence of standard supervised learning
          problems. After introducing the ﬁtted Q iteration algorithm which formalizes this framework, we
          have studied the properties and performances of the algorithm when combined with three classical
          tree-based methods (Kd-Trees, CART Trees, Tree Bagging) and two newly proposed tree-based
          ensemble methods namely Extra-Trees and Totally Randomized Trees.
           Compared with grid-based methods on low-dimensional problems, as well as with kNN and
          single tree-based methods in higher dimensions, we found out that the ﬁtted Q iteration algorithm
          was giving excellent results when combined with any one of the considered tree-based ensemble
          methods(Extra-Trees,TreeBaggingandTotallyRandomizedTrees). Onthedifferentcasesstudied,
          Extra-Treeswasthesupervisedlearningmethodabletoextractatbestinformationfromasetoffour-
          tuples. It is also faster than Tree Bagging and was performing signiﬁcantly better than this latter
          algorithm, especially on the higher dimensional problems and on low-dimensional problems with
          small sample sizes. We also found out that ﬁtted Q iteration combined with tree-based methods
          wasperformingmuchbetterthanQ-learningcombinedwithpiecewise-constantorpiecewise-linear
          grids.
           Since Extra-Trees and Tree Bagging, the two best performing supervised learning algorithms,
          readjust their approximation architecture to the output variable at each iteration, they do not en-
          sure the convergence of the ﬁtted Q iteration algorithm. However, and contrary to many parametric
          approximation schemes, they do not lead to divergence to inﬁnity problems. The convergence prop-
          erty is satisﬁed by the Totally Randomized Trees because their set of trees is frozen at the beginning
          of the iteration. They perform however less well than Extra-Trees and Tree Bagging, especially in
          the presence of irrelevant variables. They are nevertheless better than some other methods that also
          ensure the convergence of the sequence, like kNN kernel methods and piecewise-constant grids, in
          terms of performances as well as scalability to large numbers of variables and four-tuples. Within
          this context, it would be worth to study versions of Extra-Trees and Tree Bagging which would
          freeze their trees at some stage of the iteration process, and thus recover the convergence property.
           Fromatheoretical point of view, it would certainly be very interesting to further study the con-
          sistency of the ﬁtted Q iteration algorithm, in order to determine general conditions under which the
                               546
                                         TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                    algorithmconvergestoanoptimalpolicywhenthenumberoffour-tuplescollectedgrowstoinﬁnity.
                    With this in mind, one could possibly seek inspiration in the work of Ormoneit and Sen (2002) and
                    OrmoneitandGlynn(2002),whoprovideconsistencyconditionsforkernel-basedsupervisedlearn-
                    ing methodswithinthecontextofﬁttedQiteration,andalsoinsomeofthematerialpublishedinthe
                    supervised learning literature (e.g. Lin and Jeon, 2002; Breiman, 2000). More speciﬁcally, further
                    investigation in order to characterize ensembles of regression trees with respect to consistency is
                    particularly wishful, because of their good practical performances.
                        In this paper, the score associated to a test node of a tree was the relative variance reduction.
                    Several authors who adapted regression trees in other ways to reinforcement learning have sug-
                    gested the use of other score criteria for example based on the violation of the Markov assumption
                    (McCallum, 1996; Uther and Veloso, 1998) or on the combination of several error terms like the
                    supervised, the Bellman, and the advantage error terms (Wang and Diettrich, 1999). Investigating
                    the effect of such score measures within the ﬁtted Q iteration framework is another interesting topic
                    of research.
                        While the ﬁtted Q iteration algorithm used with tree-based ensemble methods reveals itself to
                    be very effective to extract relevant information from a set of four-tuples, it has nevertheless one
                    drawback: with increasing number of four-tuples, it involves a superlinear increase in computing
                    time and a linear increase in memory requirements. Although our algorithms offer a very good
                    accuracy/efﬁciency tradeoff, we believe that further research should explore different ways to try to
                    improve the computational efﬁciency and the memory usage, by introducing algorithm modiﬁca-
                    tions speciﬁc to the reinforcement learning context.
                    Acknowledgments
                    Damien Ernst and Pierre Geurts gratefully acknowledge the ﬁnancial support of the Belgian Na-
                    tional Fund of Scientiﬁc Research (FNRS). The authors thank the action editor Michael Littman
                    and the three anonymous reviewers for their many suggestions for improving the quality of the
                    manuscript. They are also very grateful to Bruno Scherrer for reviewing an earlier version of this
                    work, to Mania Pavella for her helpful comments and to Michail Lagoudakis for pointing out rele-
                    vant information about the bicycle control problems.
                    AppendixA. Extra-Trees Induction Algorithm
                    The procedure used by the Extra-Trees algorithm to build a tree from a training set is described in
                    Figure 26. This algorithm has two parameters: n   , the minimum number of elements required to
                                                                    min
                    split a node and K, the maximum number of cut-directions evaluated at each node. If K = 1 then
                    at each test node the cut-direction and the cut-point are chosen totally at random. If in addition
                    the condition (iii) is dropped, then the tree structure is completely independent of the output values
                    found in the T S, and the algorithm generates Totally Randomized Trees.
                    The score measure used is the relative variance reduction. In other words, if T Sl (resp. T Sr)
                    denotes the subset of cases from T S such that [ij < t] (resp. [ij ≥ t]), then the Score is deﬁned as
                    follows:
                                                    var(o|T S)− #TSlvar(o|T Sl)− #TSrvar(o|T Sr)
                               Score([ij <t],T S) =               #T S              #T S            ,           (25)
                                                                       var(o|T S)
                                                                  547
                                                                      ERNST, GEURTS AND WEHENKEL
                           Build a tree(T S)
                           Input: a training set T S
                           Output: a tree T;
                                • If
                                      (i) #T S < nmin, or
                                     (ii) all input variables are constant in T S, or
                                    (iii) the output variable is constant over the T S,
                                                                                            1        l
                                    return a leaf labeled by the average value                  ∑ o .
                                                                                          #T S    l
                                • Otherwise:
                                       1. Let [ij <tj] = Find a test(T S).
                                       2. Split T S into T Sl and T Sr according to the test [ij < t].
                                       3. Build T =Build a tree(T S ) and T =Build a tree(T S ) from these subsets;
                                                    l                          l         r                          r
                                       4. Create a node with the test [i < t ], attach T and T as left and right subtrees of this
                                                                                  j     j             l         r
                                           node and return the resulting tree.
                           Find a test(T S)
                           Input: a training set T S
                           Output: a test [ij < tj]:
                               1. Select K inputs,{i ,...,i }, at random, without replacement, among all (non constant) input
                                                           1       K
                                    variables.
                               2. For k going from 1 to K:
                                     (a) Compute the maximal and minimal value of ik in T S, denoted respectively iTS                                   and
                                                                                                                                                 k,min
                                            TS
                                           i      .
                                            k,max
                                     (b) Draw a discretization threshold tk uniformly in ]iT S ,iT S                    ]
                                                                                                          k,min   k,max
                                     (c) Compute the score S =Score([i <t ],T S)
                                                                      k              k     k
                               3. Return a test [ij < tj] such that Sj = max                      S .
                                                                                         k=1,...,K  k
                           Figure 26: Procedure used by the Extra-Trees algorithm to build a tree. The Totally Randomized
                                           Trees algorithm is obtained from this algorithm by setting K = 1 and by dropping the
                                           stopping condition (iii).
                                                                                          548
                                           TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                      where var(o|X) is the variance of the output o in the training set X.
                                                                             ˆ
                      AppendixB. ConvergenceoftheSequenceofQN-Functions
                      Theorem1 IftheﬁttedQiterationalgorithmiscombinedwithasupervisedlearningmethodwhich
                      produces a model of the type (17) with the kernel k   being the same from one iteration to the other
                                                                         TS                ˆ
                      andsatisfying the normalizing condition (18), then the sequence of Q -functions converges.
                                                                                            N
                      Proof Theproofisadapted in a straightforward way from Ormoneit and Sen (2002) to the fact that
                                       l  l                                                         0  l      l
                      the kernel k  ((x ,u ),(x,u)) may not be decomposed here into the product k (x ,x)δ(u ,u).
                                 TS    t  t                                                            t      t
                         Let us ﬁrst observe that in such conditions, the sequence of functions computed by the ﬁtted Q
                      iteration algorithm is determined by the recursive equation:
                                                  #F
                                   ˆ                        l  l          l         ˆ      l    0
                                  Q (x,u) =          k   ((x ,u ),(x,u))[r +γmaxQ        (x   ,u )],  ∀N>0            (26)
                                    N             ∑ TS      t  t          t     0    N−1 t+1
                                                  l=1                          u ∈U
                           ˆ
                      with Q (x,u) =0 ∀(x,u)∈X×U. Equation (26) may be rewritten:
                             0
                                                                ˆ         ˆ ˆ
                                                               Q     = HQ                                             (27)
                                                                 N           N−1
                             ˆ
                      where H is an operator mapping any function K : X ×U → R and deﬁned as follows:
                                                          #F
                                          ˆ                          l  l         l            l     0
                                        (HK)(x,u) = ∑kTS((x,u),(x,u))[r +γmaxK(x                  ,u )].              (28)
                                                                     t  t         t     0      t+1
                                                          l=1                          u ∈U
                      This operator is a contraction on the Banach space of functions deﬁned over X ×U and the supre-
                      mumnorm. Indeed, we have:
                                ˆ     ˆ                      #F       l  l               l    0           l    0
                              kHK−HKk         = γ max |        k   ((x ,u ),(x,u))[maxK(x   ,u )−maxK(x      ,u )]|
                                          ∞        (x,u)∈X×U ∑ TS     t  t        u0∈U   t+1      u0∈U    t+1
                                                            l=1
                                                             #F       l  l               l    0       l    0
                                              ≤ γ max |[        k  ((x ,u ),(x,u))max[K(x   ,u )−K(x     ,u )]|
                                                   (x,u)∈X×U ∑ TS     t  t       u0∈U    t+1          t+1
                                                             l=1
                                              ≤ γ max |K(x,u)−K(x,u)|
                                                   (x,u)∈X×U
                                              = γkK−Kk
                                                           ∞
                                              < kK−Kk .
                                                          ∞
                      By virtue of the ﬁxed-point theorem (Luenberger, 1969) the sequence converges, independently
                                                                ˆ
                      of the initial conditions, to the function Q : X ×U → R which is unique solution of the equation
                      ˆ    ˆ ˆ
                      Q=HQ.
                      AppendixC. Deﬁnition of the Benchmark Optimal Control Problems
                      Wedeﬁneinthissectionthedifferentoptimalcontrolproblemsusedinourexperiments. Simulators,
                      additional documentation and sets of four-tuples are available upon request.
                                                                      549
                                                           ERNST, GEURTS AND WEHENKEL
                       C.1 The“LeftorRight”ControlProblem
                       Systemdynamics:
                                                                 x    =x +u +w
                                                                  t+1     t    t    t
                       where w is drawn according the standard (zero mean, unit variance) Gaussian distribution.
                       If x    is such that |x   | > 10 or |x    | < 0 then a terminal state is reached.
                           t+1                t+1             t+1
                       State space: The state space X is composed of {x ∈ R|x ∈ [0,10]} and of a terminal state.
                       Action space: The action spaceU ={−2,2}.
                       Rewardfunction: The reward function r(x,u,w) is deﬁned through the following expression:
                                                                   
                                                                   
                                                                      0        if   x    ∈[0,10]
                                                                                     t+1
                                                    r(x ,u ,w ) =                                                             (29)
                                                       t  t   t       50       if   x    <0
                                                                                     t+1
                                                                   
                                                                   
                                                                      100      if   x    >10.
                                                                                     t+1
                       Decayfactor: The decay factor γ is equal to 0.75.
                       C.2 The“CarontheHill”ControlProblem
                       Systemdynamics: Thesystemhasacontinuous-timedynamicsdescribedbythesetwodifferential
                       equations:
                                          p˙  = s                                                                              (30)
                                           s˙ =             u          − gHill0(p) −s2Hill0(p)Hill00(p)                        (31)
                                                               0    2             0    2                0   2
                                                   m(1+Hill (p) )        1+Hill (p)            1+Hill (p)
                       wheremandgareparametersequalrespectivelyto1and9.81andwhereHill(p)isafunctionof p
                       deﬁned by the following expression:
                                                                    2
                                                       Hill(p) =p +p              if   p<0                                    (32)
                                                                      √ p          if   p≥0.
                                                                            2
                                                                        1+5p
                       The discrete-time dynamics is obtained by discretizing the time with the time between t and t +1
                       chosen equal to 0.100s.
                       If p    and s     are such that |p    | > 1 or |s    | > 3 then a terminal state is reached.
                           t+1       t+1                  t+1           t+1              2
                       State space: The state space X is composed of {(p,s) ∈ R ||p| ≤ 1and|s| ≤ 3} and of a terminal
                       state. X \{terminal state} is represented on Figure 8a.
                       Action space: The action spaceU ={−4,4}.
                       Rewardfunction: The reward function r(x,u) is deﬁned through the following expression:
                                                          
                                                          
                                                            −1       if   p    <−1 or |s |>3
                                                                           t+1                 t+1
                                              r(x ,u ) =  1         if   p    >1 and |s |≤3                                   (33)
                                                 t  t                      t+1                t+1
                                                          
                                                          0         otherwise.
                       Decayfactor: The decay factor γ has been chosen equal to 0.95.
                       Integration: The dynamical system is integrated by using an Euler method with a 0.001s integra-
                       tion time step.
                                                                            550
                                         TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                    Remark: This“CarontheHill”problemissimilartotheonefoundinMooreandAtkeson(1995)
                                           2   0     00
                    except that the term −s Hill (p)Hill (p) is not neglected here in the system dynamics.
                                                  0  2
                                             1+Hill (p)
                    Variants of the control problem: In our experiments, we have also considered two other variants
                    of this problem:
                        • The “Car on the Hill” with irrelevant variables: some irrelevant variables are added to the
                           state vector. The value of an irrelevant variable at time t is determined by drawing at random
                           a number in [−2,2] with a uniform probability (used in Section 5.3.5).
                        • The “Car on the Hill” with continuous action space: the action space is not yet discrete
                           anymore. It is continuous and equal to [−4,4] (used in Section 5.3.7).
                    C.3 The“AcrobotSwingUp”ControlProblem
                    System dynamics: The system has a continuous-time dynamics described by these two second-
                    order differential equations (taken from Yoshimoto et al., 1999):
                                                     ¨       ¨                      ˙
                                                  d θ +d θ +c +φ           = −µ θ                               (34)
                                                   11 1    12 2    1    1         1 1
                                                     ¨       ¨                        ˙
                                                  d θ +d θ +c +φ           = u−µ θ                              (35)
                                                   12 1    22 2    2    2           2 2
                                                                                                                (36)
                    where
                                         d     = ML2+M(L2+L2+2L L cos(θ ))                                      (37)
                                           11        1 1     2  1    2     1 2      2
                                         d     = ML2                                                            (38)
                                           22        2 2
                                         d     = M(L2+L L cos(θ ))                                              (39)
                                           12        2  2    1 2      2
                                                             ˙    ˙   ˙
                                          c    = −MLLθ(2θ +θ sin(θ ))                                           (40)
                                           1           2 1 2 2    1    2     2
                                                           ˙ 2
                                          c2   = M2L1L2θ1 sin(θ2)                                               (41)
                                          φ1   = (M1L1+M2L1)gsin(θ1)+M2L2gsin(θ1+θ2)                            (42)
                                          φ2   = M2L2gsin(θ1+θ2).                                               (43)
                    M (M ), L (L ) and µ (µ ) are the mass, length, and friction, respectively, of the ﬁrst (second)
                       1   2    1   2       1   2
                    link. θ1 is the angle of the ﬁrst link from a downward position and θ2 is the angle of the second
                                                                       ˙       ˙
                    link from the direction of the ﬁrst link (Figure 20). θ1 and θ2 are the angular velocities of the ﬁrst
                                                                                                              ˙  ˙
                    and second links, respectively. The system has four continuous state variables x = (θ1,θ2,θ1,θ2).
                    Thephysical parameters have been chosen equal to M =M =1.0, L =L =1.0, µ =µ =0.01,
                                                                         1    2         1    2         1    2
                    g=9.81.
                    The discrete-time dynamics is obtained by discretizing the time with the time between t and t +1
                    chosen equal to 0.100s.
                    Let us denote by O the set composed of the states x = ((2∗k+1)∗π,0,0,0) k ∈ Z and by d(x,O)
                    the value minkx−ok.
                              o∈O
                    If x   is such that d(x  ,O)<1thenaterminalstateis reached.
                        t+1               t+1                          4
                    State space: The state space is composed of {x ∈ R |d(x,O) ≥ 1} and of a terminal state.
                    Action space: The action spaceU ={−5,5}.
                                                                   551
                                                                                                                   ERNST, GEURTS AND WEHENKEL
                                            Rewardfunction: The reward function r(x,u) is deﬁned through the following expression:
                                                                                                                    (
                                                                                                                        0                                   if      d(x           ,O)≥1
                                                                                             r(x ,u ) =                                                                     t+1                                                                        (44)
                                                                                                    t     t             1−d(x                 ,O) if d(x                          ,O)<1.
                                                                                                                                        t+1                                 t+1
                                            Decayfactor: The decay factor γ has been chosen equal to 0.95.
                                            Integration: The dynamical system is integrated by using an Euler method with a 0.001s integra-
                                            tion time step.
                                            C.4 The“Bicycle Balancing” and “Bicycle Balancing and Riding” Control Problems
                                            Wedeﬁnehereafter the “Bicycle Balancing” and the “Bicycle Balancing and Riding” control prob-
                                            lems. These optimal control problems differ only by their reward functions.
                                            Systemdynamics: Thesystemstudied has the following dynamics:
                                                                                                                        ˙
                                                                                 ω              = ω +∆tω                                                                                                                                               (45)
                                                                                     t+1                    t             t
                                                                                  ˙                      ˙                            1
                                                                                 ω              = ω +∆t(                                               (Mhgsin(ϕ )−cos(ϕ )                                                                             (46)
                                                                                     t+1                    t            I                                                   t                  t
                                                                                                                           bicycle and cyclist
                                                                                                                ˙ ˙                          2
                                                                                                        (I     σθ +sign(θ )v (M r(invr +invr )+Mhinvr                                                              ))))
                                                                                                            dc       t                  t            d              ft             b                        CMt
                                                                                                                                                                                     t
                                                                                                        (                 ˙                                                         ˙           80
                                                                                                            θ +∆tθ                                           if     |θ +∆tθ |≤                       π
                                                                                  θ             =              t            t                                           t             t        180                                                     (47)
                                                                                     t+1                                            ˙      80                                       ˙           80
                                                                                                            sign(θ +∆tθ )                       π            if     |θ +∆tθ |>                       π
                                                                                                                         t            t   180                           t             t        180
                                                                                                        (                           ˙ ˙
                                                                                                                          T−I σω                                                   80
                                                                                                             ˙                   dv     t                             ˙
                                                                                                            θ +∆t                              if      |θ +∆tθ |≤                       π
                                                                                   ˙                           t                I                          t            t         180
                                                                                  θ             =                                dl                                                                                                                    (48)
                                                                                     t+1                                                                              ˙            80
                                                                                                            0                                  if      |θ +∆tθ |>                       π
                                                                                                                                                           t            t         180
                                                                                  x             = x +∆tvcos(ψ)                                                                                                                                         (49)
                                                                                    b                     b                           t
                                                                                      t+1                   t
                                                                                  y             = y +∆tvsin(ψ)                                                                                                                                         (50)
                                                                                    b                     b                          t
                                                                                      t+1                   t
                                                                                 ψ              = ψ +∆tsign(θ )vinvr                                                                                                                                   (51)
                                                                                     t+1                    t                       t            b
                                                                                                                                                   t
                                              with
                                                                                                                                              arctan(d +w )
                                                                                                                  ϕ        = ω +                               t        t                                                                              (52)
                                                                                                                     t                 t                    h
                                                                                                                                     |sin(θ )|
                                                                                                            invrf          =                    t                                                                                                      (53)
                                                                                                                     t                      l
                                                                                                                                     |tan(θ )|
                                                                                                            invr           =                     t                                                                                                     (54)
                                                                                                                   b
                                                                                                                     t                      l
                                                                                                                                    √                  1
                                                                                                                                                                              if      θ 6=0
                                                                                                                                             ((l−c)2+(          1    )2)                 t
                                                                                                                                                             invr
                                                                                                        invr               =                                      b                                                                                    (55)
                                                                                                                CM                                                 t
                                                                                                                     t
                                                                                                                                    
                                                                                                                                        0                                      otherwise
                                              where w is drawn according to a uniform distribution in the interval [−0.02,0.02]. The different
                                                                t                                                                                                                   10
                                            parameters are equal to the following values: ∆t = 0.01, v =                                                                                 , g = 9.82, d                      =0.3, c = 0.66,
                                                                                                                                                                                   3.6                              CM
                                                                                                                                                                                                           ˙         v
                                            h = 0.94, Mc = 15, M = 1.7, Mp = 60.0, M = (Mc+Mp), r = 0.34, σ = , I                                                                                                                                          =
                                               13            2                                d2                               2                     3          2                     1          2                   r      bicycle and cyclist
                                            (       Mh +M (h+d ) ),I =(M r ),I =( M r ),I =( M r )andl=1.11. Thisdynam-
                                                3       c                p              CM               dc               d            dv            2      d            dl           2      d
                                            ics holds valid if |ω                            | ≤ 12 π. When |ω                               | > 12 π, the bicycle is supposed to have fallen down
                                                                                       t+1            180                             t+1             180
                                            and a terminal state is reached.
                                                                                                                                                                            ˙          ˙                               7                  80          80
                                            Statespace: Thestatespaceforthiscontrolproblemis{(ω,ω,θ,θ,x ,y ,ψ)∈R |θ∈[−                                                                                                                         π,          π] and ω∈
                                                                                                                                                                                             b      b                                    180         180
                                                                                                                                                  552
                                               TREE-BASED BATCH MODE REINFORCEMENT LEARNING
                       [− 12 π, 12 π]} plus a terminal state.
                          180    180
                       Action space: The action spaceU ={(d,T)∈{−0.02,0,0.02}×{−2,0,2}}. U is composed of 9
                       elements.
                       Reward functions: The reward function for the “Bicycle Balancing” control problem is deﬁned
                       hereafter:
                                                                        (                         12
                                                                          −1       if  |ω    | >     π
                                                    r(x ,u ,w )    =                      t+1     180                          (56)
                                                        t  t   t          0        otherwise.
                       Therewardfunction for “Bicycle Balancing and Riding” control problem is:
                                                       (                                                           12
                                                         −1                                        if   |ω    | >    π
                                    r(x ,u ,w )   =                                                       t+1     180          (57)
                                       t  t   t          c      (d      (ψ )−d       (ψ     ))     otherwise
                                                           reward  angle  t      angle  t+1
                       where c        =0.1andd           : R →Rsuchthatd           (ψ)=min|ψ+2kπ|.
                                reward             angle                      angle        k∈Z
                       Decayfactor: The decay factor γ is equal to 0.98.
                       Remark: The bicycle dynamics is based on the one found in Randløv and Alstrøm (1998) and in
                                                                                          ∼
                       their corresponding simulator available at http://www.nbi.dk/ randlov/bike.html.
                       References
                       D. Bagnell, S. Kakade, A. Y. Ng, and J. Schneider. Policy search by dynamic programming. In
                          Proceedings of Neural Information Processing Systems, 2003.
                       L. C. Baird. Residual algorithms: reinforcement learning with function approximation. In Armand
                          Prieditis and Stuart Russell, editors, Machine Learning: Proceedings of the Twelfth International
                          Conference, pages 9–12, San Francisco, CA, July 1995. Morgan Kaufman.
                       R. Bellman. Dynamic Programming. Princeton University Press, 1957.
                       R. Bellman, R. Kalaba, and B. Kotkin. Polynomial approximation - a new computational technique
                          in dynamic programming: allocation processes. Mathematical Computation, 17:155–161, 1973.
                       J. A. Boyan. Technical update: least-squares temporal difference learning. Machine Learning, 49
                          (2-3):233–246, 2002.
                       J. A. Boyan and A. W. Moore. Generalization in reinforcement learning: safely approximating the
                          value function. Advances in Neural Information Processing Systems, 7:369–376, 1995.
                       L. Breiman. Bagging predictors. Machine Learning, 24(2):123–140, 1996.
                       L. Breiman. Some inﬁnity theory for predictor ensembles. Technical Report 577, University of
                          California, Department of Statistics, 2000.
                       L. Breiman. Random forests. Machine Learning, 45(1):5–32, 2001.
                       L. Breiman, J. H. Friedman, R. A. Olsen, and C. J. Stone. Classiﬁcation and Regression Trees.
                          WadsworthInternational (California), 1984.
                                                                            553
                        ERNST, GEURTS AND WEHENKEL
          D. Ernst. Near Optimal Closed-Loop Control. Application to Electric Power Systems. PhD thesis,
                  `
           University of Liege, Belgium, March 2003.
          D.Ernst, P. Geurts, and L. Wehenkel. Iteratively extending time horizon reinforcement learning. In
           N. Lavra, L. Gamberger, and L. Todorovski, editors, Proceedings of the 14th European Confer-
           enceonMachineLearning,pages96–107,Dubrovnik,Croatia,September2003.Springer-Verlag
           Heidelberg.
          D. Ernst, M. Glavic, P. Geurts, and L. Wehenkel. Approximate value iteration in the reinforce-
           ment learning context. Application to electrical power system control. To appear in Intelligent
           Automation and Soft Computing, 2005.
          P. Geurts, D. Ernst, and L. Wehenkel. Extremely randomized trees. Submitted, 2004.
          G. J. Gordon. Online ﬁtted reinforcement learning. In VFA workshop at ML-95, 1995a.
          G. J. Gordon. Stable function approximation in dynamic programming. In Proceedings of the
           Twelfth International Conference on Machine Learning, pages 261–268, San Francisco, CA,
           1995b. Morgan Kaufmann.
          G. J. Gordon. Approximate Solutions to Markov Decision Processes. PhD thesis, Carnegie Mellon
           University, June 1999.
             ´
          O. Hernandez-Lerma and B. Lasserre. Discrete-Time Markov Control Processes. Springer, New-
           York, 1996.
          L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: a survey. Journal of
           Artiﬁcial Intelligence Research, 4:237–285, 1996.
          S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Pro-
           ceedingsoftheNineteenthInternationalConferenceonMachineLearning,pages267–274,2002.
          M. G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Re-
           search, 4:1107–1149, 2003a.
          M.G.LagoudakisandR.Parr. Reinforcement learning as classiﬁcation: leveraging modern classi-
           ﬁers. In Proceedings of ICML 2003, pages 424–431, 2003b.
          J. Langford and B. Zadrozny. Reducing T-step reinforcement learning to classiﬁcation. Submitted,
           2004.
          L. J. Lin. Reinforcement Learning for Robots Using Neural Networks. PhD thesis, Carnegie Mellon
           University, Pittsburgh, USA, 1993.
          Y. Lin and Y. Jeon. Random forests and adaptive nearest neighbors. Technical Report 1005, De-
           partment of Statistics, University of Wisconsin, 2002.
          D. G. Luenberger. Optimization by Vector Space Methods. Wiley, N.Y., 1969.
          A.K.McCallum. ReinforcementLearningwithSelective Perception and Hidden State. PhD thesis,
           University of Rochester, Rochester, New-York, 1996.
                               554
                   TREE-BASED BATCH MODE REINFORCEMENT LEARNING
          A. W. Moore and C. G. Atkeson. Prioritized sweeping: reinforcement learning with less data and
           less real time. Machine Learning, 13:103–130, 1993.
          A. W. Moore and C. G. Atkeson. The parti-game algorithm for variable resolution reinforcement
           learning in multidimensional state-spaces. Machine Learning, 21(3):199–233, 1995.
          A. Y. Ng and M. Jordan. PEGASUS: a policy search method for large MDPs and POMDPs. In
           Proceedings of the Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence, pages 406–
           415, 1999.
          D. Ormoneit and P. Glynn. Kernel-based reinforcement learning in average-cost problems. IEEE
           Transactions on Automatic Control, 47(10):1624–1636, 2002.
          D. Ormoneit and S. Sen. Kernel-based reinforcement learning. Machine Learning, 49(2-3):161–
           178, 2002.
          J. Randløv and P. Alstrøm. Learning to drive a bicycle using reinforcement learning and shaping.
           In Proceedings of the Fifteenth International Conference on Machine Learning, pages 463–471,
           SanFrancisco, CA, USA, 1998. Morgan Kaufmann Publishers Inc.
          J. Rust. Using randomization to break the curse of dimensionality. Econometrica, 65(3):487–516,
           1997.
          S. P. Singh, T. Jaakkola, and M. I. Jordan. Reinforcement learning with soft state aggregation. In
           G. Tesauro, D. S. Touretzky, and T. Leen, editors, Advances in Neural Information Processing
           Systems : Proceedings of the 1994 Conference, pages 359–368, Cambridge, MA, 1995. MIT
           press.
          W. D. Smart and L. P. Kaelbling. Practical reinforcement learning in continuous spaces. In Pro-
           ceedings of the Sixteenth International Conference on Machine Learning, pages 903–910, 2000.
          M.W.Spong.SwingupcontroloftheAcrobot. In1994IEEEInternationalConferenceonRobotics
           andAutomation, pages 2356–2361, San Diego, CA, May 1994.
          R. S. Sutton. Learning to predict by the method of temporal differences. Machine Learning, 3(1):
           9–44, 1988.
          R. S. Sutton. Generalization in reinforcement learning: successful examples using sparse coarse
           coding. Advances in Neural Information Processing Systems, 8:1038–1044, 1996.
          R. S. Sutton and A. G. Barto. Reinforcement Learning, an Introduction. MIT Press, 1998.
          J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-learning. Machine Learning, 16(3):
           185–202, 1994.
          J. N. Tsitsiklis and B. Van Roy. Feature-based methods for large-scale dynamic programming.
           Machine Learning, 22:59–94, 1996.
          W.T.B.UtherandM.M.Veloso.Treebaseddiscretizationforcontinuousstatespacereinforcement
           learning. In Proceedings of AAAI-98, pages 769–774, 1998.
                               555
                        ERNST, GEURTS AND WEHENKEL
          X.WangandT.G.Diettrich. Efﬁcient value function approximation using regression trees. In Pro-
           ceedings of IJCAI-99 Workshop on Statistical Machine Learning for Large-Scale Optimization,
           Stockholm, Sweden, 1999.
          C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge University, Cam-
           bridge, England, 1989.
          J. Yoshimoto, S. Ishii, and M. Sato. Application of reinforcement learning to balancing Acrobot.
           In Proceedings of the 1999 IEEE International Conference on Systems, Man and Cybernetics,
           pages 516–521, 1999.
                               556
