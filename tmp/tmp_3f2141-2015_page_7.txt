                       Bigram ﬂipping  The source side is restricted to even-length sequences. The target is produced
                       by swapping, for all odd source sequence indices i ∈ [1,|seq|] ∧ odd(i), the ith symbol with the
                       (i + 1)th symbol. Sequences have the form:
                                          hsia a a a ...a   a |||a a a a ...a a  h/si
                                              1 2 3 4    k−1 k   2 1 4 3    k k−1
                       4.2 ITGTransductionTasks
                       Thefollowingtasks examine how well models can approach sequence transduction problems where
                       the source and target sequence are jointly generated by Inversion Transduction Grammars (ITG) [8],
                       a subclass of Synchronous Context-Free Grammars [17] often used in machine translation [18]. We
                       present two simple ITG-based datasets with interesting linguistic properties and their underlying
                       grammars. WeshowthesegrammarsinTable1,inAppendixCofthesupplementarymaterials. For
                       each synchronised non-terminal, an expansion is chosen according to the probability distribution
                       speciﬁed by the rule probability p at the beginning of each rule. For each grammar, ‘A’ is always the
                       root of the ITG tree.
                       Wetunedthegenerative probabilities for recursive rules by hand so that the grammars generate left
                       and right sequences of lengths 8 to 128 with relatively uniform distribution. We generate training
                       data by rejecting samples that are outside of the range [8,64], and testing data by rejecting samples
                       outside of the range [65,128]. For terminal symbol-generating rules, we balance the classes so
                       that for k terminal-generating symbols in the grammar, each terminal-generating non-terminal ‘X’
                       generates a vocabulary of approximately 128/k, and each each vocabulary word under that class is
                       equiprobable. These design choices were made to maximise the similarity between the experimental
                       settings of the ITG tasks described here and the synthetic tasks described above.
                       Subj–Verb–Obj to Subj–Obj–Verb  Apersistent challenge in machine translation is to learn to
                       faithfully reproduce high-level syntactic divergences between languages. For instance, when trans-
                       lating an English sentence with a non-ﬁnite verb into German, a transducer must locate and move
                       the verb over the object to the ﬁnal position. We simulate this phenomena with a synchronous
                       grammar which generates strings exhibiting verb movements. To add an extra challenge, we also
                       simulate simple relative clause embeddings to test the models’ ability to transduce in the presence
                       of unbounded recursive structures.
                       Asample output of the grammar is presented here, with spaces between words being included for
                       stylistic purposes, and where s, o, and v indicate subject, object, and verb terminals respectively, i
                       and o mark input and output, and rp indicates a relative pronoun:
                        si1 vi28 oi5 oi7 si15 rpi si19 vi16 oi10 oi24 ||| so1 oo5 oo7 so15 rpo so19 vo16 oo10 oo24 vo28
                       Genderless to gendered grammar  Wedesign a small grammar to simulate translations from a
                       language with gender-free articles to one with gender-speciﬁc deﬁnite and indeﬁnite articles. A
                       real world example of such a translation would be from English (the, a) to German (der/die/das,
                       ein/eine/ein).
                       The grammar simulates sentences in (NP/(V/NP)) or (NP/V) form, where every noun phrase
                       canbecomeaninﬁnitesequenceofnounsjoinedbyaconjunction. Eachnouninthesourcelanguage
                       has a neutral deﬁnite or indeﬁnite article. The matching word in the target language then needs to be
                       preceeded by its appropriate article. A sample output of the grammar is presented here, with spaces
                       between words being included for stylistic purposes:
                                      we11theen19andtheem17|||wg11dasgn19unddergm17
                       4.3 Evaluation
                       For each task, test data is generated through the same procedure as training data, with the key dif-
                       ference that the length of the source sequence is sampled from unif {65,128}. As a result of this
                       change, we not only are assured that the models cannot observe any test sequences during training,
                       but are also measuring how well the sequence transduction capabilities of the evaluated models gen-
                       eralise beyond the sequence lengths observed during training. To control for generalisation ability,
                       wealso report accuracy scores on sequences separately sampled from the training set, which given
                       the size of the sample space are unlikely to have ever been observed during actual model training.
                                                               7
