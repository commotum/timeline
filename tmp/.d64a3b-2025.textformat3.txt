            Published as a conference paper at ICLR 2025
            WAVELET-BASED POSITIONAL REPRESENTATION
            FOR LONG CONTEXT
             Yui Oka, Taku Hasegawa, Kyosuke Nishida, Kuniko Saito
             NTTHumanInformaticsLaboratories, NTT Corporation
             yui.oka@ntt.com
                               ABSTRACT
                In the realm of large-scale language models, a significant challenge arises when
                extrapolating sequences beyond the maximum allowable length. This is because
                the model’s position embedding mechanisms are limited to positions encountered
                during training, thus preventing effective representation of positions in longer se-
                quences. We analyzed conventional position encoding methods for long contexts
                andfoundthefollowingcharacteristics. (1) When the representation dimension is
                regarded as the time axis, Rotary Position Embedding (RoPE) can be interpreted
                as a restricted wavelet transform using Haar-like wavelets. However, because
                it uses only a fixed scale parameter, it does not fully exploit the advantages of
                wavelet transforms, which capture the fine movements of non-stationary signals
                using multiple scales (window sizes). This limitation could explain why RoPE
                performs poorly in extrapolation. (2) Previous research as well as our own anal-
                ysis indicates that Attention with Linear Biases (ALiBi) functions similarly to
                windowed attention, using windows of varying sizes. However, it has limitations
                in capturing deepdependenciesbecauseitrestrictsthereceptivefieldofthemodel.
                From these insights, we propose a new position representation method that cap-
                tures multiple scales (i.e., window sizes) by leveraging wavelet transforms with-
                out limiting the model’s attention field. Experimental results show that this new
                methodimprovestheperformanceofthemodelinbothshortandlongcontexts. In
                particular, our method allows extrapolation of position information without limit-
                ing the model’s attention field.
            1 INTRODUCTION
            Several pre-trained large language models based on Transformer architecture (Vaswani et al., 2017)
            have demonstrated robust capabilities in various generative tasks (Devlin et al., 2019; Raffel et al.,
            2020; Brown et al., 2020; Touvron et al., 2023a; Jiang et al., 2023). However, limitations on the
            input sequence length arise due to the computational resource constraints encountered during the
            pre-training phase. Such constraints necessitate a determination of the maximum allowable length
            of sequences, hereinafter Ltrain, prior to the pre-training process, thus hindering the model’s per-
            formance in processing sequences longer than those encountered during training. This weakness is
            primarily attributed to the positional encoding’s ineffectiveness in handling sequences that exceed
            the length of those encountered during the model’s training phase (Devlin et al., 2019; Press et al.,
            2022).
            Rotary Position Embedding (RoPE) (Su et al., 2021) has become a common approach in many
            language models that handle long contexts, and it employs a rotation matrix to encode positional
            information and facilitate the processing of long sequences. To manage sequences longer than those
            encountered during training, various scaling strategies (Chen et al., 2023; bloc97, 2023; Peng et al.,
            2024; Liu et al., 2024) have been applied to RoPE, although these often require additional fine-
            tuning and incur further learning costs in addition to those of pre-training. In contrast, Attention
            with Linear Biases (ALiBi) (Press et al., 2022) is able to sequence length estimation beyond the
            limits of pre-training without requiring additional fine-tuning. However, ALiBi limits the attention’s
            receptive field (Chi et al., 2023) in the manner of windowed attention (Beltagy et al., 2020). For this
            reason, a model using ALiBi may not be able to obtain information that is in a distant dependency
            relationship. In this paper, we analyze conventional positional encoding methods for long contexts,
                                  1
                           Published as a conference paper at ICLR 2025
                           Figure 1: Overview of Wavelet-based Relative Positional Representation As in RPE (Shaw et al.,
                           2018), our method computes a relative positional representation (p    )T to the query q   and the
                                                                                             m,n                  m
                           key kn. Instead of learnable embedding in RPE, the position is computed based on the wavelet
                           function. Different wavelet functions ψ   are used for each dimension of the head d. Furthermore,
                                                                  a,b
                           the scale parameter a and the shift parameter b change depending on the dimension of the head d.
                           andweproposeanovelpositionalrepresentationthatpermitsextrapolation without constraining the
                           attention mechanism’s receptive field. First, we mathematically show that RoPE performs a pro-
                           cess similar to a wavelet transformation—considered the gold standard of time-frequency analysis
                           methodology. We interpreted the position of each token in the sequence as a time point in time-
                           frequency analysis. However, RoPE does not perform a transformation in accordance with the order
                           of positions but rather in accordance with the number of dimensions, and it does not capture the
                           dynamic change in a signal over time. Furthermore, the values corresponding to the wavelet scale
                           (i.e., window size) are constant, so RoPE does not make good use of the key characteristic of wavelet
                           transforms, which is the ability to analyze signals on multiple scales. In other words, RoPE may fail
                           to capture the dynamic change in a signal over time, such as what occurs in natural language. In this
                           study, we also show that ALiBi provides different window sizes for each head.
                           Based on these insights, we propose a wavelet transform-based method, using multiple window
                           sizes, to offer a robust and flexible approach to positional encoding. By performing a wavelet trans-
                           form along the order of positions and introducing various scale parameters, our method can capture
                           the dynamic changes in a sequence over positions in the manner of the original feature of wavelet
                           transformation, i.e., time-frequency analysis. Following the methodology of Relative Position Rep-
                           resentation (RPE) (Shaw et al., 2018), we implement our method with relative ease.
                           From our experiments on extrapolation capabilities using the wikitext-103 dataset (Merity et al.,
                           2017), the results demonstrate that our method surpasses traditional positional encoding methods in
                           perplexity. We also report that our method has lower perplexity than RoPE in experiments with long
                           contexts using the Llama-2 model (Touvron et al., 2023b) and the CodeParrot dataset.
                           2    BACKGROUND
                           2.1   POSITIONAL REPRESENTATION
                           Within the Transformer architecture, positional encoding is employed to accurately represent the
                           sequential position of each token. Positional encoding can be divided into two main types: absolute
                           position, which expresses the position of a token from the static beginning of the sequence, and
                           relative position, which expresses the position of each token in relation to the other tokens within
                           the sequence. RoPE(Suetal.,2021),whichadoptsatypeofabsoluteposition,usesarotationmatrix
                           to compute the position and then multiplies it by the query and key to represent the position. RPE
                           (Shaw et al., 2018), based on a type of relative position, uses a learnable embedding that represents
                           the position of distances of up to 16 or 32 tokens by clipping. Two other variations include T5
                           Bias (Raffel et al., 2020), which has an enlarged RPE window size, and Transformer-XL (Dai et al.,
                           2019), which uses a sine wave for position representation instead of learnable embedding.
                           Position encoding plays a critical role in enabling models to effectively handle long context se-
                           quences, and it allows for extrapolation. Relative position is not a position expression that depends
                           on the length of the sequence, so it is effective in extrapolation. ALiBi (Press et al., 2022) is an
                                                                           2
                           Published as a conference paper at ICLR 2025
                           effective position representation method for extrapolation: It uses the relative position bias of all
                           tokens by adding a linear bias to each head’s attention score, rather than using position embed-
                           ding. However, ALiBi is unable to obtain information in a distant dependency relationship due to
                           its constraints on the self-attention mechanism’s receptive field(Chi et al., 2023). On the other hand,
                           absolute position is unsuitable for extrapolation because it expresses the position of all words in the
                           sequence. For this reason, many methods have been proposed for fine-tuning RoPE by interpolating
                           positions in using absolute position (Chen et al., 2023; bloc97, 2023; Peng et al., 2024).
                           2.2   FREQUENCY ANALYSIS AND TIME-FREQUENCY ANALYSIS
                           Frequency analysis in signal processing involves analyzing the frequency components of a signal
                           to understand its behavior. The Fourier transform (FT) (Bracewell & Bracewell, 1986) is a key
                           method for frequency analysis, converting a signal from the time domain to the frequency domain,
                           thus providing a global view of its frequency content. However, the FT does not provide any infor-
                           mation about when specific frequencies occur. To address this limitation, time-frequency analysis
                           techniques have been applied. The wavelet transform (WT) (Grossmann & Morlet, 1984; Mallat,
                           1989) offers a more flexible approach by analyzing the signal at multiple scales or resolutions. The
                           WTadaptively provides high time resolution for high-frequency components and high frequency
                           resolution for low-frequency components, making it well-suited for analyzing signals with non-
                           stationary or transient features. This adaptability allows the wavelet transform to capture both time
                           and frequency information with varying degrees of precision.
                           3    ROPEANDWAVELETTRANSFORM
                           3.1   PRELIMINARY
                           Wavelet Transform      Awaveletis a wave that decays quickly and locally as it approaches zero. A
                           function ψ defined on a real R is called a wavelet function if it belongs to the space L2(R) of square
                           integrable functions and satisfies the following conditions:
                                                                Z ∞           2
                                                                  −∞|ψ(x)| dx<∞.                                           (1)
                           Thewavelet function is defined as follows.                 
                                                                            1     t −b
                                                                ψa,b(t) = √ ψ       a   .                                  (2)
                                                                             a
                           In this case, b is the shift and a > 0 is the scale parameter. The scale parameter a simultaneously
                           changes the range over which the wavelet is localized as well as the wavelet’s amplitude. Typical
                           wavelets include the Haar wavelet (Haar, 1910), Ricker wavelet (Ricker, 1944), and Morlet wavelet
                           (Bernardino & Santos-Victor, 2005). Suppose that we sample T values at regular intervals from a
                           continuous signal. Wavelet transform (WT) (Grossmann & Morlet, 1984) is the process of trans-
                           forming a signal x(t) into the frequency domain and time domain by computing the inner product
                           of the wavelet function ψ   (t) and signal x(t).
                                                    a,b
                                                                          T−1
                                                               W(a,b) = Xψ (t)x(t).                                        (3)
                                                                                a,b
                                                                          t=0
                           In some cases, the term ”Discrete Wavelet Transform” or ”Wavelet Transform” is used to refer to
                           multi-resolution analysis (Mallat, 1989), but in this paper we follow the original definition. We can
                           see that the FT only converts to the frequency domain, whereas the WT converts to two domains:
                           scale a and shift b. For example, consider the case of a conversion to two scales and four shifts.
                           Whena∈[2,4]andb∈[0,1,2,3],thewavelettransformcanbeexpressedintermsofdeterminants
                           as follows:
                                                                                                             
                                      W(2,0)         ψ (0) ψ (1) ψ (2) ... ψ (T −1)
                                                       2,0       2,0       2,0            2,0              x(0)
                                                                                                 
                                      W(4,0)         ψ (0) ψ (1) ψ (2) ... ψ (T −1)
                                                4,0           4,0       4,0            4,0        x(1) 
                                      W(2,1)         ψ (0) ψ (1) ψ (2) ... ψ (T −1)
                                                2,1           2,1       2,1            2,1        x(2) 
                                              =                                                              .       (4)
                                      W(4,1)         ψ (0) ψ (1) ψ (2) ... ψ (T −1)
                                                4,1           4,1       4,1            4,1             .     
                                         .      .               .        .      .          .           .     
                                         .      .               .        .       ..        .            .
                                          .              .         .        .                 .         x(T −1)
                                      W(4,3)         ψ (0) ψ (1) ψ (2) ... ψ (T −1)
                                                       4,3       4,3       4,3            4,3
                                                                            3
                             Published as a conference paper at ICLR 2025
                             Furthermore, since ψ    (t) = ψ    (t−b)fromEq.2,ψofthewavelettransforminEq4isexpressed
                                                  a,b        a,0
                             as follows.
                                                                                                       
                                       W(2,0)          ψ (0)       ψ (1)       ψ (2)      ...  ψ (T −1)               
                                                         2,0        2,0         2,0             2,0              x(0)
                                       W(4,0)          ψ (0)       ψ (1)       ψ (2)      ...  ψ (T −1)
                                                4,0              4,0         4,0             4,0             x(1)
                                                                                                                  
                                       W(2,1)         ψ (−1)       ψ (0)       ψ (1)      ...  ψ (T −2)
                                              = 2,0              2,0         2,0             2,0        x(2) .            (5)
                                                                                                                  
                                       W(4,1)         ψ (−1)       ψ (0)       ψ (1)      ...  ψ (T −2)
                                                4,0              4,0         4,0             4,0              .   
                                          .     .                  .           .       .         .             .
                                           .               .          .           .       ..        .              .
                                           .               .          .           .                 .         x(T −1)
                                       W(4,3)         ψ (−3) ψ (−2) ψ (−1) ... ψ (T −3)
                                                        4,0         4,0        4,0              4,0
                             Due to the characteristics of the scale parameter a, the values of the wavelet matrix become 0 or
                             approach 0 outside a certain range that depends on the specific wavelet function.
                             RoPE RoPEincorporatespositional information directly into the self-attention mechanism by ro-
                             tating the query and key vectors in complex space. When divided into even and odd dimensions,
                             the following calculations are performed for the m-th query in each sequence. In even dimensions,
                             RoPEisexpressed as follows.
                               m                                                                                    qm 
                                q          cosmθ1     −sinmθ1         0          0       ...       0            0           0
                                  0                                                                                        qm
                              qm        0              0       cosmθ      −sinmθ      ...       0            0       1 
                               2                                      2            2                                 . 
                                  .    =       .          .           .          .       .         .            .         . .
                               .            .          .           .          .        ..       .            .       . 
                                  .            .          .           .          .                 .            .          qm
                                qm             0          0           0          0       ...  cosmθ        −sinmθ           d−2
                                 d−2                                                                 d/2            d/2    qm
                                                                                                                            d−1 (6)
                             where qm ∈ R1×d is the m-th query when the number of dimensions is d and θ                          =
                                                                                                                              i
                                   −2(i−1)/d
                             10000           , i ∈ [1,2,...,d/2]. For RoPE in odd dimensions, see Appendix A.1. The same
                             process is also performed for the n-th key kn ∈ R1×d.
                             3.2   THEORETICAL ANALYSIS
                             First, we show the wavelet transform using the following two Haar-like wavelets (Haar, 1910).
                                                     cosf(t)         0 ≤ t<1,            sinf(t)       0 ≤ t<1,
                                                                                         
                                              ψ(t) = −sinf(t)        1 ≤ t<2,    ψ′(t) = cosf(t)       1 ≤ t<2,                (7)
                                                                                         
                                                                                         
                                                        0            otherwise.              0         otherwise.
                             Here, f : R → R is a function that satisfies R∞ ψ(t)dt = 0 and Eq.(1). Assuming that when
                                                                              −∞
                             x(t)(0 ≤ t ≤ d − 1) is a signal with d elements, the wavelet ψ is used and wavelet transform is
                             performed at each scale a = 1. We define the shift parameter as b = j − δ(j)(j = 0,2,..,d − 2).
                                                                                                 j
                             Here, δ(t) is a function such that 0 ≤ t ≤ d − 1 and 0 ≤ δ(t) < 1. When the wavelet function is
                             Haar-like wavelet ψ(t) in Eq.(7) and a = 1 and b ∈ [b ,b ,..,d         ], the wavelet matrix ψ in the
                                                                                       0  2      d−2
                             wavelet transform w = ψx can be expressed in terms of determinants as follows.
                                                                                                                 x(0) 
                                 W(1,b )          cosϕ    −sinϕ         0        0       ...     0           0
                                        0              0         1                                                    x(1) 
                               W(1,b )         0           0      cosϕ     −sinϕ      ...     0           0     
                                       2                               2          3                                   .   
                                     .       =      .         .         .        .       .       .           .            .   .
                                    .       .              .         .         ..     .       .           .           .   
                                     .              .         .         .                .       .           .         x(d−2)
                                W(1,b     )         0         0         0        0       ... cosϕ       −sinϕ
                                      d−2                                                         d−2          d−1     x(d−1)
                                                                                                                                (8)
                             To simplify the notation in the matrix representation above, we write ϕj for j = 0,1,...,d − 1,
                             whereϕj = f(1+δ(j))ifj isodd,andϕj = f(δ(j))otherwise. Let x be the query qm, and define
                                                                                                                     −2(i−1)/d
                             f such that ϕ = ϕ         = mθ j+1 for j = 0,2,4,...,d − 2, where θ = 10000                        and
                                           j      j+1        ⌈    ⌉                                      i
                                                                2
                             i ∈ [1,2,...,d/2]. Under this definition, the transformation matrix of Eq. (8) becomes identical to
                             that of Eq. (6) in RoPE. 1 In other words, RoPE can be viewed as a wavelet transform using Haar-
                             like wavelets that change amplitude on a fixed scale. Furthermore, the same result as RoPE in odd
                                1The proof of the existence of f(t) that satisfies this condition is provided in Appendix A.2.
                                                                               4
                         Published as a conference paper at ICLR 2025
                         Figure 2: Heatmap of scaled attention scores via softmax normalization in ALiBi without non-
                         overlapping inference. The vertical axis represents the query, while the horizontal axis corresponds
                         to the key in the attention map. For clarity, values of 0.001 or more are mapped to black, while values
                         belowthat are mapped to yellow. The maximum allowable length of sequences is Ltrain = 512, and
                         the inference length is 1012.
                         dimensions can be obtained when using ψ′ for wavelet transformation. 2 This wavelet transform
                         in RoPE is performed across the number of query head dimensions d. Therefore, RoPE can be
                         considered a wavelet transformation along the head dimension using a wavelet with a fixed scale of
                         2.3
                         4   WINDOWSIZEVARIABILITY IN ALIBI
                         ALiBi has a restricted receptive field and behaves in the manner of windowed attention (Chi et al.,
                         2023; Beltagy et al., 2020). A receptive field refers to the specific region of the input space that
                         significantly influences the model’s output, typically representing the area where the most relevant
                         features are captured. ALiBi is expressed as
                                            softmax(q KT +slope·[−(m−1),...,−2,−1,0]),                         (9)
                                                      m
                         where the slope is a head-specific slope fixed before training and KT ∈ Rm×d is the first m keys.
                         In this section, we analyzed the window size in ALiBi using the attention map.
                         4.1  INSIGHTS FROM ATTENTION MAP ANALYSIS
                         Aheatmapofscaledattention scores obtained through softmax normalization is shown in Figure 2.
                         The number of heads N is 8, and the slope of ALiBi is [1, 1, 1, 1 , 1 , 1 , 1 , 1 ]. In extrapola-
                                                                            2 4 8 16 32 64 128 256
                         tion, sequences are often divided, but in this section the sequences are not divided. The experimental
                         setting was set to the same as that in Section 6.1. The perplexity results are shown in Table 1.
                         Theattention map shows that ALiBi uses multiple window sizes corresponding to relative positions
                         and that the window size increases as the slope decreases. Moreover, previous research (Chi et al.,
                         2023)showsthatconstrainingthewindowsize(slope)toasinglevalueleadstoincreasedperplexity.
                         Consequently, one of the reasons ALiBi is effective, compared to a previous relative position using
                         fixed window sizes in T5 Bias (Raffel et al., 2020), is its ability to accommodate multiple window
                         sizes. ALiBi does not perform calculations like those in Eq. (3), so it does not exactly match the
                         wavelet transform. However, having windows of various sizes is similar to the role of the scale
                         parameter used in wavelet transforms.
                         5   WAVELET-BASED POSITIONAL REPRESENTATION
                         Wavelet transform (WT) is a method of analyzing signals using variable-scale wavelets, and it is
                         possible to adjust the scale of the window. This scalability allows both broad and fine signal features
                         to be efficiently extracted by shifting the wavelet while changing the window size. In particular,
                         this is suitable for investigating non-stationary signals. For this reason, we believe that the wavelet
                            2Additionally, when sinmθ = cosmθ , the Haar wavelet matrix and RoPE are the same when the scale is
                                                  i        i
                         2, and the shift is [2,4,...,d/2]. Refer to Appendix A.3 for the detailed proof.
                            3Frompreviousresearch(Tanciketal.,2020),wealsohypothesizedthatthiscouldbeequivalenttoaFourier
                         transform. However, this hypothesis does not hold (refer to Appendix A.4 for details).
                                                                    5
                           Published as a conference paper at ICLR 2025
                           transform approach is effective for capturing the dynamic fluctuations of signals that change over
                           time, and it is also effective for the fluid nature of natural language, which is not constrained by
                           periodicity. Furthermore, whenextrapolating,itisimportanttobeabletorespondflexiblytochanges
                           in context and information. For this reason, we believe that the wavelet transform is also an effective
                           method for extrapolation.
                           When applying wavelet transforms to positional encoding, a key question arises: Which features
                           should be leveraged for handling long-context dependencies? Notably, RoPE shares conceptual
                           similarities with the wavelet transform (Section 3); however, RoPE depends on absolute positional
                           information, which limits its effective context window to the training length (Ltrain) and restricts
                           its extrapolation capabilities. In contrast, ALiBi offers extrapolation capabilities by using relative
                           position, and it supports varying window sizes (Section 4). However, ALiBi’s linear bias constrains
                           its receptive field, making it insufficient for capturing long-range dependencies. According to Press
                           et al. (2022), conventional relative positional encoding (RPE) methods (Shaw et al., 2018; Raf-
                           fel et al., 2020), which rely on a fixed window size, are similarly ineffective for extrapolation. In
                           conclusion, we adopt relative position with flexible window sizes to handle long-context and extrap-
                           olation.
                           Accordingly, we propose positional representation based on wavelet transform with the following
                           characteristics:
                                 1. Position-based Transformation: RoPE predominantly relies on independent transforma-
                                    tion based on the ’head’ dimensions. ALiBi employs multiple windows based on the rela-
                                    tive position of the sentence, rather than the dimension of the head, which may contribute
                                    to its performance. Therefore, we apply a wavelet transform based on the relative position
                                    of the sentence.
                                 2. Type of Wavelet: RoPE can be thought of as a wavelet transform using the Haar wavelet,
                                    which is the simplest wavelet. However, Haar wavelets might fall short in capturing the
                                    intricacies of natural languages. Transitioning toward the use of more sophisticated wavelet
                                    functions could enhance our approach to distilling and representing a broader spectrum of
                                    features inherent in natural languages.
                                 3. Diversification of Window Sizes (Scale Parameters): From our analysis of ALiBi, we
                                    found that having multiple windows is effective for long contexts. The original version of
                                    RoPEworkswithasinglefixed scale. To address this limitation, we introduce a variety of
                                    scale and shift parameters.
                           5.1   METHODOLOGY
                           Incorporating Wavelet Transform into PE        Due to the wavelet shift feature, we adopt relative
                           positionrepresentationusingALiBibecauseitismoresuitablethanabsolutepositionrepresentation.
                           4 In a transformer model (Vaswani et al., 2017), the self-attention mechanism operates by projecting
                           theinputsequenceintothreedistinctrepresentations—queries(Q),keys(K),andvalues(V)—using
                           learnable weight matrices. Self-attention sublayers employ N attention heads. In self-attention
                           sublayers, em,n is the attention score for each query, and then the key is calculated. RPE(Shaw
                           et al., 2018) expresses position by calculating the inner product of the query and the relative position
                           embedding. We incorporate the wavelet function into RPE as follows.
                                                                      q kT +q (p       )T
                                                                       m n      m m,n
                                                             em,n =           √d          ,                              (10)
                           where q   is the mth query (q   ∈R1×d,1 ≤ m ≤ L)ofasentence of length L, k is the nth key
                                   m                     m                                                   n
                           (k ∈ R1×d,1 ≤ n ≤ L) for q , and d is the number of dimensions of each head. Here, p            is
                             n                             m                                                           m,n
                           the relative position from the m-th query to the n-th key. RPE (Shaw et al., 2018) uses learnable
                           embedding for p       ∈ Rd and a fixed scale by clipping. However, instead of using learnable
                                            m,n
                           embeddings to represent p     , we use d-pattern wavelet functions with multiple scales to calculate
                                                     m,n
                           the position. In our method, there is no clipping, and the distance of the position expression is fixed
                           regardless of the length of the sentence.
                              4We also considered incorporating wavelet transforms into RoPE, but decided not to do this because it
                           would make the computational cost even higher. A discussion on this is included in Appendix A.5.
                                                                           6
                             Published as a conference paper at ICLR 2025
                             Wavelet Function     In conventional wavelets, such as in Eq. (2), the amplitude also varies depend-
                             ing on the scale parameter a. In the proposed method, all amplitudes are the same.
                                                                                 t−b
                                                                     ψa,b(t) = ψ     a    .                                    (11)
                             The variable t is assigned the relative position, which is t = m − n. We used the Ricker wavelet
                             (Ricker, 1944) as a base wavelet, which is formulated as follows.
                                                                                2     −t2
                                                                 ψ(t) = (1−t )exp         2   .                                (12)
                             Shift and scale parameters      Weuse s distinct patterns for the scale parameter a and d patterns
                             for the shift parameter b.                                                                  s
                                                                0  1   2     s−1                    d
                                                    (a,b) ∈ {2 ,2 ,2 ,...2      }×{0,1,2,3,..., s −1}.                         (13)
                             The scale parameter is a power of 2 derived from the principles of the discrete wavelet transform.
                             By combining the d-pattern shift parameters b with the s-pattern scale parameters a, we generate
                                                 s
                             d distinct wavelets. In this way, our method can set the s-pattern context window size using the
                             scale parameter a and the d-pattern context window using both the scale parameter a and the shift
                             parameter b. For instance, with a head dimension of d = 128, we use s = 8 scale variants (a ∈
                               0   1      7
                             {2 ,2 ,...,2 }) and 16 shift variants (b ∈ {0,1,2,...,15}), resulting in 8 × 16 = 128 unique
                             wavelets. Finally, p    is computed as follows.5
                                                 m,n
                                                                                                    
                                                                  m−n−b 2 exp −1 m−n−b 2 .                                     (14)
                                                  p     = 1−
                                                   m,n                 a                  2       a
                             6   SHORT-CONTEXT EXPERIMENT
                             6.1   EXPERIMENTAL SETTINGS
                             First, we conducted a small-scale experiment to compare our approach with various position encod-
                             ings. We used the WikiText-103 dataset (Merity et al., 2017), which consists of over 103 million
                             tokens of English Wikipedia articles. We performed a comparative evaluation using a Transformer-
                             based language model (Baevski & Auli, 2019). The dimensionality of the word embedding d
                                                                                                                              model
                             is 1024, the number of heads N is 8, the dimensionality of the heads d is 128, and the number
                             of layers is 16. The implementation was based on the fairseq (Ott et al., 2019)-based code6 pro-
                             vided in a previous work(Press et al., 2022), and all hyperparameters were set to the same values as
                                                                      7
                             those in the literature(Press et al., 2022). The maximum allowable lengths of sequences were set to
                             Ltrain = 512 and Ltrain = 1024.
                             Compared Methods Although θ = 10,000 is usually used for RoPE, it has been found that ex-
                             tending θ to 500,000 is effective for long contexts (Xiong et al., 2024). Therefore, we compared
                             θ = 10,000 with θ = 500,000. In addition to ALiBi and RoPE, the following position representa-
                             tions were also compared: NoPE (Kazemnejad et al., 2023), in which position information is given,
                             and TransXL (Dai et al., 2019), which is a relative positional representation that uses sine waves.
                             Evaluation Metric      We use perplexity as our evaluation metric. Following previous research
                             (Press et al., 2022), we evaluated the validation set. To evaluate sequences longer than L     tokens,
                                                                                                                       train
                             it is common to divide the sequence into Ltrain-length sub-sequences, evaluate each independently,
                             and report the average score. However, methods that use relative positions to express a wide range,
                             such as ALiBi, Trans-Xl, and the proposed method, are able to consider a wider range of contexts
                             than Ltrain. For this reason, in this paper, we report not only the perplexity of non-overlapping in-
                             ference but also the normal perplexity when the sequence is not divided into partial sequences. Note
                                5Implementation tips for reducing the memory and computational efficiency of the proposed method are
                             included in Appendix A.6.
                                6https://github.com/ofirpress/attention_with_linear_biases
                                7See Appendix A.7 for more details of hyperparameters.
                                                                               7
                                    Published as a conference paper at ICLR 2025
                                    Table 1: Perplexity of validation set in extrapolation experiments using Wikitext-103. Maximum
                                    allowable lengths of sequences in pre-training are L                        =512andL               =1024.
                                                                                                         train                   train
                                                                                                                 Sequence Length
                                                                                               L       =512                                   L       =1024
                                                                                                 train                                          train
                                                                    pos     128       256      512     1012      1512      2512      1024     1524     3024      5024
                                                                              Perplexity in Non-overlapping Inference with Ltrain
                                      NoPE(Kazemnejadetal.,2023)      -     26.38    23.23    21.53    21.52     21.53     21.53     20.81    21.52    21.49     21.45
                                      RoPE(Suetal.,2021)             abs    23.82    20.98    19.39    19.35     19.39     19.38     18.42    19.51    19.52     19.48
                                      RoPE(Xiongetal.,2024)          abs    23.81    20.95    19.35    19.32     19.35     19.33     18.50    19.53    19.54     19.50
                                      Trans-XL (Daietal., 2019)      rel    24.16    21.53    19.96    19.92     19.93     19.96     18.67    19.75    19.74     19.70
                                      ALiBi(Press et al., 2022)      rel    24.18    21.32    19.69    19.64     19.69     19.64     18.66    19.64    19.65     19.62
                                      Wavelet(Ricker)                rel    23.64    20.82    19.19    19.15     19.17     19.20     18.26    19.30    19.34     19.26
                                                                                 Perplexity without Non-overlapping Inference
                                      NoPE(Kazemnejadetal.,2023)      -     26.38    23.23    21.53    21.03     21.58     48.48     20.81    20.45    22.11     59.37
                                      RoPE(Suetal.,2021)             abs    23.82    20.98    19.39    23.25     44.38     93.94     18.42    18.29    33.20    122.52
                                      RoPE(Xiongetal.,2024)          abs    23.81    20.95    19.35    23.70     40.39     77.90     18.50    18.30    29.25     83.43
                                      Trans-XL(Daietal., 2019)       rel    24.16    21.53    19.96    19.09     18.92     19.05     18.67    18.25    18.17     18.76
                                      ALiBi(Press et al., 2022)      rel    24.18    21.32    19.69    18.71     18.42     18.41     18.66    18.14    17.86     17.88
                                      Wavelet(Ricker)                rel    23.64    20.82    19.19    18.23     18.00     17.99     18.26    17.13    17.14     17.44
                                      Haar (Fixed scale)             rel    24.98    22.07    20.49    51.61    116.87     299.26      -        -        -         -
                                      Haar                           rel    23.73    20.89    19.27    18.34     18.11     18.17       -        -        -         -
                                      Morlet                         rel    24.15    21.28    19.65    19.02     20.46     26.56       -        -        -         -
                                      Gaussian                       rel    23.77    20.90    19.30    18.31     18.02     17.88       -        -        -         -
                                    that when the sequence length is less than L                      , the scores for the perplexity of non-overlapping
                                                                                                train
                                    inference and the normal perplexity without division into partial sequences are the same. Of course,
                                    when perplexity is considered without division into partial sequences, the performance of RoPE is
                                    expectedtodecreasegreatlybecauseunknownvaluesareusedforRoPEwhenprocessingasequence
                                    longer than the length encountered during training.
                                    6.2     MAINRESULTS
                                    TheexperimentalresultsareshowninTable1. Theresultsofperplexityininferencewithoutoverlap
                                    showthattheproposedmethodusingwaveletsachievedthelowestperplexityandwasalsoeffective
                                    for extrapolation. In RoPE, the values used during training are also used in inference without over-
                                    lap, so the perplexity remains low even when the sequence length exceeds Ltrain. At the same time,
                                    however, perplexity is higher for ALiBi and Trans-XL than for RoPE, which is attributed to the lim-
                                    ited context range of the position representation’s applicability due to the division of the sequence
                                    into sub-sequences. In contrast, the proposed method maintains low perplexity even in the case of
                                    division into sub-sequences, suggesting that the wavelet position representation is highly effective.
                                    Ontheother hand, perplexity without non-overlapping inference showed the opposite results. First,
                                    sinceRoPEusesabsolutepositions,itisnecessarytousenewvaluesforunknownpositions,andthus
                                    perplexity increased significantly. However, in the case of θ = 500,000, the increase in perplexity
                                    was relatively small. On the contrary, Trans-XL and ALiBi, which use relative positions, were
                                    able to handle longer contexts, and perplexity decreased as the range of position representations
                                    expanded. In the proposed method, perplexity also decreased and the best score was achieved.
                                    Trans-XL uses a position representation based on a periodic sine wave function, but the proposed
                                    method, which uses wavelets, could further decrease perplexity. This result supports our claim
                                    (section 5) that an approach like wavelet transformation is more effective than periodic functions in
                                    capturing the fluid nature of natural language, which is not constrained by periodicity.
                                    6.3     ANALYSIS
                                    6.3.1      HOWEFFECTIVE ARE THE OTHER WAVELET TYPES?
                                    We also conducted experiments to see whether the same effect could be obtained with other
                                    wavelets. The wavelets tested were the Gaussian-based wavelet ψ(t) = exp(−t2), the Morlet-
                                                                            2
                                    based waveletψ(t) = exp(−t )cos(at), and the Haar-based wavelet. Note that when ψ(t/a) exists
                                    in our Morlet wavelet, the frequency of this cosine wavelet is not affected by the scale parameter a.
                                                                                                     8
                        Published as a conference paper at ICLR 2025
                        Figure 3: Heatmap of scaled attention scores via softmax normalization in 4th head after softmax
                        operation without non-overlapping inference. The vertical axis represents the query, while the hor-
                        izontal axis corresponds to the key. For clarity, values of 0.001 or more are mapped to black, while
                        valuesbelowthataremappedtoyellow. Themaximumallowablelengthofsequencesinpre-training
                        is L    =512andtheinferencelength is 1012. See Appendix A.12 for other heads.
                           train
                        Weusedthefollowing formula for the Haar wavelet.
                                                           
                                                             1        −0.5 ≤ t<0,
                                                    ψ(t) = −1 −1≤t<−0.5,                                 (15)
                                                           
                                                             0          otherwise.
                        Wekepttheshift and scale parameters constant, only changing the wavelet function. We also tested
                                                        0  0  0   0
                        the Haar wavelet when set to a ∈ {2 ,2 ,2 ,...2 }. Consequently, this restricted Haar wavelet had
                        the same scale parameter setting as the RoPE demonstrated in Section 3.2. 8 The graphs of these
                        wavelet functions are shown in Appendix A.10 (Fig. 6). Extrapolation experiments were conducted
                        underthesameconditionsastheexperimentalsetupinSection6,withLtrain = 512duringtraining.
                        As shown in Table 1, the Ricker-, Haar- and Gaussian-based wavelets had lower perplexity than
                        the Morlet wavelet. One possibility is that complex wavelets with multiplied cosine waves, such as
                        Morlet wavelets, are not suitable for relative positional representation. On the other hand, wavelets
                        with all positive values, such as Gaussian-based wavelets, are expected to represent positions within
                        a narrower distance than the window specified by the scale parameter due to softmax normaliza-
                        tion. This suggests that wavelets with a specific range of negative values are suitable, like a Ricker
                        wavelet, for positional representation. Although the Haar wavelet is simple, it is such a wavelet with
                        negative values within a specific range. Therefore, it is considered effective, although not as much
                                                                                          0     0
                        as a Ricker wavelet. However, when the scale parameter is restricted ( a ∈ {2 ,...,2 }), as in RoPE,
                        the perplexity increases. This demonstrates the importance of having multiple scales, or in this case,
                        window sizes. We also performed ablation studies for each shift and scale parameter (Appendix
                        A.13) and for discrete wavelets as well as continuous wavelets (Appendix A.14).
                        6.3.2  CANITHANDLETOKENSWITHLONG-RANGEDEPENDENCIES?
                        Figure 3 shows the attention map of scaled attention scores obtained through softmax normalization
                        for the proposed method. The inference length is L = 1012 without non-overlapping inference. The
                        mostnotablefeatureoftheproposedmethodisthatitisalwaysabletoattendtospecifictokens. The
                        words that always receive attention are those that are important in the sentence, such as the special
                        token, the first token, and the subject of the sequence. On the other hand, ALiBi has a restricted
                        receptive field for attention, making it unable to capture long-distance dependencies. Similar to the
                        proposed method, RoPE emphasizes important and special words but struggles to capture those that
                        are farther apart. Moreover, as the sentence lengthens, it loses the ability to attend to the initial word.
                        This tendency was also seen in sentences shorter than Ltrain. Accordingly, the proposed method has
                        demonstrated its superiority at capturing long dependencies without restricting the receptive field of
                        attention.
                           8Normally, the wave is localized when t > 0 in the Haar wavelet, but in the decoder model, only the range
                        t < 0 is used. Therefore, we transformed the Haar wavelet into a form that reflects the original function f(x)
                        across the y-axis.
                                                                  9
                                      Published as a conference paper at ICLR 2025
                                                         Table 2: Perplexity in Non-overlapping Inference with Ltrain = 4096.
                                                                                                               Sequence Length
                                                                                                         4 k      8 k    16 k     32 k
                                                                              RoPE(Xiongetal.,2024)      9.45    9.33    9.12     8.90
                                                                              Wavelet                    9.00    9.01    8.83     8.60
                                      7     LONGCONTEXT
                                      7.1     EXPERIMENTAL SETTINGS
                                      Next, we conducted a large-scale experiment using a Llama-based model (Touvron et al., 2023b).
                                                                                  9
                                      We pre-trained the Llama-2-7B model from scratch. For pre-training, we used the RedPajama
                                      dataset (Computer, 2023), which selects a 1B-token sample of all samples. The maximum allowable
                                      lengthofsequencesinpre-trainingwassettoLtrain = 4096. ForthesamereasonasgiveninSection
                                                                                                                                                            0    1        7
                                      6.1, we set θ = 500,000 for RoPE. Furthermore, when the scale parameter is a ∈ {2 ,2 ,...,2 },
                                      the range within which the wavelet is localized becomes narrow. Therefore, in our method, we
                                                                                          2    3        9
                                      changedthescaleparametertoa ∈ {2 ,2 ,...,2 }. Theotherparametersarethesameasthoseused
                                      for the Llama-2-7B model(Touvron et al., 2023b). We used CodeParrot 10 for evaluation, which is
                                      goodforlong-distancetesting because it requires an understanding of patterns and contextualization
                                      of information over long distances. 11
                                      7.2     MAINRESULTS
                                      Theexperimental results are shown in Table 2. Regardless of whether interpolation or extrapolation
                                      was applied, the perplexity of our method was lower than RoPE. Therefore, even with large-scale
                                      models and long contexts, our method was found to be effective. Moreover, the results in Section
                                      6.2 show that not dividing the sequence further reduces perplexity. Therefore, our method might
                                      also be able to further reduce perplexity. We investigated the use of LongBench(Bai et al., 2024),
                                      with the results given in Appendix A.15.
                                      In addition, position interpolation methods (Chen et al., 2023; bloc97, 2023; Peng et al., 2024; Ding
                                      et al., 2024) have been proposed to adapt RoPE for longer contexts. We believe these methods can be
                                      integrated into our approach for the following reasons. First, the parameter θ in RoPE corresponds
                                      to the scale parameter a in our method, implying compatibility between the two frameworks. Both
                                      θ and a refer to the upper limit of the number of positions to be expressed. Second, the LongRoPE
                                      paper (Ding et al., 2024) reveals that performance improves when extrapolation is avoided for the
                                      initial positions, which likely aligns with the shift parameter b in our method. Thus, it is highly
                                      likely that existing position interpolation methods will integrate seamlessly with our approach.
                                      8     CONCLUSION
                                      In this paper, we demonstrated that RoPE can be interpreted as a wavelet transform, and we intro-
                                      duced a novel positional representation method that leverages the wavelet transform’s advantages,
                                      effectively capturing positional information across various window sizes. Our experimental results
                                      demonstrate the proposed method’s superior performance in extrapolation tasks when compared to
                                      traditional positional representation techniques. Importantly, our approach offers the advantage of
                                      not constraining the receptive field, which allows more flexible and comprehensive analysis of po-
                                      sitions. Calculating relative positions is known to require more resources than calculating absolute
                                      positions, so we show methods for reducing memory consumption in Appendix A.6. However, the
                                      computational overhead of calculating relative positions may still impose a bottleneck, and thus
                                      reducing it is an important direction for future work.
                                          9https://huggingface.co/meta-llama/Llama-2-7b
                                         10https://huggingface.co/datasets/codeparrot/codeparrot-clean
                                         11See Appendix A.8 for more details of the hyperparameters.
                                                                                                        10
                                 Published as a conference paper at ICLR 2025
                                 REFERENCES
                                 Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In
                                    International Conference on Learning Representations, 2019. URL https://openreview.
                                    net/forum?id=ByxZX20qFQ.
                                 Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,
                                    Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: A bilin-
                                    gual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and
                                    Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Com-
                                    putational Linguistics (Volume 1: Long Papers), pp. 3119–3137, Bangkok, Thailand, August
                                    2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.172. URL
                                    https://aclanthology.org/2024.acl-long.172.
                                 Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
                                    arXiv:2004.05150, 2020.
                                                                      ´
                                 Alexandre Bernardino and Jose Santos-Victor. A real-time gabor primal sketch for visual attention.
                                                                      ´     ´
                                    In Jorge S. Marques, Nicolas Perez de la Blanca, and Pedro Pina (eds.), Pattern Recognition
                                    and Image Analysis, pp. 335–342, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg. ISBN
                                    978-3-540-32237-5.
                                 bloc97.         Ntk-aware scaled rope allows llama models to have extended (8k+) con-
                                    text size without any fine-tuning and minimal perplexity degradation., 2023.                                   URL
                                    https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_
                                    scaled_rope_allows_llama_models_to_have/.
                                 Ronald Newbold Bracewell and Ronald N Bracewell. The Fourier transform and its applications,
                                    volume 31999. McGraw-Hill New York, 1986.
                                 Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
                                    wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
                                    wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
                                    Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz
                                    Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
                                    Radford, Ilya Sutskever, and Dario Amodei.                  Language models are few-shot learners.                In
                                    H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-
                                    ral Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc.,
                                    2020.      URL https://proceedings.neurips.cc/paper_files/paper/2020/
                                    file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
                                 Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window
                                    of large language models via positional interpolation, 2023. URL https://arxiv.org/
                                    abs/2306.15595.
                                 Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky, and Peter Ramadge. Dissecting transformer
                                    length extrapolation via the lens of receptive field analysis. In Proceedings of the 61st Annual
                                    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13522–
                                    13537, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/
                                    v1/2023.acl-long.756. URL https://aclanthology.org/2023.acl-long.756.
                                 Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.
                                    URLhttps://github.com/togethercomputer/RedPajama-Data.
                                 Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
                                    Transformer-XL: Attentive language models beyond a fixed-length context. In Anna Korhonen,
                                                               ´     `
                                    David Traum, and Lluıs Marquez (eds.), Proceedings of the 57th Annual Meeting of the Associ-
                                    ation for Computational Linguistics, pp. 2978–2988, Florence, Italy, July 2019. Association for
                                    Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.
                                    org/P19-1285.
                                 Ingrid Daubechies. Ten lectures on wavelets. Society for industrial and applied mathematics, 1992.
                                                                                           11
                        Published as a conference paper at ICLR 2025
                        Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.   BERT: Pre-training of
                           deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and
                           Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of
                           the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long
                           and Short Papers), pp. 4171–4186, Minneapolis, Minnesota, June 2019. Association for Com-
                           putational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/
                           N19-1423.
                        Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan
                           Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens, 2024.
                           URLhttps://arxiv.org/abs/2402.13753.
                        W. M. Gentleman and G. Sande. Fast fourier transforms: for fun and profit. In Proceedings of
                           the November 7-10, 1966, Fall Joint Computer Conference, AFIPS ’66 (Fall), pp. 563–578, New
                           York, NY, USA, 1966. Association for Computing Machinery. ISBN 9781450378932. doi: 10.
                           1145/1464291.1464352. URL https://doi.org/10.1145/1464291.1464352.
                        A. Grossmann and J. Morlet. Decomposition of hardy functions into square integrable wavelets of
                           constant shape. SIAM Journal on Mathematical Analysis, 15(4):723–736, 1984. doi: 10.1137/
                           0515056. URLhttps://doi.org/10.1137/0515056.
                        A. Haar. Zur theorie der orthogonalen funktionensysteme. (erste mitteilung). Mathematische An-
                           nalen, 69:331–371, 1910. URL http://eudml.org/doc/158469.
                        Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chap-
                           lot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,
                            ´
                           Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,
                                                ´
                           Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https:
                           //arxiv.org/abs/2310.06825.
                        Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva
                           Reddy. The impact of positional encoding on length generalization in transformers. In A. Oh,
                           T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neu-
                           ral Information Processing Systems, volume 36, pp. 24892–24928. Curran Associates, Inc.,
                           2023.   URL https://proceedings.neurips.cc/paper_files/paper/2023/
                           file/4e85362c02172c0c6567ce593122d31c-Paper-Conference.pdf.
                        Gregory R. Lee, Ralf Gommers, Filip Waselewski, Kai Wohlfahrt, and Aaron O8217;Leary. Py-
                           wavelets: A python package for wavelet analysis. Journal of Open Source Software, 4(36):1237,
                           2019. doi: 10.21105/joss.01237. URL https://doi.org/10.21105/joss.01237.
                        Yang Li, Si Si, Gang Li, Cho-Jui Hsieh, and Samy Bengio. Learnable fourier features for multi-
                           dimensionalspatialpositionalencoding. InA.Beygelzimer,Y.Dauphin,P.Liang,andJ.Wortman
                           Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://
                           openreview.net/forum?id=R0h3NUMao_U.
                        Xiaoran Liu, Hang Yan, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of roPE-based
                           extrapolation. In The Twelfth International Conference on Learning Representations, 2024. URL
                           https://openreview.net/forum?id=JO7k0SJ5V6.
                        Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
                           ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
                           Bkg6RiCqY7.
                        S.G. Mallat. A theory for multiresolution signal decomposition: the wavelet representation. IEEE
                           Transactions on Pattern Analysis and Machine Intelligence, 11(7):674–693, 1989. doi: 10.1109/
                           34.192463.
                        Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.   Pointer sentinel mix-
                           ture models. In International Conference on Learning Representations, 2017. URL https:
                           //openreview.net/forum?id=Byj72udxe.
                                                                   12
                          Published as a conference paper at ICLR 2025
                          NhatKhangNgo,TruongSonHy,andRisiKondor. Multiresolutiongraphtransformersandwavelet
                            positional encoding for learning long-range and hierarchical structures. The Journal of Chemical
                            Physics, 159(3):034109, 07 2023a. ISSN 0021-9606. doi: 10.1063/5.0152833. URL https:
                            //doi.org/10.1063/5.0152833.
                          NhatKhangNgo,TruongSonHy,andRisiKondor. Multiresolutiongraphtransformersandwavelet
                            positional encoding for learning hierarchical structures. arXiv preprint arXiv:2302.08647, 2023b.
                          Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
                            and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of
                            NAACL-HLT2019: Demonstrations, 2019.
                          Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context win-
                            dow extension of large language models. In The Twelfth International Conference on Learning
                            Representations, 2024. URL https://openreview.net/forum?id=wHBfxhZu1u.
                          Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables
                            input length extrapolation. In International Conference on Learning Representations, 2022. URL
                            https://openreview.net/forum?id=R8sQPpGCv0.
                          ColinRaffel, NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
                            Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-
                            text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http:
                            //jmlr.org/papers/v21/20-074.html.
                          NormanRicker.Waveletfunctionsandtheirpolynomials.Geophysics,9(3):314–323,071944.ISSN
                            0016-8033. doi: 10.1190/1.1445082. URL https://doi.org/10.1190/1.1445082.
                          OhadRubinandJonathanBerant. Retrieval-pretrained transformer: Long-range language modeling
                            with self-retrieval, 2024. URL https://arxiv.org/abs/2306.13421.
                          Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position represen-
                            tations. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Con-
                            ference of the North American Chapter of the Association for Computational Linguistics: Hu-
                            man Language Technologies, Volume 2 (Short Papers), pp. 464–468, New Orleans, Louisiana,
                            June 2018. Association for Computational Linguistics.   doi: 10.18653/v1/N18-2074.    URL
                            https://aclanthology.org/N18-2074.
                          Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer
                            with rotary position embedding, 2021.
                          Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaud-
                            hary, Xia Song, and Furu Wei. A length-extrapolatable transformer. In Anna Rogers, Jordan
                            Boyd-Graber,andNaoakiOkazaki(eds.),Proceedingsofthe61stAnnualMeetingoftheAssocia-
                            tion for Computational Linguistics (Volume 1: Long Papers), pp. 14590–14604, Toronto, Canada,
                            July2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.acl-long.816. URL
                            https://aclanthology.org/2023.acl-long.816.
                          Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan,
                            Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let net-
                            works learn high frequency functions in low dimensional domains. In Proceedings of the 34th
                            International Conference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY,
                            USA,2020.CurranAssociates Inc. ISBN 9781713829546.
                          Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
                            Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient
                            transformers. In International Conference on Learning Representations, 2021. URL https:
                            //openreview.net/forum?id=qVyeW-grC2k.
                          R. Tian, Z. Wu, Q. Dai, H. Hu, Y. Qiao, and Y. Jiang.    Resformer: Scaling vits with multi-
                            resolution training. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition
                            (CVPR), pp. 22721–22731, Los Alamitos, CA, USA, jun 2023. IEEE Computer Society. doi:
                            10.1109/CVPR52729.2023.02176. URL https://doi.ieeecomputersociety.org/
                            10.1109/CVPR52729.2023.02176.
                                                                       13
                         Published as a conference paper at ICLR 2025
                                                                                                                   ´
                         Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee
                                                 `
                            Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-
                            mandJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation lan-
                            guage models. ArXiv, abs/2302.13971, 2023a. URL https://api.semanticscholar.
                            org/CorpusID:257219404.
                         HugoTouvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
                            lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
                            Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
                            Fu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,
                            Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
                            Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,
                            Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
                            Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
                            Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
                            Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,Yuchen
                            Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
                            Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
                            2023b. URLhttps://arxiv.org/abs/2307.09288.
                         Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
                            Ł ukasz Kaiser, and Illia Polosukhin.  Attention is all you need.   In I. Guyon, U. Von
                            Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
                            vances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,
                            2017.   URL https://proceedings.neurips.cc/paper_files/paper/2017/
                            file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
                         Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simon-
                            sen. Encoding word order in complex embeddings. In International Conference on Learning
                            Representations, 2020. URL https://openreview.net/forum?id=Hke-WTVtwr.
                         Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy.        Memorizing
                            transformers. In International Conference on Learning Representations, 2022. URL https:
                            //openreview.net/forum?id=TrjbxzRcnf-.
                         Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Mar-
                            tin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang,
                            Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov,
                            Mike Lewis, Sinong Wang, and Hao Ma. Effective longcontext scaling of foundation model.
                            In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Confer-
                            ence of the North American Chapter of the Association for Computational Linguistics: Human
                            Language Technologies (Volume 1: Long Papers), pp. 4643–4663, Mexico City, Mexico, June
                            2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.260. URL
                            https://aclanthology.org/2024.naacl-long.260.
                         Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. Soaring from
                            4k to 400k: Extending llm’s context with activation beacon, 2024. URL https://arxiv.
                            org/abs/2401.03462.
                                                                      14
                            Published as a conference paper at ICLR 2025
                            A APPENDIX
                            A.1    ROTARY POSITION EMBEDDING
                            RoPEincorporates positional information directly into the self-attention mechanism by rotating the
                            query and key vectors in the complex space. When divided into even and odd dimensions, the
                            following calculations are performed for the m-th query in each sequence. In even dimensions,
                            RoPEisexpressed as follows.
                               m                                                                                   qm 
                                q          cosmθ1    −sinmθ1         0           0       ...      0            0            0
                                 0                                                                                         qm
                              qm        0              0       cosmθ     −sinmθ       ...      0            0       1 
                               2                                      2           2                                 . 
                                 .    =        .          .          .           .       .        .             .        . .
                               .            .          .          .           .        ..      .             .      . 
                                 .             .          .          .           .                .             .         qm
                               qm             0           0          0           0       ...  cosmθ       −sinmθ           d−2
                                d−2                                                                  d/2           d/2    qm
                                                                                                                           d−1(16)
                            In odds dimensions, RoPE is expressed as follows.
                                  m                                                                        qm 
                                    q          sinθ1   cosθ1      0         0      ...      0            0          0
                                     1                                                                             qm
                                 qm         0         0     sinmθ     cosmθ     ...      0            0     1 
                                  3                               2         2                               . 
                                     .    =      .       .        .         .      .         .           .       . ,        (17)
                                  .         .         .        .         .       ..       .           .     . 
                                     .           .       .        .         .                .           .        qm
                                   qm            0       0        0         0      ...  sinmθ       cosmθ          d−2
                                    d−1                                                        d/2         d/2    qm
                                                                                                                   d−1
                            where qm ∈ R1×d is the m-th query when the number of dimensions is d and θ                           =
                                                                                                                             i
                                   −2(i−1)/d                                                                            n     1×d
                            10000            , i ∈ [1,2,...,d/2]. The same process is also performed for the n-th key k   ∈R .
                            A.2    PROOF OF THE EXISTENCE OF f(t)
                            Weprove the existence of f(t) as described in 3.2 such that ϕ = ϕ            =mθ j+1 , where θ =
                                                                                             j      j+1        ⌈    ⌉          i
                                                                                                                  2
                                   −2(i−1)/d
                            10000            and i ∈ [1,2,...,d/2]. Here, we restrict our proof to ψ(t) in Eq.(7), but a similar
                            argument can be applied to ψ′(t), following analogous steps to establish its validity.
                            First, we revisit the definition of ψ(t):
                                                                      cosf(t)        0 ≤ t < 1,
                                                              ψ(t) = −sinf(t)        1 ≤ t < 2,                              (18)
                                                                      
                                                                        0             otherwise.
                            Here,f : R → RisamonotonousfunctionthatsatisfiesR∞ ψ(t)dt = 0andEq.(1). Assumingthat
                                                                                       −∞
                            whenx(t)(0 ≤ t ≤ d−1)is a signal with d elements, the wavelet ψ is used and wavelet transform
                            is performed at each scale a = 1. We define the shift parameter as bj = j −δ(j)(j = 0,2,..,d−2).
                            Here, δ(t) is a monotonous function such that 0 ≤ t ≤ d − 1 and 0 ≤ δ(t) < 1.
                                                                                                                        x(0) 
                               W(1,b )         cosϕ    −sinϕ       0        0       ...       ...        0          0          x(1)
                                     0             0         1                                                                      
                               W(1,b )           0        0      cosϕ    −sinϕ      ...       ...        0          0
                                    2                             2         3                                           . 
                                                                                                                                 .
                                  .      .             .        .       .         .        .          .           .     . 
                                  .      .             .        .        . .      .        .          .           .             
                                  .    = .             .        .                 .        .          .           .     x(j) .
                               W(1,b )           0        0        0       ...    cosϕ    −sinϕ          ...        0        x(j +1)
                                    j                                               j        j+1                                
                                  .      .             .        .        .        .        .          .           .     . 
                                   .             .        .        .        .        .        . .        .           .          .   
                                   .             .        .        .        .        .                   .           .           .
                              W(1,b    )         0        0        0        0       ...       ...     cosϕ      −sinϕ        x(d−2)
                                    d−2                                                                   d−2         d−1    x(d−1)
                                                                                                                              (19)
                            To simplify the notation in the matrix representation above, we write ϕj for j = 0,1,...,d − 1,
                            where ϕj = f(1 + δ(j)) if j is odd, and ϕj = f(δ(j)) otherwise. We let x be the query qm. The
                            function f(t) is defined such that 0 < f(t) ≤ 2kπ for 0 ≤ t < 1 and 0 < f(t) ≤ 2kπ for 1 ≤ t < 2,
                            where k is the smallest natural number satisfying m < 2kπ.
                                                                              15
                             Published as a conference paper at ICLR 2025
                             Do Haar-like wavelets satisfy the necessary conditions of a wavelet?             Here, f(t) must be a
                             function such that ψ(t) satisfies the conditions of a wavelet. For Eq. 1, it is evident that it holds
                             for any f satisfying 0 < f(t) ≤ 2kπ. Next, we consider the zero-mean property. As an example,
                             consider f(t) defined as f(t) = 2kπt for 0 ≤ t < 1 and f(t) = 2kπ(t−1) for 1 ≤ t < 2. If we set
                             θ = f(t), we have:
                                                     Z ∞ ψ(t)dt = Z 2kπ cosθdθ +Z 2kπ −sinθdθ = 0.                               (20)
                                                      −∞               0                0
                             Since this satisfies the zero-mean property, we conclude that there exists an f(t) such that ψ(t) is a
                             wavelet.
                             Furthermore,weobservethatthereexistsaδ(t)satisfyingϕ (= f(δ(j))) = ϕ                 (=f(1+δ(j))) =
                                                                                           j                  j+1
                             2kπδ(t) = mθ j+1 for j = 0,2,...,d−2. In other words, we can simply choose a function δ(j)
                                             ⌈ 2 ⌉mθ
                                                      ⌈j+1⌉
                             that satisfies δ(j) =      2    for j = 0,2,...,d − 2.
                                                      2kπ
                             A.3    HAARWAVELET
                             Here, we explain wavelet transform using the Haar wavelet, which is the simplest wavelet. The
                             definition of the Haar wavelet is as follows.
                                                           
                                                              1      0 ≤ t<1/2,              
                                                   ψ(t) = −1 1/2≤t<1,               ϕ(t) =    1     0 ≤ t<1,                    (21)
                                                                                              0 otherwise.
                                                              0      otherwise.
                             Haar wavelets are defined not only by a wavelet function ψ but also by a scaling function ϕ.
                             The method of analyzing signals by performing a discrete wavelet transform using these two func-
                             tions is called multi-resolution analysis. When the scale is fixed at 2 and the shift b ∈ [0,2,...,d/2],
                             the wavelet transform using the wavelet function and scaling function is expressed as follows.
                              ψ (0)         ψ (1)          ψ (2)          ψ (3)       ...  ψ (T −2) ψ (T −1)                   
                                 2,0          2,0            2,0            2,0              2,0          2,0               x(0)
                                ϕ (0)        ϕ (1)          ϕ (2)          ϕ (3)       ...  ϕ (T −2)     ϕ (T −1)
                              2,0            2,0            2,0            2,0              2,0          2,0              x(1)
                               ψ (−2)       ψ (−1)          ψ (0)          ψ (1)       ...  ψ (T −4) ψ (T −3)
                              2,0            2,0            2,0            2,0              2,0          2,0          x(2) 
                             ϕ (−2)        ϕ (−1)          ϕ (0)          ϕ (1)       ...  ϕ (T −4)     ϕ (T −3)               
                              2,0           2,0             2,0            2,0              2,0          2,0          . .
                                  .            .              .              .                  .            .        . 
                                   .            .              .              .                  .            .               .
                                  .            .              .              .        ...       .            .        x(T −2)
                               ψ (−d) ψ (−d+1) ψ (−d+2) ψ (−d+3) ...                          ψ (0)        ψ (1)
                                2,0  2     2,0   2        2,0   2        2,0   2               2,0          2,0           x(T −1)
                               ϕ (−d) ϕ (−d +1) ϕ (−d +2) ϕ (−d +3) ...                       ϕ (0)        ϕ (1)
                                2,0  2     2,0   2        2,0   2        2,0   2               2,0          2,0                  (22)
                             FromEq.(21), ψ       and ϕ    are as follows.
                                              2,0       2,0
                                                       √                                      ( √
                                                         1/ 2          0 ≤ t<1,
                                                      
                                           ψ (t)=             √                    ϕ (t)= 1/ 2             0 ≤ t<2,             (23)
                                             2,0      −1/ 2           1 ≤ t<2,       2,0        0        otherwise.
                                                      
                                                         0           otherwise.
                             Therefore, the Haar wavelet transform is a 2 × 2 block matrix.
                                                √             √                                                    
                                      ψ(2,0)          1/ 2   −1/ 2      0        0     ...   0        0           x(0)
                                                        √       √
                                       ϕ(2,0)         1/ 2    1/ 2      0        0     ...   0        0           x(1)
                                                                     √         √                                   
                                    ψ(2,2)         0         0     1/ 2    −1/ 2    ...   0        0      x(2) 
                                    ϕ(2,2)                           √        √                           x(3) 
                                              = 0            0     1/ 2     1/ 2    ...   0        0               .       (24)
                                        .       .            .        .       .            .       .      . 
                                         .              .       .        .       .            .       .             .
                                        .       .            .        .       .     ...    .       .      . 
                                    ψ(2,T −2)           0       0       0        0     ... 1/√2    −1/√2        x(T −2)
                                     ϕ(2,T −2)          0       0       0        0     ... 1/√2     1/√2        x(T −1)
                             This matrix is the Haar forward transform using matrix multiplication for a T element signal. This
                             matches the RoPE matrix with mθ = π/4.
                                                                                16
                            Published as a conference paper at ICLR 2025
                            A.4    ISN’T ROPE A FOURIER TRANSFORM?
                            Wealsohypothesized that this could be equivalent to a Fourier transform. However, this hypothesis
                            does not hold. When a signal x(t) that changes over time is Fourier transformed, its spectrum F(k)
                            is obtained. The process of converting an actual discrete signal x(t) into a spectrum F(k) is as
                            follows.
                                                                              T
                                                                    F(f) = Xx(t)wf·t.                                         (25)
                                                                             t=0
                            TheFourier transform can be expressed as a matrix formula as follows.
                                                 F(0)     w0·0    w0·1   w0·2  ...  w0·(T−1) x(0) 
                                                   F(1)     w1·0    w1·1   w1·2  ...  w1·(T−1)     x(1)
                                                 F(2)     w2·0    w2·1   w2·2  ...  w2·(T−1) x(2) 
                                                      = .           .     .    .        .             .                 (26)
                                                  .        .        .     .     .       .    . 
                                                     .       .        .     .      .      .          .
                                                     .          f·0   f·1    f·2         f·(T−1)       .
                                                   F(f)       w      w      w     ...  w           x(T −1)
                                                                .
                            Here, f ∈ R is the wave number, T ∈ R is the number of samples, and i is the imaginary unit.
                            w = exp(−2πi) is called the Twiddle Factor (Gentleman & Sande, 1966), which is a complex
                                           T
                                                                                      −iθ
                            number expressed in polar form using Euler’s formula e        =cosθ−isinθ. Inthecomplexplane,
                            wf·t represents a point on the unit circle with an argument of the complex number −ft2π. From this
                                                                                                                    T
                            formula, we can see that the Fourier transform calculates the inner product of all signals and sine
                            waves. However, in RoPE, the inner product with sine waves is calculated only within each block.
                            Next, when calculating the attention score with RoPE, does the Fourier transform hold? Attention
                            scores of the m-th query qm and the n-th key kn with RoPE are calculated as follows.
                                                                             R1K1 
                                           h                              i       n n         d/2
                                              1    1 T        d/2   d/2 T         .      X i T i                i
                                            R (Q ) ,...,R        (Q     )         .     = (Q ) R              K ,           (27)
                                              m m             m     m              .                m      n−m n
                                                                              Rd/2Kd/2        i=1
                                                                                n    n
                                     d/2                                                         d/2
                            where Q      is the query divided into every two dimensions, and R       is the rotation matrix.
                                     m                                                           m
                                                    d−1              d−1            cosmθ         −sinmθ      
                                            d/2     qm        d/2      kn        d/2             d/2            d/2
                                          Q = d ,K = d ,R =                                                          .
                                            m        q        n         k        m        sinmθ         cosmθ
                                                      m                   n                      d/2           d/2
                            Aligning with the Fourier transform, as illustrated in Equation 26, requires a process involving the
                            inner product between a frequency tensor of dimensions f×T and a signal tensor of dimensions T×1
                            (suchasthequeryvector). However,RoPEoperatesonindependent2×2blocks,whereeachblockis
                            processed separately. Consequently, RoPE’s block-wise operations do not conform to the structure
                            required by the Fourier transform. Moreover, if we focus solely on the RoPE and key operations
                            in Equation 27, they may appear to align with the structure of a Fourier transform. However, since
                            the final step involves taking the inner product with the query, the overall operation deviates from
                            the path of becoming a perfect match with the Fourier transform. Furthermore, the rotation factor
                            represents a rotation in the complex plane, and even if it is expressed as in Eq.(26) using a rotation
                            matrix, it does not completely match a rotation matrix that represents a rotation in the Euclidean
                            plane.
                            Therefore, RoPE cannot be equated with the Fourier transform. Furthermore, even if it were the
                            sameastheFouriertransform, it would be unsuitable for processing non-stationary signals and thus
                            unsuitable for processing natural language, which is a non-stationary flow.
                                                                              17
                            Published as a conference paper at ICLR 2025
                            A.5    CONSIDERATION OF WAVELET TRANSFORMATION BASED ON ROPE
                            In this paper, we explore the incorporation of wavelet transforms into RoPE (Relative Positional
                            Encoding) following our previous discussion on RPE (Relative Position Encoding). In this regard,
                            integrating wavelet transforms into RoPE presents challenges for controlling computational and
                            memorycosts. InSections3and4,wehighlightedthepotentialeffectivenessofemployingmultiple
                            scales for extrapolation. With this in mind, we present a simplified formula for applying various
                            scales and wavelet transforms to RoPE, which we refer to here as a RoPE-based Wavelet.
                            qm        cosmθ1       −sinmθ1           0            0        ...      0             0     qm 
                                0                                                                                                0
                            qm        sinmθ         cosmθ            0            0        ...      0             0     qm 
                                1                1            1                                                                  1
                            qm        cosmθ        −sinmθ        cosmθ        −sinmθ       ...      0             0     qm 
                             2                2             2           2             2                                  2 
                            qm        sinmθ         cosmθ         sinmθ        sinmθ       ...      0             0     qm 
                             3 =              2            2            2            2                                   3 ,
                             .             .            .            .            .        .         .            .      . 
                             .             .            .            .            .         ..       .            .      . 
                             .             .            .            .            .                  .            .      . 
                              qm         cosmθ       −sinmθ         cosmθ       −sinmθ        ...  cosmθ       −sinmθ          qm
                               d−1              d/2           d/2         d/2           d/2               d/2           d/2     d−2
                              qm         sinmθ        cosmθ         sinmθ        sinmθ        ...  sinmθ        cosmθ          qm
                               d−2              d/2          d/2          d/2          d/2                d/2          d/2      d−1
                                                                                                                             (28)
                            where qm ∈ R1×d is the m-th query when the number of dimensions is d and θ                         =
                                                                                                                            i
                                  −2(i−1)/d                                                                           n      1×d
                            10000           , i ∈ [1,2,...,d/2]. The same process is also performed for the n-th key k  ∈R .
                            Conversely, the method introduced in Section 5 is here called RPE-based Wavelet. The key differ-
                            ences between RoPE-based Wavelet and RPE-based Wavelet are as follows:
                                   • NumberofScaleParameters: InRPE-basedWavelet,thescaleparameterscanbeselected
                                     up to the maximum sequence length. However, in RoPE-based Wavelet, the selection is
                                     limited to a maximum of d.
                                   • Memory Usage: RoPE-based Wavelet requires a wavelet matrix that corresponds to the
                                     number of absolute positions m. Consequently, the memory usage is significantly higher.
                                     UnlikeRoPE-basedWavelet,RPE-basedWaveletdoesnotnecessitateawaveletmatrixthat
                                     matches mvalues, allowing the use of Tip 2 from Appendix A.6, which improves memory
                                     efficiency.
                                   • Absolute and Relative Positions: When applying wavelet transforms using RoPE-based
                                     Wavelet, it is necessary to use absolute positions. In contrast, RPE-based Wavelet can use
                                     relative positions, which enhances extrapolation.
                                   • ComputationalCost: ImplementingwavelettransformsviaRoPE-basedWaveletrequires
                                     processing both the query and the key, necessitating two calculations. RPE-based Wavelet,
                                     as discussed in Section 5, only requires one computation, since it processes only the query.
                            Additionally, we conducted an experiment with RoPE-based Wavelet. Unfortunately, we had to
                            halt the learning process because it took over five times longer than anticipated. Considering
                            the learning costs associated with large-scale language models in recent years, we believe the RoPE-
                            based Wavelet approach is not feasible.
                                                                             18
                            Published as a conference paper at ICLR 2025
                            A.6    IMPLEMENTATION TIPS FOR WAVELET POSITION REPRESENTATION
                            Tip 1   Similar to RPE(Shaw et al., 2018), we used Eq. (10) as
                                                                                 T           T 
                                                                              q K +q(p )
                                                           α =softmax i             √ i ij       .
                                                             ij                       d
                                                                                        k
                            Bytransforming it in this way, it is possible to reduce the computational complexity to O(batch ×
                                        2              2
                            n×length ×d+length ×d),wherebatchisthebatchsize,nisthenumberofheads,lengthis
                            the number of tokens, and d is the number of dimensions of each head. The experiments in Section
                            6 are implemented based on the methodology introduced in this section.
                            Tip 2   Whendealing with long contexts of over 4 k with a large model, the memory efficiency of
                            (d,length,length) of the wavelet position becomes a bottleneck. Therefore, we further reduce the
                            memoryusageto (d,length) by using torch.scatter to scatter the wavelet position represen-
                            tation to the attention mask. In the relative position representation in the decoder, only the position
                            information of the token before the current token is required, for example, 0,−1,−2,etc. There-
                            fore, we pre-compute the information up to 0,−1,−2,...length and reduce the memory usage by
                            using torch.scatter to distribute it. Specifically, we prepare a (d,length) wavelet tensor and
                            calculate the 2D inner product with the query, which has been transposed to (length × batch,d).
                            The tensor after the calculation becomes (length × batch,length), which is then scattered us-
                            ing torch.scatter so that it becomes a relative position in the attention mask. This reduces
                            the amount of memory used from (d,length,length) to (d,length), and the calculation can be
                            performed using calculations between 2D tensors. The experiments in Section 7 are implemented
                            based on the methodology introduced in this section.
                            A.7    EXPERIMENTAL SETTINGS IN SHORT-CONTEXT EXPERIMENT
                            The parameter settings used in the extrapolation experiments were the same as those in the original
                            ALiBipaper. The dimensionality of the word embedding d            is 1024, the number of heads N is
                                                                                        model
                            8, the dimensionality of the heads d is 128, and the number of layers is 16. The implementation was
                                                                              12
                            based on the fairseq (Ott et al., 2019)-based code   provided in a previous work(Press et al., 2022),
                            andall hyperparameters were set to the same values as those in the literature(Press et al., 2022). The
                            number of training epochs is 205, and the batch size is 9216. The learning rate was set to 1.0, and
                            the learning process was updated by 1e-7 every 16,000 steps.
                            A.8    EXPERIMENTAL SETTINGS IN LONG-CONTEXT EXPERIMENT
                            The dimensionality of the word embedding d           is 4096, the number of heads N is 32, the di-
                                                                           model
                            mensionality of the heads d is 128, and the number of layers is 32. The number of training steps is
                            30,000, and the batch size is 1. The learning rate was set to 0.0003. We used AdamW(Loshchilov
                            &Hutter, 2019) as the optimizer, with (β ,β ) = (0.9,0.95). In accordance with previous research
                                                                      1   2
                            (Rubin & Berant, 2024; Wu et al., 2022; Zhang et al., 2024), we then used 100 sampled sequences
                            in the training set for evaluation. In this experiment, due to the large model size and long sequence
                            length, we report perplexity only for non-overlapping inference using L          , since the memory
                                                                                                        train
                            capacity is exceeded.
                              12https://github.com/ofirpress/attention_with_linear_biases
                                                                             19
                     Published as a conference paper at ICLR 2025
                     A.9  RICKER WAVELET
                     Figures 4 and 5 show the Ricker wavelets with multiple scale a.
                                                                               0  1  2 3  4
                            Figure 4: Graph of compared Ricker wavelet functions with a = [2 ,2 ,2 ,2 ,2 ]
                                                                               5  6  7 8  9
                            Figure 5: Graph of compared Ricker wavelet functions with a = [2 ,2 ,2 ,2 ,2 ]
                     A.10 WAVELETTYPE
                     Figure 6 shows graphs of the wavelets compared in Section 6.3.1. It can be seen that the simplest is
                     the Haar wavelet, while the most complex is the Morlet wavelet.
                                                                                         4
                     Figure 6: Graph of compared wavelet functions. The case with scale parameter a = 2 and shift
                     parameter b = 0 is shown.
                                                           20
                          Published as a conference paper at ICLR 2025
                          A.11 EXAMPLEOFHEATMAPANDTEXTCORRESPONDENCE
                          Figure7showstheattentionmapaftersoftmaxoperationfortheproposedmethod. First,thenotable
                          feature of the proposed method is that it is always able to pay attention to specific tokens. The words
                          that always receive attention are those that are important in the sentence, such as the ’</s>’ token,
                          the first token, and words that are the subject of the sequence, such as ’he.’ Moreover, as with ALiBi,
                          the proposed method has a different scope of attention for each head.
                          Figure 7: Heatmap of attention score e  after softmax operation for the proposed method. The
                                                                ij
                          maximumsequencelengthisLmax = 512,andthesequencelengthatinferenceisL = 1012. From
                          left to right, n = 1,2,4th heads are shown. Scores above 0.01 are mapped in black and the rest in
                          yellow. Words that were always given attention in all heads are shown in red, and words that were
                          frequently given attention only in the n = 2nd head are shown in blue. Sentences are omitted in the
                          middle because they are long with 1012 tokens.
                                                                       21
           Published as a conference paper at ICLR 2025
           A.12 CANITHANDLETOKENSWITHLONG-RANGEDEPENDENCIES?
           Figure 8: Heatmap of scaled attention scores via softmax normalization in 1-3rd and 5-8th head
           after softmax operation for ALiBi, RoPE, and our method. For clarity, values of 0.001 or more are
           mappedtoblack, while values below that are mapped to yellow.
                               22
                               Published as a conference paper at ICLR 2025
                               Table 3: Perplexity of validation set in extrapolation experiments using Wikitext-103. Maximum
                               allowable length of sequences in pre-training is L           =512.
                                                                                      train
                                                                                                       Sequence Length
                                                       scale a           shift b        128    256     512    1012    1512     2512
                                                                    Perplexity without Non-overlapping Inference
                                        Ricker     {20,21,...,27}    {0,1,2,...,15}    23.64   20.82  19.19   18.23   18.00    17.99
                                        Ricker     {21,22...,28}     {0,1,2,...,15}    23.77   20.89  19.25   18.23   17.97    18.02
                                        Ricker     {22,23...,29}     {0,1,2,...,15}    23.92   21.03  19.40   18.41   18.14    18.07
                                                     0  1  2   3
                                        Ricker     {2 ,2 ,2 ,2 }     {0,1,2,...,31}    23.96   21.13  19.55   18.87   19.40    21.73
                                                        0  1
                                        Ricker        {2 ,2 }        {0,1,2,...,63}    24.49   21.60  19.95   20.90   32.01    70.80
                                        Ricker     {20,21...,215}     {0,1,2,...,7}    23.74   20.88  19.24   18.22   17.96    17.84
                                        Ricker     {20,21...,231}      {0,1,2,3}       23.75   20.86  19.26   18.24   17.96    17.84
                                        Ricker     {20,21...,263}        {0,1}         23.75   20.88  19.30   18.31   18.04    18.02
                                        Ricker    {20,21...,2127}         {0}          23.97   21.10  19.46   18.50   18.27    18.29
                                                          7
                                        Ricker         {2 }         {0,1,2,...,127}    24.35   21.45  19.80   20.68   20.87    21.31
                                        Gaussian   {20,21,...,27}    {0,1,2,...,15}    23.77   20.90  19.30   18.31   18.02    17.88
                                        Gaussian   {21,22...,28}     {0,1,2,...,15}    23.92   21.02  19.41   18.41   18.15    18.01
                                        Gaussian   {22,23...,29}     {0,1,2,...,15}    23.98   21.09  19.46   18.43   18.13    17.93
                                                     0  1  2   3
                                        Gaussian   {2 ,2 ,2 ,2 }     {0,1,2,...,31}    23.83   29.96  19.33   18.43   18.40    18.94
                                                        0  1
                                        Gaussian      {2 ,2 }        {0,1,2,...,63}    24.28   21.35  19.70   18.96   19.63    23.14
                                        Gaussian   {20,21...,215}     {0,1,2,...,7}    23.72   20.86  19.24   18.24   17.95    17.77
                                        Gaussian   {20,21...,231}      {0,1,2,3}       23.78   20.92  19.29   18.30   18.01    17.85
                                        Gaussian   {20,21...,263}        {0,1}         23.86   20.98  19.37   18.46   18.20    18.10
                                        Gaussian  {20,21...,2127}         {0}          24.21   21.31  19.68   18.71   18.45    18.45
                                                          7
                                        Gaussian       {2 }         {0,1,2,...,127}    24.48   21.62  20.05   19.53   22.63    35.23
                                        Haar             -                  -          24.98   22.07  20.49   51.61   116.87  299.26
                                        Haar       {20,21,...,27}    {0,1,2,...,15}    23.73   20.89  19.27   18.34   18.11    18.17
                                        Morlet     {20,21,...,27}    {0,1,2,...,15}    24.15   21.28  19.65   19.02   20.46    26.56
                               A.13 ABLATION STUDY OF SCALE AND SHIFT PARAMETERS
                               Inthissection, wepresentthefindingsfromourablationstudyfocusingontheshiftandscaleparam-
                               eters of the Ricker and Gaussian wavelets. As indicated in Table 1, both wavelet types demonstrate
                               substantial effectiveness in our method. To further evaluate their performance, we explored the con-
                               tributions of two parameters, i.e., the scale parameter a and the shift parameter b, while keeping all
                               other settings consistent with those outlined in Section 6.
                               Results    Theresults of our experiments are summarized in Table 3. Both the Ricker and Gaussian
                               wavelets exhibit similar trends regarding the influence of the scale and shift parameters on extrap-
                               olation performance. Initially, we observed that increasing the scale parameter value a while hold-
                                                               0   1      7                           1   2      8
                               ing the shift parameter b ({2 ,2 ,...,2 } × {0,1,2,...,15}, {2 ,2 ,...,2 } × {0,1,2,...,15} and
                                 2   3       9
                               {2 ,2 ,...,2 }×{0,1,2,...,15}) constant maintained the performance of extrapolation, albeit with
                               somefluctuations. Conversely, when we increased the number of shift parameters while decreasing
                                                                      0   1   2   4                              0   1
                               the number of scale parameters ({2 ,2 ,2 ,2 } × {0,1,2,...,31} and {2 ,2 } × {0,1,2,...,63}),
                               there was a noticeable decline in performance. These findings underscore the significance of the
                               scale parameters in extrapolation. Moreover, we found that increasing the number of scale pa-
                               rameters while decreasing the number of shift parameters led to performance improvements in
                                                     0  1       15                              0   1      31
                               some instances ({2 ,2 ,...,2 } × {0,1,2,...,7} and {2 ,2 ,...,2 } × {0,1,2,3}). However,
                                                                                                                0  1       63
                               whentheshift parameters were reduced to two or entirely eliminated ({2 ,2 ,...,2              }×{0,1}and
                                 0   1       127
                               {2 ,2 ,...,2     }), relying solely on the scale parameters resulted in a deterioration of extrapolation
                               performance. Moreover, even when the scale parameter was fixed and only the shift parameter was
                                        7
                               used({2 }×{0,1,2,..,127}),theextrapolationperformancedecreased. Thissuggeststhepotential
                               importance of shift parameters as well.
                               In conclusion, our analysis highlights the critical roles of both shift and scale parameters in the
                               effectiveness of our wavelet-based method.
                                                                                     23
                         Published as a conference paper at ICLR 2025
                         A.14 ABLATION STUDY OF WAVELET TYPES
                         In this section, we also explored a variety of wavelet types beyond those previously discussed. In
                         Section 6.3.1, our focus was primarily on wavelets that could be computed directly from mathemat-
                         ical formulas. However, in this section, we expand our inquiry to include wavelets with varying
                         numbers of vanishing moments as well as discrete wavelet transformations. Additionally, drawing
                         from previous research (Wang et al., 2020), we considered the necessity for a distinct approach
                         when incorporating complex numbers into positional encoding. Consequently, our study did not
                         encompass wavelets that incorporate complex numbers.
                         Wavelet types  The specific wavelets under consideration in our investigation are outlined as fol-
                         lows:
                               • Daubechies (db) (Daubechies, 1992) - Compactly supported orthonormal wavelets
                               • Symlets (sym) - Wavelets with minimum asymmetry
                               • Coiflets (coif) - The scaling and wavelet functions have the same number of vanishing
                                 moments
                               • Meyer (dmey) - Wavelets defined in the frequency domain
                               • BiorthogonalSpline(bior)-Twowaveletsareused: onefordecomposition,andtheother
                                 for reconstruction
                               • Reverse biorthogonal Spline (rbio)
                         In addition, the graphs of these wavelets are shown in Figures 9 and 10. As the number of vanishing
                         moments increases, the wave oscillation becomes larger. Therefore, we also conducted a survey
                         by vanishing point moment. The name of a wavelet is derived from the number of vanishing mo-
                         ments. For example, db6isaDaubechieswaveletwith6vanishingmoments,andsym3isaSymlet
                         wavelet with 3 vanishing moments. In the case of Coiflet wavelets, coif3 is a Coiflet wavelet
                         with 6 vanishing moments. The names of bior and rbio wavelets are derived from the number
                         of vanishing moments possessed by the decomposition and reconstruction wavelets, respectively.
                         For example, bior3.5 is Biorthogonal wavelet that has 3 vanishing moments for the decomposi-
                         tion wavelet and 5 vanishing moments for the reconstruction wavelet. Biorthogonal wavelets and
                         Reverse-Biorthogonalwaveletscancalculatetheapproximatevaluesofdecompositionwaveletsand
                         reconstruction wavelets, but in this case, we only used the values of decomposition wavelets.
                         Experimental Settings  WeusedPywavelet(Leeetal., 2019) 13 to calculate the approximate val-
                         ues of these wavelets. In addition, in this experiment, we calculated the approximate values by
                                                                                             0   1    7
                         specifying 8 levels of {1,2,...,8} instead of the 8-pattern scale parameters {2 ,2 ,...,2 }. We used
                         the shift parameter {0,1,2,...,15}. The other experimental settings are the same as those in Section
                         6.
                         Results  The experimental results are summarized in Table 4. Overall, the performance observed
                         was suboptimal for extrapolation. However, it is important to note that since the parameters were
                         fixed at levels {1,2,...,8}, we believe that performance may be enhanced with adjustments to these
                         levels. Notably, the rbio1.1 wavelet demonstrated promising extrapolation capabilities, suggesting
                         significant potential for future improvements. In contrast, the coif and dmey wavelets exhibited lim-
                         ited performance, even with shorter sequences, indicating their potential unsuitability for position
                         encoding tasks. Conversely, while the extrapolation performance (> 512) of other wavelets was
                         generally low, their interpolation performance (≤ 512) remained consistently stable, highlighting
                         another avenue for enhancement. Furthermore, the performance of the db, bior, and rbio wavelets
                         showed a positive correlation with an increasing number of vanishing points. This finding under-
                         scorestheimportanceofvanishingpointsasacriticalfactorinfluencingperformance. Inconclusion,
                         our analysis indicates that both the shape of the wavelet and the number of vanishing points play
                         significant roles in determining extrapolation performance. Future work should explore these rela-
                         tionships further to identify optimal configurations for improved performance outcomes.
                           13https://pywavelets.readthedocs.io/en/latest/index.html
                                                                    24
                                    Published as a conference paper at ICLR 2025
                                    Table 4: Perplexity without Non-overlapping Inference. We evaluated the validation set in extrapo-
                                    lation experiments using Wikitext-103. The maximum allowable length of sequences in pre-training
                                    is L        =512.
                                          train
                                                                                                           Sequence Length
                                                                 Wavelet type     128       256        512       1012      1512      2512
                                                                                        Continuous Wavelet Families
                                                                 Ricker          23.64      20.82     19.19     18.23      18.00     17.99
                                                                 Gaussian        23.77      20.90     19.30     18.31      18.02     17.88
                                                                 Morlet          24.15      21.28     19.65     19.02      20.46     26.56
                                                                                          Discrete Wavelet Families
                                                                 Haar            23.73      20.89     19.27     18.34      18.11     18.17
                                                                 db2             25.22      22.26     20.64     30.30      60.27    130.93
                                                                 db4             25.22      22.47     21.37     41.78      51.75     56.18
                                                                 db8             25.19      22.48     21.58     26.90      31.55     39.75
                                                                 db16            25.23      22.43     21.24     21.15      22.16     46.65
                                                                 db32            25.12      22.35     21.14     21.20      22.40     38.00
                                                                 sym2            25.11      22.21     20.68     31.25      61.00    126.32
                                                                 sym4            25.27      22.56     21.98     24.70      26.81     42.81
                                                                 sym8            29.27      26.13     24.63     23.97      31.47     92.36
                                                                 coif1           31.24      28.00     26.24     64.62      71.06     97.60
                                                                 coif2           25.24      22.47     21.39     27.74      27.39     44.26
                                                                 coif4           49.91      45.15     42.42     41.07      56.08    110.27
                                                                 coif8           25.15      22.39     21.26     21.31      22.26     35.73
                                                                 coif16          126.38    117.88    113.42     132.14    166.77    230.95
                                                                 dmey            30.38      27.12     25.45     25.88      46.35    131.48
                                                                 bior1.3         26.27      23.36     23.69     23.38      30.71     88.66
                                                                 bior2.2         25.28      22.51     21.59     29.71      29.43     50.25
                                                                 bior2.6         25.29      22.70     21.60     22.15      22.71     40.61
                                                                 bior3.1         26.92      24.02     22.38     59.30     113.81    205.54
                                                                 bior3.5         25.17      22.49     21.65     27.41      27.19     53.99
                                                                 bior3.9         25.24      22.48     21.51     21.89      23.86     50.14
                                                                 bior4.4         25.52      22.72     21.64     21.67      24.42     51.46
                                                                 bior5.5         25.21      22.55     21.72     23.43      24.68     36.30
                                                                 bior6.8         25.14      22.39     21.21     21.10      22.31     46.97
                                                                 rbio1.1         24.26      21.34     19.69     18.79      18.63     18.98
                                                                 rbio1.3         25.28      22.50     21.39     52.06      47.78     59.94
                                                                 rbio2.2         25.92      23.08     21.98     68.57      86.12     93.90
                                                                 rbio2.6         25.29      22.68     21.60     24.54      24.47     44.57
                                                                                                    25
           Published as a conference paper at ICLR 2025
           Figure 9: Graph of compared wavelets with level=10. Pywavelet (Lee et al., 2019) was used to
           calculate wavelets.
                               26
           Published as a conference paper at ICLR 2025
           Figure 10: Graph of compared wavelets with level=10. Pywavelet (Lee et al., 2019) was used to
           calculate wavelets.
                               27
                              Published as a conference paper at ICLR 2025
                              A.15 EVALUATION ON LONGBENCH
                              Themodelspre-trainedinSection7wereevaluatedonLongBench(Baietal.,2024). Thisevaluation
                              was conducted using a dataset that contained relatively long sentences. Furthermore, the multi-
                              document QA task and single-document QA task were evaluated on all datasets. Since pre-training
                              wasconducted using an English dataset, evaluation was conducted using only the English dataset.
                              The results are shown in Figure 11. In some datasets, the performance of the model that adopted
                              RoPEwasgood(Qasper,MuSiQue,andQMSum). InNarrativeQA,thetwomodelsattainedalmost
                              the samescore. However,intheremainingtasks,theproposedmethodwasmoreeffective. Notethat
                              this is an evaluation of a model that was pre-trained on a small dataset (redpajama-1B). As future
                              work, it will be necessary to pre-train the model with a larger dataset and conduct evaluations with
                              other models that are effective for long sentences, such as LongRangeArena (Tay et al., 2021).
                              Figure 11: Evaluation results using LongBench(Bai et al., 2024). We evaluated the model pretrained
                              in Section 7. The scores of the difference between the model using the proposed method, which
                              uses wavelet-based position representation, and the model using RoPE are shown. The tasks were
                              evaluated using the dataset, which contains relatively long sentences.
                              Table 5: Overview of the dataset statistics in LongBench (Bai et al., 2024). Avg len (average length)
                              is computed using the number of words in the English.
                                                                Dataset         Avglen     Metric   Samples
                                                                NarrativeQA      18,409     F1        200
                                                                Qasper           3,619      F1        200
                                                                MultiFieldQA-en  4,559      F1        150
                                                                HotpotQA         9,151      F1        200
                                                                2WikiMQA         4,887      F1        200
                                                                MuSiQue          11,214     F1        200
                                                                TriviaQA         8,209      F1        200
                                                                SAMSum           6258     Rouge-L     200
                                                                QMSum            10614    Rouge-L     200
                                                                                    28
