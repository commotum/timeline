            Article
            Methods                                                                      Extended Data Table 3, and we chose the environments as originally 
                                                                                         introduced. Visual control and proprioceptive control span the same 
            Baselines                                                                    20 tasks22,42 with a 1 million budget. Across all benchmarks, we use no 
                                                   7
            PPO. We employ the PPO algorithm , which has become a standard               action repeat unless prescribed by the literature.
            choice in the field, to compare Dreamer under fixed hyperparameters 
            across all benchmarks. There are a large number of PPO implementa-           Environment instances. In earlier experiments, we observed that 
            tions available publicly and they are known to substantially vary in task    the performance of both Dreamer and PPO is robust to the number 
            performance2. To ensure a comparison that is representative of the           of environment instances. On the basis of the central processing unit 
            highest performance PPO can achieve under fixed hyperparameters              resources available on our training machines, we use 16 environment 
            across domains, we choose the high-quality PPO implementation                instance by default. For BSuite, the benchmark requires using a single 
                                                51
            available in the Acme framework  and select its hyperparameters in           environment instance. We also use a single environment instance for 
                                                                     1,2
            Extended Data Table 6 following recommendations  and addition-               Atari100K because the benchmark has a budget of 400,000 environ-
            ally tune its epoch batch size to be large enough for complex environ-       ment steps whereas the maximum episode length in Atari is in principle 
                   38
            ments , its learning rate and its entropy scale. We match the discount       432,000 environment steps. For Minecraft, we use 64 environments 
            factor to Dreamer because it works well across domains and is a frequent     using remote central processing unit workers to speed up experiments 
                                   10,33
            choice in the literature   . We choose the IMPALA network architecture       because the environment is slower to step.
            that we have found performed better than alternatives38 and set the  
            minibatch size to the largest possible for one A100 GPU. We verify the       Seeds and error bars. We run five seeds for each Dreamer and PPO 
            performance of our PPO implementation and hyperparameters on                 per benchmark, with the exception of ten seeds for BSuite as required 
            the ProcGen benchmark, where a highly tuned PPO implementation               by the benchmark and ten seeds for Minecraft to reliably report the 
                                                    34
            has been reported by the PPO authors . We find that our implementa-          fraction of runs that achieve diamonds. All curves show the mean over 
            tion matches or slightly outperforms this performance reference. The         seeds with one standard deviation shaded.
            training time of the implementation is comparable to Dreamer under 
            equal replay ratio. It runs about 10-times faster than Dreamer with a        Computational choices. All Dreamer and PPO agents in this paper were 
            train ratio of 32, unless restricted by environment speed, owing to its      trained on a single Nvidia A100 GPU each. Dreamer uses the 200 million 
            inherently lower experience reuse.                                           model size by default. The replay ratio controls the trade-off between 
                                                                                         computational cost and data efficiency as analysed in Fig. 6 and is cho-
            Additional baselines. For Minecraft, we additionally tune and run            sen to fit the step budget of each benchmark.
            the IMPALA and Rainbow algorithms because successful end-to-end 
                                                                                19       Previous generations
            learning from scratch has not been reported in the literature . We 
                                               51
            use the Acme implementations  of these algorithms, use the same              We present the third generation of the Dreamer line of work. Where 
            IMPALA network we used for PPO, and tune the learning rate and               the distinction is useful, we refer to this algorithm as DreamerV3. 
            entropy regularizers. For continuous control, we run the official imple-     The DreamerV1 algorithm22 was limited to continuous control, the 
                                    43                                                                          23
            mentation of TD-MPC2  from proprioceptive inputs and from images.            DreamerV2 algorithm  surpassed human performance on Atari, and the 
            We note that the code applies data augmentation and frame stack-             DreamerV3 algorithm enables out-of-the-box learning across diverse 
            ing for visual inputs—which is not documented in its paper—which is          benchmarks.
            crucial to its performance. The training time of TD-MPC2 is 1.3 days           We summarize the changes introduced for DreamerV3 as follows:
            for proprioceptive inputs and 8.0 days from pixels. Besides that, we         •  Robustness techniques: observation symlog, combining Kullback–
            compare with a wide range of tuned expert algorithms reported in the           Leibler balance with free bits, 1% unimix for categoricals in the recur-
                      9,10,33,36,41,44,52–54
            literature               .                                                     rent state-space model and actor, percentile return normalization, 
                                                                                           symexp two-hot loss for the reward head and critic
            Benchmarks                                                                   •  Network architecture: block gated recurrent unit (block GRU), 
            Aggregated scores on all benchmarks are shown in Extended Data                 RMSNorm normalization, sigmoid linear unit (SiLu) activation
            Table 1. Scores and training curves of individual tasks are included         •  Optimizer: adaptive gradient clipping, LaProp (RMSProp before 
            in Supplementary Information.                                                  momentum)
                                                                                         •  Replay buffer: larger capacity, online queue, storing and updating 
            Protocols. Summarized in Extended Data Table 2, we follow the stand-           latent states.
                                                                                    35
            ard evaluation protocols for the benchmarks where established. Atari  
                                              55                                         Implementation
            uses 57 tasks with sticky actions . The random and human reference 
            scores used to normalize scores vary across the literature and we chose      Model sizes. To accommodate different computational budgets and 
            the most common reference values, replicated in Supplementary                analyse robustness to different model sizes, we define a range of mod-
            Information. DMLab39 uses 30 tasks52 and we use the corrected action         els shown in Extended Data Table 4. The sizes are parameterized by 
                  33,56
            space     . We evaluate at 100 million steps because running for 10 bil-     the model dimension, which approximately increases in multiples 
            lion as in some previous work was infeasible. Because existing published     of 1.5, alternating between power of 2 and power of 2 scaled by 1.5. 
            baselines perform poorly at 100 million steps, we compare with their         This yields tensor shapes that are multiples of eight as required for 
            performance at 1 billion steps instead, giving them a 10-times data          hardware efficiency. Sizes of different network components derive 
            advantage. ProcGen uses the hard difficulty setting and the unlim-           from the model dimension. The MLPs have the model dimension as 
                         38                                                       34,38
            ited level set . Previous work compares at different step budgets            the number of hidden units. The sequence model has eight times the 
            and we compare at 50 million steps owing to computational cost, as           number of recurrent units, split into eight blocks of the same size as 
            there is no action repeat. For Minecraft Diamond purely from sparse          the MLPs. The convolutional encoder and decoder layers closest to 
            rewards, we establish the evaluation protocol to report the episode          the data use 16-times-fewer channels than the model dimension. Each 
            return measured at 100 million environment steps, corresponding to           latent also uses 16-times-fewer codes than the model dimension. The 
            about 100 days of in-game time. Atari100k18 includes 26 tasks with a         number of hidden layers and number of latents is fixed across model 
            budget of 400,000 environment steps, 100,000 after action repeat.            sizes. All hyperparamters, including the learning rate and batch size, 
            Previous work has used various environment settings, summarized in           are fixed across model sizes.
