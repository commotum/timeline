               Article
               Mastering diverse control tasks through 
               world models
                                                                                                     1                           1              2                          1
               https://doi.org/10.1038/s41586-025-08744-2                        Danijar Hafner  ✉, Jurgis Pasukonis , Jimmy Ba  & Timothy Lillicrap
               Received: 5 April 2024
               Accepted: 5 February 2025                                         Developing a general algorithm that learns to solve tasks across a wide range of 
               Published online: 2 April 2025                                    applications has been a fundamental challenge in artif㘶cial intelligence. Although 
               Open access                                                       current reinforcement-learning algorithms can be readily applied to tasks similar to 
                                                                                 what they have been developed for, conf㘶guring them for new application domains 
                    Check for updates                                                                                                                                    1,2
                                                                                 requires substantial human expertise and experimentation . Here we present the 
                                                                                 third generation of Dreamer, a general algorithm that outperforms specialized 
                                                                                 methods across over 150 diverse tasks, with a single conf㘶guration. Dreamer learns a 
                                                                                 model of the environment and improves its behaviour by imagining future scenarios. 
                                                                                 Robustness techniques based on normalization, balancing and transformations 
                                                                                 enable stable learning across domains. Applied out of the box, Dreamer is, to our 
                                                                                 knowledge, the f㘶rst algorithm to collect diamonds in Minecraft from scratch without 
                                                                                 human data or curricula. This achievement has been posed as a substantial challenge 
                                                                                 in artif㘶cial intelligence that requires exploring farsighted strategies from pixels and 
                                                                                                                                 3
                                                                                 sparse rewards in an open world . Our work allows solving challenging control 
                                                                                 problems without extensive experimentation, making reinforcement learning 
                                                                                 broadly applicable.
               Reinforcement learning has enabled computers to solve tasks through                                actions to reach the best outcomes. Although intuitively appealing, 
                                                                                                           4,5
               interaction, such as surpassing humans in the games of Go and Dota .                               robustly learning and leveraging world models to achieve strong task 
                                                                                                                                                                           18
               It is also a key component for improving large language models beyond                              performance has been an open problem . Dreamer overcomes this 
                                                                                6
               what is demonstrated in their pretraining data . Although the proxi-                               challenge through a range of robustness techniques based on normali-
               mal policy optimization (PPO) algorithm7 has become a standard                                     zation, balancing and transformations. We observe robust learning 
               algorithm in the field of reinforcement learning, more specialized                                 across over 150 tasks from the domains summarized in Fig. 2, as well 
               algorithms are often used to achieve higher performance. These spe-                                as across model sizes and training budgets. Notably, larger models not 
               cialized algorithms target the unique challenges posed by different                                only achieve higher scores but also require less interaction to solve a 
                                                                                  8                       9,10
               application domains, such as continuous control , discrete actions                            ,    task, offering practitioners a predictable way to increase performance 
                                     11                    12                                13
               sparse rewards , image inputs , spatial environments  and board                                    and data efficiency.
                         14
               games . However, applying reinforcement-learning algorithms to                                        To push the boundaries of reinforcement learning, we consider 
               sufficiently new tasks—such as moving from video games to robot-                                   the popular video game Minecraft, which has become a focal point of 
               ics tasks—requires substantial effort, expertise and computational                                 research in recent years19–21, with international competitions held for 
                                                                                                      1
               resources for tweaking the hyperparameters of the algorithm . This                                 developing algorithms that autonomously learn to collect diamonds in 
                                                                                                                                3
               brittleness poses a bottleneck in applying reinforcement learning                                  Minecraft . Solving this problem without human data has been widely 
               to new problems and also limits the applicability of reinforcement                                 recognized as a substantial challenge for artificial intelligence because 
               learning to computationally expensive models or tasks where tuning                                 of the sparse rewards, exploration difficulty, long time horizons and 
                                                                                                                                                                                        19
               is prohibitive. Creating a general algorithm that learns to master new                             the procedural diversity of this open-world game . Owing to these 
               domains without having to be reconfigured has been a central challenge                             obstacles, previous approaches resorted to using human expert data 
                                                                                                                                                            20,21
               in artificial intelligence and would open up reinforcement learning to                             and domain-specific curricula                 . Applied out of the box, Dreamer is, 
               a wide range of practical applications.                                                            to our knowledge, the first algorithm to collect diamonds in Minecraft 
                  Here we present Dreamer, a general algorithm that outperforms                                   from scratch.
               specialized expert algorithms across a wide range of domains while 
               using fixed hyperparameters, making reinforcement learning read-                                   Learning algorithm
               ily applicable to new problems. The algorithm is based on the idea of 
                                                                                                                                                                                                     22,23
               learning a world model that equips the agent with rich perception and                              We present the third generation of the Dreamer algorithm                                . The 
                                                          15–17
               the ability to imagine the future              . As shown in Fig. 1, the world model               algorithm consists of three neural networks: the world model predicts 
               predicts the outcomes of potential actions, a critic neural network                                the outcomes of potential actions, the critic judges the value of each 
               judges the value of each outcome and an actor neural network chooses                               outcome, and the actor chooses actions to reach the most valuable 
               1                                            2
                                                                                                              ✉
                Google DeepMind, San Francisco, CA, USA.  University of Toronto, Toronto, Ontario, Canada.  e-mail: mail@danijar.com
                                                                                                                                                            Nature | Vol 640 | 17 April 2025 | 647
