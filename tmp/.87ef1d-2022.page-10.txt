           Published as a conference paper at ICLR 2022
           ETHICS
           The ability to memorize large databases of facts could have potential ramiﬁcations for society,
           especially if those databases include sensitive personal information or copyrighted works. However,
           one advantage of using an external memory is that the memory can be easily cleared of all such
           information, as we do at the end of each document that we train on. The same is not true of
           differentiable model parameters, which is what most existing architectures use to store facts and
           information that they are trained on.
           REPRODUCIBILITY
           Details of our architecture and training hyperparameters are given in Section 4.2. The datasets
           for C4 and PG-19 are publicly available. Our additional datasets, Github, Isabelle, and ArXiv
           Matharederived from publicly available data buckets, which we link in the main part of the paper.
           Subsection 4.1 include details on how we constructed the datasets from those datasets. We plan to
           release our code as open source.
           REFERENCES
           Joshua Ainslie, Santiago Ontañón, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham,
            Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC: encoding long and structured
            inputs in transformers. In EMNLP, 2020.
           Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,
            Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis
            with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/
            2108.07732.
           Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
            CoRR,abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.
           James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
            Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
            Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
            http://github.com/google/jax.
           TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah, Jared Kaplan, Prafulla Dhariwal,
            Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
            Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
            Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
            Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
            Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020.
           Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
            Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,
            Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,
            Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
            Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios
            Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,
            Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
            Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,
            Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob
            McGrew,DarioAmodei,SamMcCandlish,IlyaSutskever, and Wojciech Zaremba. Evaluating
            large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.
            org/abs/2107.03374.
           Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,
            TamásSarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Ben-
            jamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In
            ICLR, 2021.
                               10
