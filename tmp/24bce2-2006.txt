                 LETTER                                                 CommunicatedbyYannLeCun
                 AFastLearningAlgorithmforDeepBeliefNets
                 GeoffreyE.Hinton
                 hinton@cs.toronto.edu
                 SimonOsindero
                 osindero@cs.toronto.edu
                 Department of Computer Science, University of Toronto, Toronto, Canada M5S 3G4
                 Yee-WhyeTeh
                 tehyw@comp.nus.edu.sg
                 Department of Computer Science, National University of Singapore,
                 Singapore 117543
                 Weshowhowtouse“complementarypriors”toeliminatetheexplaining-
                 awayeffectsthatmakeinferencedifﬁcultindenselyconnectedbeliefnets
                 that have many hidden layers. Using complementary priors, we derive a
                 fast, greedy algorithm that can learn deep, directed belief networks one
                 layer at a time, provided the top two layers form an undirected associa-
                 tive memory. The fast, greedy algorithm is used to initialize a slower
                 learning procedure that ﬁne-tunes the weights using a contrastive ver-
                 sionofthewake-sleepalgorithm.Afterﬁne-tuning,anetworkwiththree
                 hidden layers forms a very good generative model of the joint distribu-
                 tion of handwritten digit images and their labels. This generative model
                 gives better digit classiﬁcation than the best discriminative learning al-
                 gorithms. The low-dimensional manifolds on which the digits lie are
                 modeled by long ravines in the free-energy landscape of the top-level
                 associative memory, and it is easy to explore these ravines by using the
                 directedconnectionstodisplaywhattheassociativememoryhasinmind.
                 1 Introduction
                 Learning is difﬁcult in densely connected, directed belief nets that have
                 manyhiddenlayersbecause it is difﬁcult to infer the conditional distribu-
                 tion of the hidden activities when given a data vector. Variational methods
                 use simple approximations to the true conditional distribution, but the ap-
                 proximations may be poor, especially at the deepest hidden layer, where
                 thepriorassumesindependence.Also,variationallearningstillrequiresall
                 of the parameters to be learned together and this makes the learning time
                 scale poorly as the number of parameters increases.
                     Wedescribe a model in which the top two hidden layers form an undi-
                 rected associative memory (see Figure 1) and the remaining hidden layers
                 Neural Computation 18, 1527–1554 (2006)   C 2006 Massachusetts Institute of Technology
              1528                                 G.Hinton,S.Osindero,andY.-W.Teh
                                     2000 top-level units
                              10 label units           500 units 
                         This could be the             500 units 
                         top level of 
                         another sensory 
                         pathway                         28 x 28    
                                                         pixel 
                                                         image
              Figure 1: The network used to model the joint distribution of digit images and
              digit labels. In this letter, each training case consists of an image and an explicit
              class label, but work in progress has shown that the same learning algorithm
              can be used if the “labels” are replaced by a multilayer pathway whose inputs
              are spectrograms from multiple different speakers saying isolated digits. The
              networkthenlearnstogeneratepairsthatconsistofanimageandaspectrogram
              of the same digit class.
              formadirectedacyclic graph that converts the representations in the asso-
              ciative memory into observable variables such as the pixels of an image.
              This hybrid model has some attractive features:
                  There is a fast, greedy learning algorithm that can ﬁnd a fairly good
                    set of parameters quickly, even in deep networks with millions of
                  parameters and manyhiddenlayers.
                    Thelearningalgorithmisunsupervisedbutcanbeappliedtolabeled
                  data by learning a model that generates both the label and the data.
                    There is a ﬁne-tuning algorithm that learns an excellent genera-
                    tive model that outperforms discriminative methods on the MNIST
                  database of hand-written digits.
                    The generative model makes it easy to interpret the distributed rep-
                    resentations in the deep hidden layers.
             AFastLearningAlgorithmforDeepBeliefNets                     1529
                 Theinferencerequiredforformingaperceptisbothfastandaccurate.
                 The learning algorithm is local. Adjustments to a synapse strength
                 dependononlythestatesofthepresynapticandpostsynapticneuron.
                  The communication is simple. Neurons need only to communicate
                  their stochastic binary states.
               Section 2 introduces the idea of a “complementary” prior that exactly
             cancels the “explaining away” phenomenon that makes inference difﬁcult
             in directed models. An example of a directed belief network with com-
             plementary priors is presented. Section 3 shows the equivalence between
             restricted Boltzmann machines and inﬁnite directed networks with tied
             weights.
               Section 4 introduces a fast, greedy learning algorithm for constructing
             multilayerdirectednetworksonelayeratatime.Usingavariationalbound,
             it shows that as each new layer is added, the overall generative model
             improves. The greedy algorithm bears some resemblance to boosting in
             its repeated use of the same “weak” learner, but instead of reweighting
             each data vector to ensure that the next step learns something new, it re-
             represents it. The “weak” learner that is used to construct deep directed
             nets is itself an undirected graphical model.
               Section 5 shows how the weights produced by the fast, greedy al-
             gorithm can be ﬁne-tuned using the “up-down” algorithm. This is a
             contrastive version of the wake-sleep algorithm (Hinton, Dayan, Frey,
             & Neal, 1995) that does not suffer from the “mode-averaging” prob-
             lems that can cause the wake-sleep algorithm to learn poor recognition
             weights.
               Section 6 shows the pattern recognition performance of a network with
             three hidden layers and about 1.7 million weights on the MNIST set of
             handwrittendigits.Whennoknowledgeofgeometryisprovidedandthere
             is no special preprocessing, the generalization performance of the network
             is 1.25% errors on the 10,000-digit ofﬁcial test set. This beats the 1.5%
             achieved by the best backpropagation nets when they are not handcrafted
             for this particular application. It is also slightly better than the 1.4% errors
             reportedbyDecosteandSchoelkopf(2002)forsupportvectormachineson
             the same task.
               Finally, section 7 shows what happens in the mind of the network when
             it is running without being constrained by visual input. The network has a
             fullgenerativemodel,soitiseasytolookintoitsmind—wesimplygenerate
             animagefromitshigh-levelrepresentations.
               Throughout the letter, we consider nets composed of stochastic binary
             variables, but the ideas can be generalized to other models in which the log
             probability of a variable is an additive function of the states of its directly
             connectedneighbors(seeappendixAfordetails).
       1530               G.Hinton,S.Osindero,andY.-W.Teh
       Figure 2: A simple logistic belief net containing two independent, rare causes
       thatbecomehighlyanticorrelatedwhenweobservethehousejumping.Thebias
       of −10 on the earthquake node means that in the absence of any observation,
       this node is e10 times more likely to be off than on. If the earthquake node is on
       and the truck node is off, the jump node has a total input of 0, which means
       that it has an even chance of being on. This is a much better explanation of
       the observation that the house jumped than the odds of e−20, which apply if
       neither of the hidden causes is active. But it is wasteful to turn on both hidden
       causes to explain the observation because the probability of both happening is
       e−10 ×e−10 = e−20. When the earthquake node is turned on, it “explains away”
       the evidence for the truck node.
       2 ComplementaryPriors
       The phenomenon of explaining away (illustrated in Figure 2) makes in-
       ference difﬁcult in directed belief nets. In densely connected networks, the
       posteriordistributionoverthehiddenvariablesisintractableexceptinafew
       special cases, such as mixture models or linear models with additive gaus-
       sian noise. Markov chain Monte Carlo methods (Neal, 1992) can be used
       to sample from the posterior, but they are typically very time-consuming.
       Variational methods (Neal & Hinton, 1998) approximate the true posterior
       withamoretractabledistribution,andtheycanbeusedtoimprovealower
       boundonthelogprobabilityofthetrainingdata.Itiscomfortingthatlearn-
       ing is guaranteed to improve a variational bound even when the inference
       of the hidden states is done incorrectly, but it would be much better to ﬁnd
       a way of eliminating explaining away altogether, even in models whose
       hiddenvariableshavehighlycorrelatedeffectsonthevisiblevariables.Itis
       widelyassumedthatthisisimpossible.
         Alogistic belief net (Neal, 1992) is composed of stochastic binary units.
       Whenthenetisusedtogenerate data, the probability of turning on unit i
       is a logistic function of the states of its immediate ancestors, j, and of the
              AFastLearningAlgorithmforDeepBeliefNets                           1531
              weights, w , on the directed connections from the ancestors:
                        ij
                    p(s = 1) =           1          ,                         (2.1)
                      i        1+exp −b − s w
                                          i    j j  ij
              where bi is the bias of unit i. If a logistic belief net has only one hidden
              layer, the prior distribution over the hidden variables is factorial because
              their binary states are chosen independently when the model is used to
              generatedata.Thenonindependenceintheposteriordistributioniscreated
              by the likelihood term coming from the data. Perhaps we could eliminate
              explaining away in the ﬁrst hidden layer by using extra hidden layers to
              createa“complementary”priorthathasexactlytheoppositecorrelationsto
              those in the likelihood term. Then, when the likelihood term is multiplied
              by the prior, we will get a posterior that is exactly factorial. It is not at
              all obvious that complementary priors exist, but Figure 3 shows a simple
              exampleofaninﬁnitelogisticbeliefnetwithtiedweightsinwhichthepriors
              arecomplementaryateveryhiddenlayer(seeappendixAforamoregeneral
              treatment of the conditions under which complementary priors exist). The
              useoftiedweightstoconstructcomplementarypriorsmayseemlikeamere
              trickformakingdirectedmodelsequivalenttoundirectedones.Asweshall
              see, however, it leads to a novel and very efﬁcient learning algorithm that
              worksbyprogressivelyuntyingtheweightsineachlayerfromtheweights
              in higher layers.
                 2.1 An Inﬁnite Directed Model with Tied Weights. We can generate
              data from the inﬁnite directed net in Figure 3 by starting with a random
                                                            1
              conﬁguration at an inﬁnitely deep hidden layer and then performing a
              top-down “ancestral” pass in which the binary state of each variable in a
              layerischosenfromtheBernoullidistributiondeterminedbythetop-down
              input coming from its active parents in the layer above. In this respect, it
              is just like any other directed acyclic belief net. Unlike other directed nets,
              however,wecansamplefromthetrueposteriordistributionoverallofthe
              hidden layers by starting with a data vector on the visible units and then
              using the transposed weight matrices to infer the factorial distributions
              over each hidden layer in turn. At each hidden layer, we sample from the
              factorial posterior before computing the factorial posterior for the layer
              above.2 Appendix A shows that this procedure gives unbiased samples
                 1 ThegenerationprocessconvergestothestationarydistributionoftheMarkovchain,
              so weneedtostartatalayerthatisdeepcomparedwiththetimeittakesforthechainto
              reach equilibrium.
                 2 This is exactly the same as the inference procedure used in the wake-sleep algorithm
              (Hintonetal.,1995)butforthemodelsdescribedinthisletternovariationalapproximation
              is required because the inference procedure gives unbiased samples.
              1532                                G.Hinton,S.Osindero,andY.-W.Teh
                                              etc.
                                      WT               W
                                             V2 v2
                                                     i
                                       W               WT
                                             H1 h1
                                                      j
                                      WT               W
                                             V1 v1
                                                     i
                                       W               WT
                                             H0 h0
                                                     j
                                      WT               W
                                             V0 v0
                                                     i
              Figure3: Aninﬁnitelogisticbeliefnetwithtiedweights.Thedownwardarrows
              represent the generative model. The upward arrows are not part of the model.
              Theyrepresenttheparametersthatareusedtoinfersamplesfromtheposterior
              distribution at each hidden layer of the net when a data vector is clamped
              onV.
                  0
              because the complementary prior at each layer ensures that the posterior
              distribution really is factorial.
                 Sincewecansamplefromthetrueposterior,wecancomputethederiva-
              tivesofthelogprobabilityofthedata.Letusstartbycomputingthederiva-
              tive for a generative weight, w00, from a unit j in layer H to unit i in layer
                                          ij                      0
              V (see Figure 3). In a logistic belief net, the maximum likelihood learning
               0
              rule for a single data vector, v0,is
                    ∂ log p(v0)         
                              = h0 v0 −vˆ0 ,                                  (2.2)
                         00       j i    i
                      ∂w
                         ij
              where · denotes an average over the sampled states and vˆ0 is the proba-
                                                                     i
              bility that unit i would be turned on if the visible vector was stochastically
                         AFastLearningAlgorithmforDeepBeliefNets                                                                               1533
                         reconstructed from the sampled hidden states. Computing the posterior
                         distribution over the second hidden layer, V , from the sampled binary
                                                                                                       1
                         states in the ﬁrst hidden layer, H , is exactly the same process as recon-
                                                                                   0
                         structing the data, so v1 is a sample from a Bernoulli random variable with
                                              0                i
                         probability vˆ . The learning rule can therefore be written as
                                              i
                                   ∂ log p(v0)                            
                                                      = h0 v0 −v1 .                                                                            (2.3)
                                             00              j    i       i
                                        ∂w
                                             ij
                         Thedependenceofv1 onh0 isunproblematicinthederivationofequation
                                                            i         j
                         2.3 from equation 2.2 because vˆ0 is an expectation that is conditional on h0.
                                                                             i                                                                      j
                         Since the weights are replicated, the full derivative for a generative weight
                         is obtained by summing the derivatives of the generative weights between
                         all pairs of layers:
                                   ∂ log p(v0)                                                                      
                                                     = h0 v0 −v1 + v1 h0 −h1 + h1 v1 −v2 +···                                                  (2.4)
                                        ∂w                   j   i       i           i     j        j           j   i        i
                                             ij
                              All of the pairwise products except the ﬁrst and last cancel, leaving the
                         Boltzmannmachinelearningruleofequation3.1.
                         3 Restricted Boltzmann MachinesandContrastiveDivergence
                             Learning
                         It may not be immediately obvious that the inﬁnite directed net in
                         Figure 3 is equivalent to a restricted Boltzmann machine (RBM). An RBM
                         has a single layer of hidden units that are not connected to each other and
                         have undirected, symmetrical connections to a layer of visible units. To
                         generate data from an RBM, we can start with a random state in one of
                         the layers and then perform alternating Gibbs sampling. All of the units
                         in one layer are updated in parallel given the current states of the units
                         in the other layer, and this is repeated until the system is sampling from
                         its equilibrium distribution. Notice that this is exactly the same process as
                         generating data from the inﬁnite belief net with tied weights. To perform
                         maximum likelihood learning in an RBM, we can use the difference be-
                         tween two correlations. For each weight, w , between a visible unit i and
                                                                                                  ij          
                         a hidden unit, j, we measure the correlation v0h0 when a data vector is
                                                                                                          i   j
                         clamped on the visible units and the hidden states are sampled from their
                         conditional distribution, which is factorial. Then, using alternating Gibbs
                         sampling, we run the Markov chain shown in Figure 4 until it reaches its
                                                                                                                 ∞ ∞
                         stationarydistributionandmeasurethecorrelation v h                                                . Thegradientof
                                                                                                                  i     j
                         the log probability of the training data is then
                                   ∂ log p(v0)                                  
                                                      = v0h0 − v∞h∞ .                                                                          (3.1)
                                        ∂w                   i   j         i    j
                                             ij
                             1534                                                                       G.Hinton,S.Osindero,andY.-W.Teh
                                                               t = 0                 t = 1                  t = 2                       t = infinity
                                                                   j                       j                      j                             j
                                            <v0h0>                                                                                         <v∞h∞>
                                                   i     j                                                                                        i     j
                                                       i                      i                      i                              i
                                                                                                                                 t = infinity
                             Figure 4: This depicts a Markov chain that uses alternating Gibbs sampling.
                             In one full step of Gibbs sampling, the hidden units in the top layer are all
                             updatedinparallelbyapplyingequation2.1totheinputsreceivedfromthethe
                             current states of the visible units in the bottom layer; then the visible units are
                             all updated in parallel given the current hidden states. The chain is initialized
                             by setting the binary states of the visible units to be the same as a data vector.
                             The correlations in the activities of a visible and a hidden unit are measured
                             after the ﬁrst update of the hidden units and again at the end of the chain. The
                             difference of these two correlations provides the learning signal for updating
                             the weight on the connection.
                             This learning rule is the same as the maximum likelihood learning rule
                             for the inﬁnite logistic belief net with tied weights, and each step of Gibbs
                             sampling corresponds to computing the exact posterior distribution in a
                             layer of the inﬁnite logistic belief net.
                                   Maximizingthelogprobabilityofthedataisexactlythesameasminimiz-
                             ingtheKullback-Leiblerdivergence, KL(P0||P∞),betweenthedistribution
                                                                                                                       θ
                             of the data, P0, and the equilibrium distribution deﬁned by the model, P∞.
                                                                                                                                                                         θ
                             Incontrastivedivergencelearning(Hinton,2002),weruntheMarkovchain
                             foronlynfullstepsbeforemeasuringthesecondcorrelation.3 Thisisequiv-
                             alent to ignoring the derivatives that come from the higher layers of the
                             inﬁnite net. The sum of all these ignored derivatives is the derivative of the
                             log probability of the posterior distribution in layer V , which is also the
                                                                                                                                        n
                             derivative of the Kullback-Leibler divergence between the posterior dis-
                             tribution in layer V , Pn, and the equilibrium distribution deﬁned by the
                                                                   n      θ
                             model. So contrastive divergence learning minimizes the difference of two
                             Kullback-Leibler divergences:
                                                0 ∞                        n ∞
                                        KLP P                    −KLP P .                                                                                          (3.2)
                                                           θ                      θ      θ
                                   Ignoringsamplingnoise,thisdifferenceisnevernegativebecauseGibbs
                             sampling is used to produce Pn from P0, and Gibbs sampling always re-
                                                                                          θ
                             ducestheKullback-Leiblerdivergencewiththeequilibriumdistribution.It
                                   3 Each full step consists of updating h given v,thenupdatingv given h.
             AFastLearningAlgorithmforDeepBeliefNets                    1535
             is important to notice that Pn depends on the current model parameters,
                                  n  θ
             and the way in which P changes as the parameters change is being ig-
                                  θ
             noredbycontrastivedivergencelearning.Thisproblemdoesnotarisewith
             P0 because the training data do not depend on the parameters. An empiri-
             cal investigation of the relationship between the maximum likelihood and
             thecontrastivedivergencelearningrulescanbefoundinCarreira-Perpinan
             andHinton(2005).
               Contrastive divergence learning in a restricted Boltzmann machine is
             efﬁcient enough to be practical (Mayraz & Hinton, 2001). Variations that
             usereal-valuedunitsanddifferentsamplingschemesaredescribedinTeh,
             Welling, Osindero, and Hinton (2003) and have been quite successful for
             modelingtheformationoftopographicmaps(Welling,Hinton,&Osindero,
             2003) for denoising natural images (Roth & Black, 2005) or images of bio-
             logical cells (Ning et al., 2005). Marks and Movellan (2001) describe a way
             of using contrastive divergence to perform factor analysis and Welling,
             Rosen-Zvi, and Hinton (2005) show that a network with logistic, binary
             visible units and linear, gaussian hidden units can be used for rapid doc-
             ument retrieval. However, it appears that the efﬁciency has been bought
             at a high price: When applied in the obvious way, contrastive divergence
             learning fails for deep, multilayer networks with different weights at each
             layer because these networks take far too long even to reach conditional
             equilibriumwithaclampeddatavector.Wenowshowthattheequivalence
             betweenRBMsandinﬁnitedirectednetswithtiedweightssuggestsanef-
             ﬁcientlearningalgorithmformultilayernetworksinwhichtheweightsare
             not tied.
             4 AGreedyLearningAlgorithmforTransformingRepresentations
             Anefﬁcientwaytolearnacomplicatedmodelistocombineasetofsimpler
             modelsthatarelearnedsequentially.Toforceeachmodelinthesequenceto
             learn something different from the previous models, the data are modiﬁed
             insomewayaftereachmodelhasbeenlearned.Inboosting(Freund,1995),
             each model in the sequence is trained on reweighted data that emphasize
             the cases that the preceding models got wrong. In one version of principal
             componentsanalysis,thevarianceinamodeleddirectionisremoved,thus
             forcingthenextmodeleddirectiontolieintheorthogonalsubspace(Sanger,
             1989). In projection pursuit (Friedman & Stuetzle, 1981), the data are trans-
             formedbynonlinearlydistortingonedirectioninthedataspacetoremove
             all nongaussianity in that direction. The idea behind our greedy algorithm
             is to allow each model in the sequence to receive a different representation
             of the data. The model performs a nonlinear transformation on its input
             vectors and produces as output the vectors that will be used as input for
             the next model in the sequence.
               Figure 5 shows a multilayer generative model in which the top two
             layers interact via undirected connections and all of the other connections
             1536                            G.Hinton,S.Osindero,andY.-W.Teh
             Figure 5: A hybrid network. The top two layers have undirected connections
             and form an associative memory. The layers below have directed, top-down
             generativeconnectionsthatcanbeusedtomapastateoftheassociativememory
             toanimage.Therearealsodirected,bottom-uprecognitionconnectionsthatare
             usedtoinferafactorial representation in one layer from the binary activities in
             the layer below. In the greedy initial learning, the recognition connections are
             tied to the generative connections.
             aredirected.Theundirectedconnectionsatthetopareequivalenttohaving
             inﬁnitely many higher layers with tied weights. There are no intralayer
             connections, and to simplify the analysis, all layers have the same number
             of units. It is possible to learn sensible (though not optimal) values for the
             parametersW0byassumingthattheparametersbetweenhigherlayerswill
             be used to construct a complementary prior for W0. This is equivalent to
             assuming that all of the weight matrices are constrained to be equal. The
             task of learning W0 under this assumption reduces to the task of learning
             anRBM,andalthoughthisisstilldifﬁcult,goodapproximatesolutionscan
             befoundrapidlybyminimizingcontrastivedivergence.OnceW0 hasbeen
             learned, the data can be mapped through WT to create higher-level “data”
             at the ﬁrst hidden layer.            0
               If the RBMisaperfectmodeloftheoriginaldata,thehigher-level“data”
             willalreadybemodeledperfectlybythehigher-levelweightmatrices.Gen-
             erally,however,theRBMwillnotbeabletomodeltheoriginaldataperfectly,
             and we can make the generative model better using the following greedy
             algorithm:
             AFastLearningAlgorithmforDeepBeliefNets                    1537
               1. Learn W0 assumingall the weight matrices are tied.
               2. Freeze W0 and commit ourselves to using WT to infer factorial ap-
                                                        0
                  proximateposteriordistributionsoverthestatesofthevariablesinthe
                  ﬁrsthiddenlayer,evenifsubsequentchangesinhigher-levelweights
                  meanthatthisinferencemethodisnolongercorrect.
               3. Keeping all the higher-weight matrices tied to each other, but untied
                  from W0, learn an RBM model of the higher-level “data” that was
                  producedbyusingWT totransformtheoriginaldata.
                                    0
               If this greedy algorithm changes the higher-level weight matrices, it
             is guaranteed to improve the generative model. As shown in Neal and
             Hinton(1998),thenegativelogprobabilityofasingledatavector,v0,under
             the multilayer generative model is bounded by a variational free en-
             ergy, which is the expected energy under the approximating distribution,
             Q(h0|v0), minus the entropy of that distribution. For a directed model, the
             “energy”oftheconﬁgurationv0,h0 isgivenby
                 E(v0,h0) =−[log p(h0)+log p(v0|h0)],                  (4.1)
             so the bound is
                 log p(v0) ≥  Q(h0|v0)[log p(h0)+log p(v0|h0)]
                            allh0
                           − Q(h0|v0)logQ(h0|v0),                     (4.2)
                             allh0
             whereh0isabinaryconﬁgurationoftheunitsintheﬁrsthiddenlayer, p(h0)
             is the prior probability of h0 under the current model (which is deﬁned by
             the weights above H ), and Q(·|v0) is any probability distribution over
                               0
             the binary conﬁgurations in the ﬁrst hidden layer. The bound becomes an
             equality if and only if Q(·|v0) is the true posterior distribution.
               Whenalloftheweightmatrices are tied together, the factorial distribu-
             tionover H producedbyapplyingWT toadatavectoristhetrueposterior
                      0                     0
             distribution, so at step 2 of the greedy algorithm, log p(v0) is equal to the
             bound.Step2freezesboth Q(·|v0)andp(v0|h0),andwiththesetermsﬁxed,
             the derivative of the bound is the same as the derivative of
                  Q(h0|v0)logp(h0).                                   (4.3)
                 allh0
             So maximizing the bound with respect to the weights in the higher layers
             is exactly equivalent to maximizing the log probability of a data set in
             whichh0 occurswithprobability Q(h0|v0). If the bound becomes tighter, it
              1538                                G.Hinton,S.Osindero,andY.-W.Teh
              is possible for log p(v0) to fall even though the lower bound on it increases,
              butlog p(v0)canneverfallbelowitsvalueatstep2ofthegreedyalgorithm
              because the bound is tight at this point and the bound always increases.
                 Thegreedyalgorithmcanclearlybeappliedrecursively,soifweusethe
              full maximum likelihood Boltzmann machine learning algorithm to learn
              each set of tied weights and then we untie the bottom layer of the set from
              the weights above, we can learn the weights one layer at a time with a
              guarantee that we will never decrease the bound on the log probability of
              thedataunderthemodel.4 Inpractice,wereplacethemaximumlikelihood
              Boltzmannmachinelearningalgorithmbycontrastivedivergencelearning
              becauseitworkswellandismuchfaster.Theuseofcontrastivedivergence
              voids the guarantee, but it is still reassuring to know that extra layers
              are guaranteed to improve imperfect models if we learn each layer with
              sufﬁcient patience.
                 Toguaranteethatthegenerativemodelisimprovedbygreedilylearning
              morelayers, it is convenient to consider models in which all layers are the
              same size so that the higher-level weights can be initialized to the values
              learned before they are untied from the weights in the layer below. The
              samegreedyalgorithm, however, can be applied even when the layers are
              different sizes.
              5 Back-Fitting with the Up-Down Algorithm
              Learningtheweightmatricesonelayeratatimeisefﬁcientbutnotoptimal.
              Once the weights in higher layers have been learned, neither the weights
              nor the simple inference procedure are optimal for the lower layers. The
              suboptimality produced by greedy learning is relatively innocuous for su-
              pervisedmethodslikeboosting.Labelsareoftenscarce,andeachlabelmay
              provide only a few bits of constraint on the parameters, so overﬁtting is
              typically more of a problem than underﬁtting. Going back and reﬁtting the
              earlier models may therefore cause more harm than good. Unsupervised
              methods, however, can use very large unlabeled data sets, and each case
              maybeveryhigh-dimensional, thus providing many bits of constraint on
              a generative model. Underﬁtting is then a serious problem, which can be
              alleviated by a subsequent stage of back-ﬁtting in which the weights that
              were learned ﬁrst are revised to ﬁt in better with the weights that were
              learned later.
                 Aftergreedilylearninggoodinitialvaluesfortheweightsineverylayer,
              we untie the “recognition” weights that are used for inference from the
              “generative” weights that deﬁne the model, but retain the restriction that
              theposteriorineachlayermustbeapproximatedbyafactorialdistribution
              in which the variables within a layer are conditionally independent given
                 4 The guarantee is on the expected change in the bound.
              AFastLearningAlgorithmforDeepBeliefNets                           1539
              the values of the variables in the layer below. A variant of the wake-sleep
              algorithm described in Hinton et al. (1995) can then be used to allow the
              higher-levelweightstoinﬂuencethelower-levelones.Inthe“up-pass,”the
              recognition weights are used in a bottom-up pass that stochastically picks
              a state for every hidden variable. The generative weights on the directed
              connections are then adjusted using the maximum likelihood learning rule
                             5
              in equation 2.2. The weights on the undirected connections at the top
              level are learned as before by ﬁtting the top-level RBM to the posterior
              distribution of the penultimate layer.
                 The “down-pass” starts with a state of the top-level associative mem-
              oryandusesthetop-downgenerativeconnectionstostochasticallyactivate
              each lower layer in turn. During the down-pass, the top-level undirected
              connectionsandthegenerativedirectedconnectionsarenotchanged.Only
              the bottom-up recognition weights are modiﬁed. This is equivalent to the
              sleep phase of the wake-sleep algorithm if the associative memory is al-
              lowed to settle to its equilibrium distribution before initiating the down-
              pass. But if the associative memory is initialized by an up-pass and then
              only allowed to run for a few iterations of alternating Gibbs sampling be-
              fore initiating the down-pass, this is a “contrastive” form of the wake-sleep
              algorithm that eliminates the need to sample from the equilibrium distri-
              bution of the associative memory. The contrastive form also ﬁxes several
              other problems of the sleep phase. It ensures that the recognition weights
              arebeinglearnedforrepresentationsthatresemblethoseusedforrealdata,
              and it also helps to eliminate the problem of mode averaging. If, given a
              particulardatavector,thecurrentrecognitionweightsalwayspickapartic-
              ularmodeatthelevelaboveandignoreotherverydifferentmodesthatare
              equally goodatgeneratingthedata,thelearninginthedown-passwillnot
              trytoalterthoserecognitionweightstorecoveranyoftheothermodesasit
              wouldif the sleep phase used a pure ancestral pass. A pure ancestral pass
              wouldhavetostart by using prolonged Gibbs sampling to get an equilib-
              rium sample from the top-level associative memory. By using a top-level
              associative memory, we also eliminate a problem in the wake phase: inde-
              pendenttop-level units seem to be required to allow an ancestral pass, but
              theymeanthatthevariationalapproximationisverypoorforthetoplayer
              of weights.
                 Appendix B speciﬁes the details of the up-down algorithm using
              MATLAB-style pseudocode for the network shown in Figure 1. For sim-
              plicity, there is no penalty on the weights, no momentum, and the same
              learning rate for all parameters. Also, the training data are reduced to a
              single case.
                 5 Becauseweightsarenolongertiedtotheweightsabovethem,vˆ0 mustbecomputed
                                                                    i
              usingthestatesofthevariablesinthelayerabovei andthegenerativeweightsfromthese
              variables to i.
              1540                                G.Hinton,S.Osindero,andY.-W.Teh
              6 PerformanceontheMNISTDatabase
                 6.1 Training the Network. The MNIST database of handwritten digits
              contains 60,000 training images and 10,000 test images. Results for many
              different pattern recognition techniquesarealreadypublishedforthispub-
              licly available database,soitisidealforevaluatingnewpatternrecognition
              methods. For the basic version of the MNIST learning task, no knowledge
              of geometry is provided, and there is no special preprocessing or enhance-
              mentofthetraining set, so an unknown but ﬁxed random permutation of
              the pixels would not affect the learning algorithm. For this “permutation-
              invariant” version of the task, the generalization performance of our net-
              work was 1.25% errors on the ofﬁcial test set. The network shown in
              Figure 1 was trained on 44,000 of the training images that were divided
              into 440 balanced mini-batches, each containing 10 examples of each digit
              class.6 The weights were updated after each mini-batch.
                 Intheinitialphaseoftraining,thegreedyalgorithmdescribedinsection4
              was used to train each layer of weights separately, starting at the bot-
              tom. Each layer was trained for 30 sweeps through the training set (called
              “epochs”). During training, the units in the “visible” layer of each RBM
              had real-valued activities between 0 and 1. These were the normalized
              pixel intensities when learning the bottom layer of weights. For training
              higherlayersofweights,thereal-valuedactivitiesofthevisibleunitsinthe
              RBMweretheactivationprobabilitiesofthehiddenunitsinthelower-level
              RBM. The hidden layer of each RBM used stochastic binary values when
              thatRBMwasbeingtrained.Thegreedytrainingtookafewhoursperlayer
              in MATLABona3GHzXeonprocessor,andwhenitwasdone,theerror
              rate on the test set was 2.49% (see below for details of how the network is
              tested).
                 Whentrainingthetoplayerofweights(theonesintheassociativemem-
              ory), the labels were provided as part of the input. The labels were repre-
              sented by turning on one unit in a “softmax” group of 10 units. When the
              activities in this group were reconstructed from the activities in the layer
              above, exactly one unit was allowed to be active, and the probability of
              picking unit i was given by
                          exp(x )
                    p =       i   ,                                           (6.1)
                     i      exp(x )
                           j     j
              where x is the total input received by unit i. Curiously, the learning rules
                      i
              are unaffected by the competition between units in a softmax group, so the
                 6 Preliminary experiments with 16×16 images of handwritten digits from the USPS
              databaseshowedthatagoodwaytomodelthejointdistributionofdigitimagesandtheir
              labels was to use an architecture of this type, but for 16 × 16 images, only three-ﬁfths as
              manyunitswereusedineachhiddenlayer.
              AFastLearningAlgorithmforDeepBeliefNets                           1541
              synapses do not need to know which unit is competing with which other
              unit. The competition affects the probability of a unit turning on, but it is
              only this probability that affects the learning.
                 Afterthegreedylayer-by-layertraining,thenetworkwastrained,witha
              different learning rate and weightdecay,for300epochsusingtheup-down
              algorithmdescribedinsection5.Thelearningrate,momentum,andweight
                   7
              decay werechosenbytrainingthenetworkseveraltimesandobservingits
              performance on a separate validation set of 10,000 images that were taken
              from the remainder of the full training set. For the ﬁrst 100 epochs of the
              up-down algorithm, the up-pass was followed by three full iterations of
              alternating Gibbs sampling in the associative memory before performing
              thedown-pass.Forthesecond100epochs,6iterationswereperformed,and
              forthelast100epochs,10iterationswereperformed.Eachtimethenumber
              of iterations of Gibbs sampling was raised, the error on the validation set
              decreased noticeably.
                 The network that performed best on the validation set was tested and
              had an error rate of 1.39%. This network was then trained on all 60,000
                             8
              training images until its error rate on the full training set was as low as
              its ﬁnal error rate had been on the initial training set of 44,000 images. This
              took a further 59 epochs, making the total learning time about a week. The
              ﬁnalnetworkhadanerrorrateof1.25%.9 Theerrorsmadebythenetwork
              are shown in Figure 6. The 49 cases that the network gets correct but for
              which the second-best probability is within 0.3 of the best probability are
              showninFigure7.
                 The error rate of 1.25% compares very favorably with the error rates
              achievedbyfeedforwardneuralnetworksthathaveoneortwohiddenlay-
              ers and are trained to optimize discrimination using the backpropagation
              algorithm (see Table 1). When the detailed connectivity of these networks
              is not handcrafted for this particular task, the best reported error rate for
              stochastic online learning with a separate squared error on each of the 10
              output units is 2.95%. These error rates can be reduced to 1.53% in a net
              withonehiddenlayerof800unitsbyusingsmallinitialweights,aseparate
              cross-entropy error function on each output unit, and very gentle learning
                 7 No attempt was made to use different learning rates or weight decays for different
              layers,andthelearningrateandmomentumwerealwayssetquiteconservativelytoavoid
              oscillations. It is highly likely that the learning speed could be considerably improved by
              amorecarefulchoiceoflearningparameters,thoughitispossiblethatthiswouldleadto
              worsesolutions.
                 8 The training set has unequal numbers of each class, so images were assigned ran-
              domlytoeachofthe600mini-batches.
                 9 To check that further learning would not have signiﬁcantly improved the error rate,
              the network was then left running with a very small learning rate and with the test error
              beingdisplayedaftereveryepoch.Aftersixweeks,thetesterrorwasﬂuctuatingbetween
              1.12% and 1.31% and was 1.18% for the epoch on which number of training errors was
              smallest.
       1542               G.Hinton,S.Osindero,andY.-W.Teh
       Figure6: The125testcasesthatthenetworkgotwrong.Eachcaseislabeledby
       the network’s guess. The true classes are arranged in standard scan order.
       (John Platt, personal communication, 2005). An almost identical result of
       1.51%wasachievedinanetthathad500unitsintheﬁrsthiddenlayerand
       300 in the second hidden layer by using “softmax” output units and a reg-
       ularizer that penalizes the squared weights by an amount carefully chosen
       usingavalidationset.Forcomparison,nearestneighborhasareportederror
       rate (http://oldmill.uchicago.edu/wilder/Mnist/) of 3.1% if all 60,000
       training cases are used (which is extremely slow) and 4.4% if 20,000 are
       used. This can be reduced to 2.8% and 4.0% by using an L3 norm.
         The only standard machine learning technique that comes close to the
       1.25%errorrateofourgenerativemodelonthebasictaskisasupportvector
       machine that gives an error rate of 1.4% (Decoste & Schoelkopf, 2002). But
       it is hard to see how support vector machines can make use of the domain-
       speciﬁctricks, like weight sharing and subsampling,whichLeCun,Bottou,
       and Haffner (1998) use to improve the performance of discriminative
             AFastLearningAlgorithmforDeepBeliefNets                     1543
             Figure 7: All 49 cases in which the network guessed right but had a second
             guesswhoseprobabilitywaswithin0.3oftheprobabilityofthebestguess.The
             true classes are arranged in standard scan order.
             neuralnetworksfrom1.5%to0.95%.Thereisnoobviousreasonwhyweight
             sharingandsubsamplingcannotbeusedtoreducetheerrorrateofthegen-
             erative model, and we are currently investigating this approach. Further
             improvementscanalwaysbeachievedbyaveragingtheopinionsofmulti-
             ple networks, but this technique is available to all methods.
               Substantial reductions in the error rate can be achieved by supplement-
             ingthedatasetwithslightlytransformedversionsofthetrainingdata.Us-
             ingone-andtwo-pixeltranslations,DecosteandSchoelkopf(2002)achieve
             0.56%. Using local elastic deformations in a convolutional neural network,
             Simard, Steinkraus, and Platt (2003) achieve 0.4%, which is slightly bet-
             ter than the 0.63% achieved by the best hand-coded recognition algorithm
             (Belongie, Malik, & Puzicha, 2002). We have not yet explored the use of
             distorted data for learning generative models because many types of dis-
             tortion need to be investigated, and the ﬁne-tuning algorithm is currently
             too slow.
                6.2 Testing the Network. One way to test the network is to use a
             stochastic up-pass from the image to ﬁx the binary states of the 500 units in
             the lower layer of the associative memory. With these states ﬁxed, the label
             units are given initial real-valued activities of 0.1, and a few iterations of
             alternating Gibbs sampling are then used to activate the correct label unit.
             This method of testing gives error rates that are almost 1% higher than the
             rates reported above.
                1544                                      G.Hinton,S.Osindero,andY.-W.Teh
                Table1: ErrorratesofVariousLearningAlgorithmsontheMNISTDigitRecog-
                nition Task.
                Version of MNIST Task                  Learning Algorithm             Test Error %
                Permutationinvariant                  Ourgenerativemodel:                1.25
                                                  784 →500→500↔2000↔10
                Permutationinvariant             Supportvectormachine:degree9            1.4
                                                        polynomialkernel
                Permutationinvariant            Backprop: 784 → 500 → 300 → 10           1.51
                                                  cross-entropy and weight-decay
                Permutationinvariant                Backprop: 784 → 800 → 10             1.53
                                                 cross-entropy and early stopping
                Permutationinvariant            Backprop: 784 → 500 → 150 → 10           2.95
                                                 squarederrorandon-lineupdates
                Permutationinvariant           Nearest neighbor: all 60,000 examples     2.8
                                                          andL3norm
                Permutationinvariant           Nearest neighbor: all 60,000 examples     3.1
                                                          andL2norm
                Permutationinvariant          Nearest neighbor: 20,000 examples and      4.0
                                                            L3norm
                Permutationinvariant          Nearest neighbor: 20,000 examples and      4.4
                                                            L2norm
                Unpermutedimages;extra             Backprop: cross-entropy and           0.4
                  data from elastic           early-stoppingconvolutionalneuralnet
                  deformations
                Unpermutedde-skewed              Virtual SVM: degree 9 polynomial        0.56
                  images; extra data from 2                  kernel
                  pixel translations
                Unpermutedimages                Shape-context features: hand-coded       0.63
                                                            matching
                Unpermutedimages;extra          BackpropinLeNet5:convolutional           0.8
                  data from afﬁne                           neural net
                  transformations
                Unpermutedimages                BackpropinLeNet5:convolutional           0.95
                                                            neural net
                    Abetter method is to ﬁrst ﬁx the binary states of the 500 units in the
                lower layer of the associative memory and to then turn on each of the
                label units in turn and compute the exact free energy of the resulting
                510-component binary vector. Almost all the computation required is in-
                dependent of which label unit is turned on (Teh & Hinton, 2001), and this
                methodcomputestheexactconditionalequilibriumdistributionoverlabels
                instead of approximating it by Gibbs sampling, which is what the previ-
                ous method is doing. This method gives error rates that are about 0.5%
                higher than the ones quoted because of the stochastic decisions made in
                the up-pass. We can remove this noise in two ways. The simpler is to make
                the up-pass deterministic by using probabilities of activation in place of
             AFastLearningAlgorithmforDeepBeliefNets                     1545
             Figure8: Eachrowshows10samplesfromthegenerativemodelwithaparticu-
             larlabelclampedon.Thetop-levelassociativememoryisrunfor1000iterations
             of alternating Gibbs sampling between samples.
             stochastic binary states. The second is to repeat the stochastic up-pass
             20 times and average either the label probabilities or the label log prob-
             abilities over the 20 repetitions before picking the best one. The two types
             of averagegivealmostidenticalresults,andtheseresultsarealsoverysim-
             ilar to using a single deterministic up-pass, which was the method usedfor
             the reported results.
             7 LookingintotheMindofaNeuralNetwork
             To generate samples from the model, we perform alternating Gibbs sam-
             plinginthetop-levelassociativememoryuntiltheMarkovchainconverges
             totheequilibriumdistribution.Thenweuseasamplefromthisdistribution
             as input to the layers below and generate an image by a single down-pass
             throughthegenerativeconnections. If we clamp the label units to a partic-
             ular class during the Gibbs sampling, we can see images from the model’s
             class-conditional distributions. Figure 8 shows a sequence of images for
             each class that were generated by allowing 1000 iterations of Gibbs sam-
             pling between samples.
               We can also initialize the state of the top two layers by providing a
             random binary image as input. Figure 9 shows how the class-conditional
             stateoftheassociativememorythenevolveswhenitisallowedtorunfreely,
             butwiththelabelclamped.Thisinternalstateis“observed”byperforming
             a down-pass every 20 iterations to see what the associative memory has
       1546               G.Hinton,S.Osindero,andY.-W.Teh
       Figure 9: Each row shows 10 samples from the generative model with a par-
       ticular label clamped on. The top-level associative memory is initialized by an
       up-passfromarandombinaryimageinwhicheachpixelisonwithaprobability
       of 0.5. The ﬁrst column shows the results of a down-pass from this initial high-
       level state. Subsequent columns are produced by 20 iterations of alternating
       Gibbssamplingintheassociativememory.
       in mind. This use of the word mind is not intended to be metaphorical.
       Webelievethat a mental state is the state of a hypothetical, external world
       in which a high-level internal representation would constitute veridical
       perception. That hypothetical world is what the ﬁgure shows.
       8Conclusion
       Wehave shown that it is possible to learn a deep, densely connected be-
       lief network one layer at a time. The obvious way to do this is to assume
       that the higher layers do not exist when learning the lower layers, but
       this is not compatible with the use of simple factorial approximations to
       replace the intractable posterior distribution. For these approximations to
       work well, we need the true posterior to be as close to factorial as pos-
       sible. So instead of ignoring the higher layers, we assume that they exist
       but have tied weights that are constrained to implement a complementary
       prior that makes the true posterior exactly factorial. This is equivalent to
       havinganundirectedmodelthatcanbelearnedefﬁcientlyusingcontrastive
       divergence. It can also be viewed as constrained variational learning be-
       cause a penalty term—the divergence between the approximate and true
             AFastLearningAlgorithmforDeepBeliefNets                     1547
             posteriors—has been replaced by the constraint that the prior must make
             the variational approximation exact.
               Aftereachlayerhasbeenlearned,itsweightsareuntiedfromtheweights
             in higher layers. As these higher-level weights change, the priors for lower
             layers cease to be complementary, so the true posterior distributions in
             lower layers are no longer factorial, and the use of the transpose of the
             generative weights for inference is no longer correct. Nevertheless, we can
             use a variational bound to show that adapting the higher-level weights
             improvestheoverallgenerativemodel.
               To demonstrate the power of our fast, greedy learning algorithm, we
             used it to initialize the weights for a much slower ﬁne-tuning algorithm
             that learns an excellent generative model of digit images and their labels. It
             isnotclearthatthisisthebestwaytousethefast,greedyalgorithm.Itmight
             bebettertoomittheﬁne-tuningandusethespeedofthegreedyalgorithm
             to learn an ensemble of larger, deeper networks or a much larger training
             set. The network in Figure 1 has about as many parameters as 0.002 cubic
             millimeters of mouse cortex (Horace Barlow, personal communication,
             1999), and several hundred networks of this complexity could ﬁt within
             asingle voxel of a high-resolution fMRI scan. This suggests that much big-
             ger networks may be required to compete with human shape recognition
             abilities.
               Ourcurrentgenerativemodelislimitedinmanyways(Lee&Mumford,
             2003).It is designedforimagesinwhichnonbinaryvaluescanbetreatedas
             probabilities (which is not the case for natural images); its use of top-down
             feedbackduringperceptionislimitedtotheassociativememoryinthetop
             two layers; it does not have a systematic way of dealing with perceptual
             invariances;itassumesthatsegmentationhasalreadybeenperformed;and
             it does not learn to sequentially attend to the most informative parts of
             objects whendiscriminationisdifﬁcult.Itdoes,however,illustratesomeof
             the major advantages of generative models as compared to discriminative
             ones:
                 Generative models can learn low-level features without requir-
                  ing feedback from the label, and they can learn many more
                  parameters than discriminative models without overﬁtting. In dis-
                  criminative learning, each training case constrains the parameters
                  only by as many bits of information as are required to specify
                  the label. For a generative model, each training case constrains
                  the parameters by the number of bits required to specify the
                 input.
                  It is easy to see what the network has learned by generating from its
                 model.
                  It is possible to interpret the nonlinear, distributed representations in
                  the deep hidden layers by generating images from them.
             1548                           G.Hinton,S.Osindero,andY.-W.Teh
                The superior classiﬁcation performance of discriminative learning
                 methods holds only for domains in which it is not possible to learn
                 a good generative model. This set of domains is being eroded by
                 Moore’slaw.
             AppendixA: ComplementaryPriors
               A.1 General Complementarity. Consider a joint distribution over ob-
             servables,x,andhiddenvariables,y.Foragivenlikelihoodfunction, P(x|y),
             we deﬁne the corresponding family of complementary priors to be those
             distributions, P(y), for which the joint distribution, P(x,y) = P(x|y)P(y),
             leadstoposteriors, P(y|x),thatexactlyfactorize,thatis,leadstoaposterior
             that can be expressed as P(y|x) = j P(yj|x).
               Not all functional forms of likelihood admit a complementary prior. In
             this appendix, we show that the following family constitutes all likelihood
             functions admitting a complementary prior,
                 P(x|y)=  1  exp	j(x,yj)+β(x)

                         (y)      j
                            	                        

                       =exp     j(x,yj)+β(x)−log(y) ,               (A.1)
                               j
             where  is the normalization term. For this assertion to hold, we need to
             assume positivity of distributions: that both P(y) > 0andP(x|y) > 0 for
             everyvalueofyandx.Thecorrespondingfamilyofcomplementarypriors
             then assumes the form
                 P(y) = 1 exp	log(y)+αj(yj)
,                       (A.2)
                        C                j
             where C is a constant to ensure normalization. This combination of func-
             tional forms leads to the following expression for the joint,
                 P(x,y) = 1 exp	j(x,yj)+β(x)+αj(yj)
.              (A.3)
                          C       j                 j
               To prove our assertion, we need to show that every likelihood func-
             tion of form equation A.1 admits a complementary prior and vice versa.
             First, it can be directly veriﬁed that equation A.2 is a complementary prior
             for the likelihood functions of equation A.1. To show the converse, let us
             assume that P(y) is a complementary prior for some likelihood function
             P(x|y). Notice that the factorial form of the posterior simply means that the
                                  AFastLearningAlgorithmforDeepBeliefNets                                                                                                                           1549
                                  joint distribution P(x,y) = P(y)P(x|y) satisﬁes the following set of condi-
                                  tional independencies: y ⊥⊥ y |x for every j 	= k. This set of conditional
                                                                                           j             k
                                  independencies corresponds exactly to the relations satisﬁed by an undi-
                                  rected graphical model having edges between every hidden and observed
                                  variable and among all observed variables. By the Hammersley-Clifford
                                  theorem and using our positivity assumption, the joint distribution must
                                  be of the form of equation A.3, and the forms for the likelihood function
                                  equation A.1 and prior equation A.2 follow from this.
                                        A.2 ComplementarityforInﬁniteStacks. Wenowconsiderasubsetof
                                  modelsoftheforminequationA.3forwhichthelikelihoodalsofactorizes.
                                  This meansthatwenowhavetwosetsofconditionalindependencies:
                                               P(x|y)=P(x|y)                                                                                                                                     (A.4)
                                                                                   i
                                                                      i
                                               P(y|x)=P(yj|x).                                                                                                                                   (A.5)
                                                                      j
                                  Thisconditionisusefulforourconstructionoftheinﬁnitestackofdirected
                                  graphical models.
                                        Identifying the conditional independencies in equations A.4 and A.5
                                  as those satisﬁed by a complete bipartite undirected graphical model, and
                                  againusingtheHammersley-Cliffordtheorem(assumingpositivity),wesee
                                  thatthefollowingformfullycharacterizesalljointdistributionsofinterest,
                                               P(x,y) = 1 exp	 (x,y )+γ(x)+α (y )
,                                                                                                           (A.6)
                                                                       Z                             i, j     i       j                    i     i                    j      j
                                                                                           i, j                                    i                          j
                                  while the likelihood functions take on the form
                                               P(x|y) = exp	 (x,y )+γ(x)−log(y)
.                                                                                                             (A.7)
                                                                                              i, j     i       j                   i     i
                                                                                    i, j                                    i
                                        Although it is not immediately obvious, the marginal distribution over
                                  the observables, x, in equation A.6 can also be expressed as an inﬁnite
                                  directed model in which the parameters deﬁning the conditional distribu-
                                  tions between layers are tied together.
                                        Anintuitivewayofvalidatingthisassertionisasfollows.Consideroneof
                                  themethodsbywhichwemightdrawsamplesfromthemarginaldistribu-
                                  tion P(x) implied by equation A.6. Starting from an arbitrary conﬁguration
                                  of y, we would iteratively perform Gibbs sampling using, in alternation,
                                  the distributions given in equations A.4 and A.5. If we run this Markov
                                  chain for long enough, then, under the mild assumption that the chain
                                  1550                                                                                      G.Hinton,S.Osindero,andY.-W.Teh
                                  mixes properly, we will eventually obtain unbiased samples from the joint
                                  distribution given in equation A.6.
                                         Nowlet us imagine that we unroll this sequence of Gibbs updates in
                                  space, suchthatweconsidereachparallelupdateofthevariablestoconsti-
                                  tutestatesofaseparatelayerinagraph.Thisunrolledsequenceofstateshas
                                  a purely directed structure (with conditional distributions taking the form
                                  of equations A.4 and A.5 and in alternation). By equivalence to the Gibbs
                                  sampling scheme, after many layers in such an unrolled graph, adjacent
                                  pairs of layers will have a joint distribution as given in equation A.6.
                                         Wecanformalizetheaboveintuitionforunrollingthegraphasfollows.
                                  Thebasicideaistounrollthegraph“upwards”(i.e.,movingawayfromthe
                                  data),sothatwecanputawell-deﬁneddistributionovertheinﬁnitestackof
                                  variables.Thenweverifysomesimplemarginalandconditionalproperties
                                  ofthejointdistributionandthusdemonstratetherequiredpropertiesofthe
                                  graphinthe“downwards”direction.
                                         Let x = x(0),y = y(0),x(1),y(1),x(2),y(2),...be a sequence (stack) of vari-
                                  ables, the ﬁrst two of which are identiﬁed as our original observed and
                                  hiddenvariable. Deﬁne the functions
                                                        
      
         1             	                        
      
         
  


                                                 f (x , y ) =                 exp                    (x,y)+                             γ (x ) +                   α (y )                         (A.8)
                                                                         Z                              i, j     i     i                    i     i                     j     j
                                                                                             i, j                                   i                          j
                                                      fx(x
)= f(x
,y
)                                                                                                                            (A.9)
                                                                            

                                                                          y
                                                      fy(y
)= f(x
,y
)                                                                                                                         (A.10)
                                                                            

                                                                          x
                                                gx(x
|y
)= f(x
,y
)/fy(y
)                                                                                                                      (A.11)
                                                gy(y
|x
)= f(x
,y
)/fx(x
),                                                                                                                     (A.12)
                                  anddeﬁneajointdistributionoveroursequenceofvariablesasfollows:
                                                           (0)       (0)              (0)         (0)
                                                   P(x ,y )= f x ,y                                                                                                                             (A.13)
                                                        (i)      (i−1)                 (i)        (i−1)
                                                P(x |y                   ) =gxx |y                                       i = 1,2,...                                                          (A.14)
                                                     P(y(i)|x(i))=gy y(i)|x(i) .                                           i = 1,2,...                                                          (A.15)
                                  We verify by induction that the distribution has the following marginal
                                  distributions:
                                                P(x(i))= fxx(i)                              i = 0,1,2,...                                                                                    (A.16)
                                                P(y(i))= fyy(i)                              i = 0,1,2,...                                                                                    (A.17)
                 AFastLearningAlgorithmforDeepBeliefNets                                           1551
                 Fori = 0thisisgivenbydeﬁnitionofthedistributioninequationA.13.For
                 i > 0, we have:
                            (i)     (i) (i−1)  (i−1)          fx(i),y(i−1)        (i−1)
                        P(x )=         P x |y        P y        =          fyy(i−1)   fy y
                                   (i−1)                            (i−1)
                                  y                                y
                                =fxx(i)                                                        (A.18)
                 andsimilarly for P(y(i)). Now we see that the following conditional distri-
                 butions also hold true:
                             (i)  (i)     (i)  (i)   (i)       (i)  (i)
                           P x |y      =P x ,y         P y      =gx x |y                         (A.19)
                           (i)  (i+1)     (i)  (i+1)    (i+1)       (i) (i+1)
                        P y |x         =P y ,x            P x       =gy y |x          .          (A.20)
                 Soourjointdistribution over the stack of variables also leads to the appro-
                 priateconditionaldistributionsfortheunrolledgraphinthe“downwards”
                 direction. Inference in this inﬁnite graph is equivalent to inference in the
                 joint distribution over the sequence of variables, that is, given x(0),wecan
                 obtain a sample from the posterior simply by sampling y(0)|x(0), x(1)|y(0),
                 y(1)|x(1),....This directly shows that our inference procedure is exact for
                 the unrolled graph.
                 AppendixB: PseudocodeforUp-DownAlgorithm
                 WenowpresentMATLAB-stylepseudocodeforanimplementationofthe
                 up-down algorithm described in section 5 and used for back-ﬁtting. (This
                 methodisacontrastive version of the wake-sleep algorithm; Hinton et al.,
                 1995.)
                     The code outlined below assumes a network of the type shown in
                 Figure 1 with visible inputs, label nodes, and three layers of hidden units.
                 Beforeapplyingtheup-downalgorithm,wewouldﬁrstperformlayer-wise
                 greedytraining as described in sections 3 and 4.
                 \% UP-DOWN ALGORITHM
                 \%
                 \% the data and all biases are row vectors.
                 \% the generative model is: lab <--> top <--> pen --> hid --> vis
                 \% the number of units in layer foo is numfoo
                 \% weight matrices have names fromlayer tolayer
                 \% "rec" is for recognition biases and "gen" is for generative
                 \% biases.
                 \% for simplicity, the same learning rate, r, is used everywhere.
       1552               G.Hinton,S.Osindero,andY.-W.Teh
       \% PERFORM A BOTTOM-UP PASS TO GET WAKE/POSITIVE PHASE
       \% PROBABILITIES AND SAMPLE STATES
       wakehidprobs = logistic(data*vishid + hidrecbiases);
       wakehidstates = wakehidprobs > rand(1, numhid);
       wakepenprobs = logistic(wakehidstates*hidpen + penrecbiases);
       wakepenstates = wakepenprobs > rand(1, numpen);
       wakeopprobs = logistic(wakepenstates*pentop + targets*labtop +
         topbiases);
       wakeopstates = wakeopprobs > rand(1, numtop);
       \% POSITIVE PHASE STATISTICS FOR CONTRASTIVE DIVERGENCE
       poslabtopstatistics = targets’ * waketopstates;
       pospentopstatistics = wakepenstates’ * waketopstates;
       \% PERFORM numCDiters GIBBS SAMPLING ITERATIONS USING THE TOP LEVEL
       \% UNDIRECTED ASSOCIATIVE MEMORY
       negtopstates = waketopstates; \% to initialize loop
       for iter=1:numCDiters
         negpenprobs = logistic(negtopstates*pentop’ + pengenbiases);
         negpenstates = negpenprobs > rand(1, numpen);
         neglabprobs = softmax(negtopstates*labtop’ + labgenbiases);
         negtopprobs = logistic(negpenstates*pentop+neglabprobs*labtop+
          topbiases);
       negtopstates = negtopprobs > rand(1, numtop));
       end;
       \% NEGATIVE PHASE STATISTICS FOR CONTRASTIVE DIVERGENCE
       negpentopstatistics = negpenstates’*negtopstates;
       neglabtopstatistics = neglabprobs’*negtopstates;
       \% STARTING FROM THE END OF THE GIBBS SAMPLING RUN, PERFORM A
       \% TOP-DOWN GENERATIVE PASS TO GET SLEEP/NEGATIVE PHASE
       \% PROBABILITIES AND SAMPLE STATES
       sleeppenstates = negpenstates;
       sleephidprobs = logistic(sleeppenstates*penhid + hidgenbiases);
       sleephidstates = sleephidprobs > rand(1, numhid);
       sleepvisprobs = logistic(sleephidstates*hidvis + visgenbiases);
       \% PREDICTIONS
       psleeppenstates = logistic(sleephidstates*hidpen + penrecbiases);
       psleephidstates = logistic(sleepvisprobs*vishid + hidrecbiases);
       pvisprobs = logistic(wakehidstates*hidvis + visgenbiases);
       phidprobs = logistic(wakepenstates*penhid + hidgenbiases);
       \% UPDATES TO GENERATIVE PARAMETERS
       hidvis = hidvis + r*poshidstates’*(data-pvisprobs);
             AFastLearningAlgorithmforDeepBeliefNets                     1553
             visgenbiases = visgenbiases + r*(data - pvisprobs);
             penhid = penhid + r*wakepenstates’*(wakehidstates-phidprobs);
             hidgenbiases = hidgenbiases + r*(wakehidstates - phidprobs);
             \% UPDATES TO TOP LEVEL ASSOCIATIVE MEMORY PARAMETERS
             labtop = labtop + r*(poslabtopstatistics-neglabtopstatistics);
             labgenbiases = labgenbiases + r*(targets - neglabprobs);
             pentop = pentop + r*(pospentopstatistics - negpentopstatistics);
             pengenbiases = pengenbiases + r*(wakepenstates - negpenstates);
             topbiases = topbiases + r*(waketopstates - negtopstates);
             \%UPDATES TO RECOGNITION/INFERENCE APPROXIMATION PARAMETERS
             hidpen = hidpen + r*(sleephidstates’*(sleeppenstates-
               psleeppenstates));
             penrecbiases = penrecbiases + r*(sleeppenstates-psleeppenstates);
             vishid = vishid + r*(sleepvisprobs’*(sleephidstates-
               psleephidstates));
             hidrecbiases = hidrecbiases + r*(sleephidstates-psleephidstates);
             Acknowledgments
             Wethank Peter Dayan, Zoubin Ghahramani, Yann Le Cun, Andriy Mnih,
             Radford Neal, Terry Sejnowski, and Max Welling for helpful discussions
             and the referees for greatly improving the manuscript. The research was
             supported by NSERC, the Gatsby Charitable Foundation, CFI, and OIT.
             G.E.H. is a fellow of the Canadian Institute for Advanced Research and
             holds a Canada Research Chair in machine learning.
             References
             Belongie, S., Malik, J., & Puzicha, J. (2002). Shape matching and object recognition
               using shape contexts. IEEE Transactions on Pattern Analysis and Machine Intelli-
               gence, 24(4), 509–522.
             Carreira-Perpinan, M. A., & Hinton, G. E. (2005). On contrastive divergence learn-
               ing. In R. G. Cowell & Z. Ghahramani (Eds.), Artiﬁcial Intelligence and Statistics,
               2005. (pp. 33–41). Fort Lauderdale, FL: Society for Artiﬁcial Intelligence and
               Statistics.
             Decoste, D., & Schoelkopf, B. (2002). Training invariant support vector machines,
               MachineLearning, 46, 161–190.
             Freund, Y. (1995). Boosting a weak learning algorithm by majority. Information and
               Computation, 12(2), 256–285.
             Friedman,J.,&Stuetzle,W.(1981).Projectionpursuitregression.JournaloftheAmer-
               ican Statistical Association, 76, 817–823.
             Hinton, G. E. (2002). Training products of experts by minimizing contrastive diver-
               gence, Neural Computation, 14(8), 1711–1800.
        1554                 G.Hinton,S.Osindero,andY.-W.Teh
        Hinton, G. E., Dayan, P., Frey, B. J., & Neal, R. (1995). The wake-sleep algorithm for
          self-organizing neural networks. Science, 268, 1158–1161.
        LeCun, Y., Bottou, L., & Haffner, P. (1998). Gradient-based learning applied to doc-
          umentrecognition. Proceedings of the IEEE, 86(11), 2278–2324.
        Lee,T.S.,&Mumford,D.(2003).HierarchicalBayesianinferenceinthevisualcortex.
          Journal of the Optical Society of America, A, 20, 1434–1448.
        Marks, T. K., & Movellan, J. R. (2001). Diffusion networks, product of experts, and
          factor analysis. In T. W. Lee, T.-P. Jung, S. Makeig, & T. J. Sejnowski (Eds.), Proc.
          Int. Conf. on Independent Component Analysis (pp. 481–485). San Diego.
        Mayraz, G., & Hinton, G. E. (2001). Recognizing hand-written digits using hier-
          archical products of experts. IEEE Transactions on Pattern Analysis and Machine
          Intelligence, 24, 189–197.
        Neal, R. (1992). Connectionist learning of belief networks, Artiﬁcial Intelligence, 56,
          71–113.
        Neal, R. M., & Hinton, G. E. (1998). A new view of the EM algorithm that justiﬁes
          incremental, sparse and other variants. In M. I. Jordan (Ed.), Learning in graphical
          models (pp. 355–368). Norwell, MA: Kluwer.
        Ning,F.,Delhomme,D.,LeCun,Y.,Piano,F.,Bottou,L.,&Barbano,P.(2005).Toward
          automaticphenotypingofdevelopingembryosfromvideos.IEEETransactionson
          Image Processing, 14(9), 1360–1371.
        Roth, S., & Black, M. J. (2005). Fields of experts: A framework for learning image
          priors. In IEEE Conf. on Computer Vision and Pattern Recognition (pp. 860–867).
          Piscataway, NJ: IEEE.
        Sanger, T. D. (1989). Optimal unsupervised learning in a single-layer linear feedfor-
          wardneuralnetworks.NeuralNetworks,2(6),459–473.
        Simard,P.Y., Steinkraus, D., & Platt, J. (2003). Best practice for convolutional neural
          networksappliedtovisualdocumentanalysis.InInternationalConferenceonDoc-
          ument Analysis and Recogntion (ICDAR) (pp. 958–962). Los Alamitos, CA: IEEE
          ComputerSociety.
        Teh, Y., & Hinton, G. E. (2001). Rate-coded restricted Boltzmann machines for face
          recognition. In T. K. Leen, T. G. Dietterich, & V. Tresp (Eds.), Advances in neural
          information processing systems, 13 (pp. 908–914). Cambridge, MA: MIT Press.
        Teh, Y., Welling, M., Osindero, S., & Hinton, G. E. (2003). Energy-based models
          for sparse overcomplete representations. Journal of Machine Learning Research, 4,
          1235–1260.
        Welling, M., Hinton, G., & Osindero, S. (2003). Learning sparse topographic rep-
          resentations with products of Student-t distributions. In S. Becker, S. Thrun,
          & K. Obermayer (Eds.), Advances in neural information processing systems, 15
          (pp. 1359–1366). Cambridge, MA: MIT Press.
        Welling, M., Rosen-Zvi, M., & Hinton, G. E. (2005). Exponential family harmoni-
          ums with an application to information retrieval. In L. K. Saul, Y. Weiss, & L.
          Bottou(Eds.),Advancesinneuralinformationprocessingsystems,17(pp.1481–1488).
          Cambridge,MA:MITPress.
        ReceivedJune8,2005;acceptedNovember8,2005.
