                                              Training data-efﬁcient image transformers & distillation through attention
                                                                                           the output of the teacher, as in a regular distillation, while
                                                                                           remaining complementary to the class embedding.
                                                                                           Fine-tuning with distillation.         Weuseboththetruelabel
                                                                                           and teacher prediction during the ﬁne-tuning stage at higher
                                                                                           resolution. We use a teacher with the same target resolution,
                                                                                           typically obtained from the lower-resolution teacher by the
                                                                                           methodofTouvronetal.(2019). We have also tested with
                                                                                           true labels only but this reduces the beneﬁt of the teacher
                                                                                           and leads to a lower performance.
                                          FFN:	residual+MLP                                Classiﬁcation with our approach: joint classiﬁers.                 At
                                            self-attention
                                                                                           test time, both the class or the distillation embeddings pro-
                                                                                           duced by the transformer are associated with linear classi-
                                                                                           ﬁers and able to infer the image label. Our referent method
                                                                                           is the late fusion of these two separate heads, for which we
                             class              patch            distillation
                             token             tokens              token                   add the softmax output by the two classiﬁers to make the
                                                                                           prediction. We evaluate these three options in Section 5.
                 Figure 2. Our distillation procedure: we simply include a new dis-
                 tillation token. It interacts with the class and patch tokens through     5. Experiments
                 the self-attention layers. This distillation token is employed in
                 a similar fashion as the class token, except that on output of the        This section presents a few analytical experiments and re-
                 network its objective is to reproduce the (hard) label predicted by       sults. We ﬁrst discuss our distillation strategy. Then we
                 the teacher, instead of true label. Both the class and distillation
                 tokens input to the transformers are learned by back-propagation.         comparatively analyze the efﬁciency and accuracy of con-
                                                                                           vnets and vision transformers.
                 true label. Let yt = argmaxcZt(c) be the hard decision                    5.1. Transformer models
                 of the teacher, the objective associated with this hard-label
                 distillation is:                                                          Asmentionedearlier, our architecture design is identical to
                                    1                      1                               the one proposed by Dosovitskiy et al. (2020) with no con-
                  LhardDistill =      L (ψ(Z ),y)+ L (ψ(Z ),y ). (3)                       volutions. Our only differences are the training strategies,
                    global          2 CE         s         2 CE          s    t
                                                                                           and the distillation token. Also we do not use a MLP head
                 For a given image, the hard label associated with the teacher             for the pre-training but only a linear classiﬁer. To avoid
                 maychangedepending on the speciﬁc data augmentation.                      any confusion, we refer to the results obtained in the prior
                 Wewillseethat this choice is better than the traditional one,             workbyViT,andpreﬁxoursbyDeiT.Ifnotspeciﬁed,DeiT
                 while being parameter-free and conceptually simpler: The                  refers to our referent model DeiT-B, which has the same
                 teacher prediction yt plays the same role as the true label y.            architecture as ViT-B. When we ﬁne-tune DeiT at a larger
                                                                                           resolution, we append the resulting operating resolution at
                 Label smoothing.         Hard labels can also be converted into           the end, e.g, DeiT-B↑384. Last, when using our distillation
                                                                                           procedure, we identify it with an alembic sign as DeiT .
                 soft labels with label smoothing (Szegedy et al., 2016),                                                                                   ⚗
                 where the true label is considered to have a probability                  TheparametersofViT-B(andthereforeofDeiT-B)areﬁxed
                 of 1−ε,andtheremainingεissharedacross the remaining                       as D = 768, h = 12 and d = D/h = 64. We introduce
                 classes. We ﬁx ε = 0.1 in our all experiments that use true               two smaller models, namely DeiT-S and DeiT-Ti, for which
                 labels. Note that we do not smooth pseudo-labels provided                 wechange the number of heads, keeping d ﬁxed. Table 1
                 bythe teacher (e.g., in hard distillation).                               summarizes the models that we consider in our paper.
                 Distillation token.      Wenowfocusonourproposal,which                    5.2. Distillation
                 is illustrated in Figure 2. We add a new token, the distillation
                 token, to the initial embeddings (patches and class token).               Ourdistillation method produces a vision transformer that
                 Our distillation token is used similarly as the class token:              becomes on par with the best convnets in terms of the trade-
                 it interacts with other embeddings through self-attention,                off between accuracy and throughput, see Table 5. Interest-
                 and is output by the network after the last layer. Its target             ingly, the distilled model outperforms its teacher in terms
                 objective is given by the distillation component of the loss.             of the trade-off between accuracy and throughput. Our
                 Thedistillation embedding allows our model to learn from                  best model on ImageNet-1k is 85.2% top-1 accuracy out-
