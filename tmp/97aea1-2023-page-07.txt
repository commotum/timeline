ADE2OK [115

Al [09
15

rots 12]
oun |
LVIS [43]
‘ZewWaste-f [6]
‘6
ce

DOORS [73}

NDISPark [21.22]

0 #20
ToU deltaat 1 center point
(b) SAM vs, RITM [90] on 23 datasets

DRAM [23] GTEA [33.61

OVIS [79] PPDLS [72]

Bo A . +
a| ' { : i + Ground Truth
4, ¢ ’ : ' : + SAM
Z7) ' 7 + SAM- single output
5 ' ' ' *. RITM
45 :
LVIS VISOR DRAM IBD NDD20_ OVISiShape

Datasets
(c) Mask quality ratings by human annotators

SAM (oracle)

3 3 ae

3 nn eae

<s0] ¥ : +, 8 50|-

2 SimpleClick 3)

4 FocalClick 2 |¥
123 123

3
Number of points
(e) Random points

5
Number of points
(a) Center points (default)

Figure 7: Point to mask evaluation on 23 datasets. (a) Dataset samples. (b) Mean IoU of SAM and the strongest single point
segmenter, RITM [90]. Due to ambiguity, a single mask may not match ground truth; circles show “oracle” results of the
most relevant of SAM’s 3 predictions. (c) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10
(best). Mask center is used as the prompt. (d, e) mIoU with varying number of points. SAM significantly outperforms prior
interactive segmenters with | point and is on par with more points. Low absolute mloU at | point is the result of ambiguity.

Datasets. We use a newly compiled suite of 23 datasets
with diverse image distributions, see appendix Table 4 for
more details. We use all 23 datasets for mIoU evaluation.
For the human study, we use the subset listed in Fig. 7c
(due to the resource requirements of such studies). This
subset includes both datasets for which SAM outperforms
and underperforms RITM according to automatic metrics.

Results. First, we look at automatic evaluation on the full
suite of 23 datasets using mloU. We compare per-dataset
results in Fig. 7b against RITM. SAM yields higher re-
sults on 16 of the 23 datasets, by as much as ~47 IoU. We
also present an “oracle” result, in which the most relevant
of SAM’s 3 masks is selected by comparing them to the
ground truth, rather than selecting the most confident mask.
This reveals the impact of ambiguity on automatic evalu-
ation. In particular, with the oracle to perform ambiguity
resolution, SAM outperforms RITM on all datasets.

Results of the human study are presented in Fig. 7c. Er-
ror bars are 95% confidence intervals (all differences are
significant; see §F for details). We observe that the annota-

tors consistently rate the quality of SAM’s masks substan-
tially higher than the strongest baseline, RITM. An ablated,
“ambiguity-unaware” version of SAM with a single output
mask has consistently lower ratings. SAM’s mean ratings
fall between 7 and 9, which corresponds to the qualitative
rating guideline: “A high score (7-9): The object is identi-
fiable and errors are small and rare (e.g., missing a small,
heavily obscured disconnected component, ...).” These re-
sults indicate that SAM has leamed to segment valid masks
from a single point. Note that for datasets like DRAM and
IBD, where SAM is worse on automatic metrics, it receives
consistently higher ratings in the human study.

Fig. 7d shows additional baselines, SimpleClick [65] and
FocalClick [17]. As the number of points increases from 1
to 9, we observe that the gap between methods decreases.
This is expected as the task becomes easier; also, SAM is
not optimized for the very high IoU regime. Finally, in
Fig. 7e we replace the default center point sampling with
random point sampling. We observe that the gap between
SAM and the baselines grows and SAM is able to achieve
comparable results under either sampling method.

4022
