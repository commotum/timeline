                                        Training data-efﬁcient image transformers & distillation through attention
               8. Acknowledgements                                                daugment: Practical automated data augmentation with a
                                                                                  reduced search space. arXiv preprint arXiv:1909.13719,
               ManythankstoRossWightmanforsharinghisViTcodeand                    2019.
               bootstrapping the training method with the community, as
               well as for valuable feedback that helped us to ﬁx different    Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
               aspects of this paper. Thanks to Vinicius Reis, MannatSingh,       Pre-training of deep bidirectional transformers for lan-
               Ari Morcos, Mark Tygert, Gabriel Synnaeve, and other col-          guage understanding. arXiv preprint arXiv:1810.04805,
               leagues atFacebook for brainstorming and some exploration          2018.
               onthis axis. Thanks to Ross Girshick and Piotr Dollar for       Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
               constructive comments.                                             D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
                                                                                  Heigold, G., Gelly, S., et al. An image is worth 16x16
               References                                                         words: Transformersforimagerecognitionatscale. arXiv
                                                                                  preprint arXiv:2010.11929, 2020.
               Abnar, S., Dehghani, M., and Zuidema, W. Transferring           Fan, A., Grave, E., and Joulin, A. Reducing transformer
                 inductive biases through knowledge distillation. arXiv           depth on demand with structured dropout. arXiv preprint
                 preprint arXiv:2006.00555, 2020.                                 arXiv:1909.11556, 2019.
               Ba,J.L.,Kiros,J.R.,andHinton,G.E. Layernormalization.           Fan, A., Stock, P., Graham, B., Grave, E., Gribonval, R.,
                 arXiv preprint arXiv:1607.06450, 2016.                            ´
                                                                                  Jegou, H., and Joulin, A. Training with quantization
               Bello, I.  Lambdanetworks: Modeling long-range inter-              noise for extreme model compression. arXiv preprint
                 actions without attention. International Conference on           arXiv:2004.07320, 2020.
                 Learning Representations, 2021.                               Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,
               Bello, I., Zoph, B., Vaswani, A., Shlens, J., and Le, Q. V. At-    Y. N. Convolutional sequence to sequence learning. arXiv
                 tention augmented convolutional networks. International          preprint arXiv:1705.03122, 2017.
                 Conference on Computer Vision, 2019.                                            ´
                                                                               Goyal, P., Dollar, P., Girshick, R. B., Noordhuis, P.,
                               ´                                                  Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and
               Berman, M., Jegou, H., Vedaldi, A., Kokkinos, I., and
                 Douze, M. Multigrain: a uniﬁed image embedding for               He, K. Accurate, large minibatch sgd: Training imagenet
                 classes and instances. arXiv preprint arXiv:1902.05509,          in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
                 2019.                                                         Hanin, B. and Rolnick, D. How to start training: The effect
                             ´                                                    of initialization and architecture. Advances in Neural
               Beyer, L., Henaff, O. J., Kolesnikov, A., Zhai, X., and
                 van den Oord, A. Are we done with imagenet? arXiv                Information Processing Systems, 2018.
                 preprint arXiv:2006.07159, 2020.                              He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
               Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,        ing for image recognition. In Conference on Computer
                 A., and Zagoruyko, S. End-to-end object detection with           Vision and Pattern Recognition, 2016.
                 transformers. In European Conference on Computer Vi-          He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.
                 sion, 2020.                                                      Bagoftricks for image classiﬁcation with convolutional
               Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D.,       neural networks. In Conference on Computer Vision and
                 and Sutskever, I. Generative pretraining from pixels. In         Pattern Recognition, 2019.
                 International Conference on Machine Learning, 2020a.          Hendrycks, D. and Gimpel, K. Gaussian error linear units
               Chen, Y.-C., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan,         (gelus). arXiv preprint arXiv:1606.08415, 2016.
                 Z., Cheng, Y., and jing Liu, J. Uniter: Universal image-      Hinton, G. E., Vinyals, O., and Dean, J.           Distilling
                 text representation learning. In European Conference on          the knowledge in a neural network.        arXiv preprint
                 ComputerVision, 2020b.                                           arXiv:1503.02531, 2015.
               Cho, J. H. and Hariharan, B. On the efﬁcacy of knowledge        Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoeﬂer,
                 distillation. In International Conference on Computer            T., and Soudry, D. Augment your batch: Improving
                 Vision, 2019.                                                    generalization through instance repetition. In Conference
               Cordonnier, J.-B., Loukas, A., and Jaggi, M. On the rela-          onComputerVision and Pattern Recognition, 2020.
                 tionship between self-attention and convolutional layers.     Horn, G. V., Mac Aodha, O., Song, Y., Shepard, A., Adam,
                 arXiv preprint arXiv:1911.03584, 2020.                           H., Perona, P., and Belongie, S. J. The inaturalist chal-
                                             ´                                    lenge 2018 dataset. arXiv preprint arXiv:1707.06642,
               Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le,
                 Q. V. Autoaugment: Learning augmentation policies                2018.
                 from data. arXiv preprint arXiv:1805.09501, 2018.             Horn, G. V., Mac Aodha, O., Song, Y., Shepard, A., Adam,
               Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Ran-             H., Perona, P., and Belongie, S. J. The inaturalist chal-
