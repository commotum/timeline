                 Ethical Statement                                         2023. Openflamingo: An open-source framework for
                                                                           training large autoregressive vision-language models.
                 TheMMMU-Probenchmarkisdesignedwithethi-                  ArXiv preprint, abs/2308.01390.
                 cal considerations to ensure fair and responsible AI
                 evaluation. The dataset excludes sensitive content,    Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed
                 and the assessment focuses on testing multimodal          El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and
                 capabilities without introducing bias. We aim for         Jingjing Liu. 2020. Uniter: Universal image-text
                                                                           representation learning. In European Conference on
                 transparency in reporting model limitations and en-      Computer Vision, pages 104–120.
                 courage further research to address any societal       Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye,
                 impacts related to the use of these models in real-       Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi
                 world applications.                                       Hu, Jiapeng Luo, Zheng Ma, et al. 2024. How far
                 Limitations                                               are we to gpt-4v? closing the gap to commercial
                                                                           multimodal models with open-source suites. ArXiv
                                                                           preprint, abs/2404.16821.
                 While MMMU-Proimprovesuponexistingbench-
                 marks by filtering out text-only solvable questions    ChenhangCui, Yiyang Zhou, Xinyu Yang, Shirley Wu,
                 and introducing a vision-only setting, some limita-       Linjun Zhang, James Zou, and Huaxiu Yao. 2023.
                 tions remain. The dataset may still contain subtle        Holistic analysis of hallucination in gpt-4v (ision):
                 statistical shortcuts that models can exploit, and        Bias and interference challenges. ArXiv preprint,
                                                                           abs/2311.03287.
                 its scope is limited to predefined disciplines and
                 question formats. Additionally, while the vision-      WenliangDai,JunnanLi,DONGXULI,AnthonyTiong,
                 only input setting increases difficulty, it does not      Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N
                 fully capture the complexities of human perception.       Fung, and Steven Hoi. 2023. Instructblip: Towards
                                                                           general-purposevision-languagemodelswithinstruc-
                 Lastly, our reliance on approximated human per-           tion tuning. In Advances in Neural Information Pro-
                 formance rather than direct evaluation introduces         cessing Systems, volume 36, pages 49250–49267.
                 potential biases in reporting accurate human expert       Curran Associates, Inc.
                 performance.                                           MengnanDu,FengxiangHe,NaZou,DachengTao,and
                                                                          Xia Hu. 2023. Shortcut learning of large language
                                                                           models in natural language understanding. Commu-
                 References                                                nications of the ACM, 67(1):110–120.
                 MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,                Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,
                    Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,          Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,
                    NguyenBach,AmitBahree,ArashBakhtiari, Harki-          Akhil Mathur, Alan Schelten, Amy Yang, Angela
                    rat Behl, et al. 2024. Phi-3 technical report: A       Fan, et al. 2024. The llama 3 herd of models. ArXiv
                    highlycapablelanguagemodellocallyonyourphone.          preprint, abs/2407.21783.
                   ArXiv preprint, abs/2404.14219.
                 Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,      XingyuFu,YushiHu,BangzhengLi,YuFeng,Haoyu
                    Antoine Miech, Iain Barr, Yana Hasson, Karel          Wang,XudongLin,DanRoth,NoahASmith,Wei-
                    Lenc, Arthur Mensch, Katherine Millican, Malcolm       Chiu Ma, and Ranjay Krishna. 2024. Blink: Multi-
                    Reynolds, et al. 2022. Flamingo: a visual language     modallargelanguagemodelscanseebutnotperceive.
                    modelfor few-shot learning. In Advances in Neural     ArXiv preprint, abs/2404.12390.
                    Information Processing Systems.                     Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
                 Anthropic.    2024.         Claude    3.5    sonnet.      Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui
                    https://www.anthropic.com/news/claude-3-5-             He, Xiangyu Yue, et al. 2023. Llama-adapter v2:
                    sonnet.                                                Parameter-efficient visual instruction model. ArXiv
                                                                           preprint, abs/2304.15010.
                 Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-
                    garet Mitchell, Dhruv Batra, C. Lawrence Zitnick,   YashGoyal,TejasKhot,DouglasSummers-Stay,Dhruv
                    and Devi Parikh. 2015. VQA: visual question an-        Batra, and Devi Parikh. 2017. Making the V in VQA
                    swering. In 2015 IEEE International Conference         matter: Elevating the role of image understanding in
                    on Computer Vision, ICCV 2015, Santiago, Chile,        visual question answering. In 2017 IEEE Conference
                    December7-13, 2015, pages 2425–2433. IEEE Com-         on Computer Vision and Pattern Recognition, CVPR
                    puter Society.                                         2017, Honolulu, HI, USA, July 21-26, 2017, pages
                                                                           6325–6334. IEEE Computer Society.
                 Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes-
                    sel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,    gpt-4o. 2024.     Cheaper, better, faster, stronger.
                   Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.      https://mistral.ai/news/mixtral-8x22b/.
                                                                   15143
