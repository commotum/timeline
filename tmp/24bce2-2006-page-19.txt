             AFastLearningAlgorithmforDeepBeliefNets                     1545
             Figure8: Eachrowshows10samplesfromthegenerativemodelwithaparticu-
             larlabelclampedon.Thetop-levelassociativememoryisrunfor1000iterations
             of alternating Gibbs sampling between samples.
             stochastic binary states. The second is to repeat the stochastic up-pass
             20 times and average either the label probabilities or the label log prob-
             abilities over the 20 repetitions before picking the best one. The two types
             of averagegivealmostidenticalresults,andtheseresultsarealsoverysim-
             ilar to using a single deterministic up-pass, which was the method usedfor
             the reported results.
             7 LookingintotheMindofaNeuralNetwork
             To generate samples from the model, we perform alternating Gibbs sam-
             plinginthetop-levelassociativememoryuntiltheMarkovchainconverges
             totheequilibriumdistribution.Thenweuseasamplefromthisdistribution
             as input to the layers below and generate an image by a single down-pass
             throughthegenerativeconnections. If we clamp the label units to a partic-
             ular class during the Gibbs sampling, we can see images from the model’s
             class-conditional distributions. Figure 8 shows a sequence of images for
             each class that were generated by allowing 1000 iterations of Gibbs sam-
             pling between samples.
               We can also initialize the state of the top two layers by providing a
             random binary image as input. Figure 9 shows how the class-conditional
             stateoftheassociativememorythenevolveswhenitisallowedtorunfreely,
             butwiththelabelclamped.Thisinternalstateis“observed”byperforming
             a down-pass every 20 iterations to see what the associative memory has
