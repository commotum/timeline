           [47] Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of causal inference: founda-
              tions and learning algorithms. The MIT Press, 2017.
           [48] Fernando Pineda. Generalization of back propagation to recurrent and higher order neural
              networks. In Neural information processing systems, pages 602–611, 1987.
           [49] Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with
              implicit gradients. 2019.
           [50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark
              Chen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092,
              2021.
           [51] Stuart J Russell. Artiﬁcial intelligence a modern approach. Pearson Education, Inc., 2010.
           [52] Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van den Oord.
              Step-unrolled denoising autoencoders for text generation. arXiv preprint arXiv:2112.06749,
              2021.
           [53] Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to
              learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.
           [54] Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic
              recurrent networks. Neural Computation, 4(1):131–139, 1992.
           [55] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-
              propagation for bilevel optimization. In The 22nd International Conference on Artiﬁcial
              Intelligence and Statistics, pages 1723–1732. PMLR, 2019.
           [56] DavidSilver, AjaHuang,ChrisJMaddison,ArthurGuez,LaurentSifre,GeorgeVanDenDriess-
              che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas-
              tering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489,
              2016.
           [57] GautamSingh,FeiDeng,andSungjinAhn. IlliterateDALL·Elearnstocompose. arXivpreprint
              arXiv:2110.11405, 2021.
           [58] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
              vised learning using nonequilibrium thermodynamics. In International Conference on Machine
              Learning, pages 2256–2265. PMLR, 2015.
           [59] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data
              distribution. Advances in Neural Information Processing Systems, 32, 2019.
           [60] Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media,
              2012.
           [61] Sjoerd Van Steenkiste, Michael Chang, Klaus Greff, and Jürgen Schmidhuber. Relational neural
              expectation maximization: Unsupervised discovery of objects and their interactions. arXiv
              preprint arXiv:1802.10353, 2018.
           [62] Rishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu,
              Joshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement
              learning. In Conference on Robot Learning, pages 1439–1456. PMLR, 2020.
           [63] Nicholas Watters, Loic Matthey, Christopher P Burgess, and Alexander Lerchner. Spatial
              broadcast decoder: A simple architecture for learning disentangled representations in VAEs.
              arXiv preprint arXiv:1901.07017, 2019.
           [64] C. F. Jeff Wu. On the Convergence Properties of the EM Algorithm. The Annals of Statistics, 11
              (1):95 – 103, 1983. doi: 10.1214/aos/1176346060. URL https://doi.org/10.1214/aos/
              1176346060.
           [65] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In
              International Conference on Machine Learning, pages 2635–2644. PMLR, 2016.
           [66] Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, and Ying Nian Wu. Cooperative training
              of descriptor and generator networks. IEEE transactions on pattern analysis and machine
              intelligence, 42(1):27–45, 2018.
           [67] Jianwen Xie, Zilong Zheng, and Ping Li. Learning energy-based model with variational auto-
              encoder as amortized sampler. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,
              volume 35, pages 10441–10451, 2021.
                               13
