                         [34] K Jordan, Y Jin, V Boza, Y Jiacheng, F Cecista, L Newhouse, and J Bernstein. Muon:
                             Anoptimizer for hidden layers in neural networks, 2024b. URL https://kellerjordan. github.
                              io/posts/muon, 2024.
                         [35] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor
                              optimization. In International Conference on Machine Learning, pages 1842–1850. PMLR,
                              2018.
                         [36] Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson,
                              and Sham M. Kakade. SOAP: Improving and stabilizing shampoo using adam for language
                              modeling. In The Thirteenth International Conference on Learning Representations, 2025. URL
                              https://openreview.net/forum?id=IDxZhXrpNf.
                         [37] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
                              Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
                             Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
                         [38] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
                             Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
                              few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
                         [39] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language
                              models a mirage? Advances in neural information processing systems, 36:55565–55581, 2023.
                         [40] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,
                              and Caiming Xiong. Codegen: An open large language model for code with multi-turn program
                              synthesis. In The Eleventh International Conference on Learning Representations, 2023. URL
                              https://openreview.net/forum?id=iaYcJKpY2B_.
                         [41] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo,
                             Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended
                              decoder for vision-centric tasks. Advances in Neural Information Processing Systems, 36:
                              61501–61513, 2023.
                         [42] Sabri Eyuboglu, Ryan Ehrlich, Simran Arora, Neel Guha, Dylan Zinsley, Emily Liu, Will
                             Tennien, Atri Rudra, James Zou, Azalia Mirhoseini, et al. Cartridges: Lightweight and general-
                              purpose long context representations via self-study. arXiv preprint arXiv:2506.06266, 2025.
                         [43] hongzhouyu,TianhaoCheng,YingwenWang,WenHe,QingWang,YingCheng,YuejieZhang,
                              Rui Feng, and Xiaobo Zhang. FinemedLM-o1: Enhancing medical knowledge reasoning ability
                              of LLMfromsupervised fine-tuning to test-time training. In Second Conference on Language
                             Modeling, 2025. URL https://openreview.net/forum?id=7ZwuGZCopw.
                         [44] Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim,
                              and Jacob Andreas. The surprising effectiveness of test-time training for few-shot learning. In
                             Forty-second International Conference on Machine Learning, 2024.
                         [45] WilliamBeecherScovilleandBrendaMilner. Lossofrecentmemoryafterbilateralhippocampal
                              lesions. Journal of neurology, neurosurgery, and psychiatry, 20(1):11, 1957.
                         [46] Alvaro Pascual-Leone, Amir Amedi, Felipe Fregni, and Lotfi B Merabet. The plastic human
                              brain cortex. Annu. Rev. Neurosci., 28(1):377–401, 2005.
                         [47] Michael V Johnston. Plasticity in the developing brain: implications for rehabilitation. Devel-
                              opmental disabilities research reviews, 15(2):94–101, 2009.
                         [48] Akihiro Goto, Ayaka Bota, Ken Miya, Jingbo Wang, Suzune Tsukamoto, Xinzhi Jiang, Daichi
                              Hirai, Masanori Murayama, Tomoki Matsuda, Thomas J. McHugh, Takeharu Nagai, and
                             Yasunori Hayashi.  Stepwise synaptic plasticity events drive the early phase of memory
                              consolidation. Science, 374(6569):857–863, 2021. doi: 10.1126/science.abj9195. URL
                              https://www.science.org/doi/abs/10.1126/science.abj9195.
                         [49] UweFreyandRichardGMMorris. Synaptictaggingandlong-termpotentiation. Nature, 385
                             (6616):533–536, 1997.
                                                                    13
