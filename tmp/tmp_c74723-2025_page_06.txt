                  Table 1. Statistics for datasets used during training (top) and eval-      Table 2. We report results for direct predictions of PanSt3R on
                  uation (bottom). ”MV” denotes available multi-view data.                   rendered test images (with and without QUBO), as well as results
                                                                                             obtained via the simple 3DGS uplifting approach with LUDVIG.
                               Dataset          real  MV #scenes #classes                    †Timing for building the 3DGS. Note that given access to target
                               ScanNet++[69]     ✓     ✓      855       100                  view images, PanSt3R can make predictions without the need for
                               ASE[1]            x     ✓      750        44                  3DGS(andcameraparameters).
                           rainingInfinigen [44] x     ✓      936        76
                           T   COCO[32]          ✓     x      118k       80                                           Req.    Hyper-  Rep-   Scan       Time
                               ADE20k[76]        ✓     x      20k       150                                 Method    Poses    sim     lica   Net       (min)
                               ScanNet++[69]     ✓     ✓       50       100                          DM-NeRF[57]       ✓       51.6    44.1  41.7      ∼900
                            al ScanNet[12]       ✓     ✓       12        20                               PNF[25]      ✓       44.8    41.1  48.3         -
                            Ev Hypersim[46]      x     ✓       6         20                            PanLift [50]    ✓       60.1    57.9  58.9      ∼450
                               Replica [53]      x     ✓       7         20                     Contrastive Lift [2]   ✓       62.3    59.1  62.3      ∼420
                                                                                                         PLGS[62]      ✓       62.4    57.8  58.7      ∼120
                  wegenerate 936 indoor scenes of 25 images each. Finally,                            PCF-Lift [78]    ✓        -       -    63.5         -
                                                                                                                                                              †
                  we also leverage two widely used 2D panoptic segmenta-                        PanSt3Rw/oQUBO          †      51.6    57.3  59.5    ∼4(+35 )
                                                                                                                                                               †
                  tion datasets, COCO [32] and ADE20K [76], which consist                                  PanSt3R      †      56.5    62.0  65.7   ∼4.5(+35 )
                  of high-resolution images with precise manual annotations.                   PanSt3R+LUDVIG          ✓       66.3    60.6  67.5       ∼40
                  Adding these datasets is useful to improve generalization
                  and robustness, since they offer a larger visual diversity. To
                  simulate multi-view data on 2D images, we sample several
                  geometric and photometric variants of the input image in-
                  cluding crops, rotations, and color jittering.
                  Training details. The DINOv2 and MUSt3R backbones
                  (resp. ViT-L,andViT-L+ViT-Barchitectures)areinitialized
                  with their pretrained weights and frozen during PanSt3R
                  training. In preliminary experiments, we observed that fine-               Figure 3. Qualitative examples of novel-view panoptic segmenta-
                  tuning MUSt3R does not have a big impact on the final                      tion on HypersimandReplicascenes. Predictionsareoverlaidon
                  performance, but incurs significant additional training cost.              top of original images, and colors and their nuances denote differ-
                  Since each dataset comes with a different set of classes, we               ent classes and object instances respectively.
                  restrict the focal loss supervision L         to within the set of
                                                            cls
                  ground-truth classes of each dataset during training. Addi-                while penalizing segments with wrong matches (False Pos-
                  tional training details are provided in the Supplementary.                 itives) or without matches (False Negatives).             It can be
                  Test-time keyframes. Since the number of test views can                    seen as a combination of two terms, segmentation qual-
                  belargeduringinference(e.g.hundreds),weadoptthesame                        ity SQ = 1/|TP|P                    IoU(p,g), and a recognition
                  technique as in [3] to reduce the computational and mem-                                            (p,g)∈TP
                  ory footprint. Namely, we efficiently cluster the set of input             quality RQ = TP/(2·|TP|+|FP|+|FN|).
                  images using retrieval techniques and select a small set of                Extension to 3D scenes. PQ can be trivially computed at
                                                                                             the scene level by pretending that the scene is a concatena-
                  50keyframesusingthefarthest-point-sampling (FPS) algo-                     tion of all images, effectively tying predictions between all
                  rithm to maximize coverage. Frame tokens {f } are then                                                                       sc
                                                                         n                   images. Thismetric, coined scene-PQ(PQ ),wasfirstpro-
                  only selected from these keyframes, which is enough to                     posedin[50]toevaluatetheresultsof3Dpanopticsegmen-
                  generate relevant queries {qj}, as shown in Sec. 4.5. We                   tation. As we always use the scene-PQ metric, we omit the
                  then process the remaining views frame-by-frame, only ex-                                   sc
                                                                                             upper-script ” ” for brevity in the following. To compute
                  tracting the per-frame features F        and directly performing
                                                        n                                    the overall results for a dataset, we average the per-scene
                  maskprediction via Eq. (1), with the decoded queries {qj}                  PQsacross all scenes.
                  obtained from the keyframes.
                  4.2. Evaluation metrics                                                    4.3. Evaluation on the PanLift benchmark
                                                                                             Wefirst evaluate our method on the Panoptic Lifting (Pan-
                  Panoptic Quality (PQ). The Panoptic Quality (PQ)                           Lift) benchmark [50]. It comprises 12 scenes from Scan-
                  score [24] is defined as                                                   Net [12], 6 scenes from Hypersim [46] and 7 scenes from
                                          2P              IoU(p,g)                           Replica [53] (see details in [50]). We use the same splits
                                  PQ=          (p,g)∈TP               ,            (9)       between seen and unseen (novel) views as in [2, 50].
                                          2|TP|+|FP|+|FN|                                        For PanSt3R, we experiment with the two strategies pre-
                  wherepisapredictedinstanceandg isaGTclassinstance.                         sented in Sec. 3.3: (i) we simply render a novel image of the
                  Intuitively, this score averages IoU of matched segments                   target viewpoint using an off-the-shelf 3DGS model, and
                                                                                       5861
