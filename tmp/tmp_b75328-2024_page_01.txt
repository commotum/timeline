                                     Transformers Can Do Arithmetic with the
                                                          Right Embeddings
                                                   1*              1∗            1           1                     1
                                    SeanMcLeish ,ArpitBansal ,AlexStein ,NeelJain ,JohnKirchenbauer ,
                                                         2                    2                  1                3
                                    Brian R. Bartoldson , Bhavya Kailkhura , Abhinav Bhatele , Jonas Geiping ,
                                                         Avi Schwarzschild4, Tom Goldstein1
                            1 University of Maryland, 2 Lawrence Livermore National Laboratory, 3 ELLIS Institute Tübingen,
                             MaxPlanckInstitute for Intelligent Systems, Tübingen AI Center, 4 Carnegie Mellon University
                                                                     Abstract
                                   Thepoorperformanceoftransformers on arithmetic tasks seems to stem in large
                                   part from their inability to keep track of the exact position of each digit inside of
                                   a large span of digits. We mend this problem by adding an embedding to each
                                   digit that encodes its position relative to the start of the number. In addition to
                                   the boost these embeddings provide on their own, we show that this fix enables
                                   architectural modifications such as input injection and recurrent layers to improve
                                   performance even further.
                                   With positions resolved, we can study the logical extrapolation ability of
                                   transformers.   Can they solve arithmetic problems that are larger and more
                                   complex than those in their training data? We find that training on only 20 digit
                                   numbers with a single GPU for one day, we can reach state-of-the-art performance,
                                   achieving up to 99% accuracy on 100 digit addition problems. Finally, we show
                                   that these gains in numeracy also unlock improvements on other multi-step
                                   reasoning tasks including sorting and multiplication. 2
                                            Positional Embedding:           Positional Embedding:
                                          0           FIRE                           Abacus
                                                                                                          100
                                                                                                          75
                                         50                                                               50    Accuracy
                                                                                                          25
                                       Length of Operand One100                                           0
                                           0           50           100    0            50           100
                                                             Length of Operand Two
                           Figure 1: Zero shot exact match accuracy on addition using depth sixteen transformer (decoder only)
                           models trained on operands of up to 20 digits. Compared to state-of-the-art embeddings (left), our
                           newAbacusEmbeddings(right) dramatically improve generalization to unseen digit lengths. The
                           interior of the red square denotes the training distribution. Accuracies are averaged over three trials.
                             ∗Equal Contribution, correspondence to: smcleish@umd.edu, bansal01@umd.edu.
                              2Codeavailable on GitHub: github.com/mcleish7/arithmetic.
                           38th Conference on Neural Information Processing Systems (NeurIPS 2024).
