           Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables
            input length extrapolation. In International Conference on Learning Representations, 2022. URL
            https://openreview.net/forum?id=R8sQPpGCv0.
           Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. Limitations of language models in
            arithmetic and symbolic induction. arXiv preprint arXiv:2208.05051, 2022.
           Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
            Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
            transformer. Journal of machine learning research, 21(140):1–67, 2020.
           Gleb Rodionov and Liudmila Prokhorenkova. Discrete neural algorithmic reasoning. arXiv preprint
            arXiv:2402.11628, 2024.
           Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani,
            Shane Legg, and Joel Veness. Randomized positional encodings boost length generalization of
            transformers. arXiv preprint arXiv:2305.16843, 2023.
           David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical
            reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.
           Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum,
            and TomGoldstein. Can you learn an algorithm? generalizing from easy to hard problems with
            recurrent networks. Advances in Neural Information Processing Systems, 34, 2021.
           PeterShaw,JakobUszkoreit,andAshishVaswani. Self-attentionwithrelativepositionrepresentations.
            arXiv preprint arXiv:1803.02155, 2018.
           NoamShazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
           Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat Lee, Yuanzhi Li, and Yi Zhang. Positional
            description matters for transformers arithmetic. arXiv preprint arXiv:2311.14737, 2023.
           Jianlin Su, Murtadha Ahmed, YuLu, ShengfengPan, WenBo,andYunfengLiu. Roformer: Enhanced
            transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
           Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention
            span in transformers. In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings
            of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331–335,
            Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1032.
            URLhttps://aclanthology.org/P19-1032.
           Alberto Testolin. Can neural networks do arithmetic? a survey on the elementary numerical skills
            of state-of-the-art deep learning models. Applied Sciences, 14(2), 2024. ISSN 2076-3417. doi:
            10.3390/app14020744. URL https://www.mdpi.com/2076-3417/14/2/744.
           HugoTouvron,LouisMartin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
            Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
            and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
           Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
            Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing
            systems, 30, 2017.
                ˇ ´
           Petar Velickovic, Adrià Puigdomènech Badia, David Budden, Razvan Pascanu, Andrea Banino,
            Misha Dashevskiy, Raia Hadsell, and Charles Blundell. The clrs algorithmic reasoning benchmark.
            In International Conference on Machine Learning, pages 22084–22102. PMLR, 2022.
           HongyuWang,ShumingMa,LiDong,ShaohanHuang,DongdongZhang,andFuruWei. DeepNet:
            Scaling Transformers to 1,000 Layers. arXiv:2203.00555 [cs], March 2022. URL http://arxiv.
            org/abs/2203.00555.
           Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are
            better at learning learning algorithms. arXiv preprint arXiv:2311.12424, 2023a.
                               12
