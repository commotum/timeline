                             DensePassageRetrieval for Open-Domain Question Answering
                                                                    ∗             ˘    ∗                †
                                       Vladimir Karpukhin, Barlas Oguz, Sewon Min , Patrick Lewis,
                                                                                                    ‡
                                             Ledell Wu, Sergey Edunov, Danqi Chen , Wen-tau Yih
                                   Facebook AI              †University of Washington                 ‡Princeton University
                           {vladk, barlaso, plewis, ledell, edunov, scottyih}@fb.com
                                                            sewon@cs.washington.edu
                                                            danqic@cs.princeton.edu
                                            Abstract                                    Retrieval in open-domain QA is usually imple-
                         Open-domain question answering relies on ef-                mented using TF-IDF or BM25 (Robertson and
                         ﬁcient passage retrieval to select candidate                Zaragoza, 2009), which matches keywords efﬁ-
                         contexts, where traditional sparse vector space             ciently with an inverted index and can be seen
                         models, such as TF-IDF or BM25, are the de                  as representing the question and context in high-
                         facto method.     In this work, we show that                dimensional, sparse vectors (with weighting). Con-
                         retrieval can be practically implemented us-                versely, the dense, latent semantic encoding is com-
                         ing dense representations alone, where em-                  plementarytosparserepresentationsbydesign. For
                         beddings are learned from a small number                    example, synonyms or paraphrases that consist of
                         of questions and passages by a simple dual-                 completely different tokens may still be mapped to
                         encoder framework.       When evaluated on a                vectors close to each other. Consider the question
                         wide range of open-domain QA datasets, our                 “Whoisthebadguyinlordoftherings?”,whichcan
                         dense retriever outperforms a strong Lucene-
                         BM25system greatly by 9%-19% absolute in                    be answered from the context “Sala Baker is best
                         termsoftop-20passageretrievalaccuracy,and                   knownforportrayingthevillain Sauron in the Lord
                         helps our end-to-end QA system establish new                of the Rings trilogy.” A term-based system would
                         state-of-the-art on multiple open-domain QA                 have difﬁculty retrieving such a context, while
                                       1
                         benchmarks.                                                 a dense retrieval system would be able to better
                    1    Introduction                                                match “bad guy” with “villain” and fetch the cor-
                                                                                     rect context. Dense encodings are also learnable
                    Open-domain question answering (QA) (Voorhees,                   byadjusting the embedding functions, which pro-
                    1999) is a task that answers factoid questions us-               vides additional ﬂexibility to have a task-speciﬁc
                    ing a large collection of documents. While early                 representation. With special in-memory data struc-
                    QAsystemsareoften complicated and consist of                     tures and indexing schemes, retrieval can be done
                    multiple components (Ferrucci (2012); Moldovan                   efﬁciently using maximum inner product search
                    et al. (2003), inter alia), the advances of reading              (MIPS)algorithms(e.g., Shrivastava and Li (2014);
                    comprehension models suggest a much simpliﬁed                    Guoetal. (2016)).
                    two-stage framework: (1) a context retriever ﬁrst                   However, it is generally believed that learn-
                    selects a small subset of passages where some                    ing a good dense vector representation needs a
         arXiv:2004.04906v3  [cs.CL]  30 Sep 2020of them contain the answer to the question, andlarge number of labeled pairs of question and con-
                    then (2) a machine reader can thoroughly exam-                   texts.  Dense retrieval methods have thus never
                    ine the retrieved contexts and identify the correct              be shown to outperform TF-IDF/BM25 for open-
                    answer (Chen et al., 2017). Although reducing                    domainQAbeforeORQA(Leeetal.,2019),which
                    open-domain QA to machine reading is a very rea-                 proposes a sophisticated inverse cloze task (ICT)
                    sonable strategy, a huge performance degradation                 objective, predicting the blocks that contain the
                                                      2
                    is often observed in practice , indicating the needs             masked sentence, for additional pretraining. The
                    of improving retrieval.                                          questionencoderandthereadermodelarethenﬁne-
                         ∗Equal contribution                                         tuned using pairs of questions and answers jointly.
                        1The code and trained models have been released at           Although ORQA successfully demonstrates that
                    https://github.com/facebookresearch/DPR.
                        2For instance, the exact match score on SQuAD v1.1 drops     dense retrieval can outperform BM25, setting new
                    from above 80% to less than 40% (Yang et al., 2019a).            state-of-the-art results on multiple open-domain
                 QAdatasets, it also suffers from two weaknesses.        the extractive QA setting, in which the answer is
                 First, ICT pretraining is computationally intensive     restricted to a span appearing in one or more pas-
                 and it is not completely clear that regular sentences   sages in the corpus. Assume that our collection
                 are good surrogates of questions in the objective       contains D documents, d ,d ,··· ,d . We ﬁrst
                                                                                                    1  2        D
                 function. Second, because the context encoder is        split each of the documents into text passages of
                                                                                                                3
                 not ﬁne-tuned using pairs of questions and answers,     equallengthsasthebasicretrievalunits andgetM
                 the corresponding representations could be subop-       total passages in our corpus C = {p ,p ,...,p     },
                                                                                                             1  2       M
                 timal.                                                  whereeachpassagep canbeviewedasasequence
                                                                                              i
                    In this paper, we address the question: can we       of tokens w(i),w(i),··· ,w(i) . Given a question q,
                                                                                     1    2         |p |
                                                                                                      i
                 train a better dense embedding model using only         the task is to ﬁnd a span w(i),w(i) ,··· ,w(i) from
                 pairs of questions and passages (or answers), with-                               s     s+1        e
                 out additional pretraining? By leveraging the now       one of the passages pi that can answer the question.
                 standard BERT pretrained model (Devlin et al.,          Notice that to cover a wide variety of domains, the
                 2019) and a dual-encoder architecture (Bromley          corpus size can easily range from millions of docu-
                 et al., 1994), we focus on developing the right         ments (e.g., Wikipedia) to billions (e.g., the Web).
                 training scheme using a relatively small number         Asaresult, any open-domain QA system needs to
                 of question and passage pairs. Through a series         include an efﬁcient retriever component that can se-
                 of careful ablation studies, our ﬁnal solution is       lect a small set of relevant texts, before applying the
                                                                                                                           4
                 surprisingly simple: the embedding is optimized         reader to extract the answer (Chen et al., 2017).
                 for maximizing inner products of the question and       Formally speaking, a retriever R : (q,C) → CF
                 relevant passage vectors, with an objective compar-     is a function that takes as input a question q and a
                 ing all pairs of questions and passages in a batch.     corpus C and returns a much smaller ﬁlter set of
                                                                         texts C  ⊂C,where|C | = k  |C|. For a ﬁxed
                 OurDensePassageRetriever (DPR) is exception-                   F                F
                 ally strong. It not only outperforms BM25 by a          k, a retriever can be evaluated in isolation on top-k
                 large margin (65.2% vs. 42.9% in Top-5 accuracy),       retrieval accuracy, which is the fraction of ques-
                 but also results in a substantial improvement on        tions for which CF contains a span that answers the
                 the end-to-end QA accuracy compared to ORQA             question.
                 (41.5% vs. 33.3%) in the open Natural Questions         3   DensePassageRetriever (DPR)
                 setting (Lee et al., 2019; Kwiatkowski et al., 2019).
                    Ourcontributions are twofold. First, we demon-       We focus our research in this work on improv-
                 strate that with the proper training setup, sim-        ing the retrieval component in open-domain QA.
                 ply ﬁne-tuning the question and passage encoders        Given a collection of M text passages, the goal of
                 on existing question-passage pairs is sufﬁcient to      our dense passage retriever (DPR) is to index all
                 greatly outperform BM25. Our empirical results          the passages in a low-dimensional and continuous
                 also suggest that additional pretraining may not be     space, such that it can retrieve efﬁciently the top
                 needed. Second, we verify that, in the context of       k passages relevant to the input question for the
                 open-domainquestionanswering,ahigherretrieval           reader at run-time. Note that M can be very large
                 precision indeed translates to a higher end-to-end      (e.g., 21 million passages in our experiments, de-
                 QAaccuracy. By applying a modern reader model           scribed in Section 4.1) and k is usually small, such
                 to the top retrieved passages, we achieve compara-      as 20–100.
                 ble or better results on multiple QA datasets in the    3.1   Overview
                 open-retrieval setting, compared to several, much
                 complicated systems.                                    Our dense passage retriever (DPR) uses a dense
                                                                         encoder EP(·) which maps any text passage to a d-
                 2    Background                                         dimensionalreal-valuedvectorsandbuildsanindex
                                                                         for all the M passages that we will use for retrieval.
                 The problem of open-domain QA studied in this              3The ideal size and boundary of a text passage are func-
                 paper can be described as follows. Given a factoid      tions of both the retriever and reader. We also experimented
                 question, such as “Who ﬁrst voiced Meg on Family        withnaturalparagraphsinourpreliminarytrialsandfoundthat
                 Guy?”or“Wherewasthe8thDalaiLamaborn?”,a                 using ﬁxed-length passages performs better in both retrieval
                                                                         and ﬁnal QA accuracy, as observed by Wang et al. (2019).
                 system is required to answer it using a large corpus       4Exceptions include (Seo et al., 2019) and (Roberts et al.,
                 of diversiﬁed topics. More speciﬁcally, we assume       2020),whichretrievesandgeneratestheanswers,respectively.
                    Atrun-time, DPRappliesadifferentencoderEQ(·)                    larity) than the irrelevant ones, by learning a better
                    that maps the input question to a d-dimensional                 embedding function.
                                                                                                            + −             −     m
                    vector, and retrieves k passages of which vectors                  Let D = {hqi,p ,p ,··· ,p i}i=1 be the
                                                                                                            i    i,1        i,n
                    are the closest to the question vector. We deﬁne                training data that consists of m instances. Each
                    the similarity between the question and the passage             instance contains one question q and one relevant
                                                                                                                          i
                                                                                                          +
                    using the dot product of their vectors:                         (positive) passage p , along with n irrelevant (neg-
                                                                                                       − i
                                                                                    ative) passages p     . We optimize the loss function
                                                        |                                              i,j
                                 sim(q,p) = EQ(q) EP(p).                    (1)     as the negative log likelihood of the positive pas-
                    Although more expressive model forms for measur-                sage:
                    ing the similarity between a question and a passage                                + −             −
                                                                                               L(q ,p ,p ,··· ,p          )                (2)
                                                                                                   i   i    i,1        i,n
                    do exist, such as networks consisting of multiple                                                       +
                                                                                                                    sim(qi,p )
                    layers of cross attentions, the similarity function                                           e         i
                                                                                          = −log               +      P                − .
                                                                                                       sim(qi,p )        n     sim(qi,p  )
                    needs to be decomposable so that the represen-                                    e        i   + j=1e              i,j
                    tations of the collection of passages can be pre-               Positive and negative passages             For retrieval
                    computed. Mostdecomposablesimilarityfunctions                   problems, it is often the case that positive examples
                    are some transformations of Euclidean distance                  are available explicitly, while negative examples
                    (L2). For instance, cosine is equivalent to inner               need to be selected from an extremely large pool.
                    product for unit vectors and the Mahalanobis dis-               For instance, passages relevant to a question may
                    tance is equivalent to L2 distance in a transformed             be given in a QA dataset, or can be found using the
                    space. Inner product search has been widely used                answer. All other passages in the collection, while
                    and studied, as well as its connection to cosine                not speciﬁed explicitly, can be viewed as irrelevant
                    similarity and L2 distance (Mussmann and Ermon,                 by default. In practice, how to select negative ex-
                    2016; Ram and Gray, 2012). As our ablation study                amples is often overlooked but could be decisive
                    ﬁnds other similarity functions perform compara-                for learning a high-quality encoder. We consider
                    bly (Section 5.2; Appendix B), we thus choose                   three different types of negatives: (1) Random: any
                    the simpler inner product function and improve the              random passage from the corpus; (2) BM25: top
                    dense passage retriever by learning better encoders.            passages returned by BM25 which don’t contain
                    Encoders       Althoughinprinciple the question and             the answer but match most question tokens; (3)
                    passage encoders can be implemented by any neu-                 Gold: positive passages paired with other questions
                    ral networks, in this work we use two independent               whichappearinthetrainingset. Wewilldiscussthe
                    BERT (Devlin et al., 2019) networks (base, un-                  impact of different types of negative passages and
                    cased) and take the representation at the [CLS]                 training schemes in Section 5.2. Our best model
                    token as the output, so d = 768.                                uses gold passages from the same mini-batch and
                    Inference      During inference time, we apply the              one BM25negative passage. In particular, re-using
                    passage encoder E        to all the passages and index          gold passages from the same batch as negatives
                                          P                                         can make the computation efﬁcient while achiev-
                    them using FAISS (Johnson et al., 2017) ofﬂine.                 ing great performance. We discuss this approach
                    FAISS is an extremely efﬁcient, open-source li-                 below.
                    brary for similarity search and clustering of dense
                    vectors, which can easily be applied to billions of             In-batch negatives         Assume that we have B
                    vectors. Given a question q at run-time, we derive              questions in a mini-batch and each one is asso-
                    its embedding vq = E (q) and retrieve the top k                 ciated with a relevant passage. Let Q and P be the
                                               Q
                    passages with embeddings closest to vq.                        (B×d)matrixofquestionandpassageembeddings
                                                                                                                        T
                    3.2    Training                                                 in a batch of size B. S = QP is a (B × B) ma-
                                                                                    trix of similarity scores, where each row of which
                    Training the encoders so that the dot-product sim-              corresponds to a question, paired with B passages.
                    ilarity (Eq. (1)) becomes a good ranking function               In this way, we reuse computation and effectively
                    for retrieval is essentially a metric learning prob-            train on B2 (q , p ) question/passage pairs in each
                                                                                                    i   j
                    lem (Kulis, 2013). The goal is to create a vector               batch. Any (q , p ) pair is a positive example when
                                                                                                    i  j
                    space such that relevant pairs of questions and pas-            i = j, and negative otherwise. This creates B train-
                    sages will have smaller distance (i.e., higher simi-            ing instances in each batch, where there are B − 1
                   negative passages for each question.                            Dataset                  Train          Dev       Test
                      Thetrick of in-batch negatives has been used in              Natural Questions   79,168   58,880    8,757     3,610
                   the full batch setting (Yih et al., 2011) and more              TriviaQA            78,785   60,413    8,837   11,313
                   recently for mini-batch (Henderson et al., 2017;                WebQuestions         3,417     2,474     361     2,032
                   Gillick et al., 2019). It has been shown to be an               CuratedTREC          1,353     1,125     133      694
                                                                                   SQuAD               78,713   70,096    8,886   10,570
                   effective strategy for learning a dual-encoder model
                   that boosts the number of training examples.                  Table 1: Number of questions in each QA dataset. The
                                                                                 two columns of Train denote the original training ex-
                   4    Experimental Setup                                       amples in the dataset and the actual questions used for
                                                                                 training DPR after ﬁltering. See text for more details.
                   In this section, we describe the data we used for
                   experiments and the basic setup.                              as well as various Web sources and is intended for
                   4.1    Wikipedia Data Pre-processing                          open-domain QA from unstructured corpora.
                   Following (Lee et al., 2019), we use the English              SQuAD v1.1 (Rajpurkar et al., 2016) is a popu-
                   Wikipedia dump from Dec. 20, 2018 as the source               lar benchmark dataset for reading comprehension.
                   documents for answering questions. We ﬁrst apply              Annotators were presented with a Wikipedia para-
                   the pre-processing code released in DrQA (Chen                graph, and asked to write questions that could be
                   et al., 2017) to extract the clean, text-portion of           answered from the given text. Although SQuAD
                   articles from the Wikipedia dump. This step re-               has been used previously for open-domain QA re-
                   moves semi-structured data, such as tables, info-             search, it is not ideal because many questions lack
                   boxes, lists, as well as the disambiguation pages.            context in absence of the provided paragraph. We
                   Wethenspliteacharticleintomultiple, disjoint text             still include it in our experiments for providing
                   blocks of 100 words as passages, serving as our               a fair comparison to previous work and we will
                   basic retrieval units, following (Wang et al., 2019),         discuss more in Section 5.1.
                   which results in 21,015,324 passages in the end.5             Selection of positive passages           Because only
                   Each passage is also prepended with the title of the          pairs of questions and answers are provided in
                   Wikipedia article where the passage is from, along            TREC,WebQuestionsandTriviaQA6,weusethe
                   with an [SEP] token.                                          highest-ranked passage from BM25 that contains
                   4.2    Question Answering Datasets                            the answer as the positive passage. If none of the
                                                                                 top 100 retrieved passages has the answer, the ques-
                   We use the same ﬁve QA datasets and train-                    tion will be discarded. For SQuAD and Natural
                   ing/dev/testing splitting method as in previous               Questions, since the original passages have been
                   work(Leeetal., 2019). Below we brieﬂy describe                split and processed differently than our pool of
                   each dataset and refer readers to their paper for the         candidate passages, we match and replace each
                   details of data preparation.                                  gold passage with the corresponding passage in the
                   Natural Questions (NQ) (Kwiatkowski et al.,                   candidate pool.7 We discard the questions when
                   2019) was designed for end-to-end question an-                the matching is failed due to different Wikipedia
                   swering. The questions were mined from real                   versions or pre-processing. Table 1 shows the num-
                   Google search queries and the answers were spans              ber of questions in training/dev/test sets for all the
                   in Wikipedia articles identiﬁed by annotators.                datasets and the actual questions used for training
                   TriviaQA(Joshietal.,2017)containsasetoftrivia                 the retriever.
                   questionswithanswersthatwereoriginallyscraped
                   from the Web.                                                 5    Experiments: Passage Retrieval
                   WebQuestions(WQ)(Berantetal.,2013)consists                    In this section, we evaluate the retrieval perfor-
                   of questions selected using Google Suggest API,               mance of our Dense Passage Retriever (DPR),
                   where the answers are entities in Freebase.                   along with analysis on how its output differs from
                                                           ˇ         ˇ     `
                   CuratedTREC (TREC) (Baudis and Sedivy,
                   2015) sources questions from TREC QA tracks                      6Weusetheunﬁltered TriviaQA version and discard the
                                                                                 noisy evidence documents mined from Bing.
                       5However, Wang et al. (2019) also propose splitting docu-    7The improvement of using gold contexts over passages
                   ments into overlapping passages, which we do not ﬁnd advan-   that contain answers is small.  See Section 5.2 and Ap-
                   tageous compared to the non-overlapping version.              pendix A.
                     Training  Retriever                      Top-20                                  Top-100
                                              NQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD
                     None      BM25           59.1    66.9     55.0   70.9     68.8    73.7    76.7     71.1   84.1    80.0
                     Single    DPR            78.4    79.4     73.2   79.8     63.2    85.4    85.0     81.4   89.1    77.2
                               BM25+DPR 76.6          79.8     71.0   85.2     71.5    83.8    84.5     80.5   92.7    81.3
                     Multi     DPR            79.4    78.8     75.0   89.1     51.6    86.0    84.7     82.9   93.9    67.6
                               BM25+DPR 78.0          79.9     74.7   88.5     66.2    83.9    84.4     82.3   94.1    78.6
                  Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved
                  passages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained
                  using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.
                  traditional retrieval methods, the effects of different      90
                  training schemes and the run-time efﬁciency.
                     The DPRmodelusedinourmainexperiments                      80
                  is trained using the in-batch negative setting (Sec-
                  tion 3.2) with a batch size of 128 and one additional        70
                  BM25negative passage per question. We trained                                                BM25
                  the question and passage encoders for up to 40               60                              # Train: 1k
                  epochs for large datasets (NQ, TriviaQA, SQuAD)              Top-k accuracy (%)              # Train: 10k
                  and 100 epochs for small datasets (TREC, WQ),                50                              # Train: 20k
                                                                                                               # Train: 40k
                  with a learning rate of 10−5 using Adam, linear                                              # Train: all (59k)
                  scheduling with warm-up and dropout rate 0.1.                40        20      40       60      80      100
                     While it is good to have the ﬂexibility to adapt                      k: # of retrieved passages
                  the retriever to each dataset, it would also be de-      Figure 1: Retriever top-k accuracy with different num-
                  sirable to obtain a single retriever that works well     bers of training examples used in our dense passage re-
                  across the board. To this end, we train a multi-         triever vs BM25. The results are measured on the de-
                  dataset encoder by combining training data from          velopment set of Natural Questions. Our DPR trained
                  all datasets excluding SQuAD.8 In addition to DPR,       using 1,000 examples already outperforms BM25.
                  wealsopresent the results of BM25, the traditional
                                   9
                  retrieval method and BM25+DPR, using a linear            tiple datasets, TREC, the smallest dataset of the
                  combination of their scores as the new ranking           ﬁve, beneﬁts greatly from more training examples.
                  function. Speciﬁcally, we obtain two initial sets        In contrast, Natural Questions and WebQuestions
                  of top-2000 passages based on BM25 and DPR,              improve modestly and TriviaQA degrades slightly.
                  respectively, and rerank the union of them using         Results can be improved further in some cases by
                  BM25(q,p) + λ·sim(q,p) as the ranking function.          combining DPR with BM25 in both single- and
                  Weusedλ=1.1basedontheretrievalaccuracyin                 multi-dataset settings.
                  the development set.                                        We conjecture that the lower performance on
                  5.1   MainResults                                        SQuADis due to two reasons. First, the annota-
                  Table 2 compares different passage retrieval sys-        tors wrote questions after seeing the passage. As
                  tems on ﬁve QA datasets, using the top-k accuracy        a result, there is a high lexical overlap between
                  (k ∈ {20,100}). With the exception of SQuAD,             passages and questions, which gives BM25 a clear
                  DPRperforms consistently better than BM25 on             advantage. Second, the data was collected from
                  all datasets. The gap is especially large when k is      only 500+ Wikipedia articles and thus the distribu-
                  small (e.g., 78.4% vs. 59.1% for top-20 accuracy         tion of training examples is extremely biased, as
                  on Natural Questions). When training with mul-           argued previously by Lee et al. (2019).
                     8SQuADislimitedtoasmallset of Wikipedia documents     5.2   Ablation Study on Model Training
                  and thus introduces unwanted bias. We will discuss this issue
                  moreinSection 5.1.                                       Tounderstand further how different model training
                     9Lucene implementation. BM25 parameters b = 0.4 (doc-
                  ument length normalization) and k = 0.9 (term frequency  options affect the results, we conduct several addi-
                                                 1
                  scaling) are tuned using development sets.               tional experiments and discuss our ﬁndings below.
                  Sample efﬁciency       Weexplore how many train-          Type         #N        IB Top-5 Top-20 Top-100
                  ing examples are needed to achieve good passage           Random       7          7   47.0    64.3     77.8
                  retrieval performance. Figure 1 illustrates the top-k     BM25         7          7   50.0    63.3     74.8
                  retrieval accuracy with respect to different num-         Gold         7          7   42.6    63.1     78.3
                  bers of training examples, measured on the devel-         Gold         7         3 51.1       69.1     80.8
                  opment set of Natural Questions. As is shown, a           Gold         31        3 52.1       70.8     82.1
                                                                            Gold         127       3 55.8       73.0     83.1
                  dense passage retriever trained using only 1,000 ex-      G.+BM25(1) 31+32       3 65.0       77.3     84.4
                  amples already outperforms BM25. This suggests            G.+BM25(2) 31+64       3 64.5       76.4     84.0
                  that with a general pretrained language model, it is      G.+BM25(1) 127+128     3 65.8       78.0     84.9
                  possible to train a high-quality dense retriever with
                  a small number of question–passage pairs. Adding         Table 3: Comparison of different training schemes,
                  more training examples (from 1k to 59k) further          measured as top-k retrieval accuracy on Natural Ques-
                  improves the retrieval accuracy consistently.            tions (development set).    #N: number of negative
                                                                                                                         (1)
                                                                           examples, IB: in-batch training.    G.+BM25       and
                                                                                     (2)
                  In-batch negative training        We test different      G.+BM25       denote in-batch training with 1 or 2 ad-
                  training schemes on the development set of Natural       ditional BM25 negatives, which serve as negative pas-
                  Questions and summarize the results in Table 3.          sages for all questions in the batch.
                  Thetopblock is the standard 1-of-N training set-
                  ting, where each question in the batch is paired         Our experiments on Natural Questions show that
                  with a positive passage and its own set of n neg-        switching to distantly-supervised passages (using
                  ative passages (Eq. (2)). We ﬁnd that the choice         the highest-ranked BM25 passage that contains the
                  of negatives — random, BM25 or gold passages             answer), has only a small impact: 1 point lower
                  (positive passages from other questions) — does          top-k accuracy for retrieval. Appendix A contains
                  not impact the top-k accuracy much in this setting       moredetails.
                  whenk ≥20.                                               Similarity and loss     Besides dot product, cosine
                     Themiddlebockisthein-batchnegativetraining            andEuclideanL2distancearealsocommonlyused
                  (Section 3.2) setting. We ﬁnd that using a similar       asdecomposablesimilarityfunctions. Wetestthese
                  conﬁguration (7 gold negative passages), in-batch        alternatives and ﬁnd that L2 performs compara-
                  negative training improves the results substantially.    ble to dot product, and both of them are superior
                  Thekeydifference between the two is whether the          to cosine. Similarly, in addition to negative log-
                  gold negative passages come from the same batch          likelihood, a popular option for ranking is triplet
                  or from the whole training set. Effectively, in-batch    loss, which compares a positive passage and a nega-
                  negative training is an easy and memory-efﬁcient         tive one directly with respect to a question (Burges
                  waytoreusethenegative examples already in the            et al., 2005). Our experiments show that using
                  batch rather than creating new ones. It produces         triplet loss does not affect the results much. More
                  morepairs and thus increases the number of train-        details can be found in Appendix B.
                  ing examples, which might contribute to the good
                  model performance. As a result, accuracy consis-         Cross-dataset generalization        One interesting
                  tently improves as the batch size grows.                 question regarding DPR’s discriminative training
                     Finally, we explore in-batch negative training        is how much performance degradation it may suf-
                  with additional “hard” negative passages that have       fer from a non-iid setting. In other words, can
                  high BM25 scores given the question, but do not          it still generalize well when directly applied to
                  containtheanswerstring(thebottomblock). These            a different dataset without additional ﬁne-tuning?
                  additional passages are used as negative passages        To test the cross-dataset generalization, we train
                  for all questions in the same batch. We ﬁnd that         DPRonNaturalQuestionsonlyandtest it directly
                  adding a single BM25 negative passage improves           on the smaller WebQuestions and CuratedTREC
                  the result substantially while adding two does not       datasets. We ﬁnd that DPR generalizes well, with
                  help further.                                            3-5 points loss from the best performing ﬁne-tuned
                                                                           model in top-20 retrieval accuracy (69.9/86.3 vs.
                  Impact of gold passages       Weuse passages that        75.0/89.1 for WebQuestions and TREC, respec-
                  match the gold contexts in the original datasets         tively), while still greatly outperforming the BM25
                  (whenavailable) as positive examples (Section 4.2).      baseline (55.0/70.9).
                           5.3      Qualitative Analysis                                                         score is chosen as the ﬁnal answer. The passage
                           Although DPRperforms better than BM25 in gen-                                         selection model serves as a reranker through cross-
                           eral, passages retrieved by these two methods dif-                                    attention between the question and the passage. Al-
                           fer qualitatively.             Term-matching methods like                             though cross-attention is not feasible for retrieving
                           BM25aresensitive to highly selective keywords                                         relevant passages in a large corpus due to its non-
                           and phrases, while DPR captures lexical variations                                    decomposable nature, it has more capacity than the
                           or semantic relationships better. See Appendix C                                      dual-encoder model sim(q,p) as in Eq. (1). Apply-
                           for examples and more discussion.                                                     ing it to selecting the passage from a small number
                                                                                                                 of retrieved candidates has been shown to work
                           5.4      Run-timeEfﬁciency                                                           well (Wang et al., 2019, 2018; Lin et al., 2018).
                           Themainreasonthat we require a retrieval compo-                                           Speciﬁcally, let Pi ∈ RL×h (1 ≤ i ≤ k) be
                           nent for open-domain QA is to reduce the number                                       a BERT (base, uncased in our experiments) rep-
                           of candidate passages that the reader needs to con-                                   resentation for the i-th passage, where L is the
                           sider, which is crucial for answering user’s ques-                                    maximumlengthofthepassageandhthehidden
                           tions in real-time. We proﬁled the passage retrieval                                  dimension. The probabilities of a token being the
                           speed on a server with Intel Xeon CPU E5-2698 v4                                      starting/ending positions of an answer span and a
                          @2.20GHzand512GBmemory. Withthehelpof                                                  passage being selected are deﬁned as:
                                                                                                       10                                                                      
                           FAISSin-memoryindexforreal-valued vectors ,                                                    P        (s)      = softmax P w                         ,         (3)
                           DPRcanbemadeincrediblyefﬁcient, processing                                                       start,i                                  i    start  s
                           995.0 questions per second, returning top 100 pas-                                              Pend,i(t)        = softmax Piwendt,                             (4)
                           sages per question. In contrast, BM25/Lucene (im-                                                                                     ˆ|                  
                                                                                                                        Pselected(i)        = softmax P w                               ,   (5)
                           plemented in Java, using ﬁle index) processes 23.7                                                                                             selected i
                           questions per second per CPU thread.                                                             ˆ              [CLS]               [CLS]               h×k
                                                                                                                where P = [P                       , . . . , P         ]   ∈ R            and
                               Ontheotherhand,thetimerequiredforbuilding                                                                   1                   k
                                                                                                                wstart,wend,wselected ∈ Rh are learnable vectors.
                           an index for dense vectors is much longer. Com-                                      Wecomputeaspanscoreofthes-thtot-thwords
                           puting dense embeddings on 21-million passages                                        from the i-th passage as Pstart,i(s) × Pend,i(t), and
                           is resource intensive, but can be easily parallelized,                                a passage selection score of the i-th passage as
                           taking roughly 8.8 hours on 8 GPUs. However,                                          P           (i).
                           building the FAISS index on 21-million vectors                                          selected
                           onasingle server takes 8.5 hours. In comparison,                                          During training, we sample one positive and
                           building an inverted index using Lucene is much                                       m˜ −1negative passages from the top 100 passages
                           cheaper and takes only about 30 minutes in total.                                     returned by the retrieval system (BM25 or DPR)
                                                                                                                 for each question. m˜ is a hyper-parameter and we
                           6     Experiments: Question Answering                                                 usem˜ = 24inalltheexperiments. Thetrainingob-
                                                                                                                 jective is to maximize the marginal log-likelihood
                           In this section, we experiment with how different                                     of all the correct answer spans in the positive pas-
                           passage retrievers affect the ﬁnal QA accuracy.                                       sage (the answer string may appear multiple times
                                                                                                                 in one passage), combined with the log-likelihood
                           6.1      End-to-end QASystem                                                          of the positive passage being selected. We use the
                           Weimplementanend-to-end question answering                                            batch size of 16 for large (NQ, TriviaQA, SQuAD)
                           system in which we can plug different retriever                                       and 4 for small (TREC, WQ) datasets, and tune k
                           systems directly. Besides the retriever, our QA sys-                                  onthe development set. For experiments on small
                           tem consists of a neural reader that outputs the                                      datasets under the Multi setting, in which using
                           answer to the question. Given the top k retrieved                                     other datasets is allowed, we ﬁne-tune the reader
                           passages (up to 100 in our experiments), the reader                                   trained on Natural Questions to the target dataset.
                           assigns a passage selection score to each passage.                                   All experiments were done on eight 32GB GPUs.
                           In addition, it extracts an answer span from each                                     6.2      Results
                           passage and assigns a span score. The best span
                           fromthepassagewiththehighestpassageselection                                         Table 4 summarizes our ﬁnal end-to-end QA re-
                              10FAISSconﬁguration: weusedHNSWindextypeonCPU,                                     sults, measured by exact match with the reference
                           neighbors to store per node = 512, construction time search                           answer after minor normalization as in (Chen et al.,
                           depth = 200, search depth = 128.                                                      2017; Lee et al., 2019). From the table, we can
                        Training Model                                      NQ TriviaQA WQ TREC SQuAD
                        Single       BM25+BERT(Leeetal.,2019)               26.5      47.1       17.7    21.3       33.2
                        Single       ORQA(Leeetal.,2019)                    33.3      45.0       36.4    30.1       20.2
                        Single       HardEM(Minetal.,2019a)                 28.1      50.9        -        -         -
                        Single       GraphRetriever (Min et al., 2019b)     34.5      56.0       36.4      -         -
                        Single       PathRetriever (Asai et al., 2020)      32.6        -         -        -        56.5
                        Single       REALMWiki (Guuetal., 2020)             39.2        -        40.2    46.8        -
                        Single       REALMNews (Guuetal., 2020)             40.4        -        40.7    42.9        -
                                     BM25                                   32.6      52.4       29.9    24.9       38.1
                        Single       DPR                                    41.5      56.8       34.6    25.9       29.8
                                     BM25+DPR                               39.0      57.0       35.2    28.0       36.7
                        Multi        DPR                                    41.5      56.8       42.4    49.4       24.1
                                     BM25+DPR                               38.8      57.9       41.1    50.6       35.8
                  Table 4: End-to-end QA (Exact Match) Accuracy. The ﬁrst block of results are copied from their cited papers.
                  REALM        and REALM        are the same model but pretrained on Wikipedia and CC-News, respectively. Single
                           Wiki            News
                  andMultidenotethat our Dense Passage Retriever (DPR) is trained using individual or combined training datasets
                  (all except SQuAD). For WQ and TREC in the Multi setting, we ﬁne-tune the reader trained on NQ.
                  see that higher retriever accuracy typically leads to    trained, following Lee et al. (2019). This approach
                  better ﬁnal QA results: in all cases except SQuAD,       obtains a score of 39.8 EM, which suggests that our
                  answers extracted from the passages retrieved by         strategy of training a strong retriever and reader in
                  DPR are more likely to be correct, compared to           isolation can leverage effectively available supervi-
                  those from BM25. For large datasets like NQ and          sion, while outperforming a comparable joint train-
                  TriviaQA, models trained using multiple datasets         ing approach with a simpler design (Appendix D).
                  (Multi) perform comparably to those trained using           Onething worth noticing is that our reader does
                  the individual training set (Single). Conversely,        consider more passages compared to ORQA, al-
                  onsmaller datasets like WQ and TREC, the multi-          though it is not completely clear how much more
                  dataset setting has a clear advantage. Overall, our      time it takes for inference. While DPR processes
                  DPR-basedmodelsoutperform the previous state-            up to 100 passages for each question, the reader
                  of-the-art results on four out of the ﬁve datasets,      is able to ﬁt all of them into one batch on a sin-
                  with1%to12%absolutedifferencesinexactmatch               gle 32GB GPU, thus the latency remains almost
                  accuracy. It is interesting to contrast our results to   identical to the single passage case (around 20ms).
                  those of ORQA (Lee et al., 2019) and also the            Theexact impact on throughput is harder to mea-
                  concurrently developed approach, REALM (Guu              sure: ORQAuses2-3xlongerpassages compared
                  et al., 2020). While both methods include addi-          to DPR (288 word pieces compared to our 100
                  tional pretraining tasks and employ an expensive         tokens) and the computational complexity is super-
                  end-to-end training regime, DPR manages to out-          linear in passage length. We also note that we
                  perform them on both NQ and TriviaQA, simply             found k = 50 to be optimal for NQ, and k = 10
                  byfocusing on learning a strong passage retrieval        leads to only marginal loss in exact match accu-
                  model using pairs of questions and answers. The          racy (40.8 vs. 41.5 EM on NQ), which should be
                  additional pretraining tasks are likely more useful      roughly comparable to ORQA’s 5-passage setup.
                  only when the target training sets are small. Al-
                  though the results of DPR on WQ and TREC in the          7    Related Work
                  single-dataset setting are less competitive, adding      Passage retrieval has been an important compo-
                  morequestion–answer pairs helps boost the perfor-        nent for open-domain QA (Voorhees, 1999). It
                  mance, achieving the new state of the art.               not only effectively reduces the search space for
                     Tocompareourpipeline training approach with           answer extraction, but also identiﬁes the support
                  joint learning, we run an ablation on Natural Ques-      contextforuserstoverifytheanswer. Strongsparse
                  tions where the retriever and reader are jointly         vector space models like TF-IDF or BM25 have
                  been used as the standard method applied broadly         effective solution that shows stronger empirical per-
                  to various QA tasks (e.g., Chen et al., 2017; Yang       formance, without relying on additional pretraining
                  et al., 2019a,b; Nie et al., 2019; Min et al., 2019a;    or complex joint training schemes.
                  Wolfson et al., 2020). Augmenting text-based re-            DPRhas also been used as an important mod-
                  trieval with external structured information, such       ule in very recent work. For instance, extending
                  as knowledge graph and Wikipedia hyperlinks, has         the idea of leveraging hard negatives, Xiong et al.
                  also been explored recently (Min et al., 2019b; Asai     (2020a) use the retrieval model trained in the pre-
                  et al., 2020).                                           vious iteration to discover new negatives and con-
                     The use of dense vector representations for re-       struct a different set of examples in each training
                  trieval has a long history since Latent Semantic         iteration. Starting from our trained DPR model,
                  Analysis (Deerwester et al., 1990). Using labeled        they show that the retrieval performance can be
                  pairs of queries and documents, discriminatively         further improved. Recent work (Izacard and Grave,
                  trained dense encoders have become popular re-           2020; Lewis et al., 2020b) have also shown that
                  cently (Yih et al., 2011; Huang et al., 2013; Gillick    DPR can be combined with generation models
                  et al., 2019), with applications to cross-lingual        such as BART (Lewis et al., 2020a) and T5 (Raf-
                  document retrieval, ad relevance prediction, Web         fel et al., 2019), achieving good performance on
                  search and entity retrieval. Such approaches com-        open-domain QA and other knowledge-intensive
                  plement the sparse vector methods as they can po-        tasks.
                  tentially give high similarity scores to semantically    8   Conclusion
                  relevant text pairs, even without exact token match-
                  ing. The dense representation alone, however, is         In this work, we demonstrated that dense retrieval
                  typically inferior to the sparse one. While not the      can outperform and potentially replace the tradi-
                  focus of this work, dense representations from pre-      tional sparse retrieval component in open-domain
                  trained models, along with cross-attention mecha-        question answering. While a simple dual-encoder
                  nisms, have also been shown effective in passage         approach can be made to work surprisingly well,
                  or dialogue re-ranking tasks (Nogueira and Cho,          weshowedthatthereare some critical ingredients
                  2019; Humeauet al., 2020). Finally, a concurrent         to training a dense retriever successfully. Moreover,
                  work (Khattab and Zaharia, 2020) demonstrates            our empirical analysis and ablation studies indicate
                  the feasibility of full dense retrieval in IR tasks.     that more complex model frameworks or similarity
                  Instead of employing the dual-encoder framework,         functions do not necessarily provide additional val-
                  they introduced a late-interaction operator on top       ues. As a result of improved retrieval performance,
                  of the BERT encoders.                                    weobtainednewstate-of-the-artresults on multiple
                     Dense retrieval for open-domain QA has been           open-domain question answering benchmarks.
                  explored by Das et al. (2019), who propose to re-        Acknowledgments
                  trieve relevant passages iteratively using reformu-      Wethanktheanonymousreviewersfortheirhelpful
                  lated question vectors. As an alternative approach       commentsandsuggestions.
                  that skips passage retrieval, Seo et al. (2019) pro-
                  posetoencodecandidateanswerphrasesasvectors
                  and directly retrieve the answers to the input ques-     References
                  tions efﬁciently. Using additional pretraining with
                  the objective that matches surrogates of questions       Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,
                  andrelevantpassages, Leeetal.(2019)jointlytrain             Richard Socher, and Caiming Xiong. 2020. Learn-
                  the question encoder and reader. Their approach             ing to retrieve reasoning paths over Wikipedia graph
                                                                              for question answering. In International Conference
                  outperforms the BM25 plus reader paradigm on                onLearning Representations (ICLR).
                  multiple open-domain QA datasets in QA accuracy,                    ˇ         ˇ     `
                  and is further extended by REALM (Guu et al.,            Petr Baudis and Jan Sedivy. 2015. Modeling of the
                  2020), which includes tuning the passage encoder            question answering task in the yodaqa system. In In-
                                                                              ternational Conference of the Cross-Language Eval-
                  asynchronously by re-indexing the passages dur-             uation Forum for European Languages, pages 222–
                  ing training. The pretraining objective has also            228. Springer.
                  recently been improved by Xiong et al. (2020b).          JonathanBerant,AndrewChou,RoyFrostig,andPercy
                  In contrast, our model provides a simple and yet            Liang. 2013. Semantic parsing on Freebase from
                     question-answerpairs. InEmpiricalMethodsinNat-            clickthrough data.  In ACM International Confer-
                     ural Language Processing (EMNLP).                         ence on Information and Knowledge Management
                                                                              (CIKM), pages 2333–2338.
                  Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard
                      ¨
                     Sackinger, and RoopakShah.1994. Signatureveriﬁ-        Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
                     cation using a “Siamese” time delay neural network.       and Jason Weston. 2020. Poly-encoders: Architec-
                     In NIPS, pages 737–744.                                   tures and pre-training strategies for fast and accurate
                  Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,          multi-sentence scoring. In International Conference
                     Matt Deeds, Nicole Hamilton, and Greg Hullender.          onLearning Representations (ICLR).
                     2005. Learning to rank using gradient descent. In      Gautier Izacard and Edouard Grave. 2020. Leveraging
                     Proceedingsofthe22ndinternationalconferenceon             passageretrievalwithgenerativemodelsforopendo-
                     Machine learning, pages 89–96.                            mainquestion answering. ArXiv, abs/2007.01282.
                  Danqi Chen, Adam Fisch, Jason Weston, and Antoine                                                 ´ ´
                     Bordes. 2017. Reading Wikipedia to answer open-        Jeff Johnson, Matthijs Douze, and Herve Jegou. 2017.
                     domain questions.    In Association for Computa-          Billion-scale similarity search with GPUs.  ArXiv,
                     tional Linguistics (ACL), pages 1870–1879.                abs/1702.08734.
                  Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,         Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
                     and Andrew McCallum. 2019. Multi-step retriever-          Zettlemoyer. 2017.   TriviaQA: A large scale dis-
                     readerinteractionforscalableopen-domainquestion           tantly supervised challenge dataset for reading com-
                     answering. In International Conference on Learn-          prehension. In Association for Computational Lin-
                     ing Representations (ICLR).                               guistics (ACL), pages 1601–1611.
                  Scott Deerwester, Susan T Dumais, George W Fur-           Omar Khattab and Matei Zaharia. 2020.       ColBERT:
                     nas, Thomas K Landauer, and Richard Harshman.             Efﬁcient and effective passage search via contextu-
                     1990. Indexing by latent semantic analysis. Jour-         alized late interaction over BERT. In ACM SIGIR
                     nal of the American society for information science,     Conference on Research and Development in Infor-
                     41(6):391–407.                                            mation Retrieval (SIGIR), pages 39–48.
                  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and             Brian Kulis. 2013. Metric learning: A survey. Foun-
                     Kristina Toutanova. 2019.   BERT: Pre-training of         dations and Trends in Machine Learning, 5(4):287–
                     deep bidirectional transformers for language under-       364.
                     standing. In North American Association for Com-
                     putational Linguistics (NAACL).                        Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
                                                                               ﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,
                  David A Ferrucci. 2012. Introduction to “This is Wat-        Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
                     son”. IBM Journal of Research and Development,            Jacob Devlin, Kenton Lee, Kristina N. Toutanova,
                     56(3.4):1–1.                                              Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
                  Daniel Gillick,   Sayali Kulkarni,    Larry Lansing,         Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
                     Alessandro Presta, Jason Baldridge, Eugene Ie, and        ral questions: a benchmark for question answering
                     Diego Garcia-Olano. 2019. Learning dense repre-           research. Transactions of the Association of Compu-
                     sentations for entity retrieval. In Computational Nat-    tational Linguistics (TACL).
                     ural Language Learning (CoNLL).                        KentonLee,Ming-WeiChang,andKristinaToutanova.
                  RuiqiGuo,SanjivKumar,KrzysztofChoromanski,and                2019. Latent retrieval for weakly supervised open
                     David Simcha. 2016. Quantization based fast inner         domainquestionanswering. InAssociationforCom-
                     product search. In Artiﬁcial Intelligence and Statis-     putational Linguistics (ACL), pages 6086–6096.
                     tics, pages 482–490.                                   Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
                  Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-              jan Ghazvininejad, Abdelrahman Mohamed, Omer
                     supat, and Ming-Wei Chang. 2020.          REALM:          Levy, Veselin Stoyanov, and Luke Zettlemoyer.
                     Retrieval-augmented language model pre-training.          2020a. BART:Denoisingsequence-to-sequencepre-
                     ArXiv, abs/2002.08909.                                    training for natural language generation, translation,
                                                                               and comprehension. In Association for Computa-
                  MatthewHenderson,RamiAl-Rfou,BrianStrope,Yun-                tional Linguistics (ACL), pages 7871–7880.
                                   ´   ´     ´
                     hsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Ku-
                     mar, Balint Miklos, and Ray Kurzweil. 2017. Efﬁ-       Patrick Lewis, Ethan Perez, Aleksandara Piktus,
                     cient natural language response suggestion for smart      Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
                                                                                           ¨
                     reply. ArXiv, abs/1705.00652.                             Heinrich Kuttler,   Mike Lewis, Wen-tau Yih,
                                                                                          ¨
                                                                              Tim Rocktaschel, Sebastian Riedel, and Douwe
                  Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,            Kiela. 2020b. Retrieval-augmented generation for
                     Alex Acero, and Larry Heck. 2013. Learning deep           knowledge-intensive NLP tasks.     In Advances in
                     structured semantic models for Web search using          Neural Information Processing Systems (NeurIPS).
                   YankaiLin,HaozheJi,ZhiyuanLiu,andMaosongSun.              Anshumali Shrivastava and Ping Li. 2014. Asymmet-
                     2018. Denoising distantly supervised open-domain           ric LSH (ALSH) for sublinear time maximum inner
                     question answering.    In Association for Computa-         product search (MIPS). In Advances in Neural In-
                     tional Linguistics (ACL), pages 1736–1745.                 formation Processing Systems (NIPS), pages 2321–
                   Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and              2329.
                     Luke Zettlemoyer. 2019a. A discrete hard EM ap-         Ellen M Voorhees. 1999. The TREC-8 question an-
                     proach for weakly supervised question answering.           swering track report. In TREC, volume 99, pages
                     In Empirical Methods in Natural Language Process-          77–82.
                     ing (EMNLP).
                                                                             ShuohangWang,MoYu,XiaoxiaoGuo,ZhiguoWang,
                   Sewon Min, Danqi Chen, Luke Zettlemoyer, and Han-            Tim Klinger, Wei Zhang, Shiyu Chang, Gerry
                     naneh Hajishirzi. 2019b. Knowledge guided text re-         Tesauro, Bowen Zhou, and Jing Jiang. 2018. Rˆ3:
                     trieval and reading for open domainquestionanswer-         Reinforced ranker-reader for open-domain question
                     ing. ArXiv, abs/1911.03868.                                answering. In Conference on Artiﬁcial Intelligence
                   Dan Moldovan, Marius Pas¸ca, Sanda Harabagiu, and            (AAAI).
                     Mihai Surdeanu. 2003. Performance issues and er-        Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nalla-
                     ror analysis in an open-domain question answering          pati, and Bing Xiang. 2019. Multi-passage BERT:
                     system. ACM Transactions on Information Systems            Aglobally normalized bert model for open-domain
                     (TOIS), 21(2):133–154.                                     question answering. In Empirical Methods in Natu-
                   Stephen Mussmann and Stefano Ermon. 2016. Learn-             ral Language Processing (EMNLP).
                     ingandinferenceviamaximuminnerproductsearch.            Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
                     In International Conference on Machine Learning            ner, Yoav Goldberg, Daniel Deutch, and Jonathan
                     (ICML), pages 2587–2596.                                   Berant. 2020.   Break it down: A question under-
                   Yixin Nie, Songhe Wang, and Mohit Bansal. 2019. Re-          standing benchmark. Transactions of the Associa-
                     vealing the importance of semantic retrieval for ma-       tion of Computational Linguistics (TACL).
                     chine reading at scale. In Empirical Methods in Nat-    Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,
                     ural Language Processing (EMNLP).                          Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold
                   RodrigoNogueiraandKyunghyunCho.2019. Passage                 Overwijk. 2020a.     Approximate nearest neighbor
                     re-ranking with BERT. ArXiv, abs/1901.04085.               negative contrastive learning for dense text retrieval.
                                                                                ArXiv, abs/2007.00808.
                   Colin Raffel, Noam Shazeer, Adam Roberts, Katherine       Wenhan Xiong, Hankang Wang, and William Yang
                     Lee, Sharan Narang, Michael Matena, Yanqi Zhou,            Wang.2020b. Progressivelypretraineddensecorpus
                     Wei Li, and Peter J Liu. 2019. Exploring the limits        index for open-domain question answering. ArXiv,
                     of transfer learning with a uniﬁed text-to-text trans-     abs/2005.00038.
                     former. ArXiv, abs/1910.10683.
                   PranavRajpurkar,JianZhang,KonstantinLopyrev,and           WeiYang,YuqingXie,AileenLin,XingyuLi,Luchen
                     Percy Liang. 2016.    SQuAD: 100,000+ questions            Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019a.
                     for machine comprehension of text. In Empirical            End-to-end open-domain question answering with
                     MethodsinNaturalLanguageProcessing(EMNLP),                 bertserini. In North American Association for Com-
                     pages 2383–2392.                                           putational Linguistics (NAACL), pages 72–77.
                   Parikshit RamandAlexanderGGray.2012. Maximum              WeiYang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming
                     inner-product search using cone trees. In Proceed-         Li, and Jimmy Lin. 2019b. Data augmentation for
                     ings of the 18th ACM SIGKDD international con-             bert ﬁne-tuning in open-domain question answering.
                     ference on Knowledge discovery and data mining,            ArXiv, abs/1904.06652.
                     pages 931–939.                                          Wen-tau Yih, Kristina Toutanova, John C Platt, and
                   AdamRoberts, Colin Raffel, and Noam Shazeer. 2020.           Christopher Meek. 2011. Learning discriminative
                     Howmuchknowledgecanyoupackintotheparam-                    projections for text similarity measures.   In Com-
                     eters of a language model? ArXiv, abs/2002.08910.          putational Natural Language Learning (CoNLL),
                   Stephen Robertson and Hugo Zaragoza. 2009.        The        pages 247–256.
                     probabilistic relevance framework: BM25 and be-
                     yond. Foundations and Trends in Information Re-
                     trieval, 3(4):333–389.
                   Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur
                     Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.
                     Real-time open-domain question answering with
                     dense-sparse phrase index. In Association for Com-
                     putational Linguistics (ACL).
                  A DistantSupervision                                                  Top-1 Top-5 Top-20 Top-100
                  Whentraining our ﬁnal DPR model using Natural            Gold          44.9     66.8     78.1       85.0
                  Questions, we use the passages in our collection         Dist. Sup.    43.9     65.3     77.1       84.4
                  that best match the gold context as the positive
                  passages. As some QA datasets contain only the          Table 5: Retrieval accuracy on the development set of
                  question and answer pairs, it is thus interesting       Natural Questions, trained on passages that match the
                  to see when using the passages that contain the         gold context (Gold) or the top BM25 passage that con-
                  answers as positives (i.e., the distant supervision     tains the answer (Dist. Sup.).
                  setting), whether there is a signiﬁcant performance      Sim Loss              Retrieval Accuracy
                  degradation. Using the question and answer to-                          Top-1 Top-5 Top-20 Top-100
                  gether as the query, we run Lucene-BM25 and pick
                  the top passage that contains the answer as the pos-     DP NLL          44.9    66.8     78.1      85.0
                  itive passage. Table 5 shows the performance of                Triplet   41.6    65.0     77.2      84.5
                  DPRwhentrained using the original setting and                  NLL       43.5    64.7     76.1      83.1
                  the distant supervision setting.                         L2    Triplet   42.2    66.0     78.1      84.9
                  B Alternative Similarity Functions &                    Table 6: Retrieval Top-k accuracy on the development
                      Triplet Loss                                        set of Natural Questions using different similarity and
                  In addition to dot product (DP) and negative log-       loss functions.
                  likelihood based on softmax (NLL), we also exper-
                  iment with Euclidean distance (L2) and the triplet      the correct answer, presumably by matching “body
                  loss. We negate L2 similarity scores before ap-         of water” with semantic neighbors such as sea and
                  plying softmax and change signs of question-to-         channel, even though no lexical overlap exists. The
                  positive and question-to-negative similarities when     second example is one where BM25 does better.
                  applying the triplet loss on dot product scores. The    Thesalient phrase “Thoros of Myr” is critical, and
                  margin value of the triplet loss is set to 1. Ta-       DPRisunabletocaptureit.
                  ble 6 summarizes the results. All these additional
                  experiments are conducted using the same hyper-         D JointTrainingofRetrieverand
                  parameters tuned for the baseline (DP, NLL).                 Reader
                    Notethattheretrievalaccuracyforour“baseline”          We ﬁx the passage encoder in our joint-training
                  settings reported in Table 5 (Gold) and Table 6         schemewhileallowing only the question encoder
                  (DP, NLL) is slightly better than those reported in     to receive backpropagation signal from the com-
                  Table 3. This is due to a better hyper-parameter        bined(retriever + reader) loss function. This allows
                  setting used in these analysis experiments, which       us to leverage the HNSW-based FAISS index for
                  is documented in our code release.                      efﬁcient low-latency retrieving, without reindexing
                  C Qualitative Analysis                                  the passages during model updates. Our loss func-
                                                                          tion largely follows ORQA’s approach, which uses
                  Although DPRperforms better than BM25 in gen-           log probabilities of positive passages selected from
                  eral, the retrieved passages of these two retrievers    the retriever model, and correct spans and passages
                  actually differ qualitatively. Methods like BM25        selected from the reader model. Since the passage
                  are sensitive to highly selective keywords and          encoder is ﬁxed, we could use larger amount of
                  phrases, but cannot capture lexical variations or se-   retrieved passages when calculating the retriever
                  mantic relationships well. In contrast, DPR excels      loss. Speciﬁcally, we get top 100 passages for each
                  at semantic representation, but might lack sufﬁcient    question in a mini-batch and use the method similar
                  capacity to represent salient phrases which appear      to in-batch negative training: all retrieved passages’
                  rarely. Table 7 illustrates this phenomenon with        vectors participate in the loss calculation for all
                  two examples. In the ﬁrst example, the top scor-        questions in a batch. Our training batch size is set
                  ing passage from BM25 is irrelevant, even though        to 16, which effectively gives 1,600 passages per
                  keywords such as England and Ireland appear mul-        question to calculate retriever loss. The reader still
                  tiple times. In comparison, DPR is able to return       uses 24 passages per question, which are selected
                            Question                          Passage received by BM25                                        Passage retrieved by DPR
                            Whatisthebodyofwater              Title:British Cycling                                           Title: Irish Sea
                            between England and Ireland?      . . . England is not recognised as a region by the UCI, and     . . . Annual trafﬁc between Great Britain and Ireland amounts
                                                              there is no English cycling team outside the Commonwealth       to over 12 million passengers and of traded goods. The Irish
                                                              Games. For those occasions, British Cycling selects and sup-    Sea is connected to the North Atlantic at both its northern
                                                              ports the England team. Cycling is represented on the Isle      and southern ends. To the north, the connection is through
                                                              of Man by the Isle of Man Cycling Association. Cycling in       the North Channel between Scotland and Northern Ireland
                                                              Northern Ireland is organised under Cycling Ulster, part of     and the Malin Sea. The southern end is linked to the Atlantic
                                                              the all-Ireland governing body Cycling Ireland. Until 2006,     through the St George’s Channel between Ireland and Pem-
                                                              a rival governing body existed, ...                             brokeshire, and the Celtic Sea. ...
                                                                                                                                      ˚
                            WhoplaysThorosofMyrin             Title: No One (Game of Thrones)                                 Title: Pal Sverre Hagen
                                                                                                                                ˚
                            GameofThrones?                    . . . He may be ”no one,” but there’s still enough of a person  Pal Sverre Valheim Hagen (born 6 November 1980) is a Nor-
                                                              left in him to respect, and admire who this girl is and what    wegian stage and screen actor. He appeared in the Norwe-
                                                              she’s become. Arya ﬁnally tells us something that we’ve kind    gian ﬁlm ”Max Manus” and played Thor Heyerdahl in the
                                                              of known all along, that she’s not no one, she’s Arya Stark     Oscar-nominated 2012 ﬁlm ”Kon-Tiki”. Pl Hagen was born
                                                              of Winterfell.” ”No One” saw the reintroduction of Richard      in Stavanger, Norway, the son of Roar Hagen, a Norwegian
                                                                                                                                                                                  ´
                                                              DormerandPaulKaye,whoportrayedBericDondarrionand                cartoonist whohaslongbeenassociatedwithNorwayslargest
                                                              ThorosofMyr,respectively, in the third season, ...              daily, ”VG”. He lived in Jtten, a neighborhood in the city of
                                                                                                                              Stavanger in south-western Norway. ...
                          Table 7: Examples of passages returned from BM25 and DPR. Correct answers are written in blue and the content
                          words in the question are written in bold.
                          fromthetop5positiveandtop30negativepassages
                          (from the set of top 100 passages retrieved from
                          the same question). The question encoder’s initial
                          state is taken from a DPR model previously trained
                          on the NQ dataset. The reader’s initial state is a
                          BERT-basemodel. In terms of the end-to-end QA
                          results, our joint-training scheme does not provide
                          better results compared to the usual retriever/reader
                          training pipeline, resulting in the same 39.8 exact
                          match score on NQ dev as in our regular reader
                          modeltraining.
