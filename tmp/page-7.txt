                                    Table 1: Semantic segmentation on ADE20K val with 150 categories. Mask classiﬁcation-based
                                    MaskFormeroutperforms the best per-pixel classiﬁcation approaches while using fewer parameters
                                    and less computation. We report both single-scale (s.s.) and multi-scale (m.s.) inference results with
                                    ±std. FLOPs are computed for the given crop size. Frames-per-second (fps) is measured on a V100
                                    GPUwithabatchsizeof1.3 Backbonespre-trained on ImageNet-22K are marked with †.
                                             method                  backbone      crop size     mIoU(s.s.)      mIoU(m.s.)       #params.     FLOPs        fps
                                             OCRNet[44]                R101c     520×520           -              45.3                -           -          -
                                             DeepLabV3+[8]              R50c     512×512         44.0             44.9              44M         177G       21.0
                                         backbones                     R101c     512×512         45.5             46.4              63M         255G       14.2
                                             MaskFormer(ours)           R50      512×512         44.5 ±0.5        46.7 ±0.6         41M          53G       24.5
                                         CNN                           R101      512×512         45.5 ±0.5        47.2 ±0.2         60M          73G       19.5
                                                                       R101c     512×512         46.0 ±0.1        48.1 ±0.2         60M          80G       19.0
                                                                            †
                                             SETR[47]                 ViT-L      512×512           -              50.3             308M           -          -
                                                                      Swin-T     512×512           -              46.1              60M         236G       18.5
                                             Swin-UperNet [27, 43]    Swin-S†    512×512           -              49.3              81M         259G       15.2
                                                                      Swin-B     640×640           -              51.6             121M         471G         8.7
                                         backbones                    Swin-L†    640×640           -              53.5             234M         647G         6.2
                                                                      Swin-T     512×512         46.7 ±0.7        48.8 ±0.6         42M          55G       22.1
                                             MaskFormer(ours)         Swin-S     512×512         49.8 ±0.4        51.0 ±0.4         63M          79G       19.6
                                         ransformer                   Swin-B     640×640         51.1 ±0.2        52.3 ±0.4        102M         195G       12.6
                                         T                            Swin-B†    640×640         52.7 ±0.4        53.9 ±0.2        102M         195G       12.6
                                                                      Swin-L†    640×640         54.1 ±0.2        55.6 ±0.1        212M         375G         7.9
                                    Table 2: MaskFormervs.per-pixelclassiﬁcationbaselineson4semanticsegmentationdatasets.
                                    MaskFormer improvement is larger when the number of classes is larger. We use a ResNet-50
                                    backbone and report single scale mIoU and PQSt for ADE20K, COCO-Stuff and ADE20K-Full,
                                    whereas for higher-resolution Cityscapes we use a deeper ResNet-101 backbone following [7, 8].
                                                           Cityscapes (19 classes)    ADE20K(150classes)        COCO-Stuff(171classes)     ADE20K-Full(847classes)
                                                                             St                          St                         St                         St
                                                            mIoU          PQ           mIoU           PQ           mIoU          PQ           mIoU           PQ
                                    PerPixelBaseline      77.4         58.9          39.2          21.6          32.4         15.5          12.4           5.8
                                    PerPixelBaseline+     78.5         60.2          41.9          28.3          34.2         24.6          13.9           9.0
                                    MaskFormer(ours) 78.5(+0.0)        63.1 (+2.9)   44.5 (+2.6)   33.4 (+5.1)   37.1 (+2.9)  28.9 (+4.3)   17.4 (+3.5)   11.9 (+2.9)
                                    the general inference (Section 3.4) with the following parameters: we ﬁlter out masks with class
                                    conﬁdence below 0.8 and set masks whose contribution to the ﬁnal panoptic segmentation is less
                                    than 80% of its mask area to VOID. We report performance of single scale inference.
                                    4.3    Mainresults
                                    Semantic segmentation. In Table 1, we compare MaskFormer with state-of-the-art per-pixel classi-
                                    ﬁcation models for semantic segmentation on the ADE20K val set. With the same standard CNN
                                    backbones(e.g., ResNet [20]), MaskFormer outperforms DeepLabV3+ [8] by 1.7 mIoU. MaskFormer
                                    is also compatible with recent Vision Transformer [15] backbones (e.g., the Swin Transformer [27]),
                                    achieving a new state-of-the-art of 55.6 mIoU, which is 2.1 mIoU better than the prior state-of-the-
                                    art [27]. Observe that MaskFormer outperforms the best per-pixel classiﬁcation-based models while
                                    having fewer parameters and faster inference time. This result suggests that the mask classiﬁcation
                                    formulation has signiﬁcant potential for semantic segmentation. See appendix for results on test set.
                                    Beyond ADE20K, we further compare MaskFormer with our baselines on COCO-Stuff-10K,
                                    ADE20K-Full as well as Cityscapes in Table 2 and we refer to the appendix for comparison with
                                    state-of-the-art methods on these datasets. The improvement of MaskFormer over PerPixelBase-
                                    line+ is larger when the number of classes is larger: For Cityscapes, which has only 19 categories,
                                    MaskFormerperforms similarly well as PerPixelBaseline+; While for ADE20K-Full, which has 847
                                    classes, MaskFormer outperforms PerPixelBaseline+ by 3.5 mIoU.
                                    Although MaskFormer shows no improvement in mIoU for Cityscapes, the PQSt metric increases
                                                  St                                                                                                      St
                                    by 2.9 PQ . We ﬁnd MaskFormer performs better in terms of recognition quality (RQ ) while
                                    lagging in per-pixel segmentation quality (SQSt) (we refer to the appendix for detailed numbers).
                                    This observation suggests that on datasets where class recognition is relatively easy to solve, the main
                                    challenge for mask classiﬁcation-based approaches is pixel-level accuracy (i.e., mask quality).
                                        3It isn’t recommended to compare fps from different papers: speed is measured in different environments.
                                    DeepLabV3+fpsarefromMMSegmentation[12],andSwin-UperNetfpsarefromtheoriginalpaper[27].
                                                                                                    7
