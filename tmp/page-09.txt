                                                                              9
       is the identity mapping. But in the above experiments f is ReLU as designed
       in [1], so Eqn.(5) and (8) are approximate in the above experiments. Next we
       investigate the impact of f.
           We want to make f an identity mapping, which is done by re-arranging
       the activation functions (ReLU and/or BN). The original Residual Unit in [1]
       has a shape in Fig. 4(a) — BN is used after each weight layer, and ReLU is
       adopted after BN except that the last ReLU in a Residual Unit is after element-
       wise addition (f = ReLU). Fig. 4(b-e) show the alternatives we investigated,
       explained as following.
       4.1   Experiments on Activation
       In this section we experiment with ResNet-110 and a 164-layer Bottleneck [1]
       architecture (denoted as ResNet-164). A bottleneck Residual Unit consist of a
       1×1 layer for reducing dimension, a 3×3 layer, and a 1×1 layer for restoring
       dimension. As designed in [1], its computational complexity is similar to the
       two-3×3 Residual Unit. More details are in the appendix. The baseline ResNet-
       164 has a competitive result of 5.93% on CIFAR-10 (Table 2).
           BNafter addition. Before turning f into an identity mapping, we go the
       opposite way by adopting BN after addition (Fig. 4(b)). In this case f involves
       BN and ReLU. The results become considerably worse than the baseline (Ta-
       ble 2). Unlike the original design, now the BN layer alters the signal that passes
       through the shortcut and impedes information propagation, as reﬂected by the
       diﬃculties on reducing training loss at the beginning of training (Fib. 6 left).
           ReLU before addition. A na¨ıve choice of making f into an identity map-
       ping is to move the ReLU before addition (Fig. 4(c)). However, this leads to a
       non-negative output from the transform F, while intuitively a “residual” func-
       tion should take values in (−∞,+∞). As a result, the forward propagated sig-
       nal is monotonically increasing. This may impact the representational ability,
       and the result is worse (7.84%, Table 2) than the baseline. We expect to have
       a residual function taking values in (−∞,+∞). This condition is satisﬁed by
       other Residual Units including the following ones.
           Post-activation or pre-activation? In the original design (Eqn.(1) and
       Eqn.(2)), the activation xl+1 = f(yl) aﬀects both paths in the next Residual
       Unit: y    = f(y ) + F(f(y ),W    ). Next we develop an asymmetric form
              l+1       l         l   l+1
                          ˆ                                     ˆ
       where an activation f only aﬀects the F path: yl+1 = yl + F(f(yl),Wl+1), for
       any l (Fig. 5 (a) to (b)). By renaming the notations, we have the following form:
                                             ˆ
                               xl+1 = xl +F(f(xl),Wl),.                     (9)
       It is easy to see that Eqn.(9) is similar to Eqn.(4), and can enable a backward
       formulation similar to Eqn.(5). For this new Residual Unit as in Eqn.(9), the new
       after-addition activation becomes an identity mapping. This design means that
                                       ˆ
       if a new after-addition activation f is asymmetrically adopted, it is equivalent
                   ˆ
       to recasting f as the pre-activation of the next Residual Unit. This is illustrated
       in Fig. 5.
