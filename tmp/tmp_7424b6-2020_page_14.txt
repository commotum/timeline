                  Figure 3.3: On TriviaQA GPT3’s performance grows smoothly with model size, suggesting that language models
                  continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make signiﬁcant gains
                  over zero-shot behavior, matching and exceeding the performance of the SOTA ﬁne-tuned open-domain model, RAG
                  [LPP+20]
                  and/or the style of their answers are out-of-distribution for GPT-3. Nevertheless, GPT-3 appears able to adapt to this
                  distribution, recovering strong performance in the few-shot setting.
                  OnNaturalQuestions(NQs)GPT-3achieves14.6%inthezero-shotsetting,23.0%intheone-shotsetting,and29.9%in
                  the few-shot setting, compared to 36.6% for ﬁne-tuned T5 11B+SSM. Similar to WebQS, the large gain from zero-shot
                  to few-shot may suggest a distribution shift, and may also explain the less competitive performance compared to
                  TriviaQA and WebQS. In particular, the questions in NQs tend towards very ﬁne-grained knowledge on Wikipedia
                  speciﬁcally which could be testing the limits of GPT-3’s capacity and broad pretraining distribution.
                  Overall, on one of the three datasets GPT-3’s one-shot matches the open-domain ﬁne-tuning SOTA. On the other two
                  datasets it approaches the performance of the closed-book SOTA despite not using ﬁne-tuning. On all 3 datasets, we
                  ﬁndthat performance scales very smoothly with model size (Figure 3.3 and Appendix H Figure H.7), possibly reﬂecting
                  the idea that model capacity translates directly to more ‘knowledge’ absorbed in the parameters of the model.
                  3.3  Translation
                  For GPT-2aﬁlter was used on a multilingual collection of documents to produce an English only dataset due to capacity
                  concerns. Even with this ﬁltering GPT-2 showed some evidence of multilingual capability and performed non-trivially
                  whentranslating between French and English despite only training on 10 megabytes of remaining French text. Since we
                  increase the capacity by over two orders of magnitude from GPT-2 to GPT-3, we also expand the scope of the training
                  dataset to include more representation of other languages, though this remains an area for further improvement. As
                  discussed in 2.2 the majority of our data is derived from raw Common Crawl with only quality-based ﬁltering. Although
                  GPT-3’s training data is still primarily English (93% by word count), it also includes 7% of text in other languages.
                  These languages are documented in the supplemental material. In order to better understand translation capability, we
                  also expand our analysis to include two additional commonly studied languages, German and Romanian.
                  Existing unsupervised machine translation approaches often combine pretraining on a pair of monolingual datasets
                  with back-translation [SHB15] to bridge the two languages in a controlled way. By contrast, GPT-3 learns from a
                  blend of training data that mixes many languages together in a natural way, combining them on a word, sentence,
                  and document level. GPT-3 also uses a single training objective which is not customized or designed for any task in
                  particular. However, our one / few-shot settings aren’t strictly comparable to prior unsupervised work since they make
                  use of a small amount of paired examples (1 or 64). This corresponds to up to a page or two of in-context training data.
                  Results are shown in Table 3.4. Zero-shot GPT-3, which only receives on a natural language description of the task,
                  still underperforms recent unsupervised NMT results. However, providing only a single example demonstration for
                                                                         14
