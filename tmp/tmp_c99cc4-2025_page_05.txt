                  Pan et al.                                                                                                                                   5
                  Figure 1. Depicts the comprehensive architecture of RetentiveBEV, which comprises multiple integral components.
                  whereN      represents the total keys sampled for each attention                 feature dimension, the time complexity is O(M2  D).
                           key
                                 C3C C           0      C3C C                                      Since it involves all pairwise interactions between his-
                  head. W 2 R         Head and W   2R       Head signify the learnable
                           i                      i
                  weights associated with each attention head, where C is the                      torical and current queries, the complexity grows
                  dimensionality of the features. A 2 ½0,1 symbolizes the com-                    quadratically with the number of BEV queries. In the
                                                      ij
                                                              N                                    self-attention module, the attention matrix between
                                                                key
                  puted attention weights, initialed by P A =1. Dp 2 R2                            historical and current BEV has size M M, leading to
                                                                     ij         ij
                                                              j =1                                 a space complexity of O(M2). Besides, storing the
                  represents the predicted offset for the reference point p, facili-               input feature representations contributes a complexity
                  tating a dynamic adjustment to the point’s location. The fea-                    of O(2M D), giving a total space complexity of
                  ture extraction at the adjusted location p+Dp is denoted by                           2
                                                                      ij                           O(M +2MD).
                  x(p+Dpij), with bilinear interpolation being the default
                  method for obtaining these features (Dai et al., 2017), illus-               Combiningthese two modules, the overall time complexity
                  trating the model’s capacity to adaptively focus and refine its          of our network is O(M  N  D+M2 D), and the space com-
                  perception based on the spatial context.                                 plexity is O(N  M +M2+(N +M)D).
                     For the time and space complexity analysis of our work
                  shown in Figure 1, there are two key modules: RSCA and
                  TSA. These two modules determine the inference time com-                 Attention queries
                  plexity and space complexity of the network:
                                                                                           The Attention Query is a critical element within the attention
                     1.   RSCA: In this module, cross-attention is performed               mechanism, serving as the vector or tensor that directs the
                          based on multi-view input features. Let N be the num-            focus of attention. This query vector is essential for calculating
                          ber of multi-view features, M be the number of BEV               attention scores, determining the areas of focus within the
                          queries, and D be the feature dimension. The time                mechanism. The key and value vectors, essential for the atten-
                          complexity is O(M N D). The attention calculation              tion calculation, are derived from the feature extraction of the
                          scales linearly with the number of input views and               input image: after the original image is processed through the
                          BEV queries. During cross-attention calculation, the             backbonenetwork, it is transformed into a tensor representing
                          attention matrix of size N M needs to be stored.                high-dimensional features. Additional convolution layers then
                          Therefore, the space complexity is O(N  M). In addi-            produce the key and value vectors for the targeted ROI. In the
                          tion, storing the feature representations contributes a          traditional method of computing visual attention, a query vec-
                          complexity of O((N +M)D), resulting in a total                  tor q, along with a set of key vectors k1,k2,:::,kn,isspecified
                          space complexity of O(N M +(N +M)D).                           for an ROI. The attention score a for the given area results
                                                                                                                                  i
                     2.   TSA: This module involves self-attention between his-            from the dot product between the query vector and each key
                          torical and current BEV queries. Assuming both his-              vector ki followed by normalization via a softmax function to
                          torical and current BEV have M queries, and D is the             yield the attention weight b . The final attention output is
                                                                                                                            i
