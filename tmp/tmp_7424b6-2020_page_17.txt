                             Setting         PIQA   ARC(Easy)      ARC(Challenge)  OpenBookQA
                             Fine-tuned SOTA 79.4   92.0[KKS+20]   78.5[KKS+20]    87.2[KKS+20]
                             GPT-3Zero-Shot  80.5*  68.8           51.4            57.6
                             GPT-3One-Shot   80.5*  71.2           53.2            58.8
                             GPT-3Few-Shot   82.8*  70.1           51.5            65.4
               Table 3.6: GPT-3 results on three commonsense reasoning tasks, PIQA, ARC, and OpenBookQA. GPT-3 Few-Shot
               PIQAresult is evaluated on the test server. See Section 4 for details on potential contamination issues on the PIQA test
               set.
               Figure 3.6: GPT-3 results on PIQA in the zero-shot, one-shot, and few-shot settings. The largest model achieves a
               score on the development set in all three conditions that exceeds the best recorded score on the task.
               such as the adversarially-mined Winogrande dataset [SBBC19] still signiﬁcantly lag human performance. We test
               GPT-3’s performance on both Winograd and Winogrande, as usual in the zero-, one-, and few-shot setting.
               OnWinogradwetestGPT-3ontheoriginalsetof273Winogradschemas,usingthesame“partialevaluation” method
                              +
               described in [RWC 19]. Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which
               is presented as binary classiﬁcation and requires entity extraction to convert to the form described in this section. On
               Winograd GPT-3 achieves 88.3%, 89.7%, and 88.6% in the zero-shot, one-shot, and few-shot settings, showing no clear
               in-context learning but in all cases achieving strong results just a few points below state-of-the-art and estimated human
               performance. We note that contamination analysis found some Winograd schemas in the training data but this appears
               to have only a small effect on results (see Section 4).
               On the more difﬁcult Winogrande dataset, we do ﬁnd gains to in-context learning: GPT-3 achieves 70.2% in the
               zero-shot setting, 73.2% in the one-shot setting, and 77.7% in the few-shot setting. For comparison a ﬁne-tuned
               RoBERTAmodelachieves79%,state-of-the-art is 84.6% achieved with a ﬁne-tuned high capacity model (T5), and
               humanperformanceonthetaskasreported by [SBBC19] is 94.0%.
               3.5 CommonSenseReasoning
               Next we consider three datasets which attempt to capture physical or scientiﬁc reasoning, as distinct from sentence
               completion, reading comprehension, or broad knowledge question answering. The ﬁrst, PhysicalQA (PIQA) [BZB+19],
               asks commonsensequestionsabouthowthephysicalworldworksandisintendedasaprobeofgroundedunderstanding
               of the world. GPT-3 achieves 81.0% accuracy zero-shot, 80.5% accuracy one-shot, and 82.8% accuracy few-shot
               (the last measured on PIQA’s test server). This compares favorably to the 79.4% accuracy prior state-of-the-art of a
                                                             17
