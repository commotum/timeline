                                       TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
              Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A. A., and     Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,
                 Hardt, M. Test-time training with self-supervision for       Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-of-
                 generalization under distribution shifts. In Proceedings of  thought prompting elicits reasoning in large language
                 the 37th International Conference on Machine Learning.       models. In Advances in Neural Information Processing
                 PMLR,2020. URLhttp://proceedings.mlr.press/                  Systems 35, 2022. URL http://papers.nips.cc/pap
                v119/sun20b.html.                                             er files/paper/2022/hash/9d5609613524ecf4f15
                                                                              af0f7b31abca4-Abstract-Conference.html.
              Sun, Y., Li, X., Dalal, K., Xu, J., Vikram, A., Zhang, G.,                                  ¨
                 Dubois, Y., Chen, X., Wang, X., Koyejo, S., Hashimoto,     Wu,Z., Qiu, L., Ross, A., Akyurek, E., Chen, B., Wang, B.,
                T., and Guestrin, C. Learning to (learn at test time):        Kim, N., Andreas, J., and Kim, Y. Reasoning or reciting?
                 RNNswithexpressive hidden states, 2024. URL https:           Exploring the capabilities and limitations of language
                //arxiv.org/abs/2407.04620.                                   models through counterfactual tasks. In Proceedings of
                                                                              the 2024 Conference of the North American Chapter of
                                          ¨                                   the Association for Computational Linguistics: Human
              Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y.,
                 Chung, H. W., Chowdhery, A., Le, Q., Chi, E., Zhou, D.,      LanguageTechnologies. Association for Computational
                 and Wei, J. Challenging BIG-Bench tasks and whether          Linguistics, 2024. URL https://aclanthology.org
                 chain-of-thought can solve them. In Findings of the 2023     /2024.naacl-long.102.
                Conference of the Association for Computational Linguis-    Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao,
                 tics. Association for Computational Linguistics, 2023.       Y., and Narasimhan, K. Tree of Thoughts: Deliberate
                 doi: 10.18653/v1/2023.findings-acl.824. URL https:           problemsolvingwithlargelanguagemodels. InAdvances
                //aclanthology.org/2023.findings-acl.824.                     in Neural Information Processing Systems 36, 2023. URL
              Todd, E., Li, M., Sharma, A. S., Mueller, A., Wallace, B. C.,   http://papers.nips.cc/paper files/paper/2023/
                 and Bau, D. Function vectors in large language models.       hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstr
                 In The Twelfth International Conference on Learning          act-Conference.html.
                Representations, 2024. URL https://openreview.net           Zhao, S., Nguyen, T., and Grover, A. Probing the decision
                /forum?id=AwyxtyMwaG.                                         boundaries of in-context learning in large language mod-
              torchtune Maintainers and Contributors. torchtune: Py-          els. In ICML 2024 Workshop on In-Context Learning,
                Torch’s finetuning library, 2024. URL https://github          2024. URL https://openreview.net/forum?id=rf
                 .com/pytorch/torchtune.                                      CtCcPuSt.
              Veldkamp, K., Rosenbusch, H., Thoms, L., and Stevenson,
                 C. Solving ARCvisualanalogieswithneuralembeddings
                 and vector arithmetic: A generalized method. OSF, 2023.
                 doi: 10.17605/OSF.IO/AKP86. URL https://osf.io
                /akp86/.
              Wang, K. A., Shi, J., and Fox, E. B. Test-time regression:
                 a unifying framework for designing sequence models
                with associative memory. ArXiv preprint, 2025. URL
                 https://arxiv.org/abs/2501.12352.
              Wang, R., Zelikman, E., Poesia, G., Pu, Y., Haber, N.,
                 and Goodman, N. Hypothesis Search: Inductive reason-
                 ing with language models. In The Twelfth International
                Conference on Learning Representations, 2024. URL
                 https://openreview.net/forum?id=G7UtIGQmjm.
              Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi,
                 E. H., Narang, S., Chowdhery, A., and Zhou, D. Self-
                 consistency improves chain of thought reasoning in lan-
                 guage models. In The Eleventh International Confer-
                 ence on Learning Representations, 2023. URL https:
                //openreview.net/forum?id=1PL1NIMMrw.
                                                                        11
