                  TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                    ¨    1                  *1                  *1             1            1                1
                         EkinAkyurek       MehulDamani          AdamZweiger         Linlu Qiu    HanGuo JyothishPari
                                                            YoonKim1 JacobAndreas1
                                      Abstract
                                                                                   0.7          ARC                  BBH
                   Language models (LMs) have shown impressive
                   performance on tasks within their training distri-              0.6                                    57.8%
                   bution, but often struggle with structurally novel              0.5                         50.5%
                   tasks even when given a small number of in-                                       45.0%
                                                                                   0.4                                           Zero-
                   context task examples. We investigate the effec-                                                              Shot
                   tiveness of test-time training (TTT)—temporarily                0.3
                   updating model parameters during inference us-                 Accuracy
                   ing a loss derived from in-context examples—                    0.2    17.5%
                   as a mechanism for improving LMs’ reason-                       0.1
                   ing and few-shot learning capabilities. On the
                   Abstraction and Reasoning Corpus (ARC), per-                    0.0     FT       FT + TTT    Base    Base + TTT
                   forming TTT with in-context examples yields up             Figure 1. Pass@2 accuracy on a subset of 80 randomly selected
                   to 6× higher accuracy compared to fine-tuned               ARCvalidation tasks and overall accuracy on BIG-Bench Hard.
                   baselines—reaching 53.0% on the public valida-             Thezero-shotbaselineis0forARCand40.9%forBBH,indicated
                   tion set with an 8B-parameter LM and 61.9%                 by the dashed line. TTT boosts the performance of fine-tuned
                   when ensembled with program-synthesis meth-                models (FT) on ARC by 27.5 percentage points and increases
                   ods, matching average human performance. On                accuracy on BBH by 7.3 percentage points.
                   BIG-Bench Hard (BBH), TTT on in-context ex-
                   amples surpasses standard few-shot prompting
                   in the 10-shot setting by 7.3 percentage points            from their pre-training distributions. This question is funda-
                   (50.5%to57.8%). Ourfindings highlight the lim-             mental to understanding how, and whether, LMs can exhibit
                   itations of in-context learning for novel tasks and        the sort of flexible, novel-skill acquisition that has been
                   demonstrate the potential of test-time training to         proposed as a measure of intelligence (Chollet, 2019).
                   enhance language model adaptability.                       Solving complex and novel tasks remains extremely chal-
                                                                              lenging for LMs, and simple sampling approaches often
              1. Introduction                                                 yield poor performance on such problems (Wu et al., 2024;
                                                                              McCoyetal.,2024). However, recent progress has shown
              Large-scale neural language models (LMs) have demon-            that LMs can be substantially improved by adding extra test-
         arXiv:2411.07279v2  [cs.AI]  25 Mar 2025strated remarkable success on few-shot learning of taskstime computation. Several methods fall into this category,
              related to those seen during pre-training, as well as ele-      such as chain-of-thought prompting (Wei et al., 2022), sam-
              mentary variations or compositions of those tasks (Brown        pling with majority voting (self-consistency; Wang et al.,
              et al., 2020; Todd et al., 2024). When given natural lan-       2023), code execution (Brown et al., 2024; Snell et al., 2025;
              guage specifications or a small number of examples, LMs         Damanietal., 2025), and search (Yao et al., 2023).
              can often infer the desired task and generate appropriate out-  The idea of updating model parameters using instance-
              puts. However, an open question is whether these models         specific data at test time has roots in the literature on local
              can truly acquire new skills for which they have not been       learning (Bottou & Vapnik, 1992) and transductive learn-
              trained—particularly, tasks involving non-trivial reasoning,    ing (Joachims, 1999). In these methods, a learner refines its
              planning, and abstraction in domains that differ significantly  parameters or hypotheses after observing test inputs, adapt-
                 *Equal contribution 1Massachusetts Institute of Technology.  ing to individual examples or small clusters of examples.
              Correspondence to: Ekin Akyurek <akyurek@mit.edu>.              Suchapproaches inherently blur the line between training
                                                                              and inference, and can lead to robust adaptation in low-data
                                                                              scenarios or under distribution shift.
                                                                           1
                                            TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                Modernversions of these transductive ideas for deep neural           a sequence of input-output pairs (x ,y ),...,(x ,y ) and
                                                                                                                            1   1          k   k
                networks have been widely referred to as test-time train-            a new input x       , an LM can generate the corresponding
                                                                                                     k+1
                ing. In TTT, a model is updated at inference time using              output yˆ     bysampling from:
                                                                                              k+1
                only the current test instance or a small batch of test in-                    yˆ    ∼LM(·|x ,y ,...,x ,y ,x               )
                stances, typically through explicit gradient steps. While test-                  k+1              1   1        k   k   k+1
                time adaptation has been explored for vision models (Sun             While the possibility of in-context learning (ICL) as im-
                et al., 2020) and sequence architectures (Gandelsman et al.,         plicit machine learning simulation is discussed in previous
                                                                                                 ¨
                2022; Sun et al., 2024; Behrouz et al., 2025), its interac-          work(Akyureketal., 2023), empirical evidence shows that
                tion with other techniques for few-shot learning—especially          in-context learning with language models does not always
                in-context learning—remains less understood.                         resemble standard machine learning algorithms (Zhao et al.,
                In this paper, we investigate how to leverage TTT on top             2024; Min et al., 2022b). Furthermore, ICL often strug-
                of standard in-context learning (ICL) to boost performance           gles with novel tasks “out-of-the-box.” For example, large
                on challenging tasks that require reasoning or rule-based            language models exhibit poor performance on datasets like
                generalization. In-context learning is a powerful means              ARC(Opiełkaetal., 2024; Bober-Irizar & Banerjee, 2024).
                of adaptation without parameter updates, guided by short,            2.2. Test-Time Training
                task-specific prompts. We show that combining ICL with
                explicit gradient-based updates on test data can significantly       Test-time training (TTT) enables parametric models to adapt
                improve performance on particularly difficult tasks. Specifi-        during inference through dynamic parameter updates in re-
                cally, our main contributions1 are:                                  sponse to each test input. This approach remains relatively
                                                                                     unexplored in the era of large language models. The gen-
                  1. A systematic analysis of the key components for effec-          eral TTT process is as follows: starting with initial model
                     tive test-time training, including strategies for selecting     parameters θ0, for each test input (or batch of inputs) d, we
                     training data at inference, training objectives, and how        generate a temporary training dataset DTTT.             Wethen
                     TTTinteracts with an LM’s pre-trained parameters and            optimize these parameters to minimize a loss function
                     in-context learning.                                                                      X
                                                                                                  argmin              L(LM(d       ; θ)),
                                                                                                                                TTT
                  2. An application of TTT to two challenging benchmark                               θ     d   ∈D
                     suites—The Abstraction and Reasoning Corpus                                             TTT   TTT
                     (ARC; Chollet, 2019) and BIG-Bench Hard (BBH;                   resulting in temporarily updated parameters θd, which are
                                                                                                                          2
                     Srivastava et al., 2023; Suzgun et al., 2023).                  subsequently used for prediction.
                                                                                     In previous work (e.g., Sun et al., 2020), D         is typically
                                                                                                                                    TTT
                On ARC, our TTT approach outperforms existing open-                  constructed by applying an unsupervised objective (e.g.,
                source neural methods, attaining 53.0% accuracy with an              masked autoencoding) to the input x alone. In this paper,
                8B model and 61.9% when ensembled with a program-                    weextendTTTtothefew-shotlearningsetting, treating it
                synthesis approach (comparable to human performance).                as a form of transductive learning by leveraging few-shot
                OnBBH,TTTyields a 7.3% absolute improvement over                     demonstration examples to improve predictions. Although
                few-shot prompting, achieving 57.8% accuracy. Gains are              TTTcanalsobeappliedtochainofthought(CoT;Weietal.,
                particularly large on tasks involving structural rules or dis-       2022), we focus on direct transduction, where demonstra-
                tribution shifts (e.g., Dyck languages, Ruin names), where           tions consist of input-output pairs (x,y) without intermedi-
                TTTyields20–50percentagepoints of improvement over                   ate reasoning steps or explicit function descriptions.
                standard in-context prompting.                                       The few-shot learning setting we consider provides
                Overall, ourfindingshighlightthatTTTdrasticallyimproves              richer context in the form of demonstration pairs
                LM’sfew-shot learning ability on out-of-distribution tasks.          (x ,y ),...,(x ,y ). One simple method for TTT is Di-
                                                                                        1   1          K K
                                                                                     rect I/O training, where we directly treat each input-output
                                                                                     (x ,y ) pair as training instances. Our key insight is that
                2. Preliminaries                                                        k   k
                                                                                     the few-shot examples can also be used to construct a more
                2.1. In-context Learning                                             robust and expansive D           of synthetic in-context learn-
                                                                                                                TTT
                Atacertain scale, many LMs exhibit the ability to adapt to           ing tasks, allowing for effective model adaptation during
                newtasks without updating their parameters by simply con-                2Note that this use of “test-time training” is related but dis-
                ditioning on input examples or instructions provided. Given          tinct from the one used in recent line of work wherein an RNN’s
                                                                                     hidden state is treated as parameters and the update equation is
                   1Codeanddataareavailable at https://github.com/ekina              interpreted as optimizing a recall-based regression objective (Ravi
                kyurek/marc (ARC) and https://github.com/adamzweiger                 &Larochelle, 2017; Sun et al., 2024; Behrouz et al., 2025; Wang
                /Fewshot-TTT(BBH).                                                   et al., 2025).
                                                                                  2
                                          TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                  n                                                                3.1. Data Generation
                   Test Task
                  o
                  i
                                                  Leave One Out   Direct I/O
                  t
                  a
                  r                                                                Given a task with K training input-output pairs
                  e                                                                           K
                  n                                                                {(x ,y )}      , a test-time training dataset D        can be
                                                                                       k   k                                          TTT
                  e                                                                           k=1
                    Augmentations
                  G
                                                                                   created by either following an in-context learning setup or a
                  a
                  t
                            Horizontal 

                  a                                                                direct input-output (direct I/O) setup (top row in Figure 2):
                              Flip
                                              Applied To 
                  D
                                                                      Selected
                                                                                   Leave-one-out tasks       We begin with leave-one-out in-
                    Loss on:

                                         Loss on:

                                                             Loss on:
             context learning tasks. For each pair (x ,y ), we exclude it
                    Test Output
                                         All Outputs
                                                             Inputs + Outputs                                               j   j
                  s
                  s                                                                from the set of demonstrations and treat it as a “test” exam-
                  o
                  L                                                                ple within the newly formed synthetic task:
                            LM                  LM                 LLLMM                                                         
                                                                                                dICL =     {(x ,y )}      , x , y   .
                                                                                                 j             k   k  k̸=j   j   j
                  n
                  o
                  i
                                        K Tasks                        K Tasks
                  t
                   Task Specific                  Shared                           Here, {(x ,y )}        serves as the “in-context demonstra-
                  a                                                                           k  k   k̸=j
                  z
                  i
                     Task 1                       Task 1                           tions,” and (x ,y ) is the “synthetic test example.” To in-
                                                                                                  j   j
                  r
                  e
                  t
                     Task 2                       Task 2                           crease the number of synthetic tasks, we additionally per-
                  e                                                                                                                   ICL
                  m                                                                mutetheorderofthedemonstrations in each dj .
                     Task k                       Task k
                  a
                  r
                  a
                  P                                                                Direct input-output (I/O) tasks      Rather than constructing
               Figure 2. TTT design decisions. Data generation: A test task        in-context tasks, we treat each (x ,y ) pair independently
               consists of input-output pairs {(x ,y )}. The Leave-One-Out strat-                                      k  k
                                               i  i                                as a single training instance:
               egy removes one example at a time to form in-context learning                               I/O
                                                                                                          d   =(x ,y ).
               tasks, while augmentations further expand the dataset. An alter-                            j        j  j
               native Direct I/O approach trains directly on the examples. Loss:   In this setup, the model is fine-tuned on these training pairs
               Themodelistrained with loss computed on the Test Output (only       without in-context demonstrations. While this approach
               the test-time prediction), All Outputs (including demonstration     is more computationally efficient, our results (Sections 4
               outputs), or Inputs and Outputs (all tokens). Parametrization:      and 5) show that it underperforms methods that utilize in-
               TheTask-Specific approach trains a separate adapter per task while
               the Shared approach trains a single adapter across multiple tasks.  context demonstrations.
                                                                                   DataaugmentationForcertaintasks with structured inputs
                                                                                   (e.g., ARC), we can apply invertible transformations (e.g.,
               test time. Additionally, when task-specific knowledge is            flips, rotations, color permutations) to further augment the
               available, this structure can be leveraged to further expand        TTTdataset. Let T be a set of invertible transformations.
               the dataset, as demonstrated in our experiments on ARC              For each t∈T , we have t−1(t(x)) = x, so we can apply t to
                                                                                   each training and test instance in d to yield a transformed
               (Section 4). We also explore the general case where no task-                                             j
                                                                                   task t(d ). Since these transformations preserve the core
               specific information is used, as tested on BBH (Section 5).                 j
                                                                                   relationships in the data (e.g., the input-output pattern is
               Our experiments in this paper characterize each compo-              the same, just rotated), they effectively expand the training
               nent of the TTT pipeline, investigating different design            signal. If rule-based transformations are used, the final TTT
               choices across the following stages: (1) constructing an            dataset is: DTTT = S       S t(dj).
               input-specific training dataset D        at test-time; (2) fine-                           t∈T   j
                                                  TTT
               tuning the LM by optimizing a loss function L over the              3.2. Loss Function
               dataset P           L(LM(d;θ));and(3)samplingfromthe
                          d∈D
                              TTT                                                  We optimize the standard LM loss on D            . For the in-
               updated model with an augmented inference strategy based                                                         TTT
               onself-consistency to obtain a final prediction.                    context leave-one-out setup, we experiment with 3 different
                                                                                   waystotake the loss (middle row in Figure 2):
               3. TTTDesign                                                        • Test output (no demonstration loss) The standard for-
               Thissectiondiscussesthekeydesignchoicesandchallenges                  mulation where the loss is taken over ytest:
               of applying TTT to LLMs, including how to best leverage                      label
                                                                                          L     =L (y |x ,y ,...,x ,y ,x ;θ)
               their in-context learning capabilities, how to structure data                LM       LM test     1  1        K K test
               for effective processing, what optimization objective to use,       • All outputs3 In addition to the loss on the test output,
               and how to efficiently update model parameters. We detail              3For ARC, we start the indexing at k = 2 because the under-
               these considerations in the construction of the TTT dataset         lying transformation of an ARC task cannot be inferred without
               and the optimization setup (Figure 2).                              observing at least 1 demonstration.
                                                                                3
                                                                               TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                 the loss is also taken over the outputs of the in-context                                                                              In-Context Examples                             Test                 Model Predictions
                                 demonstrations, which encourages the model to correctly
                                                                                                                                                                                                                                            L
                                 predict the demonstrationoutputsafterseeingtheprevious                                                                                                                                                     C
                                                                                                                                                                                                                                            I
                                 demonstrations:
                                                                                                                                                               C
                                                                                                                                                               R
                                                                                                                                                               A
                                                                                K
                                             outputs             label         X                                                                                                                                                            T
                                         L               =L + L (y |x ,y ,...,x ;θ)                                                                                                                                                         T
                                                                                                                                                                                                                                            T
                                             LM                  LM                       LM k 1 1                             k                                                                                          ?
                                                                               k=1
                                                                                                                                                                                                                                            L
                                                                                                                                                                                                                     < [ ] { < ( ) 
                                                                                                                                                                     (< [ ] { < ( ) 
         ( < { ( ) { } 

                            • Loss on inputs and outputs The loss is taken over all                                                                                                                                                              } ]
                                                                                                                                                                                                                                            C
                                                                                                                                                                                                                                            I
                                                                                                                                                                                                                     > } [ ] ( { }
                                                                                                                                                               H
                                                                                                                                                                     > } [ ] ( { }   
*       } ( < > ) >      
*
                                 tokens, encouraging the model to learn the structure of x                                                                     B
                                                                                                                                                               B
                                                                                                                                                                                                                                            T
                                                                                                                                                                            ) >                       )
                                                                                                                                                                                                                            ?
                                                                                                                                                                                                                                            T
                                                                                                                                                                                                                                                 ) >   
                                 as well as y:                                                                                                                                                                                              T
                                                                            K                                                                             Figure 3. Example of ARC and BBH tasks that the model success-
                                          all            outputs          X
                                      L =L                           +            L (x |x ,y ,...,y                                  ; θ)                  fully solves only after applying TTT.
                                          LM             LM                           LM k 1 1                               k−1
                                                                          k=1                                                                              4.2. Experimental Details
                                This method, which requires learners to generate task                                                                      Model architecture & optimization                                                      For our abla-
                                 inputs as well as outputs, is analogous to existing unsu-                                                                 tion experiments, we use the 1B-parameter Llama-3.2
                                 pervised TTT objectives (Sun et al., 2020).                                                                               model (Llama Team, 2024). For our final results in Sec-
                                                                                                                                                           tion 4.6, we use the 8B Llama 3 model. We use Low-Rank
                            WefindinSections 4.3 and 5.3 that the first method (taking                                                                     Adaptation (LoRA; Hu et al., 2022) for parameter-efficient
                            the loss over both demonstration and test outputs) works                                                                       test-time training. More details are given in Appendix C.2.
                            best.
                                                                                                                                                           Fine-tuning before TTT                                  While TTToffers task-specific
                            3.3. Parametrization                                                                                                           adaptation, the initial capabilities of the base model signif-
                            Once we have the test-time training dataset D                                                               (con-              icantly influence its final performance (Section 4.4). We
                                                                                                                               TTT                         developed several approaches for generating synthetic train-
                            structed via either the in-context or direct I/O approach), we                                                                 ing data to enhance the base model’s abstract reasoning
                            perform a small number of gradient steps on task-specific                                                                      capabilities through fine-tuning, exploring both automated
                            LoRA adapters (Hu et al., 2022). This approach allows                                                                          and semi-automated methods for task generation. This is
                            computationally efficient adaptation while maintaining the                                                                     complementary to TTT as the base model is fine-tuned on
                            model’s general capabilities. By default, we learn task-                                                                       tasks distinct from those tested on, when TTT is applied. De-
                            specific LoRA adapters for each ARC or BBH task at test-                                                                       tails on our data generation strategies, as well as the effects
                            time. That is, we obtain K different LoRA adapters, where                                                                      of various data sources and model sizes on performance, are
                            Kis the number of test tasks. We also experiment with                                                                          provided in Appendix B. The fine-tuned base model serves
                            using a single shared LoRA adapter from the aggregated                                                                         as the foundation for all subsequent experiments.
                            dataset of few-shot examples drawn from multiple tasks
                            (bottom row in Figure 2)—a test-time version of meta-ICL                                                                       Evaluation                  Thesuccess criterion requires producing an
                            (Minetal., 2022a). We find that the shared adapter degrades                                                                    exact match for all test outputs (no partial credit). Following
                            performance on ARC, whereas it improves performance on                                                                         the standard ARC scoring criteria, we use the pass@2 met-
                            BBH.Wediscussthisinmoredetail in Section 5.3.                                                                                  ric and produce 2 attempts for each test input. The original
                            4. Abstraction and Reasoning Corpus                                                                                            training and validation sets consist of 400 tasks each. How-
                                                                                                                                                           ever, for efficient evaluation purposes, we randomly pick 80
                            4.1. Background                                                                                                                balanced ARC tasks from the ARC validation set, including
                                                                                                                                                           20 easy, 20 medium, 20 hard, 20 expert tasks according to
                            TheAbstraction and Reasoning Corpus (ARC) aims to eval-                                                                        the classification in (LeGris et al., 2024) (see Appendix A.2
                            uate the abstract reasoning capabilities of language models                                                                    for this task list). Except for our final results, we use this
                            through their ability to solve visual puzzles. Each puzzle                                                                     subset of ARC tasks throughout our experiments. We limit
                            (henceforth referred to as a task) consists of input-output                                                                    D           to have a maximum of 250 examples per task for ef-
                                                                                                                                                               TTT
                            pairs of 2D grids (up to 30×30 in size) containing shapes or                                                                   ficiency reasons. Appendix C.2 provides additional details
                            patterns in up to 10 different colors, as displayed in Figure 3.                                                               onthe hyperparameters.
                            Theoutput of each pair is obtained by applying an intuitive
                            and shared transformation or rule y = f(x). Each task has                                                                      Inference                Oneofthemostcommontechniquestoscale
                            2-7 demonstration examples and 1-3 test examples.                                                                              inference-time compute is to use temperature sampling to
                                                                                                                                                     4
                                                                                        TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                                                         x0     x1      x3
                                                                                                                                                     Hierarchical                                                                         Data Ablations                  Optimization Ablations
                                                                                                                                                    Majority Vote
                   30                               29
                                                                                              Identity
                                                                                                                              LM 
5
                                                                                                                                                           (top-2)
                                                                                                                              transform-1                                                                                                                                                         26
                                                                                       ?
                                                                                                                        ?
                                                                         y0     y1
                                         Test Task                                                                                                                                                                                                                             22
                                  x0     x1    x2      x3
                                                                          x2    x1      x3
                                                                                                                                                                                     20
                                                                                                                              LM 
5                                                                                                                         18
                                                                                             Horizonta
v
                                                                                               Flip
                                                                                                                              transform-1
                                                       ?
                                                                                                                        ?
                                                                                       ?
                                  y0     y1    y2
                                                                         y2     y1                                                                                                                                                       13
                                                                         x2     x0
                                                                                        x3                                                                                         asks Solved10
                                                                                                                                                                                   T
                                                                                                                              LM 
5
                                                                                              Vertica
v
                                                                                               Flip                                                                                                 5
                                                                                                                              transform-1
                                                                                       ?
                                                                                                                        ?
                                                                         y2     y0
                                                                      Leave-one-out Queries           Rule Based Augmentations                                                         0
                                                                                                                                                                                                   FT                 TT                                                       TT
                               Figure 4. Augmented inference and hierarchical voting. We                                                                                                                                                                                                        Loss
                                                                                                                                                                                                               FT + T               mations          Direct I/O       Shared T            No Demo
                                use leave-one-out tasks and invertible geometric transformations                                                                                                                              No Transfor
                                to obtain multiple equivalent versions of the task for augmented                                                                              Figure 5. Accuracy of different data and optimization ablations
                                inference. Predictions from these versions are aggregated with a                                                                              in TTTonARC.OurdataablationsrevealthattheICLdatafor-
                                hierarchical voting strategy: first, voting is performed within each                                                                          mat is crucial for effective TTT, and that applying transformations
                                transformation, and then the top candidates from each transforma-                                                                             to augment the TTT dataset notably enhances performance. For
                                tion undergo global voting to yield the top two predictions.                                                                                  optimization, learning task-specific adapters significantly outper-
                                                                                                                                                                              forms using a single adapter and taking a loss on the in-context
                                                                                                                                                                              demonstrations provides a minor performance boost.
                                obtain multiple responses and select the best according to
                                a ranker, called self-consistency (Wang et al., 2023). How-
                                ever, this is not viable in ARC (where the output grid is
                                directly predicted) as there is no way to directly enforce
                                diversity across samples while ensuring coherence within
                                samples. As an alternative self-consistency approach, we
                                try an augmented inference strategy that combines greedy
                                decoding with multiple versions of the input. Specifically,
                                wegeneratemultiplepredictioncandidatesbyusinggeomet-
                                ric transformations. We then employ a hierarchical voting
                                strategy to determine the final prediction from the set of
                                generated candidates. This approach involves two stages of                                                                                    Figure 6. Performance results across model sizes. Fine-tuned
                                voting to progressively narrow down the best candidates: (1)                                                                                  modelperformance improves with increasing size. However, the
                                Intra-transformation voting: Group predictions by their                                                                                       scaling behavior after TTT is less clear. For instance, the final
                                corresponding transformation t. Within each group, select                                                                                     performance of the 1B and 3B models is identical after TTT.
                                the top-3 most frequent predictions. (2) Global voting:
                                Take the selected transformation-specific candidates from                                                                                     portant; using the direct input-output data to construct D
                                the previous step and select the top-2 most frequent predic-                                                                                                                                                                                                              TTT
                                tions across all transformations. The augmented inference                                                                                     causes an 11-task drop (38%). Removing transformations
                                pipeline is summarized in Figure 4 and full details of the                                                                                    causes a 16 task drop (55%). Regarding optimization, per-
                                pipeline are in Appendix E.                                                                                                                   task LoRA adapters outperform a single shared adapter by 7
                                                                                                                                                                              tasks (24%). Including losses on the demonstration outputs
                                4.3. Impact of TTT Design                                                                                                                     yields a modest but consistent gain (26% → 29%).
                                In this section, we compare the final implementation of our                                                                                   4.4. Impact of Model Size
                                method with different design choices for TTT. FT serves as                                                                                    We perform full fine-tuning of 1B and 3B Llama 3.2
                                the baseline, using only the fine-tuned model with demon-                                                                                     (instruction-tuned) and 8B Llama 3 (instruction-tuned) us-
                                strations in-context. No Transformations omits the aug-                                                                                       ing synthetically generated data, as detailed in Appendix B,
                                mentation step. Direct I/O Data replaces in-context tasks                                                                                     and then use our default TTT implementation. We show
                                with the direct input-output task formulation (Section 3.1).                                                                                  results using different model sizes in Figure 6. Increasing
                                Shared TTT uses a single LoRA adapter across all tasks                                                                                        the model size consistently improves FT performance, with
                                instead of learning one per task. No Demonstration Loss                                                                                       the 8B model achieving the highest accuracy of 36%. At
                                removes the loss on demonstration outputs (Section 3.1).                                                                                      all model sizes, TTT leads to significant improvements in
                                Results are presented in Figure 5. Our TTT method is effec-                                                                                   performance. We also observe that for smaller model sizes,
                                tive, improving fine-tuned model accuracy approximately                                                                                       TTTeffectively closes the performance gap, with the 1B
                                6×(5%→29%). In-contextformatting is especially im-                                                                                            and 3B models achieving similar accuracy after TTT.
                                                                                                                                                                       5
                                             TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                     Individual             Aggregated 32                    PS         Fine-tuned LM       TTTMethod           Score
                     30                                    27      29                        X          Ours                X                  18.3%
                            23              21      22                                       X          Ours                Ours               47.1%
                     20             18                                                       X          BARC                Ours               53.0%
                                                                                             BARC Ours                      Ours               58.5%
                                                                                             BARC BARC                      Ours              62.8%
                    asks Solved10                                                                                            Avg. Human        60.2%
                    T                                                                                                         Best Human      97.8%
                       0                                                                                               BARC(ensemble)          54.4%
                         Rotate           Flip   anilla                Oracle                                     BARC(nosynthesizer)          39.3%
                              Transpose         V     Flattened                                                        Claude 3.5 Sonnet       21.0%
                                                            Hierarchical                                                          GPT-4o        9.0%
                Figure 7. Accuracy of different transformations and voting                                            OpenAIo1preview          21.0%
                schemes. While individual transformations generally perform                                                  DeepSeekr1        20.5%
                at a modest level and are comparable to one another, aggregat-                                                 OpenAIo3       82.8%
                ing across them through voting yields substantial improvements.         Table 1. Pass@2 Scores of different systems on the ARC vali-
                Notably, a hierarchical voting strategy with two voting stages sur-     dation set. Our TTT pipeline improves base models consistently.
                passes a flat voting approach. Our hierarchical method approaches       Weachieve 47.1% accuracy when applied to our fine-tuned model
                oracle-level performance, demonstrating its effectiveness in accu-      and 53.0% when applied to the BARC model (Li et al., 2025). We
                rately selecting the correct answer when present.                       ensemble our method with program synthesis (PS) based models,
                4.5. Impact of Augmented Inference                                      where we achieve score of 61.9%, comparable to the average hu-
                                                                                        manperformanceof60.2%.
                Toanalyze the impact of augmented inference and voting,
                werunseveral ablations: (1) Vanilla, which generates two                racybycombiningneuralandprogramsynthesisapproaches.
                predictions without transformations or advanced voting; (2)             While their fully neural approach shares similarities with
                TransformedInference, applying a single transformation                  our system, our TTT and inference pipeline has several ad-
                (Rotate, Transpose, or Flip) to measure its isolated effect;            ditional components (per-task LoRA, more augmentations,
                (3) Hierarchical Voting, our full pipeline combining aug-               hierarchical voting) that boost performance. To validate
                mented inference and structured voting; (4) Flattened Vot-              our improvements, we applied our TTT pipeline to BARC’s
                ing, which selects the top-2 predictions from a single voting           fully neural model, achieving 53.0% accuracy—a 35% im-
                round over all generated outputs; and (5) Oracle, an upper              provement over their original TTT method.
                bound that selects the correct answer if present.                       Building on these results, we explored combinations of
                Asshown,individual transformations are modestly effective               our approach with BARC. Combining our TTT pipeline
                ontheir own (with Transpose performing worst), but their                and neural model with BARC’s synthesizer raised accuracy
                aggregation improves results markedly. Hierarchical voting              to 58.5%. Combining our TTT pipeline with BARC’s
                further outperforms a flattened voting approach and closely             neural model and synthesizer raised accuracy to 61.9%.
                approaches the oracle’s accuracy, suggesting that our two-              This configuration matches average human performance of
                stage aggregation effectively identifies the correct solution           60.2%(LeGris et al., 2024) on the benchmark.
                whenit is present.                                                      Comparingprogramgenerationandend-to-endmodel-
                                                                                        ing    Li et al. (2025) found that program synthesis and fully
                4.6. Comparison to Other Systems                                        neural predictors for ARC are highly complementary. Their
                Following our experiments on 80 tasks, we present com-                  end-to-end neural model can only solve 42.2% of the tasks
                prehensive results on the full ARC public evaluation set,               solved by the program synthesis model. However, we find
                comparing our system against existing approaches. Our                   that when equipped with our TTT pipeline, BARC’s fine-
                analysis focuses on three key aspects: the impact of our                tuned fully neural model solves 73.5% of the tasks that are
                TTTmethodology,the benefits of combining our approach                   solved by the program synthesis model. This suggests that
                with existing methods, and the differences between fully                our TTT pipeline significantly improves the neural model’s
                neural and program synthesis methods.                                   ability to learn systematic reasoning patterns similar to those
                                                                                        captured by program synthesis models.
                TTT WeapplyTTTandaugmentedinferenceprocedure                            Semi-private evaluation          ARC-AGIchallengeprovides
                to our base fine-tuned model (fine-tuned 8B model). TTT                 a hidden “semi-private dataset” and performs external tests
                significantly improves accuracy from 18.3% to 47.1%.                    for submissions. We submitted our ensemble solution to
                Integration with existing methods            Aconcurrent work           the official ARC-AGI semi-private evaluation and observed
                byLietal.(2025)introducedBARC,achieving54.4%accu-                       47.5% accuracy. This decline may be attributed to more
                                                                                     6
                                                                                                 TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                   significant distribution shifts in the semi-private evaluation                                                                                                     70                                                        Data Ablations                   Optimization Ablations
                                   dataset. If the semi-private evaluation results become pub-                                                                                                        60                                         57.8                                           59.8
                                   licly available in future, we will provide a detailed analysis                                                                                                                                                                55.7                                           54.4           55.9
                                                                                                                                                                                                      50                          50.5                                          51.5
                                   of these performance differences.                                                                                                                                              40.9
                                                                                                                                                                                                      40
                                   5. BIG-Bench Hard                                                                                                                                                  30
                                                                                                                                                                                                     Accuracy (%)20
                                   5.1. Background                                                                                                                                                    10
                                   BIG-Bench Hard (BBH; Srivastava et al., 2023; Suzgun                                                                                                                 0                                          T                                              T
                                                                                                                                                                                                                                  ICL            TT                                              T
                                   et al., 2023) is a benchmark comprising 27 challenging tasks                                                                                                             Zero-Shot                                 No Example          Direct I/O    Shared T
                                                                                                                                                                                                                                                        ermutations                                 No Demo Loss      and Outputs
                                   across 23 task types, designed to evaluate large language                                                                                                                                                           P                                                          Loss on Inputs
                                   models on reasoning, compositionality, and generalization.                                                                                                  Figure 8. Overall BIG-Bench Hard Results. TTT outperforms
                                   UnlikeARC,BBHfeaturesabroadernaturallanguagestruc-                                                                                                          standard in-context learning by 7.3 absolute percentage points,
                                   ture and lacks a shared input format, making it unsuitable                                                                                                  from 50.5% to 57.8%. Our performance improvement over direct
                                   for invertible transformations. However, this broader scope                                                                                                 input-output data shows that using in-context leave-one-out tasks
                                   offers a valuable testbed for evaluating TTT’s effectiveness                                                                                                is crucial. Not taking demonstration loss or taking loss on inputs
                                   in a more generalized setting. Despite the absence of invert-                                                                                               results in a performance decrease. Unlike with ARC, using a
                                   ible transformations—previouslyusedinARCtoexpandthe                                                                                                         shared adapter across all tasks improves performance.
                                   TTTdataset and enhance inference—TTT still significantly
                                   improves performance on BBH.
                                                                                                                                                                                               benefit the most from TTT (Section 5.4). Unlike with ARC,
                                   5.2. Experimental Details                                                                                                                                   wedonothaveacollection of invertible transformations to
                                   Model architecture & optimization                                                                        We use Llama                                       run augmented inference. Instead, we use greedy decoding.
                                   3.1 (8B; Llama Team, 2024). For each task d, we train                                                                                                       Further hyperparameter details and evaluation details are
                                   a separate set of LoRA parameters at test-time, with a LoRA                                                                                                 given in Appendix F.2.
                                   rank of 64 over 40 random shuffles of the demonstration                                                                                                     5.3. Impact of TTT Design
                                   pairs to produce leave-one-out in-context tasks. More hy-
                                   perparameter details are given in Appendix F.1.                                                                                                             In this section, we evaluate our method and its ablations,
                                   OnBIG-Bench Hard, our base language model is able to                                                                                                        primarily comparing the zero-shot baseline, ICL, and TTT.
                                   achieve non-trivial scores out-of-the-box. Consequently, we                                                                                                 NoExamplePermutationupdatesthemodelonasingle
                                   do not perform any initial fine-tuning on synthetic tasks                                                                                                   in-context prompt instead of multiple shuffled versions. Di-
                                   outside of BBH like we do for ARC. Furthermore, since                                                                                                       rect I/O treats each input-output pair as separate training
                                   models achieve nonzero performance in a zero-shot setting,                                                                                                  instances. Shared TTT uses a single adapter across tasks
                                   weprovide the zero-shot results and analyze how TTT and                                                                                                     instead of task-specific adapters. No Demonstration Loss
                                   ICLimproveuponthem.                                                                                                                                         removes the loss applied to demonstration outputs. Loss
                                                                                                                                                                                               on Inputs and Outputs extends the loss calculation to
                                   Evaluation                         For the 27 tasks in BBH, we consider the 10-                                                                             both inputs and outputs. These ablations are as detailed in
                                   shot setting, where we select 10 random pairs from each                                                                                                     Section 3. As these results are averages over 5 runs, the
                                   task’s dataset to be demonstration pairs and evaluate on                                                                                                    standard errors of the mean for each method are given in
                                   the remaining data. Each of the 27 tasks is analogous to                                                                                                    Appendix F.1, averaging 0.4%.
                                   a single ARC task, consisting of 10 labeled examples as                                                                                                     TheresultsinFigure8showthatTTTachievesanoverallac-
                                   demonstration pairs given at test-time. We report average                                                                                                   curacy of 57.8%, outperforming standard ICL (50.5%) and
                                   results over five random seeds, where each seed specifies                                                                                                   Direct I/O learning (51.5%). This demonstrates that TTT’s
                                   which10examplesformthedemonstrationsubset. Formore                                                                                                          capabilities extend beyond ARC to more diverse and com-
                                   control over the evaluation process with test-time training,                                                                                                plex reasoning tasks, proving its effectiveness in a broader
                                   we write our own evaluation function, which is available                                                                                                    range of natural language problem-solving scenarios.
                                   in our codebase (for more details, see Appendix F.1). The
                                   number of evaluation examples for each task is then 240                                                                                                     We observe that TTT without example permuta-
                                   for all tasks except three: Causal Judgment, Penguins in a                                                                                                  tions—performing multiple gradient steps on a single
                                  Table, and Snarks, which have 177, 136, and 168 evaluation                                                                                                   in-context prompt before inference—reduces accuracy to a
                                   examples respectively. Note that the large number of evalua-                                                                                                still-impressive 55.7%. Computing the loss only on the test
                                   tion samples for each task compared to ARC means we can                                                                                                     output lowers accuracy to 54.4%, while applying it to both
                                   doatask-specific analysis to analyze which types of tasks                                                                                                   inputs and outputs achieves 55.9%.
                                                                                                                                                                                        7
                                                    TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                      100                                                                             align well with TTT’s ability to adapt to latent structural
                               Zero-Shot
                               ICL                            86.7                                    regularities during test-time.
                               TTT                                                    85.7
                                                 81.9                       78.7         80.1
                       80                                                                             Conversely, tasks requiring explicit step-by-step computa-
                                    68.8                   69.5                   71.0                tion show limited gains with TTT. For instance, Boolean
                                              60.5                      63.8
                       60                                            58.0                             Expressionsdeclinedfrom85.7%to80.4%underTTT.This
                                          53.6          53.2                                          task’s algorithmic nature—dependent on sequential reason-
                      Accuracy (%)40                                                                  ing rather than pattern-based transduction—and its likely
                                                                                                      pre-training exposure suggest TTT’s updates may not re-
                                18.5                                                                  solve its specific demands. While these particular obser-
                       20                                                                             vations align with our hypothesis, the reason certain tasks
                              3.6                                                                     benefit more from TTT remains an open question.
                        0       Dyck      Ruin Names    Movie Reco   Hyperbaton    Boolean Ex
                              Languages                 mmendation                 pressions
                   Figure 9. BIG-Bench Hard results for tasks with the largest                        6. Related Work
                   TTT-ICLscoredifferences. The four tasks on the left show the                       Test-time training            The idea of updating model param-
                   most significant improvements with TTT over ICL, while the task                    eters at test-time using instance-specific data traces back
                   on the right has the lowest TTT score relative to ICL. Full task-                  to early work on local learning (Bottou & Vapnik, 1992).
                   specific results are given in Appendix F.2.                                        Morerecently, Sun et al. (2020) propose a simple test-time
                   Sharedadapter UnlikeonARC,usingasharedadapter                                      self-supervision scheme to adapt an image classifier when
                   improvesperformanceonBBH,indicatingthattasksinBBH                                  facing distribution shifts. In language modeling, Hardt &
                   do not confound each other during training. On the ARC                             Sun(2024) fine-tune on retrieved neighbors at test-time for
                                                                                                                                    ¨
                   dataset, each puzzle has the same input format, so distin-                         notable gains, while Hubotter et al. (2025) optimize retrieval
                   guishing among multiple tasks is difficult, and we may have                        via active data selection.
                   conflicting gradients with a single adapter. In BBH, however,
                   distinguishing tasks is trivial (the instructions differ in plain                  ARCchallenge AbstractionandReasoningCorpus(ARC;
                   text), and many tasks are mutually helpful. For instance,                          Chollet, 2019; Chollet et al., 2025) is a collection of ex-
                   updating on Logical Deduction Five Objects also aids Logi-                         tremely challenging few-shot visual reasoning problems.
                   cal Deduction Three Objects, without hurting Word Sorting.                         Most approaches to ARC fall into two main categories:
                   Althoughthisisnolongertest-timetraining on distinct tasks                          program synthesis and fully neural. Program synthesis ap-
                   presented individually at test time, it can be interpreted as                      proaches(Buttetal.,2024;Wangetal.,2024;Lietal.,2025;
                   TTTontheentiredataset presented collectively at test time.                         Greenblatt, 2024) first try to find the transformation function
                                                                                                      f, and then apply it to the test example. Fully neural ap-
                   5.4. Task-Specific Analysis                                                        proaches (Veldkamp et al., 2023; Bober-Irizar & Banerjee,
                                                                                                      2024) try to directly predict the output ytest, only implicitly
                   Our task-specific results show that performance improve-                           modeling f. In this work, we use a fully neural approach,
                   ments from TTT are highly task-dependent. Among the                                using an LM to predict the test outputs. Recent work has
                   27 tasks in BBH, TTT results in a performance decline of                           explored hybrid methods, leveraging inference scaling and
                   at least 2% compared to ICL in only 2 tasks. In contrast,                          deep learning-guided program synthesis (Greenblatt, 2024;
                   12 tasks show an improvement of at least 2%, with 9 of                             Lietal., 2025). Similarly, we find that integrating our neural
                   these showing improvements of at least 5%. The four tasks                          model with program synthesis improves performance.
                   with the most significant performance boost from TTT over
                   ICLorzero-shot and the task with the most significant per-                         7. Conclusion
                   formance decrease are shown in Figure 9. These tasks in
                   order of TTT’s improvement over ICL are Dyck Languages                             Weconductaninvestigationoftest-timetraininganddemon-
                   (parentheses matching), Ruin Names (humorous name modi-                            strate that it can significantly improve LM performance on
                   fications), Movie Recommendation (choosing similar films),                         abstract reasoning and few-shot learning tasks, namely the
                   Hyperbaton (adjective ordering), and Boolean Expression                            Abstraction and Reasoning Corpus (ARC) and BIG-Bench
                   (evaluating a boolean expression). Detailed results for every                      Hard (BBH). Our key contributions include a robust TTT
                   task are given in Appendix F.2.                                                    framework with leave-one-out in-context task construction,
                   WehypothesizethatimprovementsfromTTTmaybedriven                                    the optimization setup, and the inference strategy after TTT.
                   by tasks involving distribution shifts and structured patterns.                    Ourresults reveal the potential of TTT to tackle novel rea-
                   Forexample,taskslikeDyckLanguagesandHyperbatonfol-                                 soning tasks, suggesting significant promise for test-time
                   low clear grammatical or programmatic rules, which could                           methods in advancing the next generation of LMs.
                                                                                                   8
                                       TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
              Limitations                                                     .1992.4.6.888. URL https://doi.org/10.1162/neco
              Optimization bias    In development of ARC, we used a set       .1992.4.6.888.
                                                                                                                                    ´
              of 80 tasks for validation/ablation experiments. Standard     Brown,B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re,
              hyper-parameters (learning rate, epochs) were optimized         C., and Mirhoseini, A. Large language monkeys: Scaling
              using this set, which might have introduced some bias.          inference compute with repeated sampling, 2024. URL
              Dataleakage     WhilethebaseLlama-3performspoorlyon             https://arxiv.org/abs/2407.21787.
              the public validation set of ARC, the public availability of  Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
              the dataset introduces the possibility that these models may    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
              have seen these examples during pre-training. Similarly,        Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
              while the base model achieves reasonable performance on         Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,
              BBH,its public availability raises similar concerns.            J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
                                                                              Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
              Acknowledgments                                                 S., Radford, A., Sutskever, I., and Amodei, D. Language
                                                                              models are few-shot learners. In Advances in Neural
              We sincerely thank the BARC team (Li et al., 2025) for          Information Processing Systems 33, 2020. URL https:
              their support and collaboration in ensembling our method        //proceedings.neurips.cc/paper/2020/hash/145
              with theirs, resulting in an official joint submission to the   7c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
              ARCpublic set. We thank Aniruddha Nrusimha for help-          Butt, N., Manczak, B., Wiggers, A., Rainone, C., Zhang,
              ful discussions on parameter efficient training. This work      D. W., Defferrard, M., and Cohen, T. CodeIt: Self-
              was supported by MIT–IBM Watson AI Lab, and by the              improving language models with prioritized hindsight
              National Science Foundation under grants IIS-2212310, IIS-      replay. In Proceedings of the 41st International Confer-
              2238240, and CCF-2217064. This work also benefited from         ence on Machine Learning. PMLR, 2024. URL https:
              manyconversations during the Simons Institute Program on        //dl.acm.org/doi/10.5555/3692070.3692267.
              Language Models and Transformers.
                                                                            Chollet, F. On the measure of intelligence, 2019. URL
              References                                                      https://arxiv.org/abs/1911.01547.
              Acquaviva, S., Pu, Y., Kryven, M., Sechopoulos, T., Wong,     Chollet, F., Knoop, M., Kamradt, G., and Landers, B. ARC
                 C., Ecanow, G. E., Nye, M. I., Tessler, M. H., and Tenen-    Prize 2024: Technical report, 2025. URL https://arxi
                 baum, J. Communicating natural programs to humans            v.org/abs/2412.04604.
                 and machines. In Advances in Neural Information Pro-       Damani, M., Shenfeld, I., Peng, A., Bobu, A., and Andreas,
                 cessing Systems 35, 2022. URL http://papers.nips.            J. Learning how hard to think: Input-adaptive allocation
                 cc/paper files/paper/2022/hash/182aed0379591                 of LM computation. In The Thirteenth International
                 ebd1d655b2bdc152075-Abstract-Datasets and Ben                Conference on Learning Representations, 2025. URL
                 chmarks.html.                                                https://openreview.net/forum?id=6qUUgw9bAZ.
                   ¨
              Akyurek, E., Schuurmans, D., Andreas, J., Ma, T., and         Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,
                 Zhou, D. What learning algorithm is in-context learning?     L. QLoRA:Efficient finetuning of quantized LLMs. In
                 Investigations with linear models. In The Eleventh Inter-    Advances in Neural Information Processing Systems 36,
                 national Conference on Learning Representations, 2023.       2023. URLhttp://papers.nips.cc/paper files/p
                 URLhttps://openreview.net/pdf?id=0g0X4H8yN4                  aper/2023/hash/1feb87871436031bdc0f2beaa62a0
                 I.                                                           49b-Abstract-Conference.html.
              Behrouz, A., Zhong, P., and Mirrokni, V. Titans: Learning     Gandelsman, Y., Sun, Y., Chen, X., and Efros, A. A. Test-
                 to memorize at test time, 2025. URL https://arxiv.           time training with masked autoencoders. In Advances in
                 org/abs/2501.00663.                                          Neural Information Processing Systems 35, 2022. URL
                                                                              http://papers.nips.cc/paper files/paper/2022/
              Bober-Irizar, M. and Banerjee, S. Neural networks for           hash/bcdec1c2d60f94a93b6e36f937aa0530-Abstr
                 abstraction and reasoning. Scientific Reports, 2024. ISSN    act-Conference.html.
                 2045-2322. doi: 10.1038/s41598-024-73582-7. URL
                 https://doi.org/10.1038/s41598-024-73582-7.                Greenblatt, R. Getting 50% (SoTA) on ARC-AGI with GPT-
                                                                              4o, 2024. URL https://redwoodresearch.substa
              Bottou, L. and Vapnik, V. Local learning algorithms. Neural     ck.com/p/getting-50-sota-on-arc-agi-with-gpt.
                Computation, 1992. ISSN 0899-7667. doi: 10.1162/neco          Accessed 09-11-2024.
                                                                         9
                                       TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
              Hardt, M. and Sun, Y. Test-time training on nearest neigh-    Loshchilov, I. and Hutter, F. Fixing weight decay regular-
                 bors for large language models. In The Twelfth Interna-       ization in Adam, 2018. URL https://openreview.n
                 tional Conference on Learning Representations, 2024.          et/forum?id=rk6qdGgCZ.
                 URLhttps://openreview.net/forum?id=CNL2bku4
                 ra.                                                        McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D., and
                                                                               Griffiths, T. L. Embers of autoregression show how large
              Hodel, M. Addressing the Abstraction and Reasoning               language models are shaped by the problem they are
                 Corpus via procedural example generation, 2024. URL           trained to solve. Proceedings of the National Academy
                 https://arxiv.org/abs/2404.07353.                             of Sciences, 2024. doi: 10.1073/pnas.2322420121. URL
                                                                               https://www.pnas.org/doi/abs/10.1073/pnas.23
              Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,    22420121.
                 S., Wang, L., and Chen, W. LoRA: Low-rank adaptation
                 of large language models. In The Tenth International       Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H.
                 Conference on Learning Representations, 2022. URL             MetaICL: Learning to learn in context. In Proceedings of
                 https://openreview.net/forum?id=nZeVKeeFYf9.                  the 2022 Conference of the North American Chapter of
                                                                               the Association for Computational Linguistics: Human
                ¨
              Hubotter, J., Bongni, S., Hakimi, I., and Krause, A. Effi-       LanguageTechnologies. Association for Computational
                 ciently learning at test-time: Active fine-tuning of LLMs.    Linguistics, 2022a. doi: 10.18653/v1/2022.naacl-mai
                 In The Thirteenth International Conference on Learning        n.201. URL https://aclanthology.org/2022.naac
                 Representations, 2025. URL https://openreview.net             l-main.201.
                 /forum?id=NS1G1Uhny3.
                                                                            Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,
              Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh,      Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of
                 A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A.,          demonstrations: What makes in-context learning work?
                 Radford, A., et al. GPT-4o system card. ArXiv preprint,       In Proceedings of the 2022 Conference on Empirical
                 2024. URLhttps://arxiv.org/abs/2410.21276.                    Methods in Natural Language Processing. Association
              Joachims, T. Transductive inference for text classification      for Computational Linguistics, 2022b. doi: 10.18653/v
                 usingsupportvectormachines. InProceedingsofthe16th            1/2022.emnlp-main.759. URL https://aclanthology
                 International Conference on Machine Learning. Morgan          .org/2022.emnlp-main.759.
                 KaufmannPublishers Inc., 1999. ISBN 1558606122.            OpenAI. GPT-4technical report, 2024. URL https://ar
              Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,          xiv.org/abs/2303.08774.
                 C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient   Opiełka, G., Rosenbusch, H., Vijverberg, V., and Steven-
                 memory management for large language model serv-              son, C. E. Do large language models solve ARC vi-
                 ing with PagedAttention. In Proceedings of the 29th           sual analogies like people do?, 2024.    URL https:
                 SymposiumonOperatingSystemsPrinciples, SOSP ’23.              //arxiv.org/abs/2403.09734.
                 Association for Computing Machinery, 2023.       ISBN
                 9798400702297. doi: 10.1145/3600006.3613165. URL           Ravi, S. and Larochelle, H. Optimization as a model for
                 https://doi.org/10.1145/3600006.3613165.                      few-shot learning. In The Fifth International Conference
                                                                               on Learning Representations, 2017. URL https://op
              LeGris, S., Vong, W. K., Lake, B. M., and Gureckis, T. M.        enreview.net/forum?id=rJY0-Kcll.
                 H-ARC:Arobustestimateofhumanperformanceonthe
                 Abstraction and Reasoning Corpus benchmark. ArXiv          Snell, C. V., Lee, J., Xu, K., and Kumar, A. Scaling test-
                 preprint, 2024. URL https://arxiv.org/abs/2409.0              time compute optimally can be more effective than scal-
                 1374.                                                         ing LLM parameters. In The Thirteenth International
                                                                               Conference on Learning Representations, 2025. URL
              Li, W.-D., Hu, K., Larsen, C., Wu, Y., Alford, S., Woo, C.,      https://openreview.net/forum?id=4FWAwZtd2n.
                 Dunn, S. M., Tang, H., Zheng, W.-L., Pu, Y., and Ellis,
                 K. Combining induction and transduction for abstract       Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,
                 reasoning. In The Thirteenth International Conference         A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,
                 on Learning Representations, 2025. URL https://op             Garriga-Alonso, A., et al. Beyond the imitation game:
                 enreview.net/forum?id=UmdotAAVDe.                             Quantifyingandextrapolatingthecapabilitiesoflanguage
                                                                               models. Transactions on Machine Learning Research,
              Llama Team. The Llama 3 herd of models, 2024. URL                2023. ISSN 2835-8856. URL https://openreview.n
                 https://arxiv.org/abs/2407.21783.                             et/forum?id=uyTL5Bvosj.
                                                                         10
                                       TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
              Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A. A., and     Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,
                 Hardt, M. Test-time training with self-supervision for       Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-of-
                 generalization under distribution shifts. In Proceedings of  thought prompting elicits reasoning in large language
                 the 37th International Conference on Machine Learning.       models. In Advances in Neural Information Processing
                 PMLR,2020. URLhttp://proceedings.mlr.press/                  Systems 35, 2022. URL http://papers.nips.cc/pap
                v119/sun20b.html.                                             er files/paper/2022/hash/9d5609613524ecf4f15
                                                                              af0f7b31abca4-Abstract-Conference.html.
              Sun, Y., Li, X., Dalal, K., Xu, J., Vikram, A., Zhang, G.,                                  ¨
                 Dubois, Y., Chen, X., Wang, X., Koyejo, S., Hashimoto,     Wu,Z., Qiu, L., Ross, A., Akyurek, E., Chen, B., Wang, B.,
                T., and Guestrin, C. Learning to (learn at test time):        Kim, N., Andreas, J., and Kim, Y. Reasoning or reciting?
                 RNNswithexpressive hidden states, 2024. URL https:           Exploring the capabilities and limitations of language
                //arxiv.org/abs/2407.04620.                                   models through counterfactual tasks. In Proceedings of
                                                                              the 2024 Conference of the North American Chapter of
                                          ¨                                   the Association for Computational Linguistics: Human
              Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y.,
                 Chung, H. W., Chowdhery, A., Le, Q., Chi, E., Zhou, D.,      LanguageTechnologies. Association for Computational
                 and Wei, J. Challenging BIG-Bench tasks and whether          Linguistics, 2024. URL https://aclanthology.org
                 chain-of-thought can solve them. In Findings of the 2023     /2024.naacl-long.102.
                Conference of the Association for Computational Linguis-    Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao,
                 tics. Association for Computational Linguistics, 2023.       Y., and Narasimhan, K. Tree of Thoughts: Deliberate
                 doi: 10.18653/v1/2023.findings-acl.824. URL https:           problemsolvingwithlargelanguagemodels. InAdvances
                //aclanthology.org/2023.findings-acl.824.                     in Neural Information Processing Systems 36, 2023. URL
              Todd, E., Li, M., Sharma, A. S., Mueller, A., Wallace, B. C.,   http://papers.nips.cc/paper files/paper/2023/
                 and Bau, D. Function vectors in large language models.       hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstr
                 In The Twelfth International Conference on Learning          act-Conference.html.
                Representations, 2024. URL https://openreview.net           Zhao, S., Nguyen, T., and Grover, A. Probing the decision
                /forum?id=AwyxtyMwaG.                                         boundaries of in-context learning in large language mod-
              torchtune Maintainers and Contributors. torchtune: Py-          els. In ICML 2024 Workshop on In-Context Learning,
                Torch’s finetuning library, 2024. URL https://github          2024. URL https://openreview.net/forum?id=rf
                 .com/pytorch/torchtune.                                      CtCcPuSt.
              Veldkamp, K., Rosenbusch, H., Thoms, L., and Stevenson,
                 C. Solving ARCvisualanalogieswithneuralembeddings
                 and vector arithmetic: A generalized method. OSF, 2023.
                 doi: 10.17605/OSF.IO/AKP86. URL https://osf.io
                /akp86/.
              Wang, K. A., Shi, J., and Fox, E. B. Test-time regression:
                 a unifying framework for designing sequence models
                with associative memory. ArXiv preprint, 2025. URL
                 https://arxiv.org/abs/2501.12352.
              Wang, R., Zelikman, E., Poesia, G., Pu, Y., Haber, N.,
                 and Goodman, N. Hypothesis Search: Inductive reason-
                 ing with language models. In The Twelfth International
                Conference on Learning Representations, 2024. URL
                 https://openreview.net/forum?id=G7UtIGQmjm.
              Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi,
                 E. H., Narang, S., Chowdhery, A., and Zhou, D. Self-
                 consistency improves chain of thought reasoning in lan-
                 guage models. In The Eleventh International Confer-
                 ence on Learning Representations, 2023. URL https:
                //openreview.net/forum?id=1PL1NIMMrw.
                                                                        11
                                      TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
              A. ARCDataset
              Wepresent the tasks in the development set, the data format and evaluation details for the ARC dataset (available at this
              https link.).
              A.1. Data Format
              Weusenumpy’sarrayprinting format for all experiments as shown in Figure 10.
              A.2. List of 80 Tasks Used For Development
              Weusethefollowing (Table 2) tasks validation tasks for our development.
                                 Table 2. Selected development tasks and their hardness level based on (LeGris et al., 2024).
                                  ID         Level   ID          Level     ID         Level   ID          Level
                                  0a1d4ef5   easy    762cd429    medium    e5c44e8f   hard    e99362f0    expert
                                  692cd3b6   easy    e7639916    medium    604001fa   hard    1acc24af    expert
                                  1da012fc   easy    e1d2900e    medium    4364c1c4   hard    f9a67cb5    expert
                                  66e6c45b   easy    aee291af    medium    506d28a5   hard    ad7e01d0    expert
                                  3194b014   easy    e95e3d8e    medium    2037f2c7   hard    ea9794b1    expert
                                  963f59bc   easy    e0fb7511    medium    d5c634a2   hard    58e15b12    expert
                                  d37a1ef5   easy    ae58858e    medium    ac605cbb   hard    891232d6    expert
                                  358ba94e   easy    93c31fbe    medium    27f8ce4f   hard    5833af48    expert
                                  f3cdc58f   easy    27a77e38    medium    66f2d22f   hard    4ff4c9da    expert
                                  55059096   easy    9bebae7a    medium    3ed85e70   hard    5b692c0f    expert
                                  c7d4e6ad   easy    9ddd00f0    medium    8b28cd80   hard    e2092e0c    expert
                                  4b6b68e5   easy    fe9372f3    medium    d19f7514   hard    47996f11    expert
                                  00576224   easy    69889d6e    medium    dc2aa30b   hard    34b99a2b    expert
                                  a04b2602   easy    15663ba9    medium    f5c89df1   hard    1c56ad9f    expert
                                  e9c9d9a1   easy    17b80ad2    medium    50f325b5   hard    e6de6e8f    expert
                                  ef26cbf6   easy    16b78196    medium    08573cc6   hard    fea12743    expert
                                  7ee1c6ea   easy    5b6cbef5    medium    3d31c5b3   hard    31d5ba1a    expert
                                  e9ac8c9e   easy    40f6cd08    medium    94133066   hard    79fb03f4    expert
                                  1a2e2828   easy    505fff84    medium    136b0064   hard    8719f442    expert
                                  770cc55f   easy    d017b73f    medium    90347967   hard    a8610ef7    expert
              A.3. Evaluation
              Wefollowthe competition rules that if any of the two pass@2 predictions of the system is correct, we consider that test
              correct. In the reported task-level accuracies, we did not give partial points if all tests are not solved, except the final table
              Section 4.6.
                                                                                              [[0 0 0 0

                                                          [[0 0 0 0

                                                   str                                 str
                                                                                                0 5 4 0

                                                            0 1 2 0

                                                                                                0 6 1 0

                                                            0 3 4 0

                                                                                                0 0 0 0]]
                                                            0 0 0 0]]
              Figure 10. Data Format: We convert grids to strings by representing them as numpy arrays of digits from 0 to 10 where each digit
              corresponds to a different color.
                                                                       12
                                                           TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                                                                                                                    Generated Generators + Annotations
                                                   Examples Generators + Annotations from the training set
                                                          Category

                                                                                                                                   Category

                                                          Extension, Lines

                                                                                                                                   Recoloring, Lines

                                                          Summary

                                                                                                                                   Summary

                                                          Connecting blocks with colored lines

                                                                                                                                   lines are re-colored to red while blocks kept the same

                                                          Description

                                                                                                                                   Description

                                                          In the input we see randomly placed blocks and we need to 
                                                                                                                                   In the input, there are blocks and full lines from edges to edges. 
                                                          connect them to their horizontal or vertical neighbors with lines 
                                                                                                                                   The lines are re-colored to red while the blocks are kept the same.
                                                          of the same color.
                                                                                                                                  def generate(diff_lb: float, diff_ub: float) -> dict:

                                                         def generate(diff_lb: float, diff_ub: float) -> dict:

                                                                                                            LLM
                                                                                                                                      # set the size limits

                                                             # set the size limits

                                                                                                                                      dim_bounds = (4, 30)

                                                             dim_bounds = (3, 30)

                                                                                                                                      # get the possible colors for the blocks

                                                             # get the possible colors for the blocks

                                                                                                                                      colopts = remove(2, interval(0, 10, 1))

                                                             colopts = remove(8, interval(0, 10, 1))

                                                                                                                                      # get random size for the canvas

                                                             # get random size for the canvas

                                                                                                                                      h = unifint(diff_lb, diff_ub, dim_bounds)

                                                             h = unifint(diff_lb, diff_ub, dim_bounds)

                                                                                                                                      w = unifint(diff_lb, diff_ub, dim_bounds)

                                                             w = unifint(diff_lb, diff_ub, dim_bounds)

                                                                                                                                      # pick a random color for the background

                                                             # get random background color

                                                                                                                                      ...

                                                             ...

                                                                                                                                      return {'input': gi, 'output': go}

                                                             return {'input': gi, 'output': go}

                     Figure 11. LLM based synthetic tasks generation: Given some seed task descriptions and task generator functions in Python, we
                     generate more generator functions to produce novel tasks. We use three different approaches: (1) few-shot prompting with only generators,
                     (2) few-shot prompting with generators and task descriptions, (3) two-stage approach: first generate free form descriptions, then condition
                     onthemtogenerate more generators (shown in Figure 12).
                     B. Fine-Tuning Before TTT
                     While test-time training facilitates task-specific adaptation, the base model’s capabilities impacts the final performance.
                     Wedeveloped several approaches for generating synthetic training data to enhance the base model’s abstract reasoning
                     capabilities through fine-tuning, exploring both automated and semi-automated methods for task generation. In this section,
                     wedetail our fine-tuning data generation strategies and analyze the impact of different data sources and model sizes on final
                     performance.
                     B.1. Preparing Fine-tuning Data
                     (Hodel, 2024) provides domain-specific language (DSL), REARC, as well as the transformation fi that solves the task-i,
                     and the data generation function g that are implemented in this DSL for each training task in the Dtrain dataset. These
                                                                          i                                                                                                    ARC
                     functions enable sampling of new input-output pairs that maintains the same underlying transformation principle:
                                                                                                d = (x,y) ∼ eval(gi)                                                                                    (1)
                     where d represents a newly generated input-output pair that can be solved using the same transformation function fi as the
                     original task-i4.
                     (a) Using Existing Generators                      Thegenerator functions g in REARC already provide an effective data augmentation tool
                     by producing different instantiations of same tasks. We generate extra samples from these training tasks by running the code
                     manytimesandrandomlysplitting these new examples (d ∼ eval(gi)) to a set of train and test examples. These augmented
                     examples are already provided with their DSL release.
                     (b) Few-shot Prompting an LLM                           Additionally, we used several approaches to generate novel tasks using an LM (in our
                     case, an ensemble of GPT4 and GPT4-o).
                     Thesimplest approach generates new task generators using few-shot examples:
                                                                                              g′ ∼ LM(g ,g ,...,g )                                                                                     (2)
                                                                                                              1     2          m
                     where g′ is a new generator function and g1,...,gm are existing generator functions (shown in Figure 11). We sample
                     different m examples by uniformly from existing training set. We repeat this process multiple times to get a good amount of
                     tasks.
                     Weaugmentthegeneratorfunctions with task descriptions and jointly generate both descriptions and generators:
                                                                                      ′   ′
                                                                                   (s ,g ) ∼ LM(s ,g ,s ,g ,...s ,g )                                                                                   (3)
                                                                                                           1    1    2    2         m m
                     where si represents the description of task i.
                          4Wecanverify the generated examples by asserting f (x) = y.
                                                                                                  i
                                                                                                               13
                                                                                                                       TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                                                                                                                                                                      Examples Generators + Annotations from the training set
                                                                                                                                                                                                     Category

                                                                                                                                                                                                     Extension, Lines

                                                                                                                                                                                                     Summary

                                                                                                                                                                                                     Connecting blocks with colored lines

                                                                                                                                                                                                     Description

                                                                                                                                                                                                     In the input we see randomly placed blocks and we need to 
                                                                                                                                                                                                     connect them to their horizontal or vertical neighbors with lines 
                                                                       )
                                                                                                                                                                                                     of the same color.
                                                                  3

                                                                                                                                                                                                                                                                                                                                            Generated Generators
                                                                       s
                                                                   
                                                                       n
                                                                                Examples Annotations from the training set
                                                                  n
                                                                       o
                                                                                                                                                                                                    def generate(diff_lb: float, diff_ub: float) -> dict:

                                                                       i
                                                                       t
                                                                                                                                                                                                        # set the size limits

                                                                  o
                                                                                                                                                                                                                                                                                                                             def generate(diff_lb: float, diff_ub: float) -> dict:

                                                                       a
                                                                  i
                                                                                                                                                                                                        dim_bounds = (3, 30)

                                                                                                                                                                                                                                                                                                                                 # set the size limits

                                                                       r
                                                                                    Category

                                                                  t
                                                                                                                                                                                                        # get the possible colors for the blocks

                                                                       e
                                                                                                                                                                                                                                                                                                                                 dim_bounds = (4, 30)

                                                                                    Extension, Lines

                                                                  a
                                                                       n
                                                                                                                                                                                                        colopts = remove(8, interval(0, 10, 1))

                                                                                    Summary
                                                                                                                                                                                                                                     # get the possible colors for the blocks

                                                                  t
                                                                       e
                                                                                                                                                                                                        # get random size for the canvas

                                                                                    Connecting blocks with colored lines

                                                                                                                                                                                                                                                                                                                                 colopts = remove(2, interval(0, 10, 1))

                                                                       g
                                                                                                                                                                                                        h = unifint(diff_lb, diff_ub, dim_bounds)

                                                                                    Description

                                                                                                                                                                                                                                                                                                                                 # get random size for the canvas

                                                                        
                                                                  n
                                                                                                                                                                                                                                                                                          LLM
                                                                                                                                                                                                        w = unifint(diff_lb, diff_ub, dim_bounds)

                                                                                    In the input we see randomly placed blocks and we need to 
                                                                       e
                                                                                                                                                                                                                                                                                                                                 h = unifint(diff_lb, diff_ub, dim_bounds)

                                                                  e
                                                                       g
                                                                                    connect them to their horizontal or vertical neighbors with lines                                                   # get random background color

                                                                                                                                                                                                                                                                                                                                 w = unifint(diff_lb, diff_ub, dim_bounds)

                                                                       a
                                                                                    of the same color.
                                                                                                                                                                                                        ...

                                                                                                                                                                                                                                                                                                                                 # pick a random color for the background

                                                                       t
                                                                  m
                                                                                                                                                                                                        return {'input': gi, 'output': go}

                                                                       s
                                                                                                                                                                                                                                                                                                                                 ...

                                                                        
                                                                                                                                                                                                                                                                                                                                 return {'input': gi, 'output': go}

                                                                  g
                                                                       o
                                                                                                                                                                    L
                                                                       w
                                                                  u
                                                                       t
                                                                                                                                                                         L
                                                                       (
                                                                  A
                                                                                                                                                                             M
                                                                                                                                                                                                                   Generated Annotations
                                                                                                                                                                                                     Category

                                                                                                                                                                                                     Recoloring, Lines

                                                                                                                                                                                                     Summary

                                                                                                                                                                                                     lines are re-colored to red while blocks kept the same

                                                                                                                                                                                                     Description

                                                                                                                                                                                                     In the input, there are blocks and full lines from edges to edges. 
                                                                                                                                                                                                     The lines are re-colored to red while the blocks are kept the same.
                                          Figure 12. Two-stage generation using an LLM: First, we prompt the LLM to generate a task description using few-shot prompting.
                                          Then, we generate the new generator based on existing task pairs and the newly created description.
                                          Toget the task descriptions, we manually created seed descriptions for 10 training tasks. These seed descriptions were then
                                           used to generate descriptions for the training and validation tasks through few-shot prompting. To increase diversity of
                                           tasks, we use task descriptions with hierarchical fields (category, summary, and description). The process of getting these
                                           descriptions is provided in Appendix D.1.
                                           Instead of jointly generating task descriptions and function generations, we additionally deployed a two-stage approach
                                          (Figure 12 ) described as following:
                                                                                                                                                                           ′
                                                                                                                                                                        s ∼LM(s ,s ,...s )                                                                                                                                                                                                                         (4)
                                                                                                                                                                                                         1         2                    m
                                                                                                                                                                           ′                                                                                                             ′
                                                                                                                                                                       g ∼LM(s ,g ,s ,g ,...,s ,g ,s )                                                                                                                                                                                                             (5)
                                                                                                                                                                                                         1          1         2         2                       m m
                                          This approach first generates a task description s′ and then conditions the generator creation on both existing task pairs
                                           and the new description. In total we collected 6426 generators with these LLM based approaches. We provide qualitative
                                           samples from these LM generated tasks in Figure 16.
                                           (c) Geometric Transformations                                                                                Finally, our synthetic tasks are enhanced through various geometric transformations,
                                           such as basic transformations (rotations, reflections, random shift and size scaling), pattern operations (random patching,
                                           tiling, and repetition), color permutations, and composite transformations involving sequential application of multiple basic
                                           transformations. These transformations are applied in three ways:
                                                   • Input grids only: (x,y) → (t(x),y)
                                                   • Output grids only: (x,y) → (x,t(y))
                                                   • Both input and output: (x,y) → (t(x),t(y))
                                          We use all the transformations given in Appendix C.1, and some additional transformations given in Table 3. In the
                                           fine-tuning case, different from TTT, we apply augmentations to only inputs, only outputs or both. These transformations
                                           are applied randomly to variants of tasks with 30% of the time.
                                          Table 3. We provide the additional augmentations use in our data generation for fine-tuning with their function signature and description.
                                                                                                   AugmentationName                                                                    Description
                                                                                                   Repeat(direction, n)                                                                Rotates a grid in horizontal or vertical direction by n
                                                                                                                                                                                       times.
                                                                                                   DropoutOutput                                                                       Randomlydeletes some patches of the output grids.
                                                                                                   DropoutInput                                                                        Randomlydeletes some patches of the input grids
                                                                                                                                                                                                                               14
                                      TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                                                FT                 FT                         36
                              35                                FT+TTT             FT+TTT
                              30                                 29                29           29
                              25              24                                                             +22
                              20                       20                                       +18
                                                                +24               +24
                             asks Solved15   +19                             asks Solved
                             T                         +15                   T
                              10    9
                                                                                                11            14
                               5    +9
                                              5         5        5                 5
                               0
                                   No FT      All               No-LM              1B            3B           8B
                                                      No-Geom
              Figure 13. Left: Accuracy when fine-tuning with different data sources. While all fine-tuned models perform similarly, their
              performance after TTT shows considerable variance. As expected, removing geometric transformations from the fine-tuning data reduces
              performance compared to the model trained on the full dataset. Surprisingly, excluding LM-generated data from fine-tuning actually
              outperforms the model trained on all data. Right: Performance results across different model sizes. As expected, performance of the
              base fine-tuned model improves with increasing model size, aligning with current scaling law trends. However, the scaling behavior after
              TTTisless clear. For instance, the final performance of the 1B and 3B models is identical after TTT. Full discussion in Section B.3.
              B.2. ARCInitial Fine-tuning Hyperparameters
              We perform full fine-tuning on LLama-3 family models by using the torchtune library. We train each model up to
              16000 steps. We use 2xNVIDIA A100 GPU for 1B models, 4xNVIDIA A100 GPU for 3B and 8B models. We present
              hyperparameters in Table 4.
                                                  Table 4. ARC Initial Fine-tuning Hyperparameters
                                            Hyperparameter      Search Space
                                            learning rate       2.5e-5
                                            epochs              2
                                            batch size          32
                                            optimizer           AdamW(Loshchilov&Hutter,2018)
                                            scheduler           Cosine LR Schedule with 2k warmup
              B.3. Results
              We perform full fine-tuning 1B, 3B Llama 3.2 instruction-tuned, and 8B Llama 3 instruction-tuned using augmented
              data. The format and training objective is same as the ones described for TTT in 3. Hyperparameter details are given in
              Appendix C.2. We do the following ablations for augmented data:
                1. No FT: The original Llama 3 instruction-tuned model without any fine-tuning.
                2. All: We use all methods described in Section B.1, including REARC, rule-based augmentation, and LM generation.
                3. No-Geom: Weremovegeometrictransformations from all tasks.
                4. No-LM:WeonlyuseREARCandrule-basedaugmentation,excludingtasksgeneratedbytheLM.
              Weshowresults using different model sizes in Figure 13. Increasing the model size consistently improves FT performance,
              with the 8B model achieving the highest accuracy of 36%. We also observe that TTT effectively closes the performance gap
              for smaller models, with the 1B and 3B models achieving similar accuracy after TTT.
                                                                       15
                                              TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                 C. TTTTransformationsforARC
                 Wepresent the transformations used in TTT and the training details.
                                                                 x0    x1      x2
                                                                                                                                          Test-Time 
                                                                                                                                      Training Data
                                                                                     Identity
                                                                y0     y1      y2
                                                                                                             TTT Label
                               Test Task
                          x0     x1    x2      x3
                                                                 x2    x1      x0
                                                                                     Horizonta
v
                                                                                       Flip
                                              ?
                                                                                                               ?
                                                                                                                                                 ?
                          y0     y1    y2
                                                                 y2    y1      y0
                                                                                                             TTT Label
                                                                 x2    x0      x1
                                                                                      Vertica
v
                                                                                       Flip
                                                                                                               ?
                                                                 y2    y0      y1
                                                                                                             TTT Label
                                                                     Step-1:
                         Step-2:

                                                               Leave-one-out Tasks           Rule Based Augmentations
                 Figure 14. TTT dataset generation for a test task (Section 3.1): We start by creating leave-one-out tasks from the given training
                 examples of the task. These tasks are then augmented through rule-based transformations to obtain the full TTT dataset. Finally, we train
                 task-specific LoRA adapters on top of the base FT model.
                 C.1. Transformations
                 Weprovidetheaugmentations used in TTT in Appendix C.1, please refer to our code base for their implementations. After
                 applying these augmentations, we additionally shuffle colors and shuffle training examples. Note that these transformations
                 are applied to all input and output grids. The procedure for generating the dataset for TTT is shown in Figure 14.
                 C.2. Training Setup & Hyperparameters
                 Weusethetorchtune(torchtune Maintainers & Contributors, 2024) library to train LoRA adapters on Llama-3 family of
                 models. WeapplyLoRAtrainingtoqueryandvalueprojectionweightsoftheself-attentionlayer, to the MLP weights and to
                 the output projection layer (was only available for Llama-3 8B in torchtune). We present hyperparameters of this training
                 in Table 6. We also found that using quantized LoRA adapters (Dettmers et al., 2023) instead of standard (full-precision)
                 LoRAleadstoonlyasmalldropinperformance(29 → 26taskssolvedwiththe1B-parameter model), making it a viable
                 option in memory-constrained settings.
                 Weresort to the vLLM (Kwon et al., 2023) library for prediction as it provides fast kernels and batched inference for our
                 models and LoRA inference. We just use greed decoding as we did not see improvements with temperature sampling in our
                 early experiments. We use 90, 180 degree rotations, horizontal, vertical, and diagonal (transpose) flips as our invertible
                 transformations.
                 With that, the whole TTT and inference process takes approximately 12 hours for 100 randomly sampled validation tasks
                 whenusinganNVIDIAA100GPU.
                                                                                      16
                                TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                    Table 5. We provide the augmentations used in our TTT procedure with their function signature and description.
            AugmentationName                            Description
            Rotate(90)                                  Rotates a grid 90 degrees.
            Rotate(270)                                 Rotates a grid -90 degrees.
            Rotate(180)                                 Rotates a grid 180 degrees.
            Flip(0)                                     Flips a grid horizontally
            Flip(1)                                     Flips a grid vertically
            Reflect(0, reverse=True)                    Flips a grid horizontally and prepends to the left of the
                                                        original grid
            Reflect(1, reverse=True)                    Flips a grid vertically and prepends to above the original
                                                        grid
            Reflect(0, reverse=False)                   Flips a grid horizontally and appends to the right of the
                                                        original grid
            Reflect(1, reverse=False)                   Flips a grid vertically and appends to below the original
                                                        grid
            RandomTranslateXY()                         Shifts a grid randomly in the horizontal and vertical direc-
                                                        tions. The maximum shift size is 4
            Transpose()                                 Reflects a grid on diagonal
            IncreaseResolution(2)                       Upscales the grid by interleaving elements in both hori-
                                                        zontal and vertical directions
            IncreaseHeight(2)                           Upscales the grid by interleaving elements in vertical di-
                                                        rection
            IncreaseWidth(2)                            Upscales the grid by interleaving elements in horizontal
                                                        direction
            Chain([Rotate(90),IncreaseResolution(2)])   Sequential  application of   Rotate(90)    and
                                                        IncreaseResolution(2)
            Chain([Rotate(270),IncreaseResolution(2)])  Sequential application of Rotate(270) and IncreaseRes-
                                                        olution(2)
            Chain([Rotate(180),IncreaseResolution(2)])  Sequential application of Rotate(180) and IncreaseRes-
                                                        olution(2)
            Chain([Flip(0),IncreaseResolution(2)])      Sequential application of Rotate(180) and IncreaseRes-
                                                        olution(2)
            Chain([Flip(1),IncreaseResolution(2)])      Sequential application of Rotate(180) and IncreaseRes-
                                                        olution(2)
            Chain([Transpose(),IncreaseResolution(2)])  Sequential application of Rotate(180) and IncreaseRes-
                                                        olution(2)
            Table 6. ARC TTT Hyperparameters. We find learning rate of 5e-5 the best for 1B and 3B models, and 1e-4 the best for 8B models.
                                     Hyperparameter   Search Space
                                     r LoRArank       128
                                     αLoRAalpha       16
                                     learning rate    [5e-5, 1e-4]
                                     epochs           2
                                     batch size       [1, 2]
                                     optimizer        AdamW(Loshchilov&Hutter,2018)
                                                           17
                  TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
       D. LMDataGeneration
       Wedescribed three approaches in Appendix B to use LM, we generated 6426 task generators by few-shot prompting GPT-4
       and GPT-4o models (OpenAI, 2024; Hurst et al., 2024).
       D.1. Getting Descriptions for Tasks
       This procedure is shown in Figure 15. We initially described 10 training tasks with the hierarchical-style shown in Figure 11.
       Then, for other training tasks tasks, we obtained less quality crowd-worker annotations from LARC (Acquaviva et al., 2022)
       project. By using our high-quality seed annotations and their LARC version, we 10-shot prompt and LM to produce high
       quality annotations for the other training tasks.
         Youare an intelligent agent that can induce task descriptions from examples. For Category, please *do not* use
         generic terms like Transformation, Pattern Recognition.
        —————-
         Task: {stringified task inputs and outputs}
         LARCDescription: {description of the task-1 from LARC dataset}
         GoodDescription: {hierarchical description}
        —————-
         [truncated]
        —————-
         Task: {stringified task inputs and outputs for task-K}
         LARCDescription: {description of the task-K from LARC dataset}
         GoodDescription: {hierarchical description}
        —————-
         Task: {stringified task inputs and outputs for query task}
         LARCDescription: {description of the query task from LARC dataset}
       D.2. Few-shot Prompting Details
       Weusethefollowing simple prompting template with k-shot prompting for all data generation procedures, where numbers
       filled with examples sampled from seed set. In simple few-shot generation, we exclude examples. We use GPT-4 and
       GPT-4otogenerate the new scripts.
         Youareaproblemgenerator on 2D grids of colors. Here are some examples of such transformations, please follow
         the format:
        —————-
         Example: {description of the generator function-1}
         Script: {generator function-1}
        —————-
         [truncated]
        —————-
         Example: {description of the generator function-K}
         Script: {generator function-K}
         Please generate more and make sure they are different:
                                  18
                                                                TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                      10 seed Manually Created and Crowdsourced Annotations                   Example with Crowdsourced Annotations (from LARC)                             Example with Refined Annotations
                                                                                                                                         In the input, you should see

                                                           In the input, you should see

                                                                                 Category

                                                                                                                                                                                                Category

                                                                                                                                         a black grid with gray shapes that are varied 
                                                           grey squares along the side of the grid

                                                                                 Insertion, Dots

                                                                                                                                                                                                Insertion, Dots

                                                                                                                                         in their size and form.

                                                           The output grid size

                                                                                 Summary

                                                                                                                                                                                                Summary

                                                                                                                                         The output grid size

                                                           should remain the same

                                                                                 Filling the intersection of horizontal and 
                                                                                                                                                                                                Filling the intersection of horizontal and 
                                                                                                                                         remains the same as the input grid.

                                                           To make the output, you have to

                                                                                 vertical axes drawn from the edge blocks

                                                                                                                                                                                                vertical axes drawn from the edge blocks

                                                                                                                                         To make the output, you have to

                                                           add red dots where the imaginary lines 
                                                                                 Description:

                                                                                                                                                                                                Description:

                                                                                                                                         copy the input grid. Then count how many 
                                                           of the gray squares intersect
                                                                                 In the input, there are blocks in on the 
                                                                                                                                                                                                In the input, there are blocks in on the edges 
                                                                                                                                                                          LLM
                                                                                                                                         gray squares make up each of the individual 
                                                                                 edges of the canvas, imagine drawing 
                                                                                                                                                                                                of the canvas, imagine drawing lines from 
                                                                                                                                         shapes. If the shape uses (6) gray squares, 
                                                                                 lines from each block to the opposite 
                                                                                                                                                                                                each block to the opposite edge. The 
                                                                                                                                         change those gray squares to red. If the 
                                                                                 edge. The intersection of these lines is 
                                                                                                                                                                                                intersection of these lines is filled with a 
                                                                                                                                         shape has any other number of gray 
                                                                                 filled with a different color.
                                                                                                                                                                                                different color.
                                                                                                                                         squares, change the gray squares to blue.
                       Figure 15. Generating quality seed descriptions: We use few-shot prompting to generate descriptions for a given task, using 10 manually
                       created seed descriptions along with crowd-worker annotations from Acquaviva et al. (2022) as few-shot examples. For a given new task,
                       wesimilarly provide the LM with examples and crowd-worker annotations (available only for training tasks).
                                             d
                                             i
                                             l
                                             a
                                             v
                                             n
                                             I
                                                         d
                                                         i
                                                         l
                                                         a
                                                         V
                       Figure 16. Example tasks generated by LM data augmentation procedure: We display three reasonable tasks that we can infer a
                       simple transformation (valid), and three tasks that we could not infer a simple transformation (invalid).
                                                                                                                        19
                                      TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
              E. AugmentedInference Pipeline
              E.1. Augmented Inference
              Recent work has shown that scaling test-time compute can significantly improve the performance of LMs. One of the
              most commontechniques to do this is by sampling multiple responses, and then selecting the best response using a ranker.
              However, while sampling is very effective in domains with multiple possible solutions (programs in code) or multiple
              possible paths to the final answer (math), it can be detrimental when generating answers directly, as there is no way to
              directly enforce diversity across samples while ensuring coherence within samples. As an alternative inference-time scaling,
              weuseanaugmentedinference strategy that generates multiple prediction candidates by using geometric transformations,
              combined with a greedy decoding scheme.
                                                             K
              For a given task with training examples (x ,y )    and test input x  , we use invertible geometric transformations to
                                                       k   k k=1                test
              produce equivalent transformed versions of the task, as shown in Figure 5. Let T be some set set of invertible geometric
              transformations (e.g., rotations and reflections). For each transformation t ∈ T , we apply t to all training demonstrations
              and the test input and run our model with these transformed inputs. We then apply the inverse transformation to obtain the
              final prediction for that transformation.
                                                 y˜ ∼ LM(t(d     )) := [t(x ),t(y ),...,t(x  )]                                (6)
                                                             input        1     1          test
                                                yt = t−1(y˜)                                                                   (7)
              Wefurther augment our predictions by permuting the order of training examples. For each transformation g, we sample
              n=2differentpermutations of the demonstration sequence, resulting in n·|T | total predictions per task. This is to mitigate
              any bias in the model’s processing of the demonstration sequence. (Bober-Irizar & Banerjee, 2024) also find transpose and
              rotation is helpful to produce extra prediction candidates.
              E.2. Ensembling Predictions (Voting Strategy)
                                                                                                               n·|T |
              Weemployahierarchicalvotingstrategy to determine the final prediction from the set of candidates {y} . This approach
                                                                                                               i=1
              involves two stages of voting to progressively narrow down the best candidates: first, by selecting the most frequent
              predictions within each transformation, and then by conducting an overall vote across transformation-specific candidates to
              identify the top-2 most frequent predictions. The details of each stage are as follows:
                1. Intra Transformation Voting: We group predictions by their corresponding transformation t and select the top-3 most
                  frequent predictions within each group. If fewer than 3 unique predictions exist within a group, we supplement the
                  candidates by computing additional predictions through:
                     • Row-basedmajority: For each row in the predicted output grid, we take the most frequent row values across all
                       predictions in the transformation group.
                     • Column-based majority: Similarly, for each column in the predicted output grid, we take the most frequent
                       column values across all predictions in the transformation group.
                2. Global Voting: Using the selected transformation-specific candidates obtained from (1), we conduct an overall vote to
                  select the top-2 most frequent predictions for submission. In case of a tie, predictions with the identity transformation
                  are given priority.
                                                                       20
                               TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
           F. BIG-Bench Hard Details
           F.1. Further Experimental Details
           Wewriteourownevaluationfunction for BIG-Bench Hard available in our codebase. We found that existing evaluation
           frameworksdidnotproperlymeasurezero-shotperformanceduetoinsufficientanswer-extractionparsingandanswer-format
           prompting. We also wanted more control in splitting each individual task’s dataset into demonstration examples and
           evaluation sets. For all results, we average results over different selections of the 10 few-shot examples with the following 5
           randomseeds: 42,43,44,45,46. The full TTT and inference process takes approximately 15 minutes on an NVIDIA A100
           GPU.
           Thestandard error of the mean for each method in Figure 8 over the 5 seeds is given in Table 7.
                                    Table 7. Standard Error of the Mean for each method in Figure 8.
                                    Method                  StandardErroroftheMean
                                    Zero-Shot                         0.01
                                    ICL                               0.19
                                    TTT                               0.20
                                    NoExamplePermutation              0.32
                                    E2E                               0.66
                                    Shared TTT                        0.72
                                    NoDemoLoss                        0.69
                                    Loss on Inputs and Outputs        0.35
           Wesearchoverthefollowing hyperparameters:
                                          Table 8. BBH TTT Fine-tuning Hyperparameters
                                          Hyperparameter   Search Space
                                          learning rate    [1e-5, 5e-5, 1e-4, 3e-4]
                                          r LoRArank       [64, 128]
                                          αLoRAalpha       [16, 32, 64, 128]
                                          epochs           1
                                          batch size       5
                                          training steps   [20, 40, 60]
                                          optimizer        AdamW
                                          scheduler        Cosine LR Schedule
           We similarly use the torchtune(torchtune Maintainers & Contributors, 2024) library for test-time training and the
           vLLM(Kwonetal., 2023) library for inference.
           F.2. Task-specific Results
           Thefull results for all tasks over all methods and ablations are shown in Figure 17.
                                                           21
                                                                                                                                                                                               TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                                                       100                  Boolean Expressions                         100                    Causal Judgement                           100                  Date Understanding                           100                   Disambiguation Qa                          100                     Dyck Languages                            100                     Formal Fallacies                         100                    Geometric Shapes                           100                        Hyperbaton                             100          Logical Deduction Five Objects
                                                                                       85.7 80.1 81.9            84.2                                                                                                                                                                                                                                                                                                                                                                                                                                                                 84.8 80.2
                                                                         80                                                    77.3       80                                                                80                                                                80                                                               80                                                                80                                                                80                                                               80                   78.7                                         80
                                                                                71.0                       69.7         71.3                                                                                                                                                                             65.9         67.9                                          68.8                       71.6 67.3                                                                                                                                                                        69.3                66.0
                                                                                                                                                         58.4         58.1         61.7         59.3                             61.6         58.7                60.8                      63.7 63.0                               64.2                                                59.7                                                              58.6                                                                                              58.0 63.8                                     60.8
                                                                         60                                                               60 53.5              57.5         53.8          55.0              60                         57.2          57.8 57.3                60                                                               60                          53.2                                  60            56.6 57.6 56.7 54.1              56.2 55.3          60                  54.7                56.5          52.7       60                                                                60                                       52.2
                                                                                                                                                                                                                    50.8 51.2                                                                                   49.3         45.8                                                                                                                                                                44.9                             50.7
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              44.3 40.0                                                                                                      41.6 43.3 41.6 41.0 38.9                       41.3
                                                                         40                                                               40                                                                40                                                                40                                                               40                                                                40                                                                40                                                               40                                                                40                                             37.8
                                                                                                                                                                                                                                                                                     29.2
                                                                         20                                                               20                                                                20                                                                20                                                               20             18.5                                               20                                                                20 14.2                                                          20                                                                20
                                                                           0                                                                0                                                                 0                                                                 0                                                                0      3.6                       2.7                              0     0.0                                                         0                                                                0                                                                 0
                                                                                      ICL    TTT                  TT                                    ICL   TTT                  TT                                    ICL    TTT                  TT                                    ICL   TTT                  TT                                     ICL   TTT                  TT                                    ICL    TTT                  TT                                    ICL   TTT                  TT                                     ICL   TTT                  TT                                    ICL    TTT                  TT
                                                                          Zero-Shot                  Direct I/O                             Zero-Shot                  Direct I/O                             Zero-Shot                 Direct I/O                             Zero-Shot                  Direct I/O                             Zero-Shot                 Direct I/O                              Zero-Shot                 Direct I/O                             Zero-Shot                  Direct I/O                             Zero-Shot                 Direct I/O                             Zero-Shot                  Direct I/O
                                                                                           No Example     Shared T                                          No Example      Shared T                                          No Example     Shared T                                           No Example     Shared T                                          No Example      Shared T                                          No Example     Shared T                                          No Example      Shared T                                          No Example     Shared T                                           No Example     Shared T
                                                                                             ermutations      No Demo Loss                                    ermutations       No Demo Loss                                    ermutations       No Demo Loss                                   ermutations       No Demo Loss                                    ermutations       No Demo Loss                                    ermutations       No Demo Loss                                   ermutations       No Demo Loss                                    ermutations       No Demo Loss                                    ermutations       No Demo Loss
                                                                                                                       and Outputs                                                      and Outputs                                                       and Outputs                                                       and Outputs                                                      and Outputs                                                       and Outputs                                                       and Outputs                                                      and Outputs                                                       and Outputs
                                                                                           P                       Loss on Inputs                            P                       Loss on Inputs                            P                      Loss on Inputs                            P                       Loss on Inputs                            P                       Loss on Inputs                            P                      Loss on Inputs                            P                       Loss on Inputs                            P                       Loss on Inputs                            P                      Loss on Inputs
                                                                       100         Logical Deduction Seven Objects                      100          Logical Deduction Three Objects                      100                Movie Recommendation                           100              Multistep Arithmetic Two                        100                          Navigate                             100                     Object Counting                          100                   Penguins In A Table                         100         Reasoning About Colored Objects                       100                        Ruin Names
                                                                                                                                                                                                                                 86.7 87.4 86.4            88.7 92.3                                                                                                                                                                                                                                                                                                                                                                                           86.7
                                                                                                                                                                                   83.4                                                              84.6                                                                                                                                                                                                                                                                                                                                                                                  81.9 83.1 81.7            85.8 84.2
                                                                         80                                                               80                                                                80                                                                80                                                               80                                                                80                                                                80                                                               80                                                                80
                                                                                                                                                         68.5 69.8 63.6                         66.8                      69.5                                                                                                                                63.8                      65.0                                                                                                                                                                                                 65.0
                                                                         60                                                               60                                62.5          60.2              60                                                                60                                                               60                   62.3 62.5                  61.9 60.2         60                                56.4                            60                                                               60 59.2 59.0 57.2 56.8                          57.8 58.1         60            60.5
                                                                                                    51.9         52.4 51.2                        51.6                                                              53.2                                                                                                                                                         54.2                                          50.8 54.7 56.1             50.8         54.4                                                51.9                                                       54.7                                   53.6
                                                                                       49.2 48.8           46.5                47.0                                                                                                                                                                                                                    43.8                                                              44.4                                                             47.3 47.1 43.5 45.0 44.4                46.3 46.6
                                                                         40 40.4                                                          40                                                                40                                                                40                                                               40                                                                40                                             41.9               40                                                               40                                                                40
                                                                         20                                                               20                                                                20                                                                20                                                               20                                                                20                                                                20                                                               20                                                                20
                                                                           0                                                                0                                                                 0                                                                 0     0.8    2.8   2.5    2.2   1.1    2.6    2.7   2.0          0                                                                 0                                                                 0                                                                0                                                                 0
                                                                                      ICL    TTT                  TT                                    ICL   TTT                  TT                                    ICL    TTT                  TT                                    ICL   TTT                  TT                                     ICL   TTT                  TT                                    ICL    TTT                  TT                                    ICL   TTT                  TT                                     ICL   TTT                  TT                                    ICL    TTT                  TT
                                                                          Zero-Shot                  Direct I/O                             Zero-Shot                  Direct I/O                             Zero-Shot                 Direct I/O                             Zero-Shot                  Direct I/O                             Zero-Shot                 Direct I/O                              Zero-Shot                 Direct I/O                             Zero-Shot                  Direct I/O                             Zero-Shot                 Direct I/O                             Zero-Shot                  Direct I/O
                                                                                           No Example     Shared T                                          No Example      Shared T                                          No Example     Shared T                                           No Example     Shared T                                          No Example      Shared T                                          No Example     Shared T                                          No Example      Shared T                                          No Example     Shared T                                           No Example     Shared T
                                                                                             ermutations      No Demo Loss                                    ermutations       No Demo Loss                                    ermutations       No Demo Loss                                   ermutations       No Demo Loss                                    ermutations       No Demo Loss                                    ermutations       No Demo Loss                                   ermutations       No Demo Loss                                    ermutations       No Demo Loss                                    ermutations       No Demo Loss
                                                                                                                       and Outputs                                                      and Outputs                                                       and Outputs                                                       and Outputs                                                      and Outputs                                                       and Outputs                                                       and Outputs                                                      and Outputs                                                       and Outputs
                                                                                           P                       Loss on Inputs                            P                       Loss on Inputs                            P                      Loss on Inputs                            P                       Loss on Inputs                            P                       Loss on Inputs                            P                      Loss on Inputs                            P                       Loss on Inputs                            P                       Loss on Inputs                            P                      Loss on Inputs
                                                                       100       Salient Translation Error Detection                    100                            Snarks                             100                 Sports Understanding                                               Temporal Sequences                          100 Tracking Shuffled Objects Five Objects 100Tracking Shuffled Objects Seven Objects 100 Tracking Shuffled Objects Three Objects 100                                                                                           Web Of Lies                            100                       Word Sorting
                                                                                                                                                                                                                                                                            100                   99.3 99.9 99.9 100.0 99.7 99.6
                                                                                                                                                                                                                                                                                     93.2
                                                                         80                                                               80                   77.6 73.0           72.3 72.1                80 76.0 75.6 75.9 74.6                   77.2         76.9        80                                                               80                                                                80                                                                80                                                               80                                                                80
                                                                                                                                                         70.2                                   66.8                                          69.5         69.6
                                                                                                                                                  60.7                      63.7
                                                                         60                                      50.2                     60                                                                60                                                                60                                                               60                                                                60                                                                60                                                               60                   50.5         50.2 52.6 51.6                  60 50.9 55.7 56.4 55.2                   53.2 56.5 54.4
                                                                                       48.1 47.2                               44.9                                                                                                                                                                                                                                                                                                                                                                                                                               49.5          49.8                      49.3                                          49.1
                                                                         40 42.0                    42.9 41.0           38.7              40                                                                40                                                                                                                                 40                                                                40                                                                40                                      37.7                     40                                                                40
                                                                                                                                                                                                                                                                              40            37.0                                                                                                                                                                                          30.0 33.0 35.2 34.8 32.8                32.8 32.1
                                                                                                                                                                                                                                                                                                                                                                    23.8 20.8 23.2 26.5 21.2                                          22.4         20.2 24.8
                                                                         20                                                               20                                                                20                                                                20                                                               20 18.7 20.1                                          19.9        20 16.8 17.0               17.8                18.8 15.8          20                                                               20                                                                20
                                                                           0                                                                0                                                                 0                                                                 0                                                                0                                                                 0                                                                 0                                                                0      0.0                                                        0
                                                                                      ICL    TTT                  TT                                    ICL   TTT                  TT                                    ICL    TTT                  TT                                    ICL   TTT                  TT                                     ICL   TTT                  TT                                    ICL    TTT                  TT                                    ICL   TTT                  TT                                     ICL   TTT                  TT                                    ICL    TTT                  TT
                                                                          Zero-Shot                  Direct I/O                             Zero-Shot                  Direct I/O                             Zero-Shot                 Direct I/O                             Zero-Shot                  Direct I/O                             Zero-Shot                 Direct I/O                              Zero-Shot                 Direct I/O                             Zero-Shot                  Direct I/O                             Zero-Shot                 Direct I/O                             Zero-Shot                  Direct I/O
                                                                                           No Example     Shared T                                          No Example      Shared T                                          No Example     Shared T                                           No Example     Shared T                                          No Example      Shared T                                          No Example     Shared T                                          No Example      Shared T                                          No Example     Shared T                                           No Example     Shared T
                                                                                             ermutations      No Demo Loss                                    ermutations       No Demo Loss                                    ermutations       No Demo Loss                                   ermutations       No Demo Loss                                    ermutations       No Demo Loss                                    ermutations       No Demo Loss                                   ermutations       No Demo Loss                                    ermutations       No Demo Loss                                    ermutations       No Demo Loss
                                                                                                                       and Outputs                                                      and Outputs                                                       and Outputs                                                       and Outputs                                                      and Outputs                                                       and Outputs                                                       and Outputs                                                      and Outputs                                                       and Outputs
                                                                                           P                       Loss on Inputs                            P                       Loss on Inputs                            P                      Loss on Inputs                            P                       Loss on Inputs                            P                       Loss on Inputs                            P                      Loss on Inputs                            P                       Loss on Inputs                            P                       Loss on Inputs                            P                      Loss on Inputs
                                                                                                                                            Figure 17. Task-specific 10-shot results for each BIG-Bench Hard task, averaged over 5 random seeds.
                                                                                         Rangefor x = TTTaccuracy − ICLAccuracy                                                                                                                                                                                                                        Tasks (Count)
                                                                                         x≤−5                                                                                                                                                                                                                                                          1 task total:
                                                                                                                                                                                                                                                                                                                                                       Boolean Expressions
                                                                                         −5<x≤−2                                                                                                                                                                                                                                                       1 task total:
                                                                                                                                                                                                                                                                                                                                                       Penguins In A Table
                                                                                                                                                                                                                                                                                                                                                       13tasks total:
                                                                                                                                                                                                                                                                                                                                                       Causal Judgement, Disambiguation Qa, Formal Fallacies,
                                                                                                                                                                                                                                                                                                                                                       Logical Deduction Five Objects,
                                                                                         −2<x<2                                                                                                                                                                                                                                                        Logical Deduction Seven Objects,
                                                                                                                                                                                                                                                                                                                                                       Logical Deduction Three Objects, Multistep Arithmetic Two,
                                                                                                                                                                                                                                                                                                                                                       Navigate, Reasoning About Colored Objects,
                                                                                                                                                                                                                                                                                                                                                       Salient Translation Error Detection, Sports Understanding,
                                                                                                                                                                                                                                                                                                                                                       WebOfLies,WordSorting
                                                                                                                                                                                                                                                                                                                                                       3 tasks total:
                                                                                         2 ≤ x < 5                                                                                                                                                                                                                                                     Object Counting, Tracking Shuffled Objects Five Objects,
                                                                                                                                                                                                                                                                                                                                                       Tracking Shuffled Objects Three Objects
                                                                                                                                                                                                                                                                                                                                                       9 tasks total:
                                                                                                                                                                                                                                                                                                                                                       Date Understanding, Dyck Languages, Geometric Shapes,
                                                                                         x≥5                                                                                                                                                                                                                                                           Hyperbaton, Movie Recommendation, Ruin Names,
                                                                                                                                                                                                                                                                                                                                                       Snarks, Temporal Sequences,
                                                                                                                                                                                                                                                                                                                                                       Tracking Shuffled Objects Seven Objects
                                                                                                                                                                                      Table 9. Tasks Categorized by the Difference x = TTT Accuracy − ICL Accuracy.
                                                                                                                                                                                                                                                                                                                                                                       22
