                  References                                                Isabel Papadimitriou and Dan Jurafsky. 2020. Pretrain-
                  Jean-Philippe Bernardy. 2018.    Can recurrent neural        ing on non-linguistic structure as a tool for analyz-
                     networks learn nested recursion?   LiLT (Linguistic       ing learning bias in language models. arXiv preprint
                     Issues in Language Technology), 16(1).                    arXiv:2004.14601.
                                                                                    ´                      ´                    ´
                  Samuel R Bowman, Gabor Angeli, Christopher Potts,         Jorge Perez, Javier Marinkovic, and Pablo Barcelo.
                     and Christopher D Manning. 2015. A large anno-            2019. On the turing completeness of modern neu-
                     tated corpus for learning natural language inference.     ral network architectures. In ICLR.
                     In EMNLP.                                              Luzi Sennhauser and Robert Berwick. 2018. Evaluat-
                                                      ¨                        ing the ability of LSTMs to learn context-free gram-
                  Noam Chomsky and Marcel P Schutzenberger. 1959.              mars. In Proceedings of the 2018 EMNLP Work-
                     The algebraic theory of context-free languages. In        shopBlackboxNLP:AnalyzingandInterpretingNeu-
                     Studies in Logic and the Foundations of Mathemat-         ral Networks for NLP.
                     ics, volume 26, pages 118–161. Elsevier.
                  Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. 1992.       Yikang Shen, Shawn Tan, Arian Hosseini, Zhouhan
                     Learning context-free grammars: Capabilities and          Lin, Alessandro Sordoni, and Aaron C Courville.
                     limitations of a recurrent neural network with an ex-     2019. Ordered memory. In NeurIPS.
                     ternal stack memory. In Proceedings of The Four-       Hava T Siegelmann and Eduardo D Sontag. 1992. On
                     teenth Annual Conference of Cognitive Science So-         the computational power of neural nets. In Proceed-
                     ciety. Indiana University, page 14.                       ings of the ﬁfth annual workshop on Computational
                  Jeffrey L Elman. 1990. Finding structure in time. Cog-       learning theory, pages 440–449.
                     nitive science, 14(2):179–211.                         Natalia Skachkova, Thomas Alexander Trost, and Di-
                  Felix A Gers and E Schmidhuber. 2001. Lstm recur-            etrich Klakow. 2018. Closing brackets with recur-
                     rent networks learn simple context-free and context-      rent neural networks. In Proceedings of the 2018
                     sensitive languages. IEEE Transactions on Neural         EMNLPWorkshopBlackboxNLP:AnalyzingandIn-
                     Networks, 12(6):1333–1340.                                terpreting Neural Networks for NLP, pages 232–
                  Michael Hahn. 2020. Theoretical limitations of self-         239.
                     attention in neural sequence models. Transactions                                  ¨
                                                                            MarkSteijvers and Peter Grunwald. 1996. A recurrent
                     of the Association for Computational Linguistics,         network that performs a context-sensitive prediction
                     8:156–171.                                                task. In Proceedings of the 18th annual conference
                  Yiding Hao, William Merrill, Dana Angluin, Robert            of the cognitive science society, pages 335–339.
                     Frank, Noah Amsel, Andrew Benz, and Simon              MiracSuzgun,SebastianGehrmann,YonatanBelinkov,
                     Mendelsohn. 2018. Context-free transductions with         andStuartMShieber.2019. Memory-augmentedre-
                     neural stacks. In Proceedings of the 2018 EMNLP           current neural networks can learn generalized dyck
                     WorkshopBlackboxNLP:AnalyzingandInterpreting              languages. arXiv preprint arXiv:1911.03329.
                     Neural Networks for NLP.
                                            ¨                               KeMTran,AriannaBisazza,andChristofMonz.2018.
                  Sepp Hochreiter and Jurgen Schmidhuber. 1997.               The importance of being recurrent for modeling hi-
                     Long short-term memory.       Neural computation,         erarchical structure. In EMNLP.
                     9(8):1735–1780.
                  Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-         Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                     pas, and Franc¸ois Fleuret. 2020. Transformers are        Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
                     rnns: Fast autoregressive transformers with linear at-    Kaiser, and Illia Polosukhin. 2017. Attention is all
                     tention. In ICML.                                         youneed. In NeurIPS.
                  Eugene Kharitonov and Rahma Chaabouni. 2020.              Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On
                     What they do when in doubt: a study of induc-             the practical computational power of ﬁnite precision
                     tive biases in seq2seq learners.    arXiv preprint        rnns for language recognition. In ACL.
                     arXiv:2006.14953.                                      Xiang Yu, Ngoc Thang Vu, and Jonas Kuhn. 2019.
                  Diederik P Kingma and Jimmy Ba. 2015. Adam: A                Learning the dyck language with attention-based
                     method for stochastic optimization. In ICLR.              seq2seq models. In Proceedings of the 2019 ACL
                                                                              WorkshopBlackboxNLP:AnalyzingandInterpreting
                  William Merrill, Gail Weiss, Yoav Goldberg, Roy             Neural Networks for NLP, pages 138–146.
                     Schwartz, Noah A Smith, and Eran Yahav. 2020. A
                     formal hierarchy of rnn architectures. In ACL.
                  Nikita Nangia and Samuel Bowman. 2018. Listops:
                     A diagnostic dataset for latent tree learning.   In
                     Proceedings of the 2018 NAACL: Student Research
                     Workshop.
                                                                       4306
