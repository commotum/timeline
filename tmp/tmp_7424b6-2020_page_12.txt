                                                            LAMBADA LAMBADA StoryCloze HellaSwag
                                        Setting                 (acc)           (ppl)          (acc)         (acc)
                                                                    a               b              c             d
                                        SOTA                    68.0            8.63           91.8          85.6
                                        GPT-3Zero-Shot          76.2            3.00           83.2           78.9
                                        GPT-3One-Shot           72.5            3.35           84.7           78.1
                                        GPT-3Few-Shot           86.4            1.92           87.7           79.3
                   Table 3.2: Performance on cloze and completion tasks. GPT-3 signiﬁcantly improves SOTA on LAMBADA while
                   achieving respectable performance on two difﬁcult completion prediction datasets. a[Tur20] b[RWC+19] c[LDL19]
                   d[LCH+20]
                   Figure 3.2: On LAMBADA,thefew-shot capability of language models results in a strong boost to accuracy. GPT-3
                   2.7B outperforms the SOTA 17B parameter Turing-NLG [Tur20] in this setting, and GPT-3 175B advances the state of
                   the art by 18%. Note zero-shot uses a different format from one-shot and few-shot as described in the text.
                   and [Tur20]) and argue that “continuing to expand hardware and data sizes by orders of magnitude is not the path
                   forward”. We ﬁnd that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of
                   8%overtheprevious state of the art.
                   LAMBADAisalsoademonstrationoftheﬂexibilityoffew-shot learning as it provides a way to address a problem that
                   classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a
                   standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but
                   also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word
                   ﬁlters [RWC+19] (which ban “continuation” words). The few-shot setting instead allows us to “frame” the task as a
                   cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We
                   use the following ﬁll-in-the-blank format:
                                         Alice was friends with Bob. Alice went to visit her friend        . →Bob
                                         George bought some baseball equipment, a ball, a glove, and a          . →
                   Whenpresented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase
                   of over 18% from the previous state-of-the-art. We observe that few-shot performance improves strongly with model
                   size. While this setting decreases the performance of the smallest model by almost 20%, for GPT-3 it improves accuracy
                   by10%. Finally, the ﬁll-in-blank method is not effective one-shot, where it always performs worse than the zero-shot
                   setting. Perhaps this is because all models still require several examples to recognize the pattern.
                                                                             12
