                                       Table 8: Ablation on head size and normalization type.
                                 Setting           Models          ImageSize  Top-1 Accuracy
                                             CoAtNet-2               2242          84.1
                              ImageNet-1K      Headsize: 32 ! 64     2242          83.9
                                               Normtype: BN!LN       2242          84.1
                              ImageNet-21K   CoAtNet-3               3842          87.9
                             )ImageNet-1K      Normtype: BN!LN       3842          87.8
                      • If we keep the total number of blocks in S2 and S3 ﬁxed and vary the number in each stage, we
                       observe that V0 is a sweet spot between V1 and V2. Basically, having more Transformer blocks in
                       S3generally leads to better performance until the number of MBConv blocks in S2 is too small to
                       generalize well.
                      • To further evaluate whether the sweet spot also holds in the transfer setting, where a higher
                       capacity is often regarded more important, we further compare V0 and V1 under the ImageNet-
                       21K transferring to ImageNet-1K setup. Interestingly, despite that V1 and V0 have the same
                       performance during ImageNet-21K pre-training, the transfer accuracy of V1 clearly falls behind
                       V0. Again, this suggests the importance of convolution in achieving good transferability and
                       generalization.
                      Lastly, we study two choices of model details, namely the dimension of each attention (default to
                      32) head as well as the type of normalization (default to BatchNorm) used in MBConv blocks. From
                      Table 8, we can see increasing head size from 32 to 64 can slightly hurt performance, though it
                      actually improves the TPU speed by a signiﬁcant amount. In practice, this will be a quality-speed
                      trade-off one can make. On the other hand, BatchNorm and LayerNorm have almost the same
                      performance, while BatchNorm is 10 - 20% faster on TPU depending on the per-core batch size.
                      5  Conclusion
                      In this paper, we systematically study the properties of convolutions and Transformers, which leads
                      to a principled way to combine them into a new family of models named CoAtNet. Extensive
                      experiments show that CoAtNet enjoys both good generalization like ConvNets and superior model
                      capacity like Transformers, achieving state-of-the-art performances under different data sizes and
                      computation budgets.
                      Note that this paper currently focuses on ImageNet classiﬁcation for model development. However,
                      we believe our approach is applicable to broader applications like object detection and semantic
                      segmentation. We will leave them for future work.
                                                            10
