         6
         Table 1. Classiﬁcation error on the CIFAR-10 test set using ResNet-110 [1], with
         diﬀerent types of shortcut connections applied to all Residual Units. We report “fail”
         when the test error is higher than 20%.
                    case          Fig.        on shortcut   on F    error (%)   remark
                 original [1]     Fig. 2(a)        1          1       6.61
                                                   0          1        fail      This is a plain net
                  constant        Fig. 2(b)       0.5         1        fail
                   scaling
                                                  0.5        0.5      12.35      frozen gating
                                               1−g(x)       g(x)       fail      init b =0 to −5
                  exclusive                                                          g
                                  Fig. 2(c)    1−g(x)       g(x)      8.70       init b =-6
                   gating                                                            g
                                               1−g(x)       g(x)      9.81       init b =-7
                                                                                     g
               shortcut-only                   1−g(x)         1       12.86      init b =0
                                  Fig. 2(d)                                          g
                   gating                      1−g(x)         1       6.91       init b =-6
                                                                                     g
             1×1 conv shortcut    Fig. 2(e)    1×1 conv       1       12.22
              dropout shortcut    Fig. 2(f)   dropout 0.5     1        fail
             Exclusive gating.FollowingtheHighwayNetworks[6,7]thatadoptagating
         mechanism [5], we consider a gating function g(x) = σ(W x + b ) where a
                                                                               g       g
         transform is represented by weights W and biases b followed by the sigmoid
                                                      g               g
         function σ(x) =        1   . In a convolutional network g(x) is realized by a 1×1
                                 −x
                             1+e
         convolutional layer. The gating function modulates the signal by element-wise
         multiplication.
             Weinvestigate the “exclusive” gates as used in [6,7] — the F path is scaled
         byg(x)andtheshortcutpathisscaledby1−g(x).SeeFig2(c).Weﬁndthatthe
         initialization of the biases b is critical for training gated models, and following
                                         g
                         2
         the guidelines in [6,7], we conduct hyper-parameter search on the initial value of
         b intherangeof0to-10withadecrementstepof-1onthetrainingsetbycross-
          g
         validation. The best value (−6 here) is then used for training on the training
         set, leading to a test result of 8.70% (Table 1), which still lags far behind the
         ResNet-110 baseline. Fig 3(b) shows the training curves. Table 1 also reports the
         results of using other initialized values, noting that the exclusive gating network
         does not converge to a good solution when b is not appropriately initialized.
                                                            g
             The impact of the exclusive gating mechanism is two-fold. When 1 − g(x)
         approaches 1, the gated shortcut connections are closer to identity which helps
         information propagation; but in this case g(x) approaches 0 and suppresses the
         function F. To isolate the eﬀects of the gating functions on the shortcut path
         alone, we investigate a non-exclusive gating mechanism in the next.
             Shortcut-only gating. In this case the function F is not scaled; only the
         shortcut path is gated by 1−g(x). See Fig 2(d). The initialized value of bg is still
         essential in this case. When the initialized bg is 0 (so initially the expectation
         of 1 − g(x) is 0.5), the network converges to a poor result of 12.86% (Table 1).
         This is also caused by higher training error (Fig 3(c)).
          2 See also: people.idsia.ch/ rupesh/very_deep_learning/ by [6,7].
                                         ~
