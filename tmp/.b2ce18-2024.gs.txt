             Neural Computing and Applications (2024) 36:6659–6680
             https://doi.org/10.1007/s00521-024-09417-3(0123456789().,-volV)(0123456789().,-volV)
              ORIGINAL ARTICLE
             Simultaneous instance pooling and bag representation selection
             approach for multiple-instance learning (MIL) using vision transformer
             Muhammad Waqas1,2         • Muhammad Atif Tahir1 • Muhammad Danish Author3 • Sumaya Al-Maadeed4 •
             Ahmed Bouridane5 • Jia Wu2
             Received: 10 April 2023/Accepted: 14 January 2024/Published online: 16 February 2024
             The Author(s) 2024
             Abstract
             In multiple-instance learning (MIL), the existing bag encoding and attention-based pooling approaches assume that the
             instances in the bag have no relationship among them. This assumption is unsuited, as the instances in the bags are rarely
             independent in diverse MIL applications. In contrast, the instance relationship assumption-based techniques incorporate the
             instance relationship information in the classiﬁcation process. However, in MIL, the bag composition process is com-
             plicated, and it may be possible that instances in one bag are related and instances in another bag are not. In present MIL
             algorithms, this relationship assumption is not explicitly modeled. The learning algorithm is trained based on one of two
             relationship assumptions (whether instances in all bags have a relationship or not). Hence, it is essential to model the
             assumption of instance relationships in the bag classiﬁcation process. This paper proposes a robust approach that generates
             vector representation for the bag for both assumptions and the representation selection process to determine whether to
             consider the instances related or unrelated in the bag classiﬁcation process. This process helps to determine the essential
             bag representation vector for every individual bag. The proposed method utilizes attention pooling and vision transformer
             approaches to generate bag representation vectors. Later, the representation selection subnetwork determines the vector
             representation essential for bag classiﬁcation in an end-to-end trainable manner. The generalization abilities of the pro-
             posed framework are demonstrated through extensive experiments on several benchmark datasets. The experiments
             demonstrate that the proposed approach outperforms other state-of-the-art MIL approaches in bag classiﬁcation.
             Keywords Multiple-instance learning (MIL)  Vision transformers  Attention-based pooling  Bag representation selection
             1 Introduction                                                image segmentation [3], medical image classiﬁcation [4],
                                                                           and others [5–7].
             The multiple-instance learning (MIL) approach is a case of       The MIL approaches can be categorized based on the
             weakly supervised learning [1]. This learning approach is     classiﬁcation granularity: the bag-space level classiﬁcation
             used where labeling cost is a major restriction for anno-     approaches [8], which compute the distance between the
             tating every data instance [2]. In MIL, the data are repre-   bags or apply maximum margin approach to train the
             sented as bags with multiple instances, with only one label   classiﬁers; embedding-space classiﬁcation [9, 10], where
             for each bag. Unlike supervised learning, the labels of the   an entire bag is transformed into a ﬁxed-size vector rep-
             instances are not available in the training process. The      resentation and applies a simple single instance classiﬁca-
             model in MIL is trained using weak bag-wise labels rather     tion algorithms; instance-space classiﬁcation [11], where
             than instance-wise labels. The case of supervised learning    the score for each instance is computed, and the bag label is
             and MIL is shown in Fig. 1a and b, respectively. In MIL,      obtained based on the instance scores. The studies in
             the primary objective is to develop a model that predicts     [12, 13] show that the ﬁrst two categories are robust in bag
             the label of the test bag using training bags and corre-      classiﬁcation compared to the last category. However, the
             sponding labels. The application of MIL is common in          bag-space and embedding-space classiﬁcation approaches
                                                                           cannot identify the key instances (the instances that trigger
                                                                           the bag label) [13]. Identifying key instances in the bag is
             Extended author information available on the last page of the article
                                                                                                                           123
               6660                                                                                  Neural Computing and Applications (2024) 36:6659–6680
               Fig. 1 Supervised learning (SL) vs Multiple-Instance learning (MIL),     approach is shown in b where the instances are grouped in bags, and
               a shows the example of instance classiﬁcation setup followed in SL,      the labels are provided at bag level
               where every data instance is labeled. The MIL bag classiﬁcation
               essential as these instances play a vital role in the bag                relationship could potentially hinder the performance of the
               classiﬁcation process and model interpretability.                        classiﬁcation algorithm. In order to obtain better general-
                  Furthermore, in the context of MIL, the bags consist of               ization, the classiﬁer must distinguish between instances
               multiple instances, and the goal is to classify the bags                 related to the fox concept, different animal species, other
               based on their contents. However, the difﬁculty arises when              objects inside the bag, and their relationship. Thus, deter-
               the bags in the training set and testing come from a dif-                mining the relationship or independence of instances in the
               ferent distribution [14]. Previous MIL studies assume that               bag may enhance the classiﬁcation process.
               the instances of the bag in the training and testing data are               MIL algorithms [9, 13, 15–19] are developed based on
               sampled from the same distribution (either related or                    one of two assumptions: whether instances have a rela-
               independent). However, this assumption is often violated in              tionship or not. However, it is not theoretically guaranteed
               real-world tasks [9, 13, 15–19].                                         that instances in all bags follow the same assumption.
                  For example, the case of MIL image classiﬁcation is                   Additionally, existing MIL algorithms do not explicitly
               illustrated in Fig. 2, where the image is considered a bag,              account for the bag-wise relationship assumption. As a
               and the extracted patches are considered instances. The                  result, their performance could be improved since weak
               instances related to the Fox concept are positive instances;             bag-level labels provide only limited supervision.
               instances related to other objects like cars and buildings are              For example, to identify the essential instances in the
               negative instances. Figure 2 illustrates the dissimilarity               bags, a weighted average bag pooling operation is proposed
               between different training bag distributions, where the                  using attention-based deep neural networks (AbDMIL)
               training set contains images of the animal of interest in                [13], where end-to-end trainable architectures are used to
               natural settings. However, some images in the training and               generate attention-based weights for each instance. The
               testing set may be captured in a diverse environment or                  concept of attention pooling is further investigated in Shi
               contain other similar animals.                                           et al. [15] by incorporating the attention loss mechanism.
                  In such cases, the instances in the bag may or may not                However, the existing attention-based pooling approaches
               have a relationship, and it can be challenging to ascertain              [13, 15] and bag encoding strategies [9, 17] are based on
               the presence or absence of any underlying instance rela-                 the assumption that instances in the bag are independent
               tionships. Therefore, determining the relationship between               and that no relationship exists between the instances of the
               the instances in the bag becomes important to model per-                 bag. In this assumption, the relationship between the
               formance, and the presumption of a speciﬁc instance
               123
             Neural Computing and Applications (2024) 36:6659–6680                                                                    6661
             Fig. 2 The example of distribution change where the training     testing example. Green boxes mark the positive instances, while the
             examples are from distribution. a Shows the positive training    negative instances are shown in red boxes
             example captured in different settings, and b presents the negative
             instances of the bags is ignored, which may result in            refer to the proposed approach as a vision transformer-
             neglecting the information in the bag [20, 21].                  based instance weighting and representation selection
                On the other hand, the assumption of relationship             subnetwork (ViT-IWRS).
             between instances is natural and may present a superior             The major contributions of the paper are:
             description of the data [22]. Considering the different          •  The vision transformer (ViT)-based approach is pro-
             image patches as interrelated is more meaningful than               posed to model the relationship between the instances
             assuming the opposite, speciﬁcally in multiple-instance             of the bag. This process helps to generate a bag
             image classiﬁcation scenarios. The assumption of instance           representation   vector by considering the instance
             relation is also considered for MIL problems by Zhou                relationship.
             et al. [16]. However, these techniques mainly focus on the       •  To select informative bag representation from sets of
             structural properties of the bag, and the instance relation-        generated bag representation vectors, a differentiable
             ships are modeled in terms of graph kernel learning.                representation selection subnetwork (RSN) is proposed.
             Additionally, this process is not end-to-end trainable.          •  The weight-sharing approach is presented for simulta-
                In this paper, we propose the idea of generating bag             neous instance weight learning and bag classiﬁcation
             representation vectors based on both assumptions and                for ViT. This method helps to strengthen the relation-
             introduce the bag representation selection process to select        ship between the loss and instance weighting processes.
             a suitable representation for each bag, which addresses the
             limitation of the instance relationship assumption in            To demonstrate the generalization ability of the proposed
             existing MIL algorithms.                                         approach, the experiments are performed on multiple types
                In the proposed algorithm, we incorporate bag-wise            of data from different MIL application domains. For binary
             instance relationship assumption in the classiﬁcation pro-       classiﬁcation, ﬁve benchmark datasets are used: Musk1 and
             cess by considering bags with varying instances as a batch,      Musk2 [23] datasets for molecular activity predictions;
             and bag representation vectors are generated for each bag        Fox, Elephant, and Tiger datasets for image classiﬁcation.
             based on the assumption of interaction and independence.         For multi-class classiﬁcation two datasets are used: multi-
             We obtain information about the relationship between             ple-instance MNIST (MIL-MNIST) [13] dataset for hand-
             instances in a bag by using a vision transformer architec-       written digit classiﬁcation; MIL-based CIFAR-10 datasets
             ture to model the dependencies among them. Furthermore,          [15] for object recognition. Additionally, the experiments
             the representation vectors for independent assumptions are       are also conducted for real-world Colon Cancer detection
             derived from the mean, max, average, and attention pool-         histopathology dataset [24].
             ing operations [13], which do not consider the relationship         The remainder of the paper is organized into the fol-
             of instances.                                                    lowing sections: Sect. 2 presents the literature review.
                In addition, we propose a differentiable representation       Section 3 explains the proposed methodology for (ViT-
             selection network to decide whether to consider instance         IWRS). The experimental setup is given in Sect. 4. The
             relationships in the classiﬁcation process for each bag. We
                                                                                                                                123
             6662                                                                        Neural Computing and Applications (2024) 36:6659–6680
             obtained results are discussed in Sect. 5, which follows the     adopt a ﬁxed-size embedding vector used for bag classiﬁ-
             conclusive Sect. 6.                                              cation. For example, Zhou et al. proposed two bag
                                                                              encoding techniques for MIL using Fisher vector encoding
                                                                              (miFV) and locally aggregated descriptors (miVLAD) [9].
             2 Literature review                                              The miFV and miVLAD keep essential bag-level infor-
                                                                              mation in generated bag encodings with the help of dic-
             This section presents a summary of MIL algorithms in the         tionary learning. However, the bag-space classiﬁcation
             literature. The MIL algorithms are divided into two cate-        algorithms lack any mechanism to learn appropriate feature
             gories: Classical MIL techniques and Neural network-             representation. Other conventional MIL algorithms include
             based techniques. These categories are discussed in detail       semi-supervised SVMs for MIL (MissSVM) [31], MIL
             in the following subsections.                                    with randomized trees [32], and many others [7].
             2.1 Classical MIL techniques                                     2.2 Neural network-based MIL techniques
             Classical MIL techniques can also be categorized into bag-       This section introduces the related work based on neural
             space and instance-space algorithms. The instance-space          network (NN) architectures for MIL. Traditionally, neural
             algorithms classify each instance in the bag individually        networks (NN) for MIL perform instance-level classiﬁca-
             and aggregate the instance labels to determine the bag label     tion [33]. The convolution neural networks (CNN) are also
             [11, 25]. Thus, these algorithms identify the key instances      used in MIL for feature extraction through multiple con-
             in the bag (instances that triggered the bag label). However,    volution layers [34–36]. The best candidate search and
             the unavailability of instance-level labels complicates the      instance positioning with the global max-pooling operation
             learning problem.                                                approach are explored in Hoffman [37]. However, the
                To tackle the complexity of the learning process,             max-pooling is not robust enough to ﬁnd the inﬂuential
             Andrews et al. [26] proposed two support vector machine          instance, especially in the bag classiﬁcation approach [15].
             (SVM)-based solutions to solve MIL problems: Mi-SVM                 To overcome the limitation of max-pooling, the concept
             for instance-space classiﬁcation and MI-SVM for bag-             of Noisy or [38], LSE, and generalized mean are intro-
             space classiﬁcation. Diversity Density (DD) and nearest          duced in Shi et al. [39]. However, these operators are non-
             neighbor approach for real-valued target in MIL are pro-         trainable. In contrast, the use of an adaptive pooling
             posed in Amar et al. [27], and a similar approach com-           approach and a fully connected network is proposed in Liu
             bining diversity density and expectation-maximization            et al. [40]. MIL-based pooling approaches, e.g., mean and
             (EM) is proposed in Zhang and Goldman [28]. These                max-pooling operations, are proposed in Wang et al. [41],
             algorithms address MIL problems by assigning bag labels          which is designed to extract features and perform back-
             to the instances and training an instance-space model.           propagation with the support of maximum response of
             However, these methods often fail when a complicated             instance feature extraction layers.
             relationship between instances determines the bag label.            Contrary to the above discussed techniques, the atten-
                Random subspace clustering and instance selection             tion-based pooling approach is considered as a kind of
             approach (RSIS) is proposed in Carbonneau et al. [29],           weighted average of instances in which the weights of the
             where key instances are selected from positive bags. The         instances are obtained by trainable attention layers [42].
             selected instances are then used in the instance-space           This technique has been applied in several real-world
             ensemble learning approach. However, the instance selec-         problems, such as image classiﬁcation and captioning [43].
             tion procedure in RSIS results in class imbalance problems       However, limited attention-based studies are available in
             and negatively affects performance. The constructive             the literature related to MIL. Attention-based instance
             clustering   ensemble (CCE) [30] approach performs               pooling approach in Ilse [13] proposed two-layer (AbD-
             instance clustering to obtain a binary vector representation     MIL) and three-layer (Gated-AbDMIL) networks to attain
             for the bag. The bit value in the binary vector determines       instance weights. This approach focuses on binary classi-
             the bag link to the clusters. However, the performance of        ﬁcation problems and uses an additional layer for bag
             CCE is comparatively low.                                        classiﬁcation. The loss-based attention (LBA) approach
                Bag-space techniques do not require access to instance        [15] proposed a weight-sharing approach among fully
             labels, although they are not as explainable as instance-        connected layers and attention layers. However, the atten-
             level approaches. For example, the graph-based kernel            tion pooling techniques [13, 15] assume no dependence
             approach (mi-Graph) [16] transforms the bag into a graph         among instances in the bag. Unlike previous attention-
             representation and employs a distance function to compare        based techniques, the proposed ViT-IWRS generates sev-
             bags. Embedding space methods for bag classiﬁcation              eral bag representations based on both assumptions and
              123
                  Neural Computing and Applications (2024) 36:6659–6680                                                                                                            6663
                  selects the suitable bag representation for the classiﬁcation
                  process.
                  3 Proposed methodology
                  The proposed ViT-IWRS consists of four steps. In the ﬁrst
                  step, we propose a vision transformer-based approach to
                  identify the dependencies between the bag instances. This
                  process transforms input instances into latent representa-
                  tions using an embedding network and provides the latent
                  transformation as input to a transformer encoder. The
                  encoding process involves a multi-head-self-attention pro-
                  cess that captures the global dependencies between the
                  instances in the bag. With the output of the encoding
                  process, we compute the weights for the bag instances in
                  the second step. The weighting process ensures the
                  assignment of higher weights to the essential instances in
                  the bag. The process of instance embedding and trans-
                  former encoding is shown in Fig. 3a, while the process of
                  instance weighting is illustrated in Fig. 3b.
                      The third step of the proposed approach involves gen-
                  erating bag representation vectors from instance weights
                  for both instance relationship assumptions using encoder
                  outputs and latent representations. Weights assigned to
                  instances determine the composition of the representation
                  vector and ensure that informative instances are repre-
                  sented more prominently. Figure 3c illustrates the vector
                  representation generation process. As a ﬁnal step, the
                  representation selection subnetwork (RSN) selects the ﬁnal
                  bag representation vector from a set of generated bag
                  representation vectors. The RSN and bag classiﬁcation
                  process function is shown in Fig. 3d. In the following
                  subsection, we present problem formulation, a brief dis-
                  cussion of the vision transformer, and each step of the
                  proposed approach in detail.
                  3.1 Problem formulation
                  In binary MIL classiﬁcation problem, for a given bag B ¼
                   i
                    xi;1; xi;2; xi;3; ...; xi;mi     of mi total instances with d
                  dimensions, where xi;j represents jth instance of ith bag.
                  The objective is to predict a bag target label Yi 2f1;0g.
                  The prediction of bag label depends on the corresponding
                                                             
                  set   of instance-level labels               yi;1; yi;2; ...; yi;m  ,   where
                  yi;j 2f1;0g. The instance-level labels remain unknown
                  while the model training and Yi for binary classiﬁcation is
                  obtained as:
                                      P
                             0 iff         m y ¼0
                  Y ¼                      j¼1 i;j                                            ð1Þ
                     i       1            otherwise :
                                                                                                                                                                           123
                  6664                                                                                                 Neural Computing and Applications (2024) 36:6659–6680
               bFig. 3 The Proposed ViT-IWRS framework. The top row in this                             3.3 Vision transformer for bag encoding in MIL
                  block represents 3 different input bags (red, green, and blue) with a
                  different number of instances (3, 4 and 5). Block (a) illustrates                     In MIL, the objective is classify a given bag B ¼
                  instance embedding and the transformer encoding process. The                           i
                  instance selection mechanism is shown in (b). The bag representation                                                                                             1d
                  generation block is presented in (c). The representation weighting and                  xi;1; xi;2; xi;3; ...; xi;mi   of mi instances, where xi;j 2 R                .
                  bag classiﬁcation process is shown in (d)                                             In this case, the ViT can be employed to generate robust
                                                                                                        bag embeddings and determine dependencies among the
                                                                                                        bag instances. The self-attention in the transformer
                                                                                                        encoding process can allow instances in the bag to interact
                  In this paper, we concentrate on bag-level classiﬁcation for                          with each other. It can provide essential details about the
                  binary and multi-class MIL applications. Therefore, a                                 relationship of instances in the bag, which can be used to
                  representation vector is generated for the bag of instances                           generate a robust representation vector for the bag.
                  and the model classiﬁes the bag representation vector                                     At ﬁrst, each instance x in the bag B is transformed
                  instead of individual instances.                                                                                            i;j                 i
                                                                                                        into a latent representation hi;j using an embedding net-
                      Given a bag representation vector and corresponding                               work. The process of instance embedding corresponds to
                  bag label, the model generates a Kdimensional vector of                              the patch embedding process in standard ViT settings.
                  class scores sK, where K represents the number of classes.
                                                                                                        However, the embedding network can consist of multi-
                  In this case, the bag label is determined by:                                         layer perceptron (MLP) or convolution layers, depending
                                         
                  Y ¼argmaxK1 fðsÞk ;                                                        ð2Þ       upon the nature of the data. We used a similar design for
                     i              k¼0
                                                                                                        the embedding network as previously used by Shi
                                              i
                  where fðsÞi ¼ PexpðÞs              is Softmax function that squashes                  et al. [15] and Ilse et al. [13]. The details about the
                                         K1      j
                                         j¼0 expðÞs                                                     embeddingnetworkdesignare discussed in Sect. 5.9.1.We
                  the score vector sk in the range between (0, 1) and all the                           refer to the generated latent instance representation hi;j as
                  resulting elements add up to 1 and are interpreted as class                           instance embeddings. Similarly, the embeddings for all the
                  probabilities.                                                                        instances in the bag Bi are grouped and referred to as bag
                                                                                                                              ½0    
                                                                                                        embeddings H             ¼ h ;h ;...:h                 .   Afterward, the
                  3.2 Vision transformer                                                                                      i          i;1   i;2       i;mi
                                                                                                        generated bag embeddings are prepended with a learnable
                                                                                                        class          token           hi;0         and           denoted            by
                  TheVision Transformer (ViT) is inspired by the concept of                                0½0   
                  transformers in language processing models and can be                                 Hi     ¼ hi;0;hi;1;hi;2;...:hi;mi .
                  seen as an alternative to the convolutional neural network                                The class token aggregates global information from the
                  (CNN) [44]. Vision Transformers (ViT) takes 1D patch                                  entire bag, and it allows the model to make high-level
                  embeddings as input. Therefore, the image is transformed                              decisions based on the overall content rather than relying
                  into a sequence of two-dimensional ﬂattened patches, and a                            solely on local instance information. The class token is
                  trainable linear projection converts the generated patches to                         typically fed into a classiﬁcation head for image classiﬁ-
                  one-dimensional vectors. The projected image patches are                              cation tasks. In the case of MIL, the class token diversiﬁes
                  called patch embeddings. A learnable embedding called                                 the set of generated vector representations for the bag. The
                  class token is also prepended to patch embeddings. More-                              classiﬁcation token is learnable embedding and can capture
                  over, the positional embeddings which are added to pre-                               global dependencies and relationships in the bag. Thus, the
                  serve the positional information of patches in the image.                             classiﬁcation token can be used as an additional bag rep-
                      Transform encoder [45] combines multi-head self-at-                               resentation vector. It can be used as an input for the rep-
                  tention     (MHSA) blocks with multi-layer perceptrons                                resentation selection network.
                  (MLP). Before each block, layer normalization (LN) is                                     The generated bag embeddings serve as input to the
                  applied, and residual connections are used after each block.                          encoder. At the start of the training process, the class token
                  There are two layers of MLP and GELU nonlinearity in the                              is randomly initialized and learned during the training
                  transformer encoder. The details of the transformer encoder                           process. The length of the class token is the same as the
                  and MHSA process are shown in Fig. 4. Vision transfer                                 length of the instance embedding in the bags. The class
                  employs one or more stacked transformer encoder blocks                                token is used in the MHSA process in the same way as
                  in the encoding generation process. The generated class                               other instance embeddings of the bag and accumulates
                  token from the last transformer encoder block is then                                 information from other instance embeddings [44]. Here,
                  employed for classiﬁcation using a classiﬁcation head. The                            the positional embeddings are not used as bag representa-
                  classiﬁcation head consists of MLP with one hidden layer.                             tion follows a permutation invariant structure. The ViT
                                                                                                        encodes the given bag embeddings H0½0 as:
                                                                                                                                                            i
                  123
                       Neural Computing and Applications (2024) 36:6659–6680                                                                                                                                                            6665
                       Fig. 4 The vision transformer block is shown in (a), while the process of multi-head self-attention [45] is illustrated in (b)
                        8             0½0      3.4Instanceweightcomputation
                        > H ¼ hi;0;hi;1;hi;2;...hi;mi ;
                        >             i
                        >
                        <                                 
                            H0½‘1 ¼ MHSA LN Hl1                               þH½‘1;‘¼1...L
                        > i                                            i                 i                                             In this step, the weight for each instance in the bag is
                        >                          
                        > 0½‘                                    0½‘1               0½‘1                                          computed using the attention approach [13, 15]. This pro-
                        :Hi ¼MLP LN Hi                                          þHi            ;‘¼1...L ð3Þ                            cess highlights essential instances from the bag and assigns
                                                                                                                                       a higher weight to the informative instance. Later, the
                       Where ‘ represents the index of the transformer encoder                                                         instances in the bag are pooled using a weighted average
                       block, and L denotes the depth or the total number of                                                           operation to obtain representation vectors for the bag. In
                       encoder blocks. Discussion related to the depth of ViT and                                                      this study, the weights of the transformer classiﬁcation
                       the number of heads in MHSA is presented in Sect. 5.9.4.                                                        head are shared to learn instance weight and bag repre-
                       Additionally, the generated output of the encoding process                                                      sentation vector classiﬁcation simultaneously. This process
                                                                 hi
                                                      0½L            ½L     ½L     ½L          ½L                      ½L        helps to enhance the connection between the loss and
                       is denoted by Hi                     ¼ h ;h ;h ...::hi;mi                            where hi;j
                                                                      i;0     i;1     i;2                                              instance weighting process.
                                 ½L                                                                                                                            dK                                                         K
                       and hi;0 denote the output of the last transformer encoder                                                           Let W 2 R                  be a weight matrix and b 2 R be a bias
                       block for the corresponding input instance embedding hi;j                                                       vector of classiﬁcation head f( : ). Given the output of the
                       and hi;0, respectively.                                                                                         last transformer encoder block H0½L the weights for the
                                                                                                                                                                                                     i
                                           0½L                                                                                        instance in the bag B are computed as:
                            Later, Hi            is used to generate bag representation vec-                                                                                 i
                                                                                                                       ½0                                           P                 
                       tors with the assumption of related instances, and Hi                                                is                                            K1exp h½Lwc þbc
                                                                                                                                                                          c0               i;j
                                                                                                                                                                                            
                       used to generate bag representation vectors without                                                                  8        ai;j ¼ Pmi PK1                             ½L             c    ;                   ð4Þ
                       instance relationship assumption, respectively. The process                                                      1jmi                       t¼1       c¼0 exp hi;t wc þ b
                       of instance embedding and bag encoding using ViT is                                                                                      d
                       illustrated in Fig. 3a.                                                                                         where wc 2 R is cth column vector of W and bc  b is
                                                                                                                                       corresponding bias. The obtained weights are then used to
                                                                                                                                       generate bag representation vectors in the next step. The
                                                                                                                                       process of weight computation is illustrated in Fig. 3b.
                                                                                                                                                                                                                              123
                         6666                                                                                                                                         Neural Computing and Applications (2024) 36:6659–6680
                         3.5 Computation of bag representation vectors                                                                           Gumbel SoftMax in an end-to-end approach [46]. This
                                                                                                                                                 process is analogous to computing the softmax over a
                         After obtaining the weights of the instance in the bag, the                                                             stochastically sampled set of points. The Gumbel-Max
                         next step is to compute bag representation vectors. This                                                                Trick separates the deterministic and stochastic parts of the
                         process transforms the bag with a variable number of                                                                    sampling process using the reparameterization trick
                         instances to a manageable vector representation and                                                                     [46, 47]. It computes the log probabilities of given scores in
                         transforms the MIL problem into a classical supervised                                                                  the distribution and adds some noise to them from the
                         learning problem. To classify the bags, one of the obtained                                                             Gumbel distribution. Finally, the argmax function is
                         vectors is selected using the representation selection                                                                  applied to ﬁnd the class with the maximum value for each
                         subnetwork.                        hirepresentationvectorandgenerateaone-hotvectorforuse
                                                0½L             ½L     ½L      ½L           ½L                                              by the rest of the neural network.
                              Given H                 ¼ h ;h ;h ...::h                                    and weights of
                                                i                i;0     i;1      i;2           i;mi                                                  At First, the previously computed n representation
                         instances ai the representation vector for the bag Bi are                                                               vectors for the bag B are combined to form a representa-
                                                                                                                                                                                          i
                         computed as:                                                                                                                                             hi
                                                                                                                                                 tion matrix R ¼ h½L1;w ;l ;max ;x                                            2Rnd, where d
                                      mi                                                                                                                                               i;cls        i     i          i      i
                         wi ¼ Xai;j h½L:                                                                                         ð5Þ           denotes the length of representation vectors. Afterward, the
                                                       i;j
                                     j¼1                                                                                                         representation matrix R is given as input to RSN (R),
                         Thecomputedbagrepresentations w involves the output of                                                                  which outputs the score vector r 2 Rn1 and representation
                                                                                             i                                                   selection code u ¼ ðÞu ;u ;...;u                                   are computed as:
                         the transformer encoder, and h½L is learned class token.                                                                                                         1     2             n
                                                                                      i;0                                                                                
                                                                                                                                                                           ðÞlogðÞr   þg
                         The learning process of these vectors considers all the                                                                                  exp              i      i
                                                                                                                                                                                   s
                                                                                                                                                                             	

                         instances in the bag. Thus, these vectors incorporate the                                                               ui ¼ P                           logðÞr þg           ;                                                    ð7Þ
                                                                                                                                                                                ðÞ
                                                                                                                                                                n     exp                i     j
                         information related to the relationship of instances in the                                                                            j¼1                     s
                         bag B.
                                    i
                              Additionally,                 bag         representation                  vectors           without                where gi Gumbel ð0;1Þ¼logðlogðqÞÞ;q Uni-
                         assuming instance relationship are obtained based on the                                                                form (0, 1). Additionally, s 2ð0;1Þ is the temperature
                                                             ½0       parameter,whichdeterminesthedegreeofapproximation
                         bag embeddings Hi ¼ hi;1;hi;2;...hi;mi                                             as:                                  for u in relation to a one-hot vector. A smaller value of s
                         8                      mi
                         >                    X ½0                                                                                              results in a harder u, whereas a higher s leads to a smoother
                         >
                         > x ¼                        a h ;
                         >             i                i;j      i;j
                         >                                                                                                                       u. The obtained u is further used to generate a one-hot
                         >
                         >                     j¼1
                         >
                         >                                                                                                                       vector as:
                         >                                 
                         <maxi ¼ max H½0 ;                                                                                        ð6Þ             H
                                                                  i                                                                               i     ¼argmax u ;
                                              1jmi                                                                                                                        fg
                         >                                                                                                                                                      i
                         >                                                                                                                                             i
                         >
                         >                                                                                                                                                                                                                                 ð8Þ
                         >                           mi                                                                                                                    
                         >                         X                                                                                                	                          H
                         >                     1              ½0
                         >                                                                                                                        e ¼OneHot i ;
                         > l ¼                             h ;
                         >             i                      i;j
                         :                    mi j¼1                                                                                                           H                                                          	
                                                                                                                                                 where i           denotes sampled index and e represents the one-
                         where the xi;l ;maxi represent the attention weighted                                                                   hot vector with the iH the element being 1. Afterward, the
                                                        i
                         average [13], mean, and max representation vectors,                                                                     bag representation vector for the bag B is selected as:
                                                                                                                                                                                                                            i
                         respectively. The computation of these representation                                                                                 T 	                                                                                         ð9Þ
                         vectors does not incorporate any dependencies or rela-                                                                  vi ¼ R e :
                         tionships between the instances of the bag. Therefore,                                                                  The selected bag representation vector vi is then used to
                         xi;li;maxi are based on the assumption of unrelated                                                                     classify the bag label by classiﬁcation head f(:)as
                         instances of Bi. Figure 3c shows the representation vector
                                                                                                                                                 Y ¼fðÞv :                                                                                              ð10Þ
                         generation process.                                                                                                         i            i
                         3.6 Representation selection subnetwork (RSN)                                                                           Furthermore, the details related to the number of layers in
                                                                                                                                                 RSN are discussed in Sect. 5.9.2.
                         The instance in the bag can either be related or unrelated.                                                             3.7 Loss function
                         Therefore, the representation vector generated by a correct
                         distribution assumption will provide critical information to                                                            This section presents the loss function for the training of
                         the classiﬁer. In this case, RSN aims to select one of the                                                              ViT-IWRS. The proposed loss scheme is derived from the
                         representation vectors, which is most informative for the                                                               concept of cross-entropy (CE) loss [15]. CE is a measure of
                         bag classiﬁcation. RSN performs hard selection using                                                                    dissimilarity between the true and predicted label.
                         123
                     Neural Computing and Applications (2024) 36:6659–6680                                                                                                                                       6667
                         Given a representation vector v for the training bag B ,                                             ThetermL1 !0ifanyoneinstanceinabagB belongs
                                                                                                                 i                                                                                       i
                     and corresponding label Yi 2f0;1;;K 1g, where K                                                 to the kth class. However, in this case, it is not theoretically
                     denotes the number of classes. Let f( : ) represent a neural                                        guaranteed that only one instance belongs to the kth class in
                     network and z ¼ fðvÞ2RK be the class score vector for                                               the bag [15]. Therefore, it results in a high false negative
                                            i
                     B.Theestimatedclass probability of B belonging to the k-                                            rate for the instances in the positive bags. To address this
                       i                                                           i
                     th class can be computed by using softmax function:                                                 issue, the L2 term is added to the objective function. This
                                                                                                                       term ensures that more than one instance with higher
                                   exp zk
                     qk ¼ P                 i        ;                                                      ð11Þ         weights contributes to the label. Furthermore, the L2 term
                       i          K1            c
                                        expðÞz
                                  c¼0            i                                                                       is inspired by the fact that the weight of instance xi;j
                     where expð:Þ represents the exponential function. For                                               become approximately zero when yi;j 6¼ Yi.
                     multi-class classiﬁcation, the loss function can be written
                     as:                                                                                                 4 Experimental setup
                                   K1
                                   Xc c                                                                     ð12Þ
                     CE¼ pilogqi;                                                                                       This Section introduces the datasets used for experiments
                                    c¼0                                                                                  along with relevant evaluation measures. Additionally, a
                     where pc 2f0;1gK denote the true probability of the bag                                             comparative analysis of existing methods is also provided.
                                  i
                     B belonging to the c                      class, and qc is the estimated
                       i                                   th                       i
                     probability.                                                                                        4.1 Details of datasets and evaluation measure
                         The target vector p is one-hot encodings in multi-class
                     classiﬁcation. In this case, if B belongs to the k-th class,                                        Theperformance of ViT-IWRS is evaluated using different
                                                                       i
                     there is only one element pk in the target vector which is                                          datasets for binary and multi-class classiﬁcation problems.
                                                                  i
                     not zero. So, only the positive class contributes to the loss                                       These datasets have been used to assess the performance of
                     computation process. Discarding the elements of the                                                 MIL algorithms in the literature and cover a range of MIL
                     summation which are zero due to target labels in equation                                           application domains, such as molecular activity prediction,
                     (12), the loss function can be written as:                                                          image classiﬁcation, object detection, and medical image
                                         !
                                                                                                                       classiﬁcation. The details of these datasets are given below.
                                                exp zk
                     CE¼log P                           i          :                                       ð13Þ
                                                k1           c
                                                     expðÞz                                                              4.1.1 Benchmark MIL datasets
                                                c¼0           i
                     Suppose that the training bag Bi belongs to the kth class. In                                       The experiments are conducted on ﬁve MIL datasets rela-
                     this      case,        given         the       output          of      ViT        H0½L ¼
                     hiitedtobinaryclassiﬁcationproblems: Musk1, Musk2,
                         ½L    ½L    ½L         ½L                                                                   Elephant, Tiger, and Fox. These datasets are related to
                       h ;h ;h ...::hi;mi , the weights of instances ai, and
                         i;0    i;1    i;2
                     corresponding bag representation vector v, the loss for the                                         binary classiﬁcation problems. The ﬁrst two datasets
                     bag B is computed as:                                                                               (Musk1 and Musk2) cover the application of MIL for
                              i                                                                                          molecular drug activity predictions [23]. These datasets are
                                        !
                                                     
                                                            k       k
                     L1 ¼log Pexp vw þb                                       ;                            ð14Þ         composed of molecular conformations of multiple shapes.
                                               K1               c       c                                               The bag is formed based on the shape similarity, and the
                                               c¼0 expðÞvw þb                                                            drug’s effect is observed if one or more conformations are
                                                               
                                mi 0             0                 ½L   k        k        1 1                           attached to the targeted bindings. The later three datasets:
                               X                         exp hi;j w þb
                                    @            @                                         A A
                                                                    
                     L2 ¼               log PK1                        ½L   c       c      ai;j    ;                  Elephant, Tiger, and Fox, are related to image classiﬁcation
                               j¼1                      c¼0 exp hi;cw þb                                                 [26]; features of image segments constitute the bags in
                                                                                                            ð15Þ         these datasets. The positive bags hold one or more
                                                                                                                         instances related to the animal of interest while the nega-
                     Loss ¼ L1þkL2:                                                                         ð16Þ         tive bags contain other animals. The details of these data-
                                   c        d                                                                            sets are shown in Table 1.
                     where w 2 R is cth column vector of weight matrix W
                     and bc is corresponding bias for classiﬁcation head f(:).
                         The ﬁrst term of the loss function focuses on bag clas-                                         4.1.2 MIL-based MNIST dataset
                     siﬁcation loss, while the second one captures the attention                                         In addition to the existing benchmark MIL dataset, an
                     loss, and k is a non-negative hyperparameter to balance                                             additional dataset for multi-class classiﬁcation is created
                     between bag and attention loss. The discussion related to                                           from well-known MNIST digits (MIL-MINST) for digit
                     the impact of k is given in Sect. 5.9.3.
                                                                                                                                                                                                        123
              6668                                                                             Neural Computing and Applications (2024) 36:6659–6680
              Table 1 The details of MIL         Datasets          Positive bags         Negative bags           Total bags          Total instances
              benchmark datasets
                                                 Tiger             100                   100                     200                 1220
                                                 Elephant          100                   100                     200                 1391
                                                 Fox               100                   100                     200                 1320
                                                 Musk1              47                     45                     92                  476
                                                 Muks2              63                     39                    102                 6598
              classiﬁcation [48]. The dataset consists of gray-scale digit         appearances, including both normal and malignant regions.
              images of size 2828, and the images are randomly                    Every image has been marked with the majority of nuclei
              selected to form a bag where each digit represents an                for each cell with a total of 22,444 nuclei and class labels
              instance. In this problem, we have used a labeling approach          such as epithelial, inﬂammatory, ﬁbroblast, and miscella-
              similar to [15], where bags with the target digits {’3’, ’5’,        neous. Every WSI represents a bag with several 2727
              ’9’} are labeled {’1’, ’2’, ’3’} accordingly and if a bag does       patches. The bag is labeled as positive if it has one or more
              not include any of the target digits, it is labeled as ’0’. in       nuclei from the epithelial class.
              the training process, the model is trained for 50, 100, 150,
              200, 300, and 400 generated training bags, respectively,             4.1.5 Evaluation measure
              while the performance is evaluated on 1000 test bags.
                                                                                   We evaluate the performance of the proposed ViT-IWRS
              4.1.3 MIL-based CIFAR-10 dataset                                     in terms of bag classiﬁcation accuracy. The experiments on
                                                                                   benchmark datasets are performed using ﬁve runs of
              We construct more challenging MIL datasets for multi-                10-fold cross-validation, and average performance is
              class classiﬁcation using images from the CIFAR-10                   reported. For the MIL-based MNIST dataset, the experi-
              dataset for object recognition MIL application [49]. The             ments are performed with 1000 test bags and different
              CIFAR-10 dataset contains 60000 images divided into ten              numbers of training bags (50, 100, 150, 200, 300, and 400).
              classes, each image is of size 32  32, and classes are              The experiments are repeated 50 times for each train and
              completely mutually exclusive. We employed a similar                 test set, and average results are compared with existing
              approach previously used in Shi et al. [15] to evaluate the          state-of-the-art techniques. Similarly, the experiments are
              performance of ViT-IWRS on this dataset. The bags are                repeated thirty times with different training and testing data
              formed by treating images as instances, and bags are nor-            for MIL-based CIFAR-10 datasets, and average perfor-
              mally distributed with a mean bag size of 10 and a variance          mance is reported. On the Colon Cancer dataset, we per-
              of 2, respectively. The target classes are set to {’airplane’,       formed a 5-fold cross-validation, and average results are
              ’automobile’, ’bird’}, and associated with the labels {’1’,          presented.
              ’2’, ’3’} accordingly. The bags related to target classes at
              most contain images from one of these three classes. The             4.2 Methods used for comparative study
              training sets are built with 500 and 5000 bags, while the
              test set is created with 1000 bags.                                  The proposed approach is compared with several state-of-
                                                                                   the-art attention-based approaches and other benchmark
              4.1.4 Colon cancer dataset                                           bag-level classiﬁcation techniques. The methods for per-
                                                                                   formance comparison are selected based on good perfor-
              Detecting cancerous regions in hematoxylin and eosin (H              mance and the wide range of MIL solutions they offer.
              &E) stained whole-slide images (WSI) are vital in clinical           Some of the methods are brieﬂy discussed below.
              settings [50]. These images, also called digital pathology           •   MIL NN [41]: This study proposes trainable pooling
              slides, can occupy several gigabytes of storage space [51].              operators for MIL. In this work, the bag-level classi-
              Presently,    supervised    approaches    require    pixel-level         ﬁcation technique (MI-NET) directly produces the bag
              annotations, which demand signiﬁcant time from patholo-                  label. The instance-level classiﬁcation technique (mi-
              gists. A successful solution to reduce pathologists’ work-               NET) pools instance-level scores to produce the bag
              load is to use weak slide levels. For this study, we                     label. The pooling approach based on the residual
              conducted experiments on colon cancer histopathology                     connection ( MI-NET RC) is also proposed.
              images [24] to test the efﬁciency of ViT-IWRS.                       •   Ranking Loss-based Simple MIL (ESMIL) [52]: This
                 This dataset consists of 100 H&E images belonging to                  paper presents a novel approach to differentiate
              binary classes. These images feature a range of tissue
               123
              Neural Computing and Applications (2024) 36:6659–6680                                                                      6669
                 between positive and negative bags by a simple                 5.1 Comparison with SOTA attention-based
                 pairwise bag-level ranking loss function. The proposed              pooling approaches
                 objective function ensures that the model assigns a
                 higher score to the positive bags. Instead of using a          The comparison of the ViT-IWRS with three SOTA
                 threshold-based    decision    function,   the   proposed      attention techniques LBA [15] and AbDMIL [13]is
                 approach penalizes the network when it generates a             depicted in Fig. 5. Similar to the proposed ViT-IWRS, the
                 lower score for positive bags compared to negative             algorithms estimate the weights of the instances using the
                 bags.                                                          attention mechanism and generate a representation vector
              •  Attention-based Deep MIL (AbDMIL) [13]: This work              for the bag. However, these techniques do not consider the
                 proposed an attention approach to identify the weights         relationship of instances in the bag. These approaches are
                 of the instances in the bag. The authors proposed two          implemented, and reproduced results are reported. The
                 architectures for attention-based pooling to solve MIL         proposed ViT-IWRS achieves better results in all ﬁve
                 binary classiﬁcation problem.                                  datasets. For the Fox dataset, the proposed approach
              •  Loss-based Attention (LBA) [15]: This method extends           achieved 62.5% accuracy compared to the 60.5% and
                 the concepts of (AbDMIL) [11] and introduces collab-           59.5% accuracy achieved by LBA [15] and AbDMIL [13],
                 orative training for attention and classiﬁcation layers of     respectively. Similarly, the ViT-IWRS approach attained
                 the network.                                                   84.5% accuracy for the Tiger dataset, superior to the pre-
              •  Multiple-instance SVM (MI-SVM and mi-SVM) [26]:                vious results of 83% by LBA [15]. In the case of the
                 In this study, two algorithms mi-SVM and MI-SVM                Elephant dataset, the proposed approach attained 87.4%
                 extend the use of SVM to solve multiple-instance               accuracy.
                 learning problems. The MI-SVM maximizes the bag                   For Musk1 and Musk2 datasets, the ViT-IWRS
                 margin while SVM updates the hyper-plane based on              approach achieved 89.5% and 87.6% compared to the
                 the instance label assignments.                                previous best performance of 88.6% and 87.3% accuracy,
              •  Classiﬁer Ensemble with constructive clustering (CCE)          respectively. Overall, the performance of ViT-IWRS is
                 [30]: This method represents the entire bag of instances       superior to the counterpart attention-based techniques on
                 from a binary vector, employing clustering and adopt-          all ﬁve benchmark datasets. The proposed ViT-IWRS is
                 ing an ensemble learning-based classiﬁcation approach.         robust enough to ascertain the association among the
                 Thebinary vector entries are set to 1 if any bag instance      instances. With the help of the RSN network, it can provide
                 is a part of the cluster. Additionally, the clustering and     superior bag encoding.
                 models are trained on different data representations.             The experimental results show that the prior assumption
              •  Multiple instances (Fisher Vector and VLAD) [9]:               of instance relationship in the bag restricts the performance
                 These methods are based on bag encoding generation             of AbDMIL and LBA. On the contrary, the proposed ViT-
                 techniques. These techniques are inspired by the widely        IWRS generates several bag representations without prior
                 used Fisher vector (FV) and VLAD encoding schemes              assumption of instance selection and simultaneously
                 for image classiﬁcation                                        selects the informative vector through RSN. This ability
                                                                                generates a more effective vector representation for the bag
                                                                                and improves the model’s generalization ability.
              5 Results and discussion                                          5.2 Comparison with benchmark techniques
              In this Section, we present the results and discuss the           Performance comparison of ViT-IWRS with benchmark
              performance of the proposed (ViT-IWRS )approach. First,           techniques is given in Table 2. ViT-IWRS outperformed
              we compare the performance of the proposed approach               the performance of existing benchmark techniques on
              with   state-of-the-art (SOTA) attention-based pooling            Elephant, Tiger, and Fox datasets. ViT-IWRS produced
              approaches for MIL classiﬁcation problems, including              62.5% accuracy for the Fox dataset compared to the
              AbDMIL [13], Gated-AbDMIL [53], and loss-based                    highest 86.2% accuracy by MI-Net [41]. For the Elephant
              attention (LBA) [15]. Later, the proposed approach is             dataset, 87.4% accuracy outperformed the previous best
              compared to benchmark bag classiﬁcation approaches.               accuracy of 62.1% accuracy of miFV [9]. Similarly, the
                                                                                ViT-IWRS produced 84.5% accuracy on the Tiger dataset
                                                                                and surpassed the previous best performance of 83.6%
                                                                                accuracy reported by MI-Net-RC [41].
                                                                                                                                   123
                6670                                                                                      Neural Computing and Applications (2024) 36:6659–6680
                Fig. 5 The performance analysis of ViT-IWRS with SOTA attention-based MIL techniques, a shows the comparison on Musk1 and Musk2
                datasets, while the performance comparison for image-related MIL dataset is given in (b)
                Table 2 The performance               Algorithms               Accuracy ondDatasets
                comparison of proposed ViT-
                IWRS with benchmark MIL                                        Elephant         Tiger             Fox               Musk1            Musk2
                techniques, the best accuracy is
                highlighted by boldface and           mi-SVM [26]              82.2             78.4              58.2              87.4             83.6
                italicized, while the second-best     MI-SVM [26]              81.4             84.0              57.8              77.9             84.3
                performance for each dataset is       Simple-MI [9]            80.1 ± (8.2)     77.8 ± (9.2)      54.6 ± (9.3)      83.2 ± (12.3)    85.3 ± (11.1)
                marked as simple boldface
                                                      EM-DD [28]               77.1 ± (9.8)     73.0 ± (10.1)     60.9 ± (10.1)     84.9 ± (9.8)     86.9 ± (10.8)
                                                      MI-Wrapper [54]          82.7 ± (8.8)     77.0 ± (9.2)      58.2 ± (10.2)     84.9 ± (10.6)    79.6 ± (10.6)
                                                      CCE [30]                 79.3 ± (7.5)     76.0 ± (12.0)     59.9 ± (13.7)     83.1 ± (2.5)     71.3 ± (2.4)
                                                      APR [23]                 75.19 ± (1.3)    55.8 ± (1.1)      53.2 ± (1.4)      92.4 – (2.7)     89.20 ± (3.0)
                                                      Citation-kNN [55]        82.6 ± (1.0)     78.8 ± (1.3)      58.2 ± (1.1)      90.3 ± (1.3)     83.7 ± (2.3)
                                                      MI-Graph [16]            85.1 ± (7.0)     81.9 ± (1.6)      61.2 ± (2.8)      90.0 ± (3.8)     90.1 – (3.8)
                                                      RSIS [29]                84.6 ± (1.0)     82.5 ± (2.3)      61.1 ± (2.0)      88.8 ± (2.3)     89.5 ± (2.6)
                                                      miVLAD [9]               85.0 ± (8.0)     81.0 ± (9.0)      62.0 ± (10.0)     87.1 ± (9.5)     87.2 ± (9.7)
                                                      miFV [56]                85.2 ± (8.0)     81.3 ± (7.0)      62.1± (10.0)      90.9 ± (8.0)     88.4 ± (9.0)
                                                      mi-Net [41]              85.8 ± (3.6)     82.4 ± (3.7)      61.3 ± (3.5)      88.9 ± (3.9)     85.2 ± (4.5)
                                                      MI-NET [41]              86.2 ± (2.5)     83.0 ± (2.2)      62.2 ± (2.2)      88.7 ± (4.1)     85.9 ± (4.5)
                                                      ESMIL [52]               82.5 ± (3.0)     82.7 ± (4.0)      61.7± (4.5)       87.8 ± (3.5)     88.2 ± (5.0)
                                                      Proposed ViT-IWRS        87.4 – (3.2)     84.5 – (3.5)      62.5 – (4.0)      89.5 – (7.5)     87.6 – (5.9)
                   In the case of Musk1 and Musk2 datasets, the ViT-                        case of image datasets, the ViT-IWRS performs consider-
                IWRS produced comparable accuracy to several bag clas-                      ably better than the benchmark approaches.
                siﬁcation approaches. The Musk1 and Musk2 datasets are
                composedofmolecular conformations with a small number                       5.3 ViT-IWRS VS benchmark MIL techniques
                of bags. It is usually difﬁcult for neural networks to per-
                form well as benchmark methods. Additionally, in the                        Benchmark MIL techniques such as mi-Net and MI-Net
                Musk1 and Musk2 datasets, molecular data follow a                           [41] adopt trainable pooling operations to generate vector
                structure and can be represented using graphs; therefore,                   representation for the bag. However, the proposed pooling
                the graph representation-based techniques [16] are more                     operation considers the equal contribution of instances in
                suitable for these types of datasets. Thus, the performance                 the bag. Additionally, these techniques do not account for
                of ViT-IWRS is limited in these datasets. However, in the                   the instance relationship information in the pooling pro-
                                                                                            cess. The bag encoding approaches such as miFV and
                123
              Neural Computing and Applications (2024) 36:6659–6680                                                                     6671
              miVLAD [56] are based on dictionary learning techniques          nodes is smaller than a preset threshold, then a weighted
              using the instance clustering process and incorporate all the    edge is established between the nodes. The weight of the
              instances of the bag in the encoding process. However,           edge expresses the afﬁnity of the two nodes. This approach
              these techniques do not incorporate any instance weighting       is useful where details of the bag structure play an essential
              technique in the encoding process which may affect the           role in the bag classiﬁcation process. In contrast, ViT-
              performance of generated encoding. Likewise, Simple-MI           IWRS models instance dependencies through the MHSA
              [9] computes the instance-wise mean vector for the bag. In       process and simultaneously incorporates bag-wise instance
              comparison with these algorithms, ViT-IWRS tackles the           relationship assumption in the classiﬁcation process.
              relationship assumption with instance weighing and bag
              representation selection process.                                5.4 Performance comparison on MIL-MNIST
                 RSIS [29] adopts a random subspace hard clustering                 dataset
              approach to select a candidate instance from positive bags
              while the instances from negative bags are sampled ran-
              domly. The selected instances are classiﬁed using an                For the multi-class classiﬁcation problem, the MIL-
              ensemble learning technique in ambient space. However,           MNIST dataset is generated. We used a bag generation
              the adopted instance selection process in RSIS results in a      approach similar to the one used in LBA [15] and AbDMIL
              class imbalance problem. Similarly, CCE [30] groups              [13]. The performance of the ViT-IWRS is compared with
              training instances into c clusters and generates a cdi-         SOTA attention-based approaches, including LBA [15],
              mensional binary vector representation for the bag. The ith      AbDMIL, and Gated-AbDMIL [13]. The two approaches,
              bits in the representation vector are set to one if corre-       AbDMIL and Gated-AbDMIL, were extended with Soft-
              sponding bag instances are part of ith cluster. The proposed     max output to support multi-class classiﬁcation problems.
              ViT-IWRS generates a robust bag representation vector by         The bag classiﬁcation is also performed for max-instance,
              incorporating the information presented in all instances of      mean-instance, max-instance embedding, and mean-in-
              the bag with different weights. Additionally, the generated      stance embedding. The max-embedding and mean-em-
              bag representation vector using ViT-IWRS offers more             bedding are computed by the output of the previously
              information in the classiﬁcation process than the classiﬁ-       discussed embedding network. The bag classiﬁcation
              cation of instances in ambient space or binary vector            results in Table 3 show that the proposed ViT-IWRS pro-
              generated by RSIS [29] and CCE [30].                             duces better performance in most cases, especially in the
                 Moreover, ESMIL [52] uses a ranking loss mechanism            case of large training sets of 150, 200, 300, and 400 bags,
              to assign a score to each instance in the bags. The proposed     respectively.
              ranking loss function ensures that the highest-scoring
              instance in a positive bag receives a higher score than the      5.5 Comparison on MIL-based CIFAR-10 dataset
              highest-scoring instance in a negative bag. ESMIL distin-
              guishes between positive and negative bags based on the          To better evaluate the performance of the proposed ViT-
              highest-scoring instances from the bag of each category,         IWRS, a larger and more challenging dataset is created
              and this process helps to maximize the AUC score. How-
              ever, ESMIL ignores the contribution of other instances in       Table 3 The performance comparison of ViT-IWRS with SOTA
              the bag classiﬁcation process. Additionally, the adopted         attention techniques on MIL-MNIST dataset. The best accuracy is
              training process lacks the ability to learn an efﬁcient score    highlighted in boldface and italicized, while the second-best perfor-
              function for bag classiﬁcation. This property is essential for   mance for each dataset is marked in simple boldface
              bag-level classiﬁcation, and the selection of a suboptimal                             Accuracy using different training set
              scoring function affects the model’s generalization ability.     Algorithms            50     100    150    200     300    400
              In contrast, ViT-IWRS assigns higher weights to the
              instances in the bag, which induces bag labels and gener-        Instance-(max)        47.7   75     84.6   88.7    89.2   89.9
              ates a robust bag representation vector by combining the         Instance-(mean)       58.7   77.4   86.5   91.7    91.9   92.2
              instance relationship and weighted impact of the instances.      Embedding-(max)       63.5   79.6   87.9   91.8    92.5   92.8
              This ability helps to learn an efﬁcient scoring function for     Embedding-(mean)      52.8   77.4   86.9   92.0    92.3   92.6
              bag-level classiﬁcation.                                         AbDMIL [13]           75.3   87.5   91.893.894.3          95.5
                 Similarly, Mi-Graph [16] assumes instances of the bag         Gated-AbDMIL [13]     72     86.9   91.1   93      93.8   94.5
              have a relationship and adopts a graph kernel learning           LBA[15]               75.9   89.0   91.7   93.9    94.395.7
              technique to transform a given bag into an undirected            Proposed ViT-IWRS     76.188.2      92.5   94.2    95.6   96.5
              weighted graph. The nodes in the generated graph represent
              instances of the bag, and if the distance between the two
                                                                                                                                  123
                6672                                                                                    Neural Computing and Applications (2024) 36:6659–6680
                based on CIFAR-10. The performance of ViT-IWRS is
                compared with SOTA methods, including LBA [15],
                AbDMIL [13], and Gated-AbDMIL [13], previously used
                for MIL-MNIST. The experiments are conducted for 500,
                and 5000 randomly generated training bags. Additionally,
                the experimental results of this dataset are presented in
                Table 4. The results show that ViT-IWRS surpasses the
                previous best performance of LBA and produces 3.1% and
                1.5%improvedperformanceon500training bags and 5000
                bags, respectively. The experimental results indicate that
                the proposed ViT-IWRS is robust in determining the
                dependencies among the bag instances in complex and
                challenging situations involving large datasets.                           Fig. 6 The performance analysis of ViT-IWRS with SOTA attention-
                                                                                           based MIL techniques on Colon Cancer histopathology dataset
                5.6 Performance comparison on colon cancer
                     dataset                                                               Using statistical analysis, this test determines if there is a
                                                                                           substantial difference between two related groups. This
                We have evaluated the performance of ViT-IWRS algo-                        technique is preferable when the normality or equal vari-
                rithms on a real-life colon cancer dataset with weak                       ance assumptions are violated. These methods are tested
                labeling. Our comparison includes state-of-the-art tech-                   using the same train-test distribution as ViT-IWRS.
                niques such as AbDMIL [13], Gated-AbDMIL [13], LBA                            Table 5 shows the pvalues for the Musk1 and Musk2
                [15], and ESMIL [52], as well as instance-level and                        datasets. A p-value below 0.05 indicates that ViT-IWRS is
                embedding level max and mean pooling operations. The                       statistically better than LBA [15], AbDMIL [13], Gated-
                results show the effectiveness of ViT-IWRS on this dataset.                AbDMIL [13], and ESMIL [52]. Likewise, in the case of
                Based on the results shown in Fig. 6, it is evident that the               the Musk2 dataset, ViT-IWRS is statistically signiﬁcant
                proposed ViT-IWRS outperforms other state-of-the-art                       compared to AbDMIL and Gated-AbDMIL. Table 6 shows
                techniques. ViT-IWRS obtained 92.4% bag-level classiﬁ-                     the p-values for the Elephant, Tiger, and Fox datasets. The
                cation accuracy compared to the previous best of 90.3% by                  proposed ViT-IWRS is statistically signiﬁcant for the Fox
                LBA [15]. ViT-IWRS achieves this by effectively                            dataset compared to LBA [15], AbDMIL [13], Gated-
                managing Global and Local information about the bag.                       AbDMIL [13], and ESMIL [52]. Similarly, for the Tiger
                Furthermore, the representation selection process ensures                  and Elephant datasets, the ViT-IWRS is statistically better
                that only the necessary bag representation vector is used in               than AbDMIL[13],Gated-AbDMIL[13],andESMIL[52].
                the classiﬁcation process.                                                 The proposed ViT-IWRS showed statistical signiﬁcance in
                                                                                           comparison with AbDMI [13] and Gated-AbDMIL [13].
                5.7 Statistical validation                                                 Similarly, ViT-IWRS exhibited statistical signiﬁcance over
                                                                                           ESMIL [52] and LBA [15] on four and two datasets,
                In this work, we evaluate the statistical signiﬁcance of ViT-              respectively.
                IWRS on MIL benchmark datasets using the Wilcoxon-                            We also used the Friedman rank test [59, 60] to assess
                signed rank test with a 95% conﬁdence interval [57, 58].                   the overall performance of various algorithms and compare
                                                                                           their performance across various datasets. This statistical
                Table 4 The experimental results on MIL-BASED CIFAR-10 dataset.            test is designed to assess whether there are statistically
                The best accuracy is highlighted in boldface and italicized, while the     signiﬁcant differences among the means of three or more
                second-best performance for each dataset is marked in simple               related groups. It involves ranking the data within each
                boldface                                                                   group and assigning a rank to each algorithm. In this
                Algorithms              Performance in accuracy                            ranking, the best algorithm is assigned the lowest rank,
                                                                                           while the algorithm with the worst performance is assigned
                                        500 (Training bags)     5000 (Training bags)       the highest rank. The rankings of the proposed and com-
                AbDMIL [13]             41.8                    51.7                       pared methods are determined with 95% signiﬁcance and a
                Gated-AbDMIL [13]       39.8                    49.1                       critical distance diagram is plotted to illustrate the results
                LBA[15]                 45.751.9                                           in Fig. 7. As shown in Fig. 7, the proposed ViT-IWRS
                Proposed ViT-IWRS       49.5                    54.8                       achieved the lowest rank (most important) among all
                                                                                           compared techniques. This indicates that the performance
                                                                                           of ViT-IWRS is superior to the compared methods.
                123
               Neural Computing and Applications (2024) 36:6659–6680                                                                                   6673
               Table 5 The obtained pvalues of Wilcoxon-signed ranked test for Musk1 and Musk2 datasets
                                 Musk1                                                         Musk2
                                 Is ViT-IWRS statistically signiﬁcant? (if p\0.05)   p-        Is ViT-IWRS statistically signiﬁcant? (if p\0.050)   p-
                                                                                     values                                                         values
               Algorithms
               AbDMIL            Yes                                                 0.0242    Yes                                                  0.0414
               Gated-            Yes                                                 0.0079    Yes                                                  0.0331
                 AbDMIL
               LBA               Yes                                                 0.0465    No                                                   0.4696
               ESMIL             Yes                                                 0.0054    No                                                   0.6001
               Table 6 The obtained pvalues of Wilcoxon-signed ranked test for Elephant, Tiger, and Fox datasets
                                          Fox                                      Tiger                                    Elephant
                                          Is ViT-IWRS           pvalues           Is ViT-IWRS            pvalues          Is ViT-IWRS            pvalues
                                          statistically                            statistically                            statistically
                                          signiﬁcant?                              signiﬁcant?                              signiﬁcant?
                                          (if p\0.05)                              (if p\0.05)                              (if p\0.05)
               Algorithms
                 AbDMIL                   Yes                   0.003              Yes                    0.019             Yes                    0.0002
                 Gated-AbDMIL             Yes                   0.004              Yes                    0.017             Yes                    0.0044
                 LBA                      Yes                   0.009              No                     0.125             No                     0.0750
                 ESMIL                    Yes                   0.048              Yes                    0.130             Yes                    0.0001
               Fig. 7 Critical distance diagram comparing the proposed ViT-IWRS         important rank at the left and the least signiﬁcant rank at the right.
               against various MIL algorithms with a 95% conﬁdence interval. The        The two algorithms are not considerably different if they are not
               diagram’s top line shows the algorithm’s average rank, with the most     connected by bold line
                                                                                        algorithms are trained for 100 Epochs, and the average
                                                                                        training time in the log scale is shown in Fig. 8. All the
               5.8 Time efficiency comparison                                           experiments are conducted on a machine with a Core i7
                                                                                        3.10 GHz CPU, RTX 3060 GPU, and 16GB of main
               In this paper, the time efﬁciency of the proposed ViT-                   memory.
               IWRS is empirically evaluated on ﬁve benchmark MIL                          Compared to AbDMIL, Gated-AbDMIL, and LBA, the
               datasets. The time costs of training do not include the time             training process for ViT-IWRS is more time-consuming.
               for data preparation. The proposed ViT-IWRS is compared                  This is because ViT uses a self-attention mechanism with
               with state-of-the-art counterparts, including LBA [15],                  quadratic complexity, making it more computationally
               AbDMIL[13],Gated-AbDMIL[13],andESMIL[52].The                             expensive than traditional attention algorithms. Notably,
                                                                                                                                                 123
             6674                                                                        Neural Computing and Applications (2024) 36:6659–6680
             Fig. 8 The time efﬁciency analysis of ViT-IWRS with SOTA attention-based MIL techniques. The time comparisons on the Elephant, Tiger, and
             Fox datasets are shown in (a–c). The time comparison of the Musk1 and Musk2 datasets is illustrated in (d) and (e), respectively
             ViT-IWRS requires less training time than ESMIL, which           approach’s effectiveness and ability to surpass the capa-
             involves a pairwise loss strategy, necessitating the adjust-     bilities of current state-of-the-art techniques.
             ment of network weights across all pairs of positive and
             negative bags.                                                   5.9 Parameter sensitivity analysis
                However, ViT-IWRS outperforms state-of-the-art algo-
             rithms on all types of datasets in terms of bag classiﬁcation    This section discusses the impact of different hyperpa-
             performance. This outcome underscores the proposed               rameters related to ViT-IWRS on performance. There are
              123
              Neural Computing and Applications (2024) 36:6659–6680                                                                     6675
              several parameters related to ViT-IWRS, such as the size         that two subnetwork layers are preferred for Musk1, Ele-
              of the RSN, the number of blocks, and the number of heads        phant, and Tiger datasets. Whereas, for Musk2 and Fox
              in ViT blocks. These parameters are tuned one at a time.         datasets, tree layer RSN is preferred. However, increasing
              While tuning one parameter, the other parameters are kept        the number of layers can result in overﬁtting. Furthermore,
              ﬁxed. Initially, the number of transformer encoder blocks        the number of layers for the MIL-MNIST, MIL-BASED
              and layers in RSN is set to two, and the number of heads in      CIFAR-10, and Colon Cancer datasets is set to one
              MHSA is ﬁxed to four, respectively. The details of the           throughout the experiments. The detailed analysis of RSN
              hyperparameters related to model training are given in           size is given in Table in 9.
              Table 7. The details of the embedding network are also
              presented in this section.                                       5.9.3 Analysis of term k in loss function
              5.9.1 Embedding network                                          The loss function presented in Sect. 3.7 comprises L1 and
                                                                               L2, where k is a hyperparameter. The value of k plays a
              The proposed ViT-IWRS ﬁrst transforms the bag instance           signiﬁcant role in the model performance and interpreta-
              to a latent representation using an embedding network. We        tion. As discussed previously, the L1 term in the loss
              adopted a similar setting for embedding networks as pre-         function can be decreased to a small value even when only
              viously used in AbDMIL [13] and LBA [15]. The                    one instance shares the label with the bag; when k ¼ 0, the
              embedding network for benchmark datasets mainly con-             L2 term is removed from the objective, the model only
              sists of fully connected layers. In contrast, the MIL-MNIST      focuses on the bag loss resulting in a low instance recall
              and MIL-based CIFAR-10 datasets network comprises                and may negatively affect the classiﬁcation performance.
              convolution layers with other related operations based on        Weevaluated the impact of k on MIL-MNIST datasets of
              the LeNet5 architecture [61]. The details of the networks        50 training and 1000 testing bags, respectively. Figure 9
              for the benchmark dataset and MIL-MNIST dataset are              shows      the    performance      of    ViT-IWRS        with
              given in Table 8.                                                k 2 0;103      2    1
                                                                                    ½;10 ;10 ;1;1;10;.                The      experiments
                                                                               demonstrate the effectiveness of k in the loss function. The
              5.9.2 Layers in representation selection subnetworks (RSN)       positive value of k between 1 and 10 improves the
                                                                               instances recall and bag classiﬁcation performance.
              This subnetwork comprises one or more fully connected
              layers, whereas the network’s last layer consists of a single    5.9.4 Analysis for ViT depth and attention heads
              output neuron. The network learns a nonlinear represen-
              tation selection function using a continuous output vector       The ViT depth and the number of attention heads are the
              during training and generates a discretized one-hot vector       essential parameters in the proposed approach. First, we
              in the testing. The layers in this subnetwork depend on the      ﬁxed the number of attention heads to four and the impact
              dataset representation diversity. The initial RSN comprises      of ViT depth. Later, the best-performing depth is used to
              a fully connected layer with ReLU activation and dropout         analyze the inﬂuence of attention heads. The experiments
              operation. Later, the layers to RSN are added with Tanh(:)       show that a depth of 3 is preferred for the Musk1 and
              followed by the dropout operation. The experiments show          Musk2 datasets, respectively, while the number of heads
              Table 7 The details of hyperparameters used in the training of ViT-IWRS
              Hyperparameter         Benchmark dataset                    MIL-MNIST dataset                     MIL CIFAR-10
                                                                                                                DATASET
              k                      221
              Optimizer              Adam                                 Adam                                  Adam
              Betas                  (0.9,0.999)                          (0.9,0.999)                           (0.9, 0.999)
              Learning-rate          0.0005                               0.0005                                0.01
              Epochs                 150                                  100                                   100
              Weight decay           0.0005                               0.0001                                0.0001
              Batch size             1                                    1                                     1
              Stopping criteria      Lowest validation error and loss     Lowest validation error and loss      Lowest validation error and loss
                                                                                                                                  123
               6676                                                                                Neural Computing and Applications (2024) 36:6659–6680
               Table 8 The details of              Layer numbers                                                  Network Details
               embedding network for
               benchmark and MIL-MNIST             Layer details for Benchmark Dataset
               datasets. The parameters of
               convolution layers are               1                                                             FC-256 ? Activation() ? Dropuout()
               constituted as                       2                                                             FC-128 ? Activation() ? Dropout()
               Convolution(a,b,c,d), where a,       3                                                             FC-64 ? Activation() ?Dropout()
               b, c, and d represent kernel size,  Layer details for MIL-MNIST Dataset
               stride, padding and the number
               of kernels, respectively             1                                                             Convolution (5,1,0,20) ? Activation()
                                                    2                                                             Max-pool (2,2)
                                                    3                                                             Convolution (5, 1, 0, 50) ? Activation()
                                                    4                                                             Max-pool (2,2)
                                                    5                                                             FC-500 ? Activation()
                                                   Layer Details for MIL-based CIFAR-10 Dataset
                                                    1                                                             Convolution (5, 3, 0, 20) ? Activation()
                                                    2                                                             Max-pool (2,2)
                                                    3                                                             Convolution (5, 1, 0, 50) ? Activation()
                                                    4                                                             Max-pool (2,2)
                                                    5                                                             FC-500 ? Activation() ? Dropout()
                                                    6                                                             FC-500 ? Activation()
                                                   Layer Details for Colon Cancer Dataset
                                                    1                                                             Convolution (4, 1, 0, 36) ? Activation()
                                                    2                                                             Max-pool (2,2)
                                                    3                                                             Convolution (3, 1, 0, 48) ? Activation()
                                                    4                                                             Max-pool (2,2)
                                                    5                                                             FC-500 ? Activation() ? Dropout()
               Table 9 Analysis of layers in RSN                                      from 2 and 4 can produce better performance. This is due
               Number of layer     Musk1     Musk2      Elephant    Fox     Tiger     to the nature of the datasets. Additionally, where the
                                                                                      structure information of the instances is important in
               1                   87.9      85.7       83.9        50.9    82.5      addition to the instance relationship, adding ViT blocks and
               2                   88.5      85.9       85.4        60.5    83.2      increasing the number of heads does not improve perfor-
               3                   87.2      86.1       83.9        61.2    82.4      mance. For the fox, tiger, and Elephant datasets, 3, 2 and 3
               4                   86.5      84.3       84.0        59.5    82.5      blocks and 4 heads tend to perform well, respectively. It
               5                   86.3      83.6       84.2        58.0    81.5      shows that these instances inside these datasets are highly
               The bold value in the table shows the highest performance achieved     related, and existing SOTA attention-based approaches do
               on a particular dataset using the corresponding number of layers in    not consider this relation. The analysis of depth is shown in
               RSN                                                                    Fig. 10, and the analysis of the number of heads in MHSA
                                                                                      is illustrated in Fig. 11, respectively. Furthermore, for the
                                                                                      MIL-MNIST dataset, the depth is set to 1 and the number
                                                                                      of heads is set to 4 throughout the experiments.
                                                                                      5.10 Ablation study
                                                                                      The proposed ViT-IWRS consists of two essential pro-
                                                                                      cessing blocks: the transformer encoding and RSN blocks.
                                                                                      These blocks are shown in Fig. 3a and d, respectively. The
                                                                                      contribution of these two blocks to overall model perfor-
                                                                                      manceisvalidated in the section. The performance of these
                                                                                      two blocks is observed on the Musk1 dataset for binary
                                                                                      classiﬁcation and the MIL-MNIST dataset for multi-class
               Fig. 9 The analysis of the term k in loss function
               123
             Neural Computing and Applications (2024) 36:6659–6680                                                                    6677
             Fig. 10 The analysis of transformer depth. The depth analysis for Musk1, Musk2, Elephant, Fox, and Tiger datasets is illustrated from (a–e),
             respectively
             Fig. 11 The analysis of the number of MHSA heads in transformer encoder. The analysis of attention heads for Musk1, Musk2, Elephant, Fox,
             and Tiger datasets is given from (a–e), respectively
             classiﬁcation problems. Additionally, the experiments on         proposed ViT-IWRS results in performance degradation.
             the MIL-MNIST dataset are performed 30 times using a             Therefore, the use of the RSN block is essential to achieve
             training set of 50 bags and a test set of 1000 bags, and the     improved results.
             average performance is presented.
                                                                              5.10.2 Effect of transformer encoding
             5.10.1 Effect of RSN block
                                                                              In order to verify the impact of transformer encoding, we
             In order to verify the impact of RSN, we replace this block      simply apply max and attention pooling on the output of
             with a simple average operation that computes the feature-       the embedding network to obtain a bag representation
             wise average of the representation matrix R. Later, the          vector. Afterward, the generated output vector is used for
             averaged vector is used for classiﬁcation. The experimental      the classiﬁcation process. This process is analogous to
             results in Table 10 show that the removal of RSN from the        existing AbDMIL and LBA algorithms. The experimental
                                                                                                                                123
              6678                                                                            Neural Computing and Applications (2024) 36:6659–6680
              Table 10 Details of ablation study, the performance is presented in classiﬁcation accuracy
              Ablation study design                                             Performance in    Impact
                                                                                accuracy
                                                                                Musk1 MIL-
                                                                                         MNIST
              Effect of RSN block                                               87.70    73.90    The model’s performance degrades when the RSN
              (RSN network block removed from the model, and representation                         network is replaced with an average operation.
                vectors in R are combined by applying average pooling operation)
              Effect of Transformer Encoding block. (Transformer encoding block 86.2     63.5     The model’s performance degrades when the RSN
                and RSN network block is removed from the model, and the output                     network is replaced with an average operation.
                of Embedding network is transformed to a vector representation
                using simple Max-pooling)
              Effect of Transformer Encoding block. (Transformer encoding block 88.4     75.3     The model’s performance degrades when the
                and RSN network block is removed from the model, and the output                     Transformer encoding and RSN is removed from
                of Embedding network is transformed to vector representation using                  model.
                attention pooling pooling)
              Performance of proposed ViT-IWRS with all proposed blocks         89.50    76.10    ViT-IWRSachievesimprovedperformance withall
                                                                                                    proposed blocks
              results in Table 10 show that the removal of Transformer            such as computer-aided diagnostic and clinical decision
              EncodingandRSNfromtheproposedViT-IWRSresultsin                      support.
              performance degradation. Therefore, the use of this block              Although the proposed approach produces promising
              is essential to attain improved results.                            results on several datasets related to images, this approach
                                                                                  is less computationally expensive as compared to existing
                                                                                  pooling techniques. Furthermore, the performance of ViT-
              6 Conclusion                                                        IWRS is effective when labels are entirely dependent on
                                                                                  the structural properties of the instances, such as molecular
              In this work, we presented the application for a vision             datasets. The proposed loss function can be further exten-
              transformer for simultaneous instance weighting and bag             ded to handle multi-instance multi-target regression prob-
              encoding processes for MIL. The existing MIL algorithms             lems,   such   as   Drug Discovery and Environmental
              presumed that the instances in the bag are either related or        Monitoring. In the future, we intend to explore the appli-
              unrelated. However, this assumption may not apply to all            cation of the proposed approach to multiple-instance and
              bags in the dataset.                                                multiple-label learning (MIML) tasks and incorporate the
                 The proposed approach avoids the instance relationship           structural details of the bag into the self-attention process.
              assumption in a two-stage process. In the ﬁrst stage, several
              bag representation vectors are generated for both relation-         Acknowledgement This publication was jointly supported by Qatar
              ship assumptions. In the second stage, the network decides          University QUHI-CENG-22/23-548. The ﬁndings achieved herein are
                                                                                  solely the responsibility of the authors. Open Access funding pro-
              whether to consider instances to be related or not using the        vided by the Qatar National Library.
              representation selection module in the classiﬁcation pro-
              cess. The experimental results show that the selection              Data availability The datasets generated during and/or analyzed
              subnetwork robustly selects bag representation vectors in           during the current study are publically available at http://www.uco.es/
                                                                                  grupos/kdis/momil/.
              the bag classiﬁcation process in an end-to-end trainable
              approach. The experiments are performed on diverse                  Declarations
              datasets related to images and molecular activity. The
              proposed approach outperformed several state-of-the-art             Conflict of interest Authors declare no conflict of interest.
              attention pooling and benchmark bag classiﬁcation tech-
              niques. Additionally, the proposed ViT-IWRS provides                Open Access   This article is licensed under a Creative Commons
                                                                                  Attribution 4.0 International License, which permits use, sharing,
              model interpretations for vision transformer architecture           adaptation, distribution and reproduction in any medium or format, as
              through an attention-based instance weighting approach.             long as you give appropriate credit to the original author(s) and the
              Thus, the proposed approach is suited for image classiﬁ-            source, provide a link to the Creative Commons licence, and indicate
              cation, object detection, and high-risk MIL applications,           if changes were made. The images or other third party material in this
                                                                                  article are included in the article’s Creative Commons licence, unless
              123
                Neural Computing and Applications (2024) 36:6659–6680                                                                                           6679
                indicated otherwise in a credit line to the material. If material is not     18. Waqas M, Tahir MA, Qureshi R (2023) Deep Gaussian mixture
                included in the article’s Creative Commons licence and your intended              model based instance relevance estimation for multiple instance
                use is not permitted by statutory regulation or exceeds the permitted             learning applications. Appl Intell 53(9):10310–10325
                use, you will need to obtain permission directly from the copyright          19. Waqas M, Tahir MA, Khan SA (2023) Robust bag classiﬁcation
                holder. To view a copy of this licence, visit http://creativecommons.             approach for multi-instance learning via subspace fuzzy cluster-
                org/licenses/by/4.0/.                                                             ing. Expert Syst Appl 214:119113
                                                                                             20. Shao Z, Bian H, Chen Y, Wang Y, Zhang J, Ji X et al (2021)
                                                                                                  Transmil: transformer based correlated multiple instance learning
                References                                                                        for whole slide image classiﬁcation. Adv Neural Inf Process Syst
                                                                                                  34:2136
                                                                                             21. Waqas M, Khan Z, Ahmed SU, Raza A (2023) MIL-Mixer: a
                 1. Zhou Z-H (2018) A brief introduction to weakly supervised                     robust bag encoding strategy for Multiple Instance Learning (mil)
                    learning. Natl Sci Rev 5(1):44–53                                             using MLP-Mixer. In 2023 18th IEEE International Conference
                 2. Li M, Li X, Jiang Y, Zhang J, Luo H, Yin S (2022) Explainable                 on Emerging Technologies (ICET) 22–26
                    multi-instance and multi-task learning for COVID-19 diagnosis            22. Wei X-S, Zhou Z-H (2016) An empirical study on image bag
                    and lesion segmentation in CT images. Knowl-Based Syst                        generators for multi-instance learning. Mach Learn 105(2):155–198
                    252:109278                                                                                                            ´
                                                                                             23. Dietterich TG, Lathrop RH, Lozano-Perez T (1997) Solving the
                 3. Liu Y, Wu YH, Wen P, Shi Y, Qiu Y, Cheng MM (2020) Lev-                       multiple instance problem with axis-parallel rectangles. Artif
                    eraging   instance-,  image-and dataset-level information for                 Intell 89(1–2):31–71
                    weakly supervised instance segmentation. IEEE Trans. Pattern             24. Sirinukunwattana K, Raza SEA, Tsang Y-W, Snead DR, Cree IA,
                    Anal. Mach. Intell. 44(3):1415–1428                                           Rajpoot NM(2016) Locality sensitive deep learning for detection
                 4. Zhang Y, Liu S, Qu X, Shang X (2022) Multi-instance discrim-                  and classiﬁcation of nuclei in routine colon cancer histology
                    inative contrastive learning for brain image representation.                  images. IEEE Trans Med Imaging 35(5):1196–1206
                    Neural Comput Appl. https://doi.org/10.1007/s00521-022-07524-            25. Raykar VC, Krishnapuram B, Bi J, Dundar M, Rao RB (2008)
                    7                                                                             Bayesian multiple instance learning: automatic feature selection
                 5. Antwi-Bekoe E, Liu G, Ainam J-P, Sun G, Xie X (2022) A deep                   and inductive transfer. In: Proceedings of the 25th international
                    learning approach for insulator instance segmentation and defect              conference on machine learning, pp. 808–815
                    detection. Neural Comput Appl 34(9):7253–7269                            26. Andrews S, Tsochantaridis I, Hofmann T (2002) Support vector
                                            ´
                 6. Wang K, Liu J, Gonzalez D (2017) Domain transfer multi-in-                    machines for multiple-instance learning. In: NIPS, vol. 2, p. 7
                    stance dictionary learning. Neural Comput Appl 28:983–992                27. Amar RA, Dooly DR, Goldman SA, Zhang Q (2001) Multiple-
                 7. Carbonneau M-A, Cheplygina V, Granger E, Gagnon G (2018)                      instance learning of real-valued data. In: ICML, pp. 3–10.
                    Multiple instance learning: a survey of problem characteristics               Citeseer
                    and applications. Pattern Recogn 77:329–353                              28. Zhang Q, Goldman S (2001) EM-DD: An improved multiple-
                 8. Cheplygina V, Tax DM, Loog M (2015) Dissimilarity-based                       instance learning technique. In: Dietterich T, Becker S, Ghahra-
                    ensembles for multiple instance learning. IEEE Trans Neural                   mani Z(ed) Advances in neural information processing systems.
                    Netw Learn Syst 27(6):1379–1391                                               MIT Press, 14. https://proceedings.neurips.cc/paper_ﬁles/paper/
                 9. Wei X-S, Wu J, Zhou Z-H (2016) Scalable algorithms for multi-                 2001/ﬁle/e4dd5528f7596dcdf871aa55cfccc53c-Paper.pdf
                    instance   learning.  IEEE Trans Neural Netw Learn Syst                  29. Carbonneau M-A, Granger E, Raymond AJ, Gagnon G (2016)
                    28(4):975–987                                                                 Robust multiple-instance learning ensembles using random sub-
                                    ´
                10. Perronnin F, Sanchez J, Mensink T (2010) Improving the ﬁsher                  space instance selection. Pattern Recogn 58:83–99
                    kernel for large-scale image classiﬁcation. In: European Confer-         30. Zhou Z-H, Zhang M-L (2007) Solving multi-instance problems
                    ence on Computer Vision, pp. 143–156. Springer                                with classiﬁer ensemble based on constructive clustering. Knowl
                11. Ramon J, De Raedt L (2000) Multi instance neural networks. In:                Inf Syst 11(2):155–170
                    Proceedings of the ICML-2000 Workshop on Attribute-value and             31. Zhou Z-H, Xu J-M (2007) On the relation between multi-instance
                    Relational Learning, pp. 53–60                                                learning and semi-supervised learning. In: Proceedings of the 24th
                12. Kandemir M, Hamprecht FA (2015) Computer-aided diagnosis                      international conference on machine learning, pp. 1167–1174
                    from weak supervision: a benchmarking study. Comput Med                  32. Leistner C, Saffari A, Bischof H (2010) Miforests: Multiple-in-
                    Imaging Graph 42:44–50                                                        stance learning with randomized trees. In: European conference
                13. Ilse M, Tomczak J, Welling M (2018) Attention-based deep                      on computer vision, pp. 29–42. Springer
                    multiple instance learning. In: International conference on              33. Li CH, Gondra I, Liu L (2012) An efﬁcient parallel neural net-
                    machine learning, pp. 2127–2136. PMLR                                         work-based multi-instance learning algorithm. J Supercomput
                14. Zhang W-J, Zhou Z-H (2014) Multi-instance learning with dis-                  62(2):724–740
                    tribution change. In: Proceedings of the AAAI conference on              34. Waqas M, Khan Z, Anjum S, Tahir MA (2020) Lung-wise
                    artiﬁcial intelligence, vol. 28                                               tuberculosis analysis and automatic CT report generation with
                15. Shi X, Xing F, Xie Y, Zhang Z, Cui L, Yang L (2020) Loss-based                hybrid feature and ensemble learning. In: CLEF (Working Notes)
                    attention for deep multiple instance learning. In: Proceedings of        35. Abro WA, Aicher A, Rach N, Ultes S, Minker W, Qi G (2022)
                    the   AAAI conference on artiﬁcial intelligence, vol. 34,                     Natural language understanding for argumentative dialogue sys-
                    pp. 5742–5749                                                                 tems in the opinion building domain. Knowl-Based Syst
                16. Zhou Z-H, Sun Y-Y, Li Y-F (2009) Multi-instance learning by                   242:108318
                    treating instances as non-IID samples. In: Proceedings of the 26th       36. Hanif M, Waqas M, Muneer A, Alwadain A, Tahir MA, Raﬁ M
                    annual    international    conference    on    machine     learning,          (2023) Deepsdc: deep ensemble learner for the classiﬁcation of
                    pp. 1249–1256                                                                 social-media ﬂooding events. Sustainability 15(7):6049
                17. Waqas M, Tahir MA, Qureshi R (2021) Ensemble-based instance              37. Hoffman J, Pathak D, Darrell T, Saenko K (2015) Detector dis-
                    relevance estimation in multiple-instance learning. In: 2021 9th              covery in the wild: joint multiple instance and representation
                    European workshop on visual information processing (EUVIP),                   learning. In: Proceedings of the IEEE conference on computer
                    pp. 1–6. IEEE                                                                 vision and pattern recognition, pp. 2883–2891
                                                                                                                                                         123
               6680                                                                                 Neural Computing and Applications (2024) 36:6659–6680
               38. Zhang C, Platt J, Viola P (2005) Multiple instance boosting for         Toronto.     https://www.cs.toronto.edu/*kriz/learning-features-
                   object detection. In: Weiss J, Sch\‘‘{o}lkopf B, Platt J(ed) Advances   2009-TR.pdf
                   in neural information processing systems. MIT Press, 18             50. Ghaznavi F, Evans A, Madabhushi A, Feldman M (2013) Digital
               39. Shi X, Xing F, Xu K, Xie Y, Su H, Yang L (2017) Supervised              imaging in pathology: whole-slide imaging and beyond. Annu
                   graph hashing for histopathology image retrieval and classiﬁca-         Rev Pathol 8:331–359
                   tion. Med Image Anal 42:117–128                                                                  ´
                                                                                       51. Dimitriou N, Arandjelovic O, Caie PD (2019) Deep learning for
               40. Liu Y, Chen H, Wang Y, Zhang P (2021) Power pooling: an                 whole slide image analysis: an overview. Front Med 6:264
                   adaptive pooling function for weakly labelled sound event           52. Asif A et al (2019) An embarrassingly simple approach to neural
                   detection. In: 2021 International joint conference on neural net-       multiple instance classiﬁcation. Pattern Recogn Lett 128:474–479
                   works (IJCNN), pp. 1–7. IEEE                                        53. Hahn M (2020) Theoretical limitations of self-attention in neural
               41. Wang X, Yan Y, Tang P, Bai X, Liu W (2018) Revisiting mul-              sequence models. Trans Assoc Comput Linguist 8:156–171
                   tiple instance neural networks. Pattern Recogn 74:15–24             54. Frank E, Xu X (2008) Applying propositional learning algorithms
               42. Li G, Li C, Wu G, Ji D, Zhang H (2021) Multi-view attention-            to multi-instance data. Working paper series, Department of
                   guided   multiple   instance  detection   network   for   inter-        computer science, The University of Waikato. https://books.goo
                   pretable breast cancer histopathological image diagnosis. IEEE          gle.com/books?id=5eaGzgEACAAJ
                   Access 9:79671–79684                                                55. Wang J, Zucker J-D (2000) Solving multiple-instance problem: a
               43. WangF,JiangM,QianC,YangS,LiC,ZhangH,WangX,Tang                          lazy learning approach. International Conference on Machine
                   X(2017) Residual attention network for image classiﬁcation. In:         Learning.   1:1119–1126.   https://api.semanticscholar.org/Corpu
                   Proceedings of the IEEE conference on computer vision and               sID:13896348
                   pattern recognition, pp. 3156–3164                                  56. Wei X-S, Wu J, Zhou Z-H (2014) Scalable multi-instance
               44. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X,            learning. In: 2014 IEEE international conference on data mining,
                   Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S               pp. 1037–1042. IEEE
                   et al.(2020) An image is worth 16x16 words: transformers for        57. Wilcoxon F (1992) Individual comparisons by ranking methods.
                   image recognition at scale. arXiv preprint arXiv:2010.11929             In: Kotz S, Johnson NL (eds) Breakthroughs in statistics:
               45. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez             methodology and distribution. Springer, Berlin, pp 196–202
                   AN, Kaiser Ł, Polosukhin I (2017) Attention is all you need.        58. Conover WJ (1999) Practical nonparametric statistics, vol 350.
                   Advances in neural information processing systems. Curran               Wiley, New York
                   Associates, Inc., 30                                                         ˇ
                                                                                       59. Demsar J (2006) Statistical comparisons of classiﬁers over mul-
               46. Jang E, Gu S, Poole B (2017) Categorical Reparametrization with         tiple data sets. J Mach Learn Res 7:1–30
                   Gumbel-Softmax. In: Proceedings international conference on         60. Friedman M (1937) The use of ranks to avoid the assumption of
                   learning representations (ICLR). https://openreview.net/pdf?id=         normality implicit in the analysis of variance. J Am Stat Assoc
                   rkE3y85ee                                                               32(200):675–701
               47. Li X-C, Zhan D-C, Yang J-Q, Shi Y (2021) Deep multiple              61. LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based
                   instance selection. Sci China Inf Sci 64(3):1–15                        learning   applied  to   document    recognition.  Proc   IEEE
               48. LeCun Y, Cortes C, Burges C (2010) Mnist handwritten digit              86(11):2278–2324
                   database. ATT Labs [Online]. Available: http://yann.lecun.com/
                   exdb/mnist2                                                         Publisher’s Note Springer Nature remains neutral with regard to
               49. Krizhevsky A, Hinton G (2009) Learning multiple layers of           jurisdictional claims in published maps and institutional afﬁliations.
                   features from tiny images. Technical report, University of
               Authors and Afﬁliations
                                       1,2   •                            1 •                                  3 •                          4 •
               Muhammad Waqas                  Muhammad Atif Tahir           Muhammad Danish Author               Sumaya Al-Maadeed
               Ahmed Bouridane5 • Jia Wu2
               & Muhammad Waqas                                                        1   FAST School of Computing, National University of
                   waqas.sheikh@nu.edu.pk; mwaqas@mdanderson.org                           Computer Emerging Science (FAST-NUCES), Karachi,
                   Muhammad Atif Tahir                                                     Pakistan
                   atif.tahir@nu.edu.pk                                                2   Department of Imaging Physics, The University of Texas MD
                   Muhammad Danish Author                                                  Anderson Cancer Center, Houston, TX, USA
                   k190887@nu.edu.pk                                                   3   College of information technology, United Arab Emirates
                   Sumaya Al-Maadeed                                                       University, Abu Dhabi, United Arab Emirates
                   Salali@qu.edu.qa                                                    4   Department of Computer Science and Engineering, Qatar
                   Ahmed Bouridane                                                         University, Doha, Qatar
                   abouridane@sharjah.ac.ae                                            5   Cybersecurity and Data Analytics Research Center,
                   Jia Wu                                                                  University of Sharjah, Sharjah, UAE
                   jiawu11@mdanderson.com
               123
