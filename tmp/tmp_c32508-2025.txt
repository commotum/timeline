                                                                                           www.nature.com/scientificdata
                      opeN a Comprehensive Behavioral 
    Data DeSCRIptoR Dataset for the abstraction and 
                                    Reasoning Corpus
                                                  1                 2                   1,2                  1
                                    Solim LeGris    ✉, Wai Keen Vong , Brenden M. Lake     & todd M. Gureckis
                                    the abstraction and Reasoning Corpus (aRC) is a visual program synthesis benchmark designed to 
                                    test out-of-distribution generalization in machines. Comparing aI algorithms to human performance 
                                    is essential to measure progress on these problems. In this paper, we present H-aRC (Human-aRC): a 
                                    novel large-scale dataset containing solution attempts from over 1700 humans on ARC problems. The 
                                    dataset spans the full set of 400 training and 400 evaluation tasks from the original ARC benchmark, 
                                    and it is the largest human evaluation to date. By publishing the dataset, we contribute human 
                                    responses to each problem, step-by-step behavioral action traces from the aRC user-interface, and 
                                    natural-language solution descriptions of the inferred program/rule. We believe this dataset will be 
                                    of value to researchers, both in cognitive science and AI, since it of㘶ers the potential to facilitate the 
                                    discovery of underlying mechanisms supporting abstraction and reasoning in people. the insights to be 
                                    gained from these data not only have value for cognitive science, but could in turn inform the design of 
                                    more e㘠陦cient, human-like AI algorithms.
                                    Background & Summary
                                    T㔴e question of how to measure intelligence in humans and machines remains a critical stepping stone towards 
                                    developing more sophisticated AI. In that spirit, the Abstraction and Reasoning Corpus (ARC) benchmark was 
                                    proposed by François Chollet1 to evaluate analogical generalization, measuring how machines handle a broad 
                                    category of novel tasks given just a few examples. Each task requires inferring an underlying transformation rule 
                                    or program from a series of training input-output pairs which consist of abstract visual grids (see Fig. 1), and 
                                    to use this rule to correctly generate an output grid given a novel test input. Although visually simple, the tasks 
                                    are conceptually rich and challenging, requiring the identif㘶cation of compositional rules involving objects and 
                                    relations, geometry, counting, visual instructions, and logical operations.
                                                                                                                             -
                                      In the last several years, large language models (LLMs) have achieved impressive performance on a wide vari
                                    ety of benchmarks, demonstrating competency in natural language understanding, coding, and mathematics2,3. 
                                    With larger and more powerful LLMs, many benchmarks have had a limited shelf life, with performance rapidly 
                                                                          4
                                    increasing to human or even superhuman levels . In contrast, ARC has proven to be a persistent and formidable 
                                    challenge for state-of-the-art AI systems, with little progress observed in the f㘶rst few years af㘶er its creation. In 
                                    a f㘶rst ARC competition held on Kaggle in 2019, the majority of approaches were based on program synthesis 
                                                                                                                             -
                                    techniques, with the winner of the competition achieving a 21% score on the private test set (kaggle.com/com
                                    petitions/abstraction-and-reasoning-challenge). Af㘶er several years of stagnation, an ARC Prize Foundation was 
                                    founded in 2024 to encourage research and development for achieving an open-source, low-resource solution 
                                    to the ARC benchmark, and spurring progress towards human-level intelligence (arcprize.org). A number of 
                                                                                                                             -
                                    advancements resulted from their f㘶rst public competition, with some open-source LLM-based models achiev
                                    ing notable gains in performance, jumping from 33% to 55.5% on the private evaluation set5. Additionally, 
                                    closed-source models from OpenAI and Anthropic have achieved major leaps in performance, reaching up to 
                                                                                                                             -
                                    75.7% on the ARC prize semi-private evaluation set (arcprize.org/leaderboard). While these results are impres
                                    sive, today’s top-performing models rely on massive amounts of text data during pretraining to achieve this level 
                                    of performance, and possibly cognitively implausible data augmentation techniques to support task-specif㘶c 
                                           6,7. Unlike people, these models are both data- and resource-intensive, suggesting that they operate 
                                    inference
                                    under fundamentally dif㘶erent mechanisms to solve ARC problems. Additionally, the ARC Prize Foundation 
                                    1                                    2
                                                                                                            ✉
                                    Department of Psychology, nYU, new York, USA.  center for Data Science, nYU, new York, USA.  e-mail: solim.legris@ 
                                    nyu.edu
          Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                     1
            www.nature.com/scientificdata/                                                                       www.nature.com/scientificdata
                                            Fig. 1  ARC Demonstration Tasks. Examples of easy (nearly everyone solved them in two attempts or fewer) 
                                            and hard (few solved them in three attempts or fewer) tasks, with corresponding training and test examples. 
                                            Below the tasks are state space graphs representing all visited grid states by participants, from starting state 
                                            (blue nodes) to correct or incorrect submitted grid (green and red nodes respectively). From lef㘶 to right: 
                                            f76d97a5.json, e9ac8c9e.json, e3497940.json and dd2401ed.json.
                                            released a second, improved version of the ARC benchmark (ARC-AGI-2) on which they report similar levels 
                                            of human accuracy to those reported here8, while the top-performing AI models from the f㘶rst competition all 
                                            achieve under 10% accuracy. For these reasons, comprehensive benchmarking of humans on ARC tasks remains 
                                            an important objective for understanding the underlying mechanisms of human reasoning, but also for gaining 
                                            insights about human intelligence that could lead to better, more e㘠陦cient and human-like AI systems.
                                               A previous attempt at benchmarking human performance on ARC found mean task accuracy for humans 
                                            to be approximately 83%, which was estimated empirically using a small, semi-randomly selected subset of 40 
                                                                       9. However, it is unclear how robust this estimate is because of its small sample size, 
                                            tasks from the training set
                                            and whether the estimate also applies to the evaluation set, which is believed to be much harder. In this work, we 
                                            introduce H-ARC (Human-ARC) which closes this gap by providing a robust estimate of human performance 
                                            on the full set of 800 publicly available ARC tasks. H-ARC is a publicly available repository of over 15,500 
                                            attempts on ARC tasks, with step-by-step action traces recorded from our user-interface and accompanying 
                                                                                                                                                     10,11
                                            natural-language solution descriptions (see Table 1). Although prior research using variants of ARC tasks     or 
                                            a modif㘶ed experimental setup12 have also released human behavioral data, to the best of our knowledge, there 
                                            was no comprehensive public dataset of human behavior at this scale on both the training and evaluation sets of 
                                            ARC prior to this work.
                                               H-ARC makes two key contributions. First, it provides a robust estimate of human performance which can 
                                                                                                                                                          -
                                            be used to benchmark AI algorithms. ARC represents a high-prof㘶le index of intelligence, and these data con
                                            tribute in important ways to the measurement of AI progress, especially when comparing to algorithms that 
                                            operate with more human-like resource-constraints. In addition, from the perspective of cognitive science, 
                                            H-ARC of㘶ers the potential to enrich our understanding of how people solve a range of analogical reasoning 
                                            problems. In particular, people’s error patterns in ARC are revealing of the underlying mechanisms that support 
                                                                                                                                                          -
                                            reasoning. Qualitative analyses of our dataset suggest that people are incorrect in systematic ways, of㘶en achiev
                                            ing partially correct responses or resorting to what appears to be surface-level statistics to approximate observed 
                                            patterns inferred from training examples (Figs. 5, 6). Additionally, the free-form, natural-language solution 
            Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                 2
              www.nature.com/scientificdata/                                                                                           www.nature.com/scientificdata
                                                      Metric                           Training Set    Evaluation Set
                                                      Number of tasks                  400             400
                                                      Total number of participants     783             946
                                                      Incomplete participants          94              242
                                                      Average participants per task    11.8            10.3
                                                      Average attempts to solution     1.3             1.4
                                                      Total attempts                   7,916           7,820
                                                      Unique number of visited states  127,146         208,214
                                                      Total action traces              241,697         344,569
                                                     Table 1.  Human ARC Descriptives. Here we report numerical values summarizing our behavioral dataset. 
                                                     “Total attempts” is the number of individual submissions across all tasks and participants, and “total action 
                                                     traces” is the number of individual actions recorded across all tasks/participants.
                                                     descriptions collected with each problem attempt expose how people use and create on-the-f㘶y abstractions to 
                                                     solve novel problems, af㘶ording the possibility of informative, natural-language analyses.
                                                     Methods
                                                     We collected human data on each of the 400 training tasks and 400 evaluation tasks from ARC in two separate 
                                                     phases (extending the subset of 40 training tasks previously collected and described in Johnson et al.9. Each 
                                                     task has 1–10 training examples and 1–3 test examples, with each example consisting of an input-output pair. 
                                                     Because only a few tasks had more than 1 test example (14 and 19 tasks in the training and evaluation sets 
                                                     respectively), we opted to evaluate humans using only the f㘶rst test example for each of these tasks. On average, 
                                                     11.8 participants completed each of the 400 training tasks, while 10.3 participants completed each of the 400 
                                                     evaluation tasks.
                                                     participants.  We recruited 783 participants (59.6% male, 37.8% female, 2.6% other) on the training set tasks 
                                                     and 946 participants (49.5% male, 48.0% female, 2.5% other) on the evaluation set tasks from Amazon Mechanical 
                                                     Turk using the CloudResearch (cloudresearch.com) platform to ensure high quality data13. Participants were 
                                                     between 18 and 78 years old (M = 40.4, SD = 10.8). T㔴ey were compensated $10 and were also given a bonus 
                                                     of $1 if they succeeded at a randomly selected task and its written solution description was judged adequate 
                                                     by the experimenters. Best judgement was used: if a description was at least one complete sentence and was 
                                                     relevant to the task, it was counted as adequate. T㔴e study was approved by the local institutional review board 
                                                     (NYU’s Committee on Activities Involving Human Subjects; IRB-FY2016-231). Participants were informed about 
                                                     the general purpose of the study, the kinds of content they would be shown, and that they would be required 
                                                     to interact with our interface using their computer. T㔴ey were informed about compensation amounts, the 
                                                     anonymization of their data, their right to withdraw at any time, and asked to consent before proceeding with the 
                                                     experiment.
                                                     Data collection.  We evaluated humans using the same evaluation procedure proposed in the original paper 
                                                                                          1
                                                     describing the ARC benchmark . In particular, human participants were allowed three attempts per task to gen-
                                                     erate a correct solution, and were only given minimal feedback on each attempt, with the interface labeling each 
                                                     submission as correct or incorrect af㘶er each attempt.
                                                     User Interface.     Participants were f㘶rst given instructions about the experiment and explanations about the 
                                                                                                                                          9
                                                     dif㘶erent aspects of the ARC user interface. As in previous experiments , the user interface closely mirrored the 
                                                                                                   1
                                                     original interface provided by Chollet  (see Fig. 2). T㔴e interface allowed participants to select dif㘶erent colors 
                                                     and either edit one cell at a time or multiple selected cells at once. More sophisticated tools allowed the partic-
                                                     ipant to copy and paste a selection from the test input to the test output grid, or use the f㘶ood f㘶ll tool to change 
                                                     the color of all neighboring cells of the same color to a new color. Participants could resize the grid height and 
                                                     width, as well as copy the full test input grid to the test output grid. A reset button allowed participants to revert 
                                                     the output grid back to the initial state, a 3  × 3 black grid. Finally, unlike in previous iterations of the interface, 
                                                     we added another tool allowing participants to undo actions and revert the state of the output grid to the pre-
                                                     vious state before the last action was taken. At any point in time, the participant could click the help button to 
                                                     display the full set of instructions.
                                                     Tutorial.    At the beginning of the experiment, participants were provided with animated instructions outlining 
                                                     the user interface with an example task, and then asked to solve the same task to familiarize themselves with the 
                                                     interface. A relatively simple task was given to participants for the tutorial, and they were required to generate 
                                                     the correct test output to proceed (see Fig. 2 for an example). For the training set experiment, we chose task 
                                                     21f83797.json from the evaluation set, whereas for the evaluation set experiment, we chose task e9af-
                                                     cf9a.json from the training set. Af㘶er the tutorial, participants were asked to answer several basic compre-
                                                     hension questions to make sure they understood the instructions. T㔴e experiment started immediately af㘶er 
                                                     successful completion of the quiz. If one of the questions was answered incorrectly, participants were told that 
                                                     one or more of the questions were incorrect. Participants were given unlimited attempts at the quiz, but could 
                                                     only continue to the main part of the experiment if they successfully answered each question.
              Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                                             3
               www.nature.com/scientificdata/                                                                                                     www.nature.com/scientificdata
                                                         Fig. 2  ARC Experiment Interface. Participants were given instructions about the dif㘶erent controls and 
                                                         layout of the interface, followed by a tutorial task. Shown here is a tutorial task chosen from the training 
                                                         set (e9afcf9a.json). T㔴e experimental platform we used is made available at exps.gureckislab.org/e/
                                                         assumption-fast-natural.
                                                         Procedure.  T㔴e experiment consisted of 5 ARC tasks which were randomly selected from either the set of 400 
                                                         training tasks or from the 400 evaluation tasks. To reduce the potential for attrition or dropouts, we reduced the 
                                                         amount of ARC tasks participants were required to solve from 10 to 5 tasks af㘶er collecting 241 out of 783 partic-
                                                         ipants from the f㘶rst phase of data collection on the training set. On average, participants completed the experi-
                                                         ment in 23 minutes and 1 second (SD = 13m 24s) for the training set, and 28 minutes and 51 seconds (SD=16m 
                                                         19s) for the evaluation set. T㔴ere was no time limit for completing a task. Participants who exceeded the total 
                                                         time limit of 90 minutes were dealt with manually by email, but were included in our dataset nonetheless. For 
                                                                                                                                                                                                        -
                                                         each task, participants were given three attempts. Af㘶er each attempt, feedback was given on whether the sub
                                                         mitted solution was correct or not. Participants were not allowed to resubmit a previously incorrect output grid, 
                                                         ensuring that each of their attempts would be unique. We implemented this feature af㘶er collecting data from the 
                                                         f㘶rst 340 participants in the training set experiment. Prior to that, we observed that approximately 8% of incor-
                                                         rect second and third submission attempts were the same as earlier submission attempts on the same task. If the 
                                                         participant failed to generate the solution af㘶er three attempts, they automatically proceeded onto the next task. 
                                                         We also collected natural-language descriptions of the inferred solutions by asking participants to write down 
                                                         their solution in words (English). T㔴is was f㘶rst done af㘶er submitting an initial attempt before any feedback 
                                                         was given. If the initial submission was incorrect, participants were asked to submit a second natural-language 
                                                         description, either af㘶er a subsequent correct submission or on their last (but still incorrect) submission.
                                                         Data Records
                                                                                                                                                                               14
                                                         T㔴e dataset is publicly available on an Open Science Framework (OSF) data repository  under a Creative 
                                                         Commons License (CC0 1.0 Universal), with this section being the primary source of information on the avail-
                                                         ability and content of the data being described. T㔴e dataset is organized in two main directories: data/ and 
                                                         survey/.
                                                         Data directory.  T㔴is directory contains three primary CSV f㘶les: 
                                                         •	   data.csv: T㔴e main dataset f㘶le, where each row represents a single action taken by a unique participant on 
                                                              a specif㘶c task and attempt. Actions refer to the use of tools and other relevant clicks within the user interface: 
                                                              edit a cell, copy-paste, select, f㘶ood f㘶ll, undo, submit, etc. Key columns include:
                                                             – hashed_id: Anonymized participant identif㘶er.
                                                             – task_name: Name of the task.
                                                             – attempt_number: Number of the current attempt.
                                                             – action_id: Number of the action taken for the current attempt.
                                                             – action: Action taken by the participant within the user-interface.
               Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                                                           4
               www.nature.com/scientificdata/                                                                                                     www.nature.com/scientificdata
                                                         Fig. 3  Predicted task success rate af㘶er three attempts. We report model-based estimates, inferred using a 
                                                         Bayesian IRT model which accounts for missing data. Tasks are ordered from lowest success rate to highest, 
                                                         showing the distribution of model-based estimates of task di㘠陦culty for the 400 tasks in the training and 
                                                         evaluation sets, respectively. Dotted lines show average accuracy across all tasks in either the training (blue) or 
                                                         evaluation (orange) set.
                                                             – test_output_grid: State of the output grid in string format.
                                                             – action_x: X-coordinate of the action.
                                                             – action_y: Y-coordinate of the action
                                                         •	   summary_data.csv: A summary f㘶le where each row represents a single attempt by a participant at a task. 
                                                              Important columns include:
                                                             – hashed_id, task_name, attempt_number.
                                                             – num_actions: Total actions taken for the current attempt.
                                                             – test_output_grid: Final submitted output grid in string format.
                                                             –  f㘶rst_written_solution and last_written_solution: Natural-language solutions provided 
                                                               by participants.
                                                         •	   incorrect_submissions.csv: Contains data on incorrect grid submissions. Columns include:
                                                             – task_name, test_output_grid, count.
                                                         Survey directory.  T㔴is directory includes three CSV f㘶les capturing participant feedback and demographics: 
                                                         •	   demographics.csv: Includes age, gender, race and education_level.
                                                         •	   feedback.csv: Contains a feedback column with textual feedback from participants.
                                                         •	   withdraw.csv: Documents participant withdrawals, including withdraw_reason and withdraw_
                                                              comment columns when given by the participant.
                                                         technical Validation
                                                         We present four checks to support the validity of the dataset we are releasing. Firstly, we conducted model-based 
                                                         analyses of performance that incorporate uncertainty about our measurements by accounting for the missing 
                                                         data. Additionally, because this data was collected to showcase the diverse patterns of behavior that could explain 
                                                         aspects of people’s thinking and creativity, we present checks on the errors that people make, attempt-by-attempt 
                                                         improvement and the hypotheses that people generate when thinking about ARC problems.
                                                         performance.  Here, we check the ability of participants to perform the ARC tasks and check the possible 
                                                         inf㘶uence of incomplete data.
                                                         Incomplete data.  Participant data collected online can be incomplete for many reasons: participants may f㘶nd 
                                                         the task too hard, have technical di㘠陦culties, f㘶nd the experiment uninteresting, misunderstand the instructions 
                                                         or even run out of time. In our experiments, a number of participants withdrew from the experiment af㘶er 
                                                         completing between 0 and 4 tasks, although most did not provide any particular reason for withdrawing. All 
                                                         withdrawal reasons, when provided by participants, are available in the released dataset. We found that 94 out 
                                                         of 783 participants’ data from the training set experiment are incomplete, while 242 out of 946 participants’ data 
                                                         from the evaluation set experiment are incomplete. Our results indicate that 7.5% and 13.3% of the training and 
                                                         evaluation set task data are missing, for a total of 10.3% missing task data. We obtained these values by comput-
                                                         ing the proportion of expected task data (number of participants  × number of tasks assigned) that was missing 
                                                         from our dataset.
               Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                                                           5
               www.nature.com/scientificdata/                                                                                              www.nature.com/scientificdata
                                                       Attempt       Set           Accuracy (avg / best %)    HDI
                                                       1             Training      54.6/96.8                  [53.3, 55.8]
                                                       1             Evaluation    49.2/95.8                  [47.9, 50.4]
                                                       2             Training      66.6/98.5                  [65.4, 67.8]
                                                       2             Evaluation    61.6/97.8                  [60.5, 62.8]
                                                       3             Training      70.5/98.8                  [69.3, 71.6]
                                                       3             Evaluation    65.7/98.8                  [64.6, 66.8]
                                                      Table 2.  Human ARC Performance Summary. Human average (avg) represents model-based human 
                                                      performance af㘶er one, two or three attempts, where performance is mean task accuracy across each respective 
                                                      set. Human best refers to the overall (empirical) proportion of tasks that any human participant successfully 
                                                      solved, across all tasks in either the training or evaluation set. We also report the 94% Highest Density Interval 
                                                      (HDI) from our Bayesian model estimates.
                                                      Fig. 4  Examples of learning from minimal feedback. In the lef㘶 column, we show the test input seen by 
                                                      participants for three dif㘶erent problems from the ARC training and evaluation sets. In the middle column, 
                                                      f㘶rst and second incorrect submissions from selected participants are shown for each problem. T㔴e last column 
                                                      corresponds to the f㘶nal, but correct submission. From top to bottom: e6721834.json, d931c21c.json 
                                                      and 3af2c5a8.json.
                                                      Model-based estimation.  To address the issue of missing data and to estimate performance more robustly, we 
                                                      f㘶t a statistical model predicting human performance (see Fig. 3). Specif㘶cally, we estimated mean task accuracy 
                                                      by modeling latent participant ability, task di㘠陦culty and feedback ef㘶ects through a hierarchical Bayesian item 
                                                                                          15                                             16
                                                      response theory (IRT) model . We adapt the standard Rasch model  to account for multiple attempts by includ-
                                                      ing an additional term and jointly f㘶tting all attempts using f㘶xed ability and item parameters across attempts. T㔴e 
                                                                                           th                   th            th
                                                      probability of success for the i  subject on the j  task and k  attempt was modeled as a logistic function 
                                                                                                         P     =            1           ,
                                                                                                          ij,,k           −−()αβ+γ
                                                                                                                              ij
                                                                                                                   1 + e             k                                                      (1)
                                                      where αi is the latent participant ability, βj is the latent item difficulty and γk is the latent feedback effect. 
                                                      Participant task attempt outcomes were distributed according to Oi,j,k ~ Bernouilli (Pi,j,k). MCMC was used to 
                                                                                                                                                              17
                                                      perform approximate Bayesian inference over the parameters (α, β, γ) using PyMC , with 4 chains and 10000 
               Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                                                  6
         www.nature.com/scientificdata/                                          www.nature.com/scientificdata
                                Fig. 5  Distribution of human errors for a8610ef7.json. In the lef㘶 column, we show 3 of 4 training 
                                examples (for illustrative purposes) and the test input seen by participants, as well as the true test output grid. In 
                                the right panel, we show a non-exhaustive selection of dif㘶erent incorrect submissions from participants in the 
                                H-ARC dataset that attempted this particular problem.
                                                        18                                   
                                                                                             R
                                samples using its NUTS sampler . Convergence was assessed using trace plots and   values, which were less 
                                than 1.01. T㔴e mean accuracy for each split was computed by averaging estimated probability of success across 
                                all participants and tasks (see Table 2), using sampled parameters during inference. For comparison, we also 
                                report empirical mean accuracy which is computed as the mean success rate across tasks on the training and 
                                evaluation splits, respectively. More details on model-based estimation can be found in the accompanying code 
                                repository.
                                performance on the training set.  According to the IRT estimate, the mean accuracy on the training set 
                                tasks is 70.5% (94% HDI [69.3%, 71.6%]). For comparison, the empirical mean task accuracy is 76.2% (SD = 
                                21.5%). We also report model estimates of mean task accuracy for participants’ f㘶rst and second attempts, 54.6% 
                                (94% HDI [53.3%, 55.8%]) and 66.6% (94% HDI [65.4%, 67.8%]), respectively. Participants solved ARC training 
                                tasks in 1.3 attempts on average, with the modal and median number of attempts being 1. Of the 400 training 
                                tasks, we f㘶nd 74 tasks (18.5% of the training set) for which all participant who attempted the task generated the 
                                correct solution within three submissions or fewer. Conversely, we also f㘶nd 5 tasks (1.3% of the training set) 
                                which no participants were able to solve correctly in three attempts or fewer. Note that since each problem is 
                                attempted by approximately 10 people, this result simply means that we did not f㘶nd anyone in a set of 10 that 
                                could solve the problem. T㔴is is not evidence that these problems are not in principle solvable by a person. Finally, 
                                we f㘶nd that 40.0% of participants solved all training set tasks they were presented and that 8.6% of participants 
                                solved none.
                                performance on the evaluation set.  According to the IRT estimate, the mean accuracy on the evalua-
                                tion set tasks is 65.7% (94% HDI [64.6%, 66.8%]). For comparison, the empirical mean task accuracy is 64.5%  
                                (SD = 22.5%). We also report model estimated mean task accuracy for participants’ f㘶rst and second attempts, 
                                49.2% (94% HDI [47.9%, 50.4%]) and 61.6% (94% HDI [60.5%, 62.8%]), respectively. On average, participants 
                                solve ARC evaluation tasks in 1.4 attempts, with the modal and median number of attempts being 1. Of the 400 
         Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                         7
            www.nature.com/scientificdata/                                                                          www.nature.com/scientificdata
                                             Fig. 6  Distribution of human errors for e40b9e2f.json. In the lef㘶 column, we show the training examples 
                                             and test input seen by participants, as well as the true test output grid. In the right panel, we show a non-
                                             exhaustive selection of dif㘶erent incorrect submissions from participants in the H-ARC dataset that attempted 
                                             this particular problem.
                                             evaluation tasks, we f㘶nd 22 tasks (5.5% of the evaluation set) for which participants always found the correct 
                                             solution. Conversely, we f㘶nd 5 tasks (1.3% of the evaluation set) which no participants were able to solve. We also 
                                             f㘶nd that 33.8% of participants solved all evaluation set tasks they tried and that 16.7% solved none.
                                             Self-correction through feedback.  In the second technical validation, we inspect to what extent people 
                                             are making use of the minimal feedback to improve their solution. T㔴e IRT model estimates that for each addi-
                                             tional attempt, there is a substantial increase in the probability of success on a given task. Specif㘶cally, we f㘶nd that 
                                             for an average participant on an average task, success increases by 27.7% (94% HDI: [26.1%, 29.3%]) on a second 
                                             attempt and by 34.4% (94% HDI: [33.1%, 35.6%]) on a third attempt. People will of㘶en make initially wrong 
                                             guesses, but they are capable of self-correction and can f㘶exibly consider alternative solutions. For example, in the 
                                             third row of Fig. 4, the participant f㘶rst infers that the rule is to simply copy the input grid into each quadrant of 
                                             an 8 × 8 grid. Next, the participant makes the correct inference that the test outputs have some kind of symme-
                                             try, which they correctly guess to be mirroring along each axis. T㔴e idea is right, but the execution is incorrect. 
                                             Finally, they correct the minor mistakes in the top and bottom right quadrants.
                                             Incorrect responses.  In the third technical validation, we examine whether there is structure to the incor-
                                             rect responses that people produced. First, qualitative inspection suggests that people’s errors are not random, 
                                             but systematic and problem-type dependent (see Figs. 5, 6). For instance, although people make a non-negligible 
                                             amount of height and width errors, within the set of incorrect submissions, we f㘶nd that 68.2% and 73.5% of 
                                             submission attempts have both the correct height and width in training and evaluation set tasks respectively. In 
                                             general, we observe that people of㘶en successfully apply crucial transformations, such as f㘶nding the right grid 
                                             dimensions or f㘶lling in cells with the relevant colors, while missing some steps to a complete solution. T㔴ese par-
                                             tial solutions demonstrate failure modes of the human cognitive system that could inform potential mechanistic 
                                             hypotheses. For example, many of the errors illustrated in Figs. 5, 6 suggest conceptual errors. In both cases, the 
                                             incorrect outputs are of㘶en visually close to the true test output, suggesting approximations of the ground truth 
            Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                     8
            www.nature.com/scientificdata/                                                                          www.nature.com/scientificdata
                                             Fig. 7  Human action traces on ARC problems. In the lef㘶 column, we show the test input seen by participants, 
                                             and the true test output grid for three dif㘶erent problems from the ARC evaluation set. In the middle column, 
                                             action traces show some successive states of the grid of a selected participant, with the last state corresponding 
                                             to a correct (green box) or incorrect (red box) submission. In the last column, we show the f㘶rst natural-
                                             language descriptions submitted by participants along with their solution. From top to bottom: 34b99a2b.
                                             json, 4364c1c4.json and a8610ef7.json.
                                             rule. In broad strokes, the solution for problem a8610ef7.json (see lef㘶 panel of Fig. 5) requires copying the 
                                             input grid and then coloring all blue cells gray or red. T㔴e condition for deciding which color to paint a blue cell 
                                             depends on evaluating whether it is symmetric along the horizontal axis of the grid: if both (mirrored) corre-
                                             sponding cells in the top and bottom halves of the grid are blue, the cells should be colored red, otherwise the blue 
                                             cell should be colored gray. In light of this solution, the output of many participants shown in Fig. 5 appear like 
                                             they are the result of inferring partially correct programs, where the condition for coloring cells red is approxi-
                                             mated using surface-level statistics.
                                             Natural language descriptions.  In our f㘶nal technical validation, we check the natural-language descrip-
                                             tions that the participants generated when solving ARC problems (see Fig. 7 for examples). To validate the infor-
                                             mativeness of these descriptions, we focused on evaluating the last submitted solution descriptions of correct 
                                             attempts only. Af㘶er f㘶ltering uninformative text (fewer than 3 words), and rare tasks (fewer than 4 successful 
                                             descriptions), we obtained a total of 5940 natural-language descriptions across 691 tasks. We trained a Bernoulli 
                                             Naive Bayes classif㘶er using bag-of-words features to predict which specif㘶c ARC task a participant was solving 
                                             from their textual description alone. Using 5-fold cross-validation for hyperparameter optimization, we per-
                                             formed a grid search over both vocabulary size v ∈ {100, 250, 500, 1000, 2000, 2777} and Laplace smoothing 
                                             parameters α ∈ {0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0}, f㘶nding optimal values of v = 1000 features and α = 0.01. On a 
                                             held-out test set (70/30 split), the classif㘶er achieved 23.7% accuracy—signif㘶cantly above the empirical null dis-
                                             tribution of 0.2% (±0.1%), with statistical signif㘶cance conf㘶rmed by permutation testing (p  < 0.0001, n = 1000 
                                             permutations). T㔴is demonstrates that participants’ verbal explanations contain meaningful task-specif㘶c infor-
                                             mation, with the most frequent terms including color descriptors (blue, red, green), spatial language (squares, 
                                             grid, pattern, shape), and directional concepts (lef㘶, right), conf㘶rming that participants of㘶en used visuospatial 
                                             concepts to articulate their reasoning about abstract transformation rules. Furthermore, qualitative inspection 
                                             reveals that the natural-language descriptions in H-ARC contain words like “f㘶ll”, “extend”, “move”, “slide” or even 
                                             “water” or “f㘶ower”. All these words capture concepts from everyday life that people used to reason about these 
                                             novel problems. At a surface-level, these concepts seem unrelated to a task that requires inferring a hidden trans-
                                             formation rule and applying it to 2D grids of numbers between 0 and 9. Reminiscent of analogical reasoning, the 
                                             use of these concepts is suggestive of people’s ability to come up with useful abstractions on-the-f㘶y that greatly 
                                             restrict their search space when solving ARC problems.
                                             Code availability
                                             T㔴e code for technical validation is available on our accompanying code repo at github.com/le-gris/h-arc. T㔴e 
                                             code is written in Python, using standard packages such as PyMC, NumPy and SciPy.
                                             Received: 23 January 2025; Accepted: 24 July 2025;
                                             Published: 7 August 2025
                                             References
                                               1.  Chollet, F. On the measure of intelligence. arXiv preprint arXiv:1911.01547, https://doi.org/10.48550/arXiv.1911.01547 (2019).
                                               2.  Achiam, J. et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, https://doi.org/10.48550/arXiv.2303.08774 (2023).
                                               3.  Wei, J. et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, https://doi.org/10.48550/arXiv.2206.07682 
                                                 (2022).
            Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                     9
        www.nature.com/scientificdata/                                www.nature.com/scientificdata
                             4.  Kaplan, J. et al. Scaling laws for neural language models (2020).
                             5. Chollet, F., Knoop, M., Kamradt, G. & Landers, B. ARC prize 2024: Technical report. arXiv [cs.AI], https://doi.org/10.48550/
                              arXiv.2412.04604 (2024).
                             6.  Li, W.-D. et al. Combining induction and transduction for abstract reasoning. arXiv [cs.LG], https://doi.org/10.48550/
                              arXiv.2411.02272 (2024).
                             7.  Akyürek, E. et al. T㔴e surprising ef㘶ectiveness of test-time training for abstract reasoning. arXiv [cs.AI], https://doi.org/10.48550/
                              arXiv.2411.07279 (2024).
                             8.  Chollet, F., Knoop, M., Kamradt, G., Landers, B., & Pinkard, H. Arc-agi-2: A new challenge for frontier ai reasoning systems. arXiv 
                              [cs.AI], https://doi.org/10.48550/arXiv.2505.11831 (2025).
                             9.  Johnson, A., Vong, W. K. Lake, B., & Gureckis, T. M. Fast and f㘶exible: Human program induction in abstract reasoning tasks. In 
                              Proceedings of the Annual Meeting of the Cognitive Science Society, 43, https://doi.org/10.48550/arXiv.2103.05823 (2021).
                            10.   Moskvichev, A. K. Odouard, V. V. & Mitchell, M. T㔴e conceptARC benchmark: Evaluating understanding and generalization in the 
                              ARC domain. Transactions on Machine Learning Research, ISSN 2835-8856 https://openreview.net/forum?id=8ykyGbtt2q (2023).
                            11. Mitchell, M. Palmarini, A. B. & Moskvichev, A. Comparing humans, gpt-4, and gpt-4v on abstraction and reasoning tasks. 
                              Proceedings of the LLM-CP Workshop, AAAI 2024, https://doi.org/10.48550/arXiv.2311.09247 (2024).
                            12.  Acquaviva,  S.  et al. Communicating natural programs to humans and machines. Advances in Neural Information Processing Systems 
                              35, 3731–3743, https://doi.org/10.48550/arXiv.2106.07824 (2022).
                            13.  Hauser, D. J. et al. Evaluating cloudresearch’s approved group as a solution for problematic data quality on mturk. Behavior Research 
                              Methods 55(8), 3953–3964, https://doi.org/10.3758/s13428-022-01999-x (2023).
                            14.  LeGris, S., Vong, W. K., Lake, B. & Gureckis, T. H-arc, https://doi.org/10.17605/OSF.IO/BH8YQ (2025).
                            15.  Fox, J. P. Bayesian item response modeling: T㔴eory and applications. Statistics for Social and Behavioral Sciences. Springer, New York, 
                              NY, 2010 edition, ISBN 9781441907417,9781441907424. https://doi.org/10.1007/978-1-4419-0742-4 (2010).
                            16.  Rasch, G. Studies in mathematical psychology: I. probabilistic models for some intelligence and attainment tests. xiii, 184 (1960).
                            17.  Abril-Pla,  O.  et al. PyMC: a modern, and comprehensive probabilistic programming framework in python. PeerJ. Computer science 
                              9, e1516, https://doi.org/10.7717/peerj-cs.1516 (2023).
                            18.  Hof㘶man, M. D. et al. T㔴e no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. J. Mach. Learn. Res. 15(1), 
                              1593–1623 (2014).
                           acknowledgements
                           We thank Aysja Johnson for collecting the data from the f㘶rst few hundred participants on the training set and 
                           Nicholas Emery Kirsch for funding the data collection and personnel ef㘶orts on the evaluation set. T㔴is work was 
                           also generously supported by NSF BCS grant 2121102 to T.M.G.
                           author contributions
                           S.L. conducted the experiments, analyzed the data and wrote the manuscript. W.K.V., B.M.L. and T.M.G. provided 
                           project guidance. All authors contributed to reviewing and writing the manuscript.
                           Competing interests
                           T㔴e authors declare no competing interests.
                           additional information
                           Correspondence and requests for materials should be addressed to S.L.
                           Reprints and permissions information is available at www.nature.com/reprints.
                           Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and 
                           institutional a㘠陦liations.
                                   Open Access T㔴is article is licensed under a Creative Commons Attribution-NonCommercial- 
                                   NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribu-
                           tion and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) 
                           and the source, provide a link to the Creative Commons licence, and indicate if you modif㘶ed the licensed mate-
                           rial. You do not have permission under this licence to share adapted material derived from this article or parts of it. 
                           T㔴e images or other third party material in this article are included in the article’s Creative Commons licence, 
                           unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative  
                           Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted  
                           use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit 
                           http://creativecommons.org/licenses/by-nc-nd/4.0/.
                            
                           © T㔴e Author(s) 2025, corrected publication 2025
        Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1          10
