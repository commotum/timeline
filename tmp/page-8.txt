                                  Table 3: Panoptic segmentation on COCO panoptic val with 133 categories. MaskFormer seam-
                                   lessly uniﬁes semantic- and instance-level segmentation without modifying the model architecture
                                   or loss. Our model, which achieves better results, can be regarded as a box-free simpliﬁcation of
                                   DETR[3]. The major improvement comes from “stuff” classes (PQSt) which are ambiguous to
                                   represent with bounding boxes. For MaskFormer (DETR) we use the exact same post-processing
                                   as DETR. Note, that in this setting MaskFormer performance is still better than DETR (+2.2 PQ).
                                   Ourmodelalsooutperforms recently proposed Max-DeepLab [38] without the need of sophisticated
                                   auxiliary losses, while being more efﬁcient. FLOPs are computed as the average FLOPs over 100
                                  validation images (COCO images have varying sizes). Frames-per-second (fps) is measured on a
                                  V100 GPU with a batch size of 1 by taking the average runtime on the entire val set including
                                   post-processing time. Backbones pre-trained on ImageNet-22K are marked with †.
                                        method                  backbone       PQ         PQTh          PQSt        SQ       RQ     #params.   FLOPs      fps
                                        DETR[3]                R50+6Enc       43.4     48.2          36.3           79.3    53.8        -         -        -
                                        MaskFormer(DETR)       R50+6Enc       45.6     50.0 (+1.8)   39.0 (+2.7)    80.2    55.8        -         -        -
                                      backbonesMaskFormer(ours)R50+6Enc       46.5     51.0 (+2.8)   39.8 (+3.5)    80.4    56.8      45M       181G     17.6
                                        DETR[3]               R101+6Enc       45.1     50.5          37.0           79.9    55.5        -         -        -
                                      CNNMaskFormer(ours)     R101+6Enc       47.6     52.5 (+2.0)   40.3 (+3.3)    80.7    58.0      64M       248G     14.0
                                        Max-DeepLab[38]          Max-S        48.4     53.0          41.5            -        -       62M       324G      7.6
                                                                 Max-L        51.1     57.0          42.2            -        -      451M      3692G       -
                                      backbones                 Swin-T        47.7     51.7          41.7           80.4    58.3      42M       179G     17.0
                                                                Swin-S        49.7     54.4          42.6           80.9    60.4      63M       259G     12.4
                                        MaskFormer(ours)        Swin-B        51.1     56.3          43.2           81.4    61.8     102M       411G      8.4
                                                                Swin-B†       51.8     56.9          44.1           81.4    62.6     102M       411G      8.4
                                      ransformer                       †
                                      T                         Swin-L        52.7     58.5          44.0           81.8    63.5     212M       792G      5.2
                                   Panoptic segmentation. In Table 3, we compare the same exact MaskFormer model with DETR [3]
                                   ontheCOCOpanopticvalset. TomatchthestandardDETRdesign,weadd6additionalTransformer
                                   encoder layers after the CNN backbone. Unlike DETR, our model does not predict bounding boxes
                                   but instead predicts masks directly. MaskFormer achieves better results while being simpler than
                                   DETR.Todisentangle the improvements from the model itself and our post-processing inference
                                   strategy we run our model following DETR post-processing (MaskFormer (DETR)) and observe that
                                   this setup outperforms DETR by 2.2 PQ. Overall, we observe a larger improvement in PQSt compared
                                   to PQTh. This suggests that detecting “stuff” with bounding boxes is suboptimal, and therefore, box-
                                   basedsegmentationmodels(e.g., MaskR-CNN[19])donotsuitsemanticsegmentation. MaskFormer
                                   also outperforms recently proposed Max-DeepLab [38] without the need of special network design
                                   as well as sophisticated auxiliary losses (i.e., instance discrimination loss, mask-ID cross entropy
                                   loss, and per-pixel classiﬁcation loss in [38]). MaskFormer, for the ﬁrst time, uniﬁes semantic- and
                                   instance-level segmentation with the exact same model, loss, and training pipeline.
                                  Wefurther evaluate our model on the panoptic segmentation version of the ADE20K dataset. Our
                                   model also achieves state-of-the-art performance. We refer to the appendix for detailed results.
                                   4.4    Ablation studies
                                  Weperformaseries of ablation studies of MaskFormer using a single ResNet-50 backbone [20].
                                   Per-pixel vs. mask classiﬁcation. In Table 4, we verify that the gains demonstrated by MaskFromer
                                   comefromshifting the paradigm to mask classiﬁcation. We start by comparing PerPixelBaseline+
                                   and MaskFormer. The models are very similar and there are only 3 differences: 1) per-pixel vs.
                                   mask classiﬁcation used by the models, 2) MaskFormer uses bipartite matching, and 3) the new
                                   model uses a combination of focal and dice losses as a mask loss, whereas PerPixelBaseline+
                                   utilizes per-pixel cross entropy loss. First, we rule out the inﬂuence of loss differences by training
                                   PerPixelBaseline+ with exactly the same losses and observing no improvement. Next, in Table 4a, we
                                   comparePerPixelBaseline+withMaskFormertrainedusingaﬁxedmatching(MaskFormer-ﬁxed),i.e.,
                                   N=Kandassignmentdonebasedoncategorylabelindicesidenticallytotheper-pixelclassiﬁcation
                                   setup. We observe that MaskFormer-ﬁxed is 1.8 mIoU better than the baseline, suggesting that
                                   shifting from per-pixel classiﬁcation to mask classiﬁcation is indeed the main reason for the gains of
                                   MaskFormer. In Table 4b, we further compare MaskFormer-ﬁxed with MaskFormer trained with
                                   bipartite matching (MaskFormer-bipartite) and ﬁnd bipartite matching is not only more ﬂexible
                                  (allowing to predict less masks than the total number of categories) but also produces better results.
                                                                                                8
