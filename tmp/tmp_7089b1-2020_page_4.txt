           sleep (10,11). In general, wake-sleep Bayesian learning (12) iterates between training a probabilistic
           generative model that deﬁnes the learner’s prior alongside a neural network recognition model that
           learns to invert this generative model given new data. During “waking” the generative model is used to
           interpret new data, guided by the recognition model. The recognition model is learned ofﬂine during
           “sleep,” from imagined data sets (“dreams” or “fantasies”) sampled from the generative model.
              DreamCoderdevelopsthewake-sleepapproachforlearningtolearnprograms: Itslearnedlanguage
           deﬁnes a generative model over programs and tasks, where each program solves a particular hypotheti-
           cal task; its neural network learns to recognize patterns across tasks in order to best predict program
           components likely to solve any given new task. During waking, the system is presented with data
           from several tasks and attempts to synthesize programs that solve then, using the neural recognition
           modeltoproposecandidateprograms. Learningoccursduringtwodistinct but interleaved sleep phases,
           alternately growing the learned language (generative model) by consolidating new abstractions from
           programs found during waking, and training the neural network (recognition model) on “fantasy”
           programs sampled from the generative model. This wake-sleep architecture builds on and further
           integrates a pair of ideas, Bayesian multitask program learning (5,13,14) and neurally-guided program
           synthesis (15,16), which have been separately inﬂuential in the recent literature but have only been
                                                   2
           brought together in our work starting with the EC algorithm (17), and now made much more scalable
           in DreamCoder (see S3 for further discussion of prior work).
              Theresulting system has wide applicability. We describe applications to eight domains (Fig. 1A):
           classic program synthesis challenges, more creative visual drawing and building problems, and
           ﬁnally, library learning that captures the basic languages of recursive programming, vector algebra,
           and physics. All of our tasks involve inducing programs from very minimal data, e.g., ﬁve to ten
           examples of a new concept or function, or a single image or scene depicting a new object. The learned
           languages span deterministic and probabilistic programs, and programs that act both generatively (e.g.,
           producing an artifact like an image or plan) and conditionally (e.g., mapping inputs to outputs). Taken
           together, we hope these applications illustrate the potential for program induction to become a practical,
           general-purpose, and data-efﬁcient approach to building intepretable, reusable knowledge in artiﬁcial
           intelligence systems.
           Wake/Sleep ProgramLearning
           WenowdescribethespeciﬁcsoflearninginDreamCoder,beginningwithanoverviewofthealgorithm
           and its mathematical formulation, then turning to the details of its three phases. Learning proceeds
           iteratively, with each iteration (Eq. 1, Fig. 2) cycling through a wake phase of trying to solve tasks
           interleaved with two sleep phases for learning to solve new tasks. In the wake phase (Fig. 2 top),
           the system searches for programs that solve tasks drawn from a training set, guided by the neural
           recognition model which ranks candidate programs based on the observed data for each task. Candidate
           programs are scored according to how well they solve the presented tasks, and how plausible they
           are a priori under the learned generative model for programs. The ﬁrst sleep phase, which we refer
           to as abstraction (Fig. 2 left), grows the library of programming primitives (the generative model)
           by replaying experiences from waking, ﬁnding common program fragments from task solutions, and
           abstracting out these fragments into new code primitives. This mechanism increases the breadth and
           depth of the learner’s declarative knowledge, its learned library as in Fig. 1B or Fig. 7, when viewed
           as a network. The second sleep phase, which we refer to as dreaming (Fig. 2 right), improves the
           learner’s procedural skill in code-writing by training the neural network that helps search for programs.
                                                   4
