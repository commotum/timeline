                     Problem: An emirp is an integer that, when its digits are written in reverse order, is a prime number. For example, 73 is
                     an emirp because 37 is prime. What is the smallest two-digit prime number that is NOT an emirp?
                     Solution: ...
                     Step7: Continue to the next smallest two-digit prime number. 19 is prime.
                     Step 8: Check if 23 is an emirp. Reverse the digits of 23: 32. Check if 32 is prime: No, 32 is not prime (divisible by 2).
                     Since 23 is a prime number and its reverse, 32, is not a prime number, 23 is not an emirp. ...
                     Qwen2.5-Math-7B-PRM800Kscore: 0.859
                     R-PRM:Analysis: Previous Steps Analysis: ...
                     . . . . . .
                     Calculation Analysis: ......Upon reviewing, the mistake in the analysis is assuming 23 is the answer without
                     properly checking if there are smaller primes that are not emirps. ......Given the oversight in the analysis
                     regarding the thorough checking of primes and their reversals, the conclusion that 23 is the smallest such prime
                     without properly validating against all primes in sequence is premature. ......
                     Conclusion: The Now Step incorrectly concludes that 23 is the smallest two-digit prime number ......
                     Verification: Is the step correct (Yes/No)? No
                     R-PRM(MajorityVoting)score: 0.055
                  Figure 6: A case study from ProcessBench MATH dataset. Red text denotes the error step, and the blue text
                  highlights our model’s critique of the error.
                  6 Conclusion                                             Carlo Tree Search (MCTS) and Beam Search re-
                  In this paper, we present Reasoning-Driven Pro-          main underexplored, although they could poten-
                  cess Reward Modeling (R-PRM), a novel frame-             tially better leverage the characteristics of PRM
                  workthat advances the process reward modeling of         and yield improved generation results.
                  mathematical reasoning. Our framework consists           Acknowledgements
                  of three components. First, we leverage stronger        We would like to thank the anonymous review-
                  LLMstoconstructseeddata,enablingourmodelto               ers for their insightful comments. Shujian Huang
                  perform a comprehensive evaluation process. Sec-         is the corresponding author. This work is sup-
                  ond,weusepreferenceoptimizationtoenhanceper-             ported by National Science Foundation of China
                  formance without requiring additional annotated          (No. 62376116, 62176120), research project of
                  data. Third, we introduce inference-time scaling         Nanjing University-China Mobile Joint Institute
                  to fully harness the model’s reasoning capabili-         (NJ20250038), the Fundamental Research Funds
                  ties. Extensive experiments demonstrate that our         for the Central Universities (No.      2024300507,
                  methodachieves significant performance improve-          2025300390), and the Postgraduate Research &
                  mentsonProcessBenchandPRMBench,whilealso                 Practice Innovation Program of Jiangsu Province
                  effectively guiding LLM reasoning. Further anal-         (No. KYCX25_0315andNo. SJCX25_0031).
                  ysis shows that R-PRM exhibits more comprehen-
                  sive, robust, and generalizable evaluation capabili-
                  ties, as its performance continues to improve with       References
                  increased inference, highlighting its substantial po-    Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
                  tential.                                                   Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
                  Limitations                                                Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
                                                                             Nakano, Christopher Hesse, and John Schulman.
                                                                             2021. Training verifiers to solve math word prob-
                  Duetocomputationalresourceconstraints,wehave               lems. Preprint, arXiv:2110.14168.
                  not yet verified our approach on larger models such      DeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-
                  as 70B, despite extensive experiments demonstrat-          soning capability in llms via reinforcement learning.
                  ing its effectiveness on 7B models. We hypothesize         Preprint, arXiv:2501.12948.
                  that larger models, given their enhanced reasoning       Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang,
                  capabilities, could achieve higher modeling accu-          HanZhao,YingboZhou,NanJiang, Doyen Sahoo,
                  racy when combined with our methodology. Ad-               CaimingXiong,andTongZhang.2024. Rlhfwork-
                  ditionally, while we have tested popular inference         flow: From reward modeling to online rlhf. Preprint,
                  strategies like Best-of-N and Guided Search, our           arXiv:2405.07863.
                  exploration of advanced search algorithms remains        ChaoqunHe,RenjieLuo,YuzhuoBai,ShengdingHu,
                  limited.  Sophisticated methods such as Monte              Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han,
                                                                      13458
