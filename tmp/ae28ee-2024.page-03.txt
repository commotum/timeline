                         Published as a conference paper at ICLR 2024
                         Through these stages of filtering, the original 90,000 PRs are filtered down to the 2,294 task in-
                         stances which comprise SWE-bench. A final breakdown of these task instances across repositories
                         is presented in Figure 3, and Table 1 highlights the key features of SWE-bench task instances. We
                         highlight that the codebases are large with thousands of files, and the reference pull requests often
                         make changes to multiple files at once. Technical details about SWE-bench’s construction pipeline
                         are discussed in Appendix A. Additional dataset statistics are in Appendix A.5.
                         2.2  TASK FORMULATION
                         Model input. A model is given an issue text description and a complete codebase. The model is
                         then tasked to make an edit to the codebase to resolve the issue. In practice, we represent edits as
                         patch files, which specify which lines in the codebase to modify in order to resolve the issue.
                         Evaluation metrics. To evaluate a proposed solution, we apply the generated patch, using unix’s
                         patch program, to the codebase and then execute the unit and system tests associated with the
                         task instance. If the patch applies successfully and all of these tests pass we consider the proposed
                         solution to have successfully resolved the issue. The metric for our benchmark is the percentage of
                         task instances that are resolved. Additional technical details in Appendix A.4.
                         2.3  FEATURES OF SWE-BENCH
                         Traditional benchmarksinNLPtypicallyinvolveonlyshortinputandoutputsequencesandconsider
                         somewhat “contrived” problems created specifically for the benchmark. In contrast, SWE-bench’s
                         realistic construction setting imbues the dataset with unique properties, which we discuss below.
                         Real-world software engineering tasks. Since each task instance in SWE-bench consists of a
                         large and complex codebase and a description of a relevant issue, solving SWE-bench requires
                         demonstrating sophisticated skills and knowledge possessed by experienced software engineers but
                         are not commonly evaluated in traditional code generation benchmarks.
                         Continually updatable. Our collection process can be easily applied to any Python repository on
                         GitHub and requires minimal human intervention. Therefore, we can extend SWE-bench with a
                         continual supply of new task instances and evaluate LMs on issues created after their training date,
                         which ensures that the solution was not included in their training corpus.
                         Diverse long inputs. Issue descriptions are typically long and detailed (195 words on average), and
                         codebases regularly contain many thousands of files. Solving SWE-bench requires identifying the
                         relatively small number of lines that need to be edited to solve an issue amongst a sea of context.
                         Robust evaluation. For each task instance, there is at least one fail-to-pass test which was used
                         to test the reference solution, and 40% of instances have at least two fail-to-pass tests. These tests
                         evaluatewhetherthemodeladdressedtheproblemintheissue. Inaddition,amedianof51additional
                         tests run to check whether prior functionality is properly maintained.
                         Cross-context code editing. Unlike prior settings that may constrain edit scope to an individ-
                         ual function or class (e.g., Chen et al., 2021; Cassano et al., 2022) or provide cloze-style fill-in
                         blanks (e.g., Lu et al., 2021; Fried et al., 2023), SWE-bench does not provide such explicit guid-
                         ance. Rather than merely having to produce a short code snippet, our benchmark challenges models
                         to generate revisions in multiple locations of a large codebase. SWE-bench’s reference solutions
                         average editing 1.7 files, 3.0 functions, and 32.8 lines (added or removed).
                         Wide scope for possible solutions. The task of repository-scale code editing can serve as a level
                         playing field to compare approaches ranging from retrieval and long-context models to decision-
                         making agents, which could reason and act in code. SWE-bench also allows creative freedom, as
                         models can generate novel solutions that may deviate from the reference PR.
                         3   SWE-LLAMA: FINE-TUNING CODELLAMA FOR SWE-BENCH
                         It is important to benchmark the performance of open models on SWE-bench alongside proprietary
                                                                                   `
                         models. At the time of writing, only the CodeLlama models (Roziere et al., 2023) are able to handle
                         the very long contexts necessary. However, we observe that the off-the-shelf CodeLlama variants
                                                                    3
