                                                                                                                í µãŒµ   points within pillar                                                   across two modalities at the fine-grained level with negli-
                                                                                                                  !                                                                         gible extra cost.
                                        N points         x
                                                   y
                                          LiDAR Points                                                                      í µãŒµ                                                              3.2. Sensor-Time 4D Attention
                                                                                                                              í µãŒµ                                         í µãŒµ
                                                                                               í µãŒµ                                                                         "
                                          Homogeneous                                            !     í µãŒµ
                                        Transformation       G                                  P       "                  â€¦                                         P                             To model the mutual correlations of sequential point
                                                      u                                                                     í µãŒµ                                                              clouds and camera features, our key motivation is to exploit
                                               v                                                                               í µãŒµ                                        í µãŒµ                 the self-attention mechanism in Transformer to aggregate
                                                                                               í µãŒµ                                                                         #                 complementary information. The classic transformer archi-
                                                                                                 !     í µãŒµ                                                            P
                                     Camera Feature Map                                         P       #                                          Grid Feature                             tecture [30] takes a sequence as input consisting of discrete
                                 Camera Feature Fetching                       Pillar Extraction              Point-wise Attention                   Feature Scatter                        tokens, each represented by a feature vector. In this work,
                                                                                                                                                                                            the input sequence consists of sequential point cloud and
                               Figure 3. Pipeline of Grid Feature Encoder. We fetch the corre-                                                                                              image features. Formally, we assign the grid-wise features
                                                                                                                                                                                                                                               L            C T
                                                                                                                                                                                            from BEV maps {M ,M }                                                            as input tokens. To adapt
                               spondingcamerafeaturesforLiDARpointsandthencapturepillar                                                                                                                                                        t           t       i=1
                                                                                                                                                                                                                                                i            i
                               features for each modality respectively. Besides, point-wise atten-                                                                                          to 3D object detection, we present three critical designs on
                               tion is conducted between two modalities within stacked pillars.                                                                                             top of the classic transformer to model the information in-
                               Finally, the pillar features are scattered into 2D BEV grids.                                                                                                teraction across sensors and time, including Sparse Window
                                                                                                                                                                                            Partition, Pyramid Context, and 4D Positional Encoding.
                               tures. Second, the fetched image feature involves a specific                                                                                                 SparseWindowPartition. Althoughthenumberoftokens
                               range of receptive field in the image, which helps to allevi-                                                                                                has been sufficiently reduced via the grid feature encoder,
                               ate the projection biases between two modalities.                                                                                                            a small grid size usually results in a high-resolution map
                                                                                                                                                                                            for favorable performance. Directly computing the token-
                               Pillar Feature Extraction. The number of raw LiDAR                                                                                                           wise relations on the whole grip map is still not manage-
                               points is huge and directly computing point-wise relations                                                                                                   able. Thus, can we further reduce the network complex-
                               is a heavy load to bear. In contrast, the number of BEV                                                                                                      ity while maintaining the detection accuracy? Motivated by
                               grids is small. As such, we encode both point clouds and                                                                                                     the window partition mechanism [16], we constrain the lo-
                               camera images into the BEV maps separately. Though the                                                                                                       cal self-attention computation within partitioned windows,
                               projection from 3D points to the 2D space yields informa-                                                                                                    which largely reduces the number of input tokens. Com-
                               tion loss of the height dimension, such a loss hardly affects                                                                                                pared to 2D vision tasks that take pixels in images as input,
                               the intrinsic geometry of 3D objects in autonomous-driving                                                                                                   our BEV map in 3D vision is highly sparse, where the pro-
                               scenes. Finally, the point-wise correlations are translated                                                                                                  portion of blank areas without any points is much larger
                               to grid-wise correlations in the BEV. Also note that the im-                                                                                                 than that of non-blank areas. To leverage the sparsity, we
                               age feature extraction is independent of point cloud feature                                                                                                 drop out the windows that only contain blank areas to fur-
                               extraction, thus the modality differences are well preserved                                                                                                 ther alleviate the computational load. Let the window size
                                                                                                                                                                                            be Hw Ã—Ww,weobtainS[ H , W ] non-overlapping win-
                               for further processing.                                                                                                                                                                                                          Hw Ww
                                      In more detail, we follow PointPillars [12] to quantize                                                                                               dows, where S denote the selected sparse non-blank win-
                                                                                                                                                                                            dows. Given the input sequence F                                                                 âˆˆ RNFÃ—f, where
                               point clouds into P vertical pillars on fixed-size 2D grids.                                                                                                                                                                                           in
                                                                                                                                                                                            N =HwÃ—WwÃ—TÃ—misthetotalnumberoftokens,
                               Then we perform linear transformation and max-pooling                                                                                                             F
                               on each pillar as grid features, which are further scattered                                                                                                 T denotes the number of frames and m is the number of
                                                                                                      L                    HÃ—WÃ—f                                                            modalities. we use dot-product attention to model the mu-
                               into BEV representation M                                                      âˆˆ R                             L, where H                                    tual correlations among input tokens. We formally have:
                               and W denote the BEV map size and fL denotes the fea-
                               ture dimension. Similarly, we obtain the camera features                                                                                                                              Q=F M ,K=F M ,V =F M ,
                                                                                                                                                                                                                                     in       q                      in       k                      in       v
                               MCâˆˆRHÃ—WÃ—fC intheBEVaswell.                                                                                                                                                                                                                        QKT                                                (1)
                                                                                                                                                                                                                     A(Q,K,V)=softmax( âˆš )V,
                               Point-wise Attention. Inside each pillar, we propose to en-                                                                                                                                                                                               d
                               hance the pillar encoding via learning a fine-grained corre-                                                                                                 where Q,K,V are the query, key and value features ob-
                               lation among points. Namely, we use two separate learn-                                                                                                      tained by a linear transformation on the input sequence, and
                               able linear layers both with N                                                     outputs to learn weights                                                  M,M ,M âˆˆRfÃ—darethetransformationmatrix. Anon-
                                                                                                           P                                                                                     q          k          v
                               w âˆˆRNP andw âˆˆ RNP. Theweightsw andw is                                                                                                                       linear transformation is applied to the attention weights to
                                    L                                        C                                                                 L                    C
                               learnedfromthecombinationofpointcloudfeatureandim-                                                                                                           produce the output features:
                               agefeatureandfollowedbythesigmoidactivationfunction.                                                                                                                                                    F =MLP(A)+F .                                                                                (2)
                               Thentwoweightsareappliedtothepointcloudandtheim-                                                                                                                                                            out                                            in
                               age features over the N                                          points within the pillar, respec-
                                                                                          P                                                                                                 Therefore the grid features are aggregated over all tokens
                               tively.            This allows for dynamic information aggregation                                                                                           with learnable attention weights.
                                                                                                                                                                                   17175
