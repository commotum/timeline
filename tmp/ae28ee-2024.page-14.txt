           Published as a conference paper at ICLR 2024
           Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. Software
            testing with large language model: Survey, landscape, and vision, 2023.
           ChunqiuStevenXiaandLingmingZhang.Lesstraining,morerepairingplease: revisitingautomated
            program repair via zero-shot learning. In Proceedings of the 30th ACM Joint European Software
            Engineering Conference and Symposium on the Foundations of Software Engineering, pp. 959–
            971, 2022.
           Chunqiu Steven Xia and Lingming Zhang. Conversational automated program repair, 2023.
           ChunqiuStevenXia,MatteoPaltenghi,JiaLeTian,MichaelPradel,andLingmingZhang. Universal
            fuzzing via large language models. arXiv preprint arXiv:2308.04748, 2023.
           John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing
            and benchmarking interactive coding with execution feedback, 2023.
           Xin Yang, Raula Gaikovina Kula, Norihiro Yoshida, and Hajimu Iida. Mining the modern code
            review repositories: A dataset of people, process and product. In Proceedings of the 13th Inter-
            national Conference on Mining Software Repositories, pp. 460–463, 2016.
           Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable
            real-world web interaction with grounded language agents, 2022.
           Pengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua How-
            land, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, and Charles Sutton.
            Natural language to code generation in interactive data science notebooks, 2022.
           Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao
            Xie,andQianxiangWang. Codereval: Abenchmarkofpragmaticcodegenerationwithgenerative
            pre-trained models, 2023.
           Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene
            Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: A large-scale
            human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In
            Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp.
            3911–3921, Brussels, Belgium, October-November 2018. Association for Computational Lin-
            guistics. doi: 10.18653/v1/D18-1425. URL https://aclanthology.org/D18-1425.
           Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu
            Chen, and Jian-Guang Lou. Cert: Continual pre-training on sketches for library-oriented code
            generation, 2022.
           Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, and
            Jian-Guang Lou. Large language models meet nl2code: A survey, 2023.
           Jiyang Zhang, Sheena Panthaplackel, Pengyu Nie, Junyi Jessy Li, and Milos Gligoric. Coditt5:
            Pretraining for source code and natural language editing, 2022.
           Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and We-
            icheng Wang. Towards an understanding of large language models in software engineering tasks.
            arXiv preprint arXiv:2308.11396, 2023.
           Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,
            Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environ-
            mentfor building autonomous agents, 2023.
                               14
