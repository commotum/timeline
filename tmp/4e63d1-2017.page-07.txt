                             the substitution list of the resulting proof state. We can prove known facts trivially by a uniﬁcation
                             with themselves, resulting in no parameter updates during training and hence no generalization.
                             Therefore, during training we are masking the calculation of the uniﬁcation success of a known
                             ground atom that we want to prove. Speciﬁcally, we set the uniﬁcation score to 0 to temporarily hide
                             that training fact and assume it can be proven from other facts and rules in the KB.
                             4.2   Neural Link Prediction as Auxiliary Loss
                             At the beginning of training all subsymbolic representations are initialized randomly. When unifying
                             a goal with all facts in a KB we consequently get very noisy success scores in early stages of training.
                             Moreover, as only the maximum success score will result in gradient updates for the respective
                             subsymbolic representations along the maximum proof path, it can take a long time until NTPs learn
                             to place similar symbols close to each other in the vector space and to make effective use of rules.
                             To speed up learning subsymbolic representations, we train NTPs jointly with ComplEx [7] (Ap-
                             pendix B). ComplEx and the NTP share the same subsymbolic representations, which is feasible
                             as the RBF kernel in unify is also deﬁned for complex vectors. While the NTP is responsible for
                             multi-hop reasoning, the neural link prediction model learns to score ground atoms locally. At test
                             time, only the NTP is used for predictions. Thus, the training loss for ComplEx can be seen as an
                             auxiliary loss for the subsymbolic representations learned by the NTP. We term the resulting model
                             NTPλ. Based on the loss in Section 4.1, the joint training loss is deﬁned as
                             L       K = L      K +      X −ylog(complex (s,i,j))−(1−y)log(1−complex (s,i,j))
                               ntpλ        ntp                                      θ                                        θ
                                     θ          θ
                                                    ([s,i,j],y) ∈ T
                             where [s,i,j] is a training atom and y its ground truth target.
                             4.3   Computational Optimizations
                             NTPsasdescribedabovesuffer from severe computational limitations since the neural network is
                             representing all possible proofs up to some predeﬁned depth. In contrast to symbolic backward
                             chainingwhereaproofcanbeabortedassoonasuniﬁcationfails,indifferentiableprovingweonlyget
                             a uniﬁcation failure for atoms whose arity does not match or when we detect cyclic rule application.
                             Wepropose two optimizations to speed up NTPs in the Appendix. First, we make use of modern
                             GPUsbybatchprocessing many proofs in parallel (Appendix C). Second, we exploit the sparseness
                             of gradients caused by the min and max operations used in the uniﬁcation and proof aggregation
                             respectively to derive a heuristic for a truncated forward and backward pass that drastically reduces
                             the number of proofs that have to be considered for calculating gradients (Appendix D).
                             5    Experiments
                             Consistent with previous work, we carry out experiments on four benchmark KBs and compare
                             ComplExwiththeNTPandNTPλintermsofareaunderthePrecision-Recall-curve(AUC-PR)on
                             the Countries KB, and Mean Reciprocal Rank (MRR) and HITS@m [34] on the other KBs described
                             below. Training details, including hyperparameters and rule templates, can be found in Appendix E.
                             Countries     TheCountries KB is a dataset introduced by [35] for testing reasoning capabilities of
                             neural link prediction models. It consists of 244 countries, 5 regions (e.g. EUROPE), 23 subregions
                             (e.g. WESTERN EUROPE, NORTHERN AMERICA),and1158factsabouttheneighborhoodofcountries,
                             and the location of countries and subregions. We follow [36] and split countries randomly into a train-
                             ing set of 204 countries (train), a development set of 20 countries (dev), and a test set of 20 countries
                             (test), such that every dev and test country has at least one neighbor in the training set. Subsequently,
                             three different task datasets are created. For all tasks, the goal is to predict locatedIn(c,r) for every
                             test country c and all ﬁve regions r, but the access to training atoms in the KB varies.
                             S1:     All ground atoms locatedIn(c,r) where c is a test country and r is a re-
                             gion are removed from the KB. Since information about the subregion of test coun-
                             tries is still contained in the KB, this task can be solved by using the transitivity rule
                             locatedIn(X,Y):–locatedIn(X,Z),locatedIn(Z,Y).
                             S2: In addition to S1, all ground atoms locatedIn(c,s) are removed where c is a test country and s
                                                                                 7
