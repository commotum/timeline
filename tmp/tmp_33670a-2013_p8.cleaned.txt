                                               N  = 1000                           N  = 50000
                                   100          train                   125          train
                                   110                                  130
                                                                        135                                            Wake-Sleep (train)
                                   120                                                                                 Wake-Sleep (test)
                                                                        140                                            MCEM (train)
                                   130                                                                                 MCEM (test)
                                                                        145                                            AEVB (train)
                                   140                                                                                 AEVB (test)
                                                                        150
                                 Marginal log-likelihood150
                                                                        155
                                   160                                  160
                                      0    10   20   30   40   50   60     0   10   20  30   40  50   60
                                   # Training samples evaluated (millions)
                              Figure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the
                              estimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an
                              on-line algorithm, and (unlike AEVB and the wake-sleep method) can’t be applied efﬁciently for
                              the full MNIST dataset.
                              Visualisation of high-dimensional data          If we choose a low-dimensional latent space (e.g. 2D),
                              we can use the learned encoders (recognition model) to project high-dimensional data to a low-
                              dimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST
                              and Frey Face datasets.
                              6    Conclusion
                              We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB
                              (SGVB),forefﬁcientapproximateinference with continuous latent variables. The proposed estima-
                              tor can be straightforwardly differentiated and optimized using standard stochastic gradient meth-
                              ods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an
                              efﬁcient algorithm for efﬁcient inference and learning, Auto-Encoding VB (AEVB), that learns an
                              approximate inference model using the SGVB estimator. The theoretical advantages are reﬂected in
                              experimental results.
                              7    Future work
                              Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and
                              learning problem with continuous latent variables, there are plenty of future directions: (i) learning
                              hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used
                              for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic
                              Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models
                              with latent variables, useful for learning complicated noise distributions.
                                                                                   8
