                                        Auto-Encoding Variational Bayes
                                      Diederik P. Kingma                       MaxWelling
                                    Machine Learning Group                Machine Learning Group
                                   Universiteit van Amsterdam            Universiteit van Amsterdam
                                    dpkingma@gmail.com                  welling.max@gmail.com
                                                              Abstract
                                How can we perform efÔ¨Åcient inference and learning in directed probabilistic
                                models, in the presence of continuous latent variables with intractable posterior
                                distributions, and large datasets? We introduce a stochastic variational inference
                                and learning algorithm that scales to large datasets and, under some mild differ-
                                entiability conditions, even works in the intractable case. Our contributions are
                                two-fold. First, we show that a reparameterization of the variational lower bound
                                yields a lower bound estimator that can be straightforwardly optimized using stan-
                                dard stochastic gradient methods. Second, we show that for i.i.d. datasets with
                                continuous latent variables per datapoint, posterior inference can be made espe-
                                cially efÔ¨Åcient by Ô¨Åtting an approximate inference model (also called a recogni-
                                tion model) to the intractable posterior using the proposed lower bound estimator.
                                Theoretical advantages are reÔ¨Çected in experimental results.
                        1   Introduction
                        HowcanweperformefÔ¨Åcientapproximateinferenceandlearningwithdirectedprobabilisticmodels
                        whose continuous latent variables and/or parameters have intractable posterior distributions? The
                        variational Bayesian (VB) approach involves the optimization of an approximation to the intractable
                        posterior. Unfortunately, the common mean-Ô¨Åeld approach requires analytical solutions of expecta-
                        tions w.r.t. the approximate posterior, which are also intractable in the general case. We show how a
                        reparameterization of the variational lower bound yields a simple differentiable unbiased estimator
                        of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for ef-
                        Ô¨Åcient approximate posterior inference in almost any model with continuous latent variables and/or
                        parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.
                        For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-
       arXiv:1312.6114v11  [stat.ML]  10 Dec 2022EncodingVB(AEVB)algorithm. IntheAEVBalgorithmwemakeinferenceandlearningespecially
                        efÔ¨ÅcientbyusingtheSGVBestimatortooptimizearecognitionmodelthatallowsustoperformvery
                        efÔ¨Åcient approximate posterior inference using simple ancestral sampling, which in turn allows us
                        to efÔ¨Åciently learn the model parameters, without the need of expensive iterative inference schemes
                        (suchasMCMC)perdatapoint. Thelearnedapproximateposteriorinferencemodelcanalsobeused
                        for a host of tasks such as recognition, denoising, representation and visualization purposes. When
                        a neural network is used for the recognition model, we arrive at the variational auto-encoder.
                        2   Method
                        The strategy in this section can be used to derive a lower bound estimator (a stochastic objective
                        function) for a variety of directed graphical models with continuous latent variables. We will restrict
                        ourselvesheretothecommoncasewherewehaveani.i.d. datasetwithlatentvariablesperdatapoint,
                        andwhereweliketoperformmaximumlikelihood(ML)ormaximumaposteriori(MAP)inference
                        on the (global) parameters, and variational inference on the latent variables. It is, for example,
                                                                  1
                                                                      œÜ          z          Œ∏
                                                                                 x
                                                                                   N
                             Figure1: Thetypeofdirectedgraphicalmodelunderconsideration. Solidlinesdenotethegenerative
                             model p (z)p (x|z), dashed lines denote the variational approximation q (z|x) to the intractable
                                      Œ∏     Œ∏                                                               œÜ
                             posterior p (z|x). The variational parameters œÜ are learned jointly with the generative model pa-
                                         Œ∏
                             rameters Œ∏.
                             straightforward to extend this scenario to the case where we also perform variational inference on
                             the global parameters; that algorithm is put in the appendix, but experiments with that case are left to
                             future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming
                             data, but here we assume a Ô¨Åxed dataset for simplicity.
                             2.1   Problemscenario
                                                                       (i) N
                             Let us consider some dataset X = {x          }     consisting of N i.i.d. samples of some continuous
                                                                           i=1
                             or discrete variable x. We assume that the data are generated by some random process, involving
                             an unobserved continuous random variable z. The process consists of two steps: (1) a value z(i)
                             is generated from some prior distribution p ‚àó(z); (2) a value x(i) is generated from some condi-
                                                                            Œ∏
                             tional distribution p ‚àó(x|z). We assume that the prior p ‚àó(z) and likelihood p ‚àó(x|z) come from
                                                  Œ∏                                       Œ∏                       Œ∏
                             parametric families of distributions p (z) and p (x|z), and that their PDFs are differentiable almost
                                                                    Œ∏           Œ∏
                             everywherew.r.t. both Œ∏ and z. Unfortunately, a lot of this process is hidden from our view: the true
                             parameters Œ∏‚àó as well as the values of the latent variables z(i) are unknown to us.
                             Very importantly, we do not make the common simplifying assumptions about the marginal or pos-
                             terior probabilities. Conversely, we are here interested in a general algorithm that even works efÔ¨Å-
                             ciently in the case of:
                                   1. Intractability:    the case where the integral of the marginal likelihood p (x)                =
                                       R                                                                                     Œ∏
                                         p (z)p (x|z)dz is intractable (so we cannot evaluate or differentiate the marginal like-
                                          Œ∏     Œ∏
                                       lihood), where the true posterior density p (z|x) = p (x|z)p (z)/p (x) is intractable
                                                                                      Œ∏            Œ∏       Œ∏       Œ∏
                                       (so the EM algorithm cannot be used), and where the required integrals for any reason-
                                       able mean-Ô¨Åeld VB algorithm are also intractable. These intractabilities are quite common
                                       and appear in cases of moderately complicated likelihood functions p (x|z), e.g. a neural
                                                                                                                 Œ∏
                                       network with a nonlinear hidden layer.
                                   2. A large dataset: we have so much data that batch optimization is too costly; we would like
                                       to make parameter updates using small minibatches or even single datapoints. Sampling-
                                       based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a
                                       typically expensive sampling loop per datapoint.
                             Weareinterested in, and propose a solution to, three related problems in the above scenario:
                                   1. EfÔ¨Åcient approximate ML or MAP estimation for the parameters Œ∏. The parameters can be
                                       of interest themselves, e.g. if we are analyzing some natural process. They also allow us to
                                       mimicthehidden random process and generate artiÔ¨Åcial data that resembles the real data.
                                   2. EfÔ¨Åcient approximate posterior inference of the latent variable z given an observed value x
                                       for a choice of parameters Œ∏. This is useful for coding or data representation tasks.
                                   3. EfÔ¨Åcient approximate marginal inference of the variable x. This allows us to perform all
                                       kindsofinferencetaskswhereaprioroverxisrequired. Commonapplicationsincomputer
                                       vision include image denoising, inpainting and super-resolution.
                                                                                 2
                                         For the purpose of solving the above problems, let us introduce a recognition model qœÜ(z|x): an
                                         approximation to the intractable true posterior p (z|x). Note that in contrast with the approximate
                                                                                                                 Œ∏
                                         posterior in mean-Ô¨Åeld variational inference, it is not necessarily factorial and its parameters œÜ are
                                         not computed from some closed-form expectation. Instead, we‚Äôll introduce a method for learning
                                         the recognition model parameters œÜ jointly with the generative model parameters Œ∏.
                                         From a coding theory perspective, the unobserved variables z have an interpretation as a latent
                                         representation or code. In this paper we will therefore also refer to the recognition model q (z|x)
                                                                                                                                                                                  œÜ
                                         as a probabilistic encoder, since given a datapoint x it produces a distribution (e.g. a Gaussian)
                                         over the possible values of the code z from which the datapoint x could have been generated. In a
                                         similar vein we will refer to p (x|z) as a probabilistic decoder, since given a code z it produces a
                                                                                      Œ∏
                                         distribution over the possible corresponding values of x.
                                         2.2     Thevariational bound
                                         Themarginallikelihoodiscomposedofasumoverthemarginallikelihoodsofindividualdatapoints
                                         logp (x(1),¬∑¬∑¬∑ ,x(N)) = PN logp (x(i)), which can each be rewritten as:
                                                Œ∏                                   i=1          Œ∏
                                                                     logp (x(i)) = D                (q (z|x(i))||p (z|x(i))) + L(Œ∏,œÜ;x(i))                                              (1)
                                                                            Œ∏                  KL œÜ                      Œ∏
                                         The Ô¨Årst RHS term is the KL divergence of the approximate from the true posterior. Since this
                                         KL-divergence is non-negative, the second RHS term L(Œ∏,œÜ;x(i)) is called the (variational) lower
                                         bound on the marginal likelihood of datapoint i, and can be written as:
                                                              logp (x(i)) ‚â• L(Œ∏,œÜ;x(i)) = E                               [‚àílogq (z|x)+logp (x,z)]                                      (2)
                                                                     Œ∏                                         qœÜ(z|x)                œÜ                     Œ∏
                                         which can also be written as:                                                                           h                      i
                                                            L(Œ∏,œÜ;x(i)) = ‚àíD                   (q (z|x(i))||p (z)) + E                             logp (x(i)|z)                        (3)
                                                                                          KL œÜ                      Œ∏              qœÜ(z|x(i))             Œ∏
                                         We want to differentiate and optimize the lower bound L(Œ∏,œÜ;x(i)) w.r.t. both the variational
                                         parameters œÜ and generative parameters Œ∏. However, the gradient of the lower bound w.r.t. œÜ
                                                                                               ¬®
                                         is a bit problematic. The usual (naƒ±ve) Monte Carlo gradient estimator for this type of problem
                                                                                                                                P
                                         is: ‚àá E             [f(z)] = E                 f(z)‚àá             logq (z) ' 1                L f(z)‚àá                    logq (z(l)) where
                                                 œÜ qœÜ(z)                      qœÜ(z)               qœÜ(z)          œÜ             L      l=1             qœÜ(z(l))          œÜ
                                         z(l) ‚àº q (z|x(i)). This gradient estimator exhibits exhibits very high variance (see e.g. [BJP12])
                                                     œÜ
                                         and is impractical for our purposes.
                                         2.3     TheSGVBestimatorandAEVBalgorithm
                                         In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the
                                         parameters. We assume an approximate posterior in the form q (z|x), but please note that the
                                                                                                                                           œÜ
                                         technique can be applied to the case q (z), i.e. where we do not condition on x, as well. The fully
                                                                                                 œÜ
                                         variational Bayesian method for inferring a posterior over the parameters is given in the appendix.
                                         Undercertainmildconditionsoutlinedinsection2.4forachosenapproximateposteriorq (z|x)we
                                                                                                                                                                             œÜ
                                                                                                    e
                                         can reparameterize the random variable z ‚àº q (z|x) using a differentiable transformation g (,x)
                                                                                                             œÜ                                                                   œÜ
                                         of an (auxiliary) noise variable :
                                                                                           e
                                                                                           z = g (,x)            with        ‚àº p()                                                   (4)
                                                                                                   œÜ
                                         See section 2.4 for general strategies for chosing such an approriate distribution p() and function
                                         gœÜ(,x). We can now form Monte Carlo estimates of expectations of some function f(z) w.r.t.
                                         q (z|x) as follows:
                                          œÜ
                                                                                  h                      i            L
                                                                                                   (i)           1 X                 (l)    (i)                       (l)
                                            E          (i)  [f(z)] = E              f(gœÜ(,x )) '                         f(gœÜ( ,x )) where                              ‚àºp() (5)
                                              qœÜ(z|x      )                 p()                                 L
                                                                                                                    l=1
                                         We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic
                                                                                                                A              (i)                     (i)
                                                                                                               e
                                         Gradient Variational Bayes (SGVB) estimator L (Œ∏,œÜ;x                                     ) ' L(Œ∏,œÜ;x ):
                                                                                                    L
                                                                      A             (i)        1 X                   (i)    (i,l)                   (i,l)    (i)
                                                                    e
                                                                   L (Œ∏,œÜ;x )=                          logp (x ,z               ) ‚àílogq (z              |x     )
                                                                                              L                Œ∏                              œÜ
                                                                                                  l=1
                                                                                    (i,l)            (i,l)     (i)                 (l)
                                                                    where         z       =gœÜ(            , x    )    and             ‚àºp()                                           (6)
                                                                                                                  3
                             Algorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm. Either of the two
                             SGVBestimatorsinsection 2.3 can be used. We use settings M = 100 and L = 1 in experiments.
                                Œ∏,œÜ‚ÜêInitialize parameters
                                repeat
                                    XM‚ÜêRandomminibatchofM datapoints(drawnfromfulldataset)
                                     ‚ÜêRandomsamplesfromnoisedistribution p()
                                                 M           M
                                                e
                                    g ‚Üê‚àá L (Œ∏,œÜ;X ,)(Gradientsofminibatchestimator(8))
                                            Œ∏,œÜ
                                    Œ∏,œÜ‚ÜêUpdateparametersusinggradientsg (e.g. SGD or Adagrad [DHS10])
                                until convergence of parameters (Œ∏,œÜ)
                                return Œ∏,œÜ
                             Often, the KL-divergence DKL(qœÜ(z|x(i))||pŒ∏(z)) of eq. (3) can be integrated analytically (see
                                                                                                                      (i)   
                             appendix B), such that only the expected reconstruction error E             (i)  logp (x |z) requires
                                                                                                   qœÜ(z|x  )       Œ∏
                             estimation by sampling. The KL-divergence term can then be interpreted as regularizing œÜ, encour-
                             aging the approximate posterior to be close to the prior p (z). This yields a second version of the
                                                                                           Œ∏
                                                  B         (i)               (i)
                                                e
                             SGVBestimator L (Œ∏,œÜ;x ) ' L(Œ∏,œÜ;x ), corresponding to eq. (3), which typically has less
                             variance than the generic estimator:
                                                                                                  L
                                             B         (i)                     (i)             1 X             (i)  (i,l)
                                            e
                                           L (Œ∏,œÜ;x )=‚àíD (q (z|x )||p (z))+                          (logp (x |z        ))
                                                                   KL œÜ              Œ∏         L           Œ∏
                                                                                                 l=1
                                                       (i,l)        (i,l) (i)            (l)
                                            where     z     =g (      , x   )   and       ‚àºp()                                   (7)
                                                                œÜ
                             Given multiple datapoints from a dataset X with N datapoints, we can construct an estimator of the
                             marginal likelihood lower bound of the full dataset, based on minibatches:
                                                                                              M
                                                                       M          M       N X               (i)
                                                                      e                           e
                                                     L(Œ∏,œÜ;X)'L (Œ∏,œÜ;X )= M                       L(Œ∏,œÜ;x )                         (8)
                                                                                              i=1
                                                       M          (i) M
                             where the minibatch X         = {x }         is a randomly drawn sample of M datapoints from the
                                                                      i=1
                             full dataset X with N datapoints. In our experiments we found that the number of samples L
                             per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100.
                                                 e      M
                             Derivatives ‚àá      L(Œ∏;X ) can be taken, and the resulting gradients can be used in conjunction
                                             Œ∏,œÜ
                             with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a
                             basic approach to compute the stochastic gradients.
                             A connection with auto-encoders becomes clear when looking at the objective function given at
                             eq. (7). The Ô¨Årst term is (the KL divergence of the approximate posterior from the prior) acts as a
                             regularizer, while the second term is a an expected negative reconstruction error. The function gœÜ(.)
                                                                           (i)                               (l)
                             is chosen such that it maps a datapoint x        and a random noise vector         to a sample from the
                                                                            (i,l)        (l)  (i)          (i,l)           (i)
                             approximate posterior for that datapoint: z         =gœÜ( ,x )wherez               ‚àºqœÜ(z|x ). Subse-
                                                     (i,l)                                    (i) (i,l)
                             quently, the sample z       is then input to function logp (x      |z    ), which equals the probability
                                                                                          Œ∏
                             density (or mass) of datapoint x(i) under the generative model, given z(i,l). This term is a negative
                             reconstruction error in auto-encoder parlance.
                             2.4   Thereparameterization trick
                             In order to solve our problem we invoked an alternative method for generating samples from
                             qœÜ(z|x). The essential parameterization trick is quite simple. Let z be a continuous random vari-
                             able, and z ‚àº qœÜ(z|x) be some conditional distribution. It is then often possible to express the
                             random variable z as a deterministic variable z = gœÜ(,x), where  is an auxiliary variable with
                             independent marginal p(), and g (.) is some vector-valued function parameterized by œÜ.
                                                                 œÜ
                             This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t
                             qœÜ(z|x) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. œÜ. A proof
                             is as follows. Given the deterministic mapping z = gœÜ(,x) we know that qœÜ(z|x)Qidzi =
                             p()Q d . Therefore1, R q (z|x)f(z)dz = R p()f(z)d = R p()f(g (,x))d. It follows
                                     i   i                   œÜ                                                 œÜ
                                 1Note that for inÔ¨Ånitesimals we use the notational convention dz = Q dz
                                                                                                    i   i
                                                                                  4
                                                                                    R                         P
                             that a differentiable estimator can be constructed:      q (z|x)f(z)dz ' 1          L f(g (x,(l)))
                                                                                       œÜ                    L    l=1    œÜ
                                     (l)
                             where     ‚àº p(). In section 2.3 we applied this trick to obtain a differentiable estimator of the
                             variational lower bound.
                             Take, for example, the univariate Gaussian case: let z ‚àº p(z|x) = N(¬µ,œÉ2). In this case, a valid
                             reparameterization is z = ¬µ + œÉ, where  is an auxiliary noise variable  ‚àº N(0,1). Therefore,
                                                                              P
                                                                            1    L             (l)         (l)
                             E        2 [f(z)] = E          [f(¬µ+œÉ)] '              f(¬µ+œÉ )where ‚àºN(0,1).
                              N(z;¬µ,œÉ )             N(;0,1)                L    l=1
                             For which qœÜ(z|x) can we choose such a differentiable transformation gœÜ(.) and auxiliary variable
                              ‚àº p()? Three basic approaches are:
                                   1. Tractable inverse CDF. In this case, let  ‚àº U(0,I), and let g (,x) be the inverse CDF of
                                                                                                     œÜ
                                      qœÜ(z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal,
                                      Gompertz, Gumbel and Erlang distributions.
                                   2. AnalogoustotheGaussianexample,forany‚Äùlocation-scale‚Äùfamilyofdistributionswecan
                                      choose the standard distribution (with location = 0, scale = 1) as the auxiliary variable
                                      , and let g(.) = location + scale ¬∑ . Examples: Laplace, Elliptical, Student‚Äôs t, Logistic,
                                      Uniform, Triangular and Gaussian distributions.
                                   3. Composition: It is often possible to express random variables as different transformations
                                      of auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed
                                      variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted
                                      sumofGammavariates),Beta, Chi-Squared, and F distributions.
                             When all three approaches fail, good approximations to the inverse CDF exist requiring computa-
                             tions with time complexity comparable to the PDF (see e.g. [Dev86] for some methods).
                             3   Example: Variational Auto-Encoder
                             In this section we‚Äôll give an example where we use a neural network for the probabilistic encoder
                             q (z|x) (the approximation to the posterior of the generative model p (x,z)) and where the param-
                              œÜ                                                                     Œ∏
                             eters œÜ and Œ∏ are optimized jointly with the AEVB algorithm.
                             Let the prior over the latent variables be the centered isotropic multivariate Gaussian p (z) =
                                                                                                                           Œ∏
                             N(z;0,I). Note that in this case, the prior lacks parameters. We let p (x|z) be a multivariate
                                                                                                          Œ∏
                             Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution pa-
                             rameters are computed from z with a MLP (a fully-connected neural network with a single hidden
                             layer, see appendix C). Note the true posterior p (z|x) is in this case intractable. While there is
                                                                                Œ∏
                             much freedom in the form qœÜ(z|x), we‚Äôll assume the true (but intractable) posterior takes on a ap-
                             proximate Gaussian form with an approximately diagonal covariance. In this case, we can let the
                             variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2:
                                                                       (i)               (i)   2(i)
                                                            logq (z|x    ) = logN(z;¬µ ,œÉ          I)                            (9)
                                                                 œÜ
                             where the mean and s.d. of the approximate posterior, ¬µ(i) and œÉ(i), are outputs of the encoding
                             MLP,i.e. nonlinear functions of datapoint x(i) and the variational parameters œÜ (see appendix C).
                                                                                               (i,l)           (i)          (i,l)
                             As explained in section 2.4, we sample from the posterior z            ‚àº qœÜ(z|x ) using z           =
                                  (i)  (l)       (i)     (i)    (l)         (l)
                             gœÜ(x , ) = ¬µ          +œÉ  where ‚àº N(0,I). With  we signify an element-wise
                             product. In this model both p (z) (the prior) and q (z|x) are Gaussian; in this case, we can use the
                                                           Œ∏                     œÜ
                             estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation
                             (see appendix B). The resulting estimator for this model and datapoint x(i) is:
                                                      J                                                L
                                  L(Œ∏,œÜ;x(i)) ' 1 X                   (i) 2       (i) 2     (i) 2     1 X            (i)  (i,l)
                                                           1+log((œÉ ) )‚àí(¬µ ) ‚àí(œÉ )                 +        logp (x |z        )
                                                   2                  j           j         j         L          Œ∏
                                                     j=1                                                l=1
                                            (i,l)    (i)    (i)    (l)          (l)
                                 where    z     =¬µ +œÉ                 and       ‚àºN(0,I)                                     (10)
                             Asexplained above and in appendix C, the decoding term logp (x(i)|z(i,l)) is a Bernoulli or Gaus-
                                                                                              Œ∏
                             sian MLP, depending on the type of data we are modelling.
                                2Note that this is just a (simplifying) choice, and not a limitation of our method.
                                                                               5
                         4   Related work
                         The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learn-
                         ing method in the literature that is applicable to the same general class of continuous latent variable
                         models. Like our method, the wake-sleep algorithm employs a recognition model that approximates
                         the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimiza-
                         tion of two objective functions, which together do not correspond to optimization of (a bound of)
                         the marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete
                         latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.
                         Stochastic variational inference [HBWP13] has recently received increasing interest. Recently,
                                                                                                      ¬®
                         [BJP12] introduced a control variate schemes to reduce the high variance of the naƒ±ve gradient
                         estimator discussed in section 2.1, and applied to exponential family approximations of the poste-
                         rior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing
                         the variance of the original gradient estimator. In [SK13], a similar reparameterization as in this
                         paper was used in an efÔ¨Åcient version of a stochastic variational inference algorithm for learning the
                         natural parameters of exponential-family approximating distributions.
                         The AEVB algorithm exposes a connection between directed probabilistic models (trained with a
                         variational objective) and auto-encoders. A connection between linear auto-encoders and a certain
                         class of generative linear-Gaussian modelshaslongbeenknown. In [Row98]itwasshownthatPCA
                         correspondstothemaximum-likelihood(ML)solutionofaspecialcaseofthelinear-Gaussianmodel
                         with a prior p(z) = N(0,I) and a conditional distribution p(x|z) = N(x;Wz,I), speciÔ¨Åcally the
                         case with inÔ¨Ånitesimally small .
                                                                   +
                         In relevant recent work on autoencoders [VLL 10] it was shown that the training criterion of un-
                         regularized autoencoders corresponds to maximization of a lower bound (see the infomax princi-
                         ple [Lin89]) of the mutual information between input X and latent representation Z. Maximiz-
                         ing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional en-
                         tropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding
                         model [VLL+10], i.e. the negative reconstrution error. However, it is well known that this recon-
                         struction criterion is in itself not sufÔ¨Åcient for learning useful representations [BCV13]. Regular-
                         ization techniques have been proposed to make autoencoders learn useful representations, such as
                         denoising, contractive and sparse autoencoder variants [BCV13]. The SGVB objective contains a
                         regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regu-
                         larization hyperparameter required to learn useful representations. Related are also encoder-decoder
                         architectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew
                         someinspiration. AlsorelevantaretherecentlyintroducedGenerativeStochasticNetworks[BTL13]
                         wherenoisyauto-encoderslearnthetransitionoperatorofaMarkovchainthatsamplesfromthedata
                         distribution. In [SL10] a recognition model was employed for efÔ¨Åcient learning with Deep Boltz-
                         mannMachines. These methods are targeted at either unnormalized models (i.e. undirected models
                         like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm
                         for learning a general class of directed probabilistic models.
                         The recently proposed DARN method [GMW13], also learns a directed probabilistic model using
                         an auto-encoding structure, however their method applies to binary latent variables. Even more
                         recently, [RMW14] also make the connection between auto-encoders, directed proabilistic models
                         and stochastic variational inference using the reparameterization trick we describe in this paper.
                         Their work was developed independently of ours and provides an additional perspective on AEVB.
                         5   Experiments
                         We trained generative models of images from the MNIST and Frey Face datasets3 and compared
                         learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.
                         Thegenerative model (encoder) and variational approximation (decoder) from section 3 were used,
                         where the described encoder and decoder have an equal number of hidden units. Since the Frey
                         Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except
                         that the means were constrained to the interval (0,1) using a sigmoidal activation function at the
                            3Available at http://www.cs.nyu.edu/ roweis/data.html
                                                                 Àú
                                                                    6
                                                  MNIST, N =3               MNIST, N =5              MNIST, N =10              MNIST, N =20             MNIST, N =200
                                                             z                        z                        z                         z                        z
                                            100                       100                      100                       100                      100
                                            110                       110                      110                       110                      110
                                         L  120                       120                      120                       120                      120
                                            130                       130                      130                       130                      130
                                            140                       140                      140                       140                      140
                                            150 5     6     7     8   150 5    6     7      8  150 5     6     7     8   150 5     6     7     8  150 5     6     7     8
                                              10    10    10    10     10    10    10     10     10    10    10    10      10    10    10    10     10    10    10    10
                                            # Training samples evaluated
                                                                          Frey Face, N =2           Frey Face, N =5          Frey Face, N =10         Frey Face, N =20
                                                                     1600               z     1600                z     1600               z     1600               z
                                                                     1400                     1400                      1400                     1400
                                               Wake-Sleep (test)     1200                     1200                      1200                     1200
                                               Wake-Sleep (train)    1000                     1000                      1000                     1000
                                               AEVB (test)         L  800                      800                       800                      800
                                               AEVB (train)           600                      600                       600                      600
                                                                      400                      400                       400                      400
                                                                      200                      200                       200                      200
                                                                        0 5     6      7      8  0 5      6      7      8  0 5     6      7      8  0 5      6      7      8
                                                                       10     10     10     10   10     10     10    10    10    10     10     10   10     10     10     10
                                       Figure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the
                                       lower bound, for different dimensionality of latent space (N ). Our method converged considerably
                                                                                                                            z
                                       faster and reached a better solution in all experiments. Interestingly enough, more latent variables
                                       does not result in more overÔ¨Åtting, which is explained by the regularizing effect of the lower bound.
                                       Vertical axis: the estimated average variational lower bound per datapoint. The estimator variance
                                       was small (< 1) and omitted. Horizontal axis: amount of training points evaluated. Computa-
                                       tion took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an
                                       effective 40 GFLOPS.
                                       decoder output. Note that with hidden units we refer to the hidden layer of the neural networks of
                                       the encoder and decoder.
                                       Parameters are updated using stochastic gradient ascent where gradients are computed by differenti-
                                       ating the lower bound estimator ‚àá                    L(Œ∏,œÜ;X)(seealgorithm 1),plusasmallweightdecayterm
                                                                                        Œ∏,œÜ
                                       corresponding to a prior p(Œ∏) = N(0,I). Optimization of this objective is equivalent to approxi-
                                       mate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower
                                       bound.
                                       We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the
                                       sameencoder(alsocalled recognition model) for the wake-sleep algorithm and the variational auto-
                                       encoder. All parameters, both variational and generative, were initialized by random sampling from
                                       N(0,0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were
                                       adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01,
                                       0.02, 0.1} based on performance on the training set in the Ô¨Årst few iterations. Minibatches of size
                                       M=100wereused,withL=1samplesperdatapoint.
                                       Likelihood lower bound                  We trained generative models (decoders) and corresponding encoders
                                       (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case
                                       oftheFreyFacedataset(topreventoverÔ¨Åtting,sinceitisaconsiderablysmallerdataset). Thechosen
                                       number of hidden units is based on prior literature on auto-encoders, and the relative performance
                                       of different algorithms was not very sensitive to these choices. Figure 2 shows the results when
                                       comparing the lower bounds. Interestingly, superÔ¨Çuous latent variables did not result in overÔ¨Åtting,
                                       which is explained by the regularizing nature of the variational bound.
                                       Marginallikelihood               For very low-dimensional latent space it is possible to estimate the marginal
                                       likelihood of the learned generative models using an MCMC estimator. More information about the
                                       marginal likelihood estimator is available in the appendix. For the encoder and decoder we again
                                       usedneural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional
                                       latent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB
                                       and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo
                                       (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for
                                       the three algorithms, for a small and large training set size. Results are in Ô¨Ågure 3.
                                                                                                             7
                                               N  = 1000                           N  = 50000
                                   100          train                   125          train
                                   110                                  130
                                                                        135                                            Wake-Sleep (train)
                                   120                                                                                 Wake-Sleep (test)
                                                                        140                                            MCEM (train)
                                   130                                                                                 MCEM (test)
                                                                        145                                            AEVB (train)
                                   140                                                                                 AEVB (test)
                                                                        150
                                 Marginal log-likelihood150
                                                                        155
                                   160                                  160
                                      0    10   20   30   40   50   60     0   10   20  30   40  50   60
                                   # Training samples evaluated (millions)
                              Figure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the
                              estimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an
                              on-line algorithm, and (unlike AEVB and the wake-sleep method) can‚Äôt be applied efÔ¨Åciently for
                              the full MNIST dataset.
                              Visualisation of high-dimensional data          If we choose a low-dimensional latent space (e.g. 2D),
                              we can use the learned encoders (recognition model) to project high-dimensional data to a low-
                              dimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST
                              and Frey Face datasets.
                              6    Conclusion
                              We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB
                              (SGVB),forefÔ¨Åcientapproximateinference with continuous latent variables. The proposed estima-
                              tor can be straightforwardly differentiated and optimized using standard stochastic gradient meth-
                              ods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an
                              efÔ¨Åcient algorithm for efÔ¨Åcient inference and learning, Auto-Encoding VB (AEVB), that learns an
                              approximate inference model using the SGVB estimator. The theoretical advantages are reÔ¨Çected in
                              experimental results.
                              7    Future work
                              Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and
                              learning problem with continuous latent variables, there are plenty of future directions: (i) learning
                              hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used
                              for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic
                              Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models
                              with latent variables, useful for learning complicated noise distributions.
                                                                                   8
                    References
                    [BCV13]  Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A re-
                             view and new perspectives. 2013.
                    [BJP12]  David M Blei, Michael I Jordan, and John W Paisley. Variational Bayesian inference
                             with Stochastic Search. In Proceedings of the 29th International Conference on Ma-
                             chine Learning (ICML-12), pages 1367‚Äì1374, 2012.
                                            ¬¥
                    [BTL13]  YoshuaBengioandEricThibodeau-Laufer. Deep generative stochastic networks train-
                             able by backprop. arXiv preprint arXiv:1306.1091, 2013.
                    [Dev86]  Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings
                             of the 18th conference on Winter simulation, pages 260‚Äì265. ACM, 1986.
                    [DHS10]  John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online
                             learning and stochastic optimization. Journal of Machine Learning Research, 12:2121‚Äì
                             2159, 2010.
                    [DKPR87] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid
                             monte carlo. Physics letters B, 195(2):216‚Äì222, 1987.
                    [GMW13] KarolGregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv
                             preprint arXiv:1310.8499, 2013.
                    [HBWP13] MatthewDHoffman,DavidMBlei,ChongWang,andJohnPaisley. Stochastic varia-
                             tional inference. The Journal of Machine Learning Research, 14(1):1303‚Äì1347, 2013.
                    [HDFN95] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The‚Äù wake-
                             sleep‚Äùalgorithmforunsupervisedneuralnetworks. SCIENCE,pages1158‚Äì1158,1995.
                    [KRL08]  KorayKavukcuoglu,Marc‚ÄôAurelioRanzato,andYannLeCun. Fastinferenceinsparse
                             coding algorithms with applications to object recognition. Technical Report CBLL-
                             TR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU,
                             2008.
                    [Lin89]  RalphLinsker. An application of the principle of maximum information preservation to
                             linear systems. Morgan Kaufmann Publishers Inc., 1989.
                    [RGB13]  Rajesh Ranganath, Sean Gerrish, and David M Blei. Black Box Variational Inference.
                             arXiv preprint arXiv:1401.0118, 2013.
                    [RMW14] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic back-
                             propagation and variational inference in deep latent gaussian models. arXiv preprint
                             arXiv:1401.4082, 2014.
                    [Row98]  Sam Roweis. EM algorithms for PCA and SPCA. Advances in neural information
                             processing systems, pages 626‚Äì632, 1998.
                    [SK13]   Tim Salimans and David A Knowles. Fixed-form variational posterior approximation
                             through stochastic linear regression. Bayesian Analysis, 8(4), 2013.
                    [SL10]   Ruslan Salakhutdinov and Hugo Larochelle. EfÔ¨Åcient learning of deep boltzmann ma-
                             chines. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages 693‚Äì
                             700, 2010.
                        +
                    [VLL 10] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine
                             Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep
                             network with a local denoising criterion. The Journal of Machine Learning Research,
                             9999:3371‚Äì3408, 2010.
                    A Visualisations
                    See Ô¨Ågures 4 and 5 for visualisations of latent space and corresponding observed space of models
                    learned with SGVB.
                                                        9
                                    (a) Learned Frey Face manifold                   (b) Learned MNIST manifold
                             Figure 4: Visualisations of learned data manifold for generative models with two-dimensional latent
                             space, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coor-
                             dinates on the unit square were transformed through the inverse CDF of the Gaussian to produce
                             values of the latent variables z. For each of these values z, we plotted the corresponding generative
                             p (x|z) with the learned parameters Œ∏.
                              Œ∏
                                   (a) 2-D latent space     (b) 5-D latent space    (c) 10-D latent space    (d) 20-D latent space
                             Figure 5: RandomsamplesfromlearnedgenerativemodelsofMNISTfordifferentdimensionalities
                             of latent space.
                             B Solutionof‚àíD                (q (z)||p (z)), Gaussian case
                                                       KL œÜ           Œ∏
                             The variational lower bound (the objective to be maximized) contains a KL term that can often be
                             integrated analytically. Here we give the solution when both the prior pŒ∏(z) = N(0,I) and the
                             posterior approximation qœÜ(z|x(i)) are Gaussian. Let J be the dimensionality of z. Let ¬µ and œÉ
                             denote the variational mean and s.d. evaluated at datapoint i, and let ¬µ and œÉ simply denote the
                                                                                                         j       j
                             j-th element of these vectors. Then:
                                                    Z q (z)logp(z)dz = Z N(z;¬µ,œÉ2)logN(z;0,I)dz
                                                        Œ∏
                                                                                                 J
                                                                               J             1 X 2          2
                                                                          =‚àí log(2œÄ)‚àí              (¬µ +œÉ )
                                                                               2             2        j     j
                                                                                               j=1
                                                                                10
                       And:            Z                   Z
                                         q (z)logq (z)dz =   N(z;¬µ,œÉ2)logN(z;¬µ,œÉ2)dz
                                          Œ∏       Œ∏
                                                                          J
                                                             J          1 X          2
                                                         =‚àí2log(2œÄ)‚àí 2      (1+logœÉj)
                                                                         j=1
                       Therefore:                        Z
                                    ‚àíD ((q (z)||p (z)) =    q (z)(logp (z)‚àílogq (z)) dz
                                       KL   œÜ     Œ∏         Œ∏        Œ∏         Œ∏
                                                            J
                                                         1 X             2       2      2
                                                       =       1+log((œÉ ) )‚àí(¬µ ) ‚àí(œÉ )
                                                         2              j       j      j
                                                           j=1
                       When using a recognition model qœÜ(z|x) then ¬µ and s.d. œÉ are simply functions of x and the
                       variational parameters œÜ, as exempliÔ¨Åed in the text.
                       C MLP‚Äôsasprobabilisticencodersanddecoders
                       In variational auto-encoders, neural networks are used as probabilistic encoders and decoders. There
                       are many possible choices of encoders and decoders, depending on the type of data and model. In
                       our example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs).
                       For the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with
                       either Gaussian or Bernoulli outputs, depending on the type of data.
                       C.1  Bernoulli MLP as decoder
                       In this case let p (x|z) be a multivariate Bernoulli whose probabilities are computed from z with a
                                    Œ∏
                       fully-connected neural network with a single hidden layer:
                                                      D
                                          logp(x|z) = Xx logy +(1‚àíx )¬∑log(1‚àíy )
                                                          i    i       i          i
                                                      i=1
                                           where y = f (W tanh(W z+b )+b )                          (11)
                                                      œÉ    2       1     1    2
                       where f (.) is the elementwise sigmoid activation function, and where Œ∏ = {W ,W ,b ,b } are
                             œÉ                                                          1   2  1   2
                       the weights and biases of the MLP.
                       C.2  Gaussian MLPasencoderordecoder
                       In this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:
                                                  logp(x|z) = logN(x;¬µ,œÉ2I)
                                                   where ¬µ = W h+b
                                                                4     4
                                                     logœÉ2 = W h+b
                                                                5     5
                                                         h=tanh(W z+b )                             (12)
                                                                    3     3
                       where {W ,W ,W ,b ,b ,b }aretheweightsandbiasesoftheMLPandpartofŒ∏ whenused
                               3    4   5  3  4  5
                       as decoder. Note that when this network is used as an encoder qœÜ(z|x), then z and x are swapped,
                       and the weights and biases are variational parameters œÜ.
                       D Marginallikelihoodestimator
                       Wederivedthefollowingmarginallikelihoodestimatorthatproducesgoodestimatesofthemarginal
                       likelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and
                       sufÔ¨Åcient samples are taken. Let p (x,z) = p (z)p (x|z) be the generative model we are sampling
                                                  Œ∏         Œ∏   Œ∏
                       from, and for a given datapoint x(i) we would like to estimate the marginal likelihood p (x(i)).
                                                                                              Œ∏
                       Theestimation process consists of three stages:
                                                              11
                                                         (l)
                                  1. SampleLvalues{z }fromtheposteriorusinggradient-basedMCMC,e.g. HybridMonte
                                     Carlo, using ‚àá logp (z|x) = ‚àá logp (z)+‚àá logp (x|z).
                                                    z      Œ∏           z     Œ∏         z      Œ∏
                                                                                     (l)
                                  2. Fit a density estimator q(z) to these samples {z  }.
                                  3. Again, sample L new values from the posterior. Plug these samples, as well as the Ô¨Åtted
                                     q(z), into the following estimator:
                                                           L                (l)      !‚àí1
                                                  (i)       1 X         q(z   )                         (l)         (i)
                                             p (x )'                                         where    z    ‚àºp (z|x )
                                               Œ∏            L     p (z)p (x(i)|z(l))                           Œ∏
                                                              l=1   Œ∏    Œ∏
                            Derivation of the estimator:
                                                         R            R q(z)pŒ∏(x(i),z) dz
                                                1          q(z)dz            pŒ∏(x(i),z)
                                                 (i)  =       (i)  =             (i)
                                            p (x )       p (x )             p (x )
                                              Œ∏          Z Œ∏                 Œ∏
                                                            p (x(i),z)    q(z)
                                                      =      Œ∏                     dz
                                                             p (x(i)) p (x(i),z)
                                                         Z    Œ∏          Œ∏
                                                      = p (z|x(i))       q(z)     dz
                                                             Œ∏        p (x(i),z)
                                                                        Œ∏
                                                            L            (l)
                                                      ' 1 X           q(z  )         where    z(l) ‚àº p (z|x(i))
                                                         L                 (i) (l)                     Œ∏
                                                                p (z)p (x |z )
                                                            l=1  Œ∏     Œ∏
                            E MonteCarloEM
                            The Monte Carlo EM algorithm does not employ an encoder, instead it samples from the pos-
                            terior of the latent variables using gradients of the posterior computed with ‚àáz logpŒ∏(z|x) =
                            ‚àá logp (z) + ‚àá logp (x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog
                              z     Œ∏          z     Œ∏
                            steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5
                            weight updates steps using the acquired sample. For all algorithms the parameters were updated
                            using the Adagrad stepsizes (with accompanying annealing schedule).
                            The marginal likelihood was estimated with the Ô¨Årst 1000 datapoints from the train and test sets,
                            for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte
                            Carlo with 4 leapfrog steps.
                            F FullVB
                            Aswritten in the paper, it is possible to perform variational inference on both the parameters Œ∏ and
                            the latent variables z, as opposed to just the latent variables as we did in the paper. Here, we‚Äôll derive
                            our estimator for that case.
                            Let p (Œ∏) be some hyperprior for the parameters introduced above, parameterized by Œ±. The
                                  Œ±
                            marginal likelihood can be written as:
                                                     logp (X) = D       (q (Œ∏)||p (Œ∏|X))+L(œÜ;X)                             (13)
                                                          Œ±          KL œÜ         Œ±
                            where the Ô¨Årst RHS term denotes a KL divergence of the approximate from the true posterior, and
                            where L(œÜ;X)denotes the variational lower bound to the marginal likelihood:
                                              L(œÜ;X)=Z q (Œ∏)(logp (X)+logp (Œ∏)‚àílogq (Œ∏)) dŒ∏                                 (14)
                                                              œÜ           Œ∏            Œ±            œÜ
                            Note that this is a lower bound since the KL divergence is non-negative; the bound equals the true
                            marginal when the approximate and true posteriors match exactly. The term logp (X) is composed
                                                                                                              Œ∏
                                                                                                             P
                            of a sum over the marginal likelihoods of individual datapoints logp (X) =          N logp (x(i)),
                                                                                                   Œ∏            i=1     Œ∏
                            which can each be rewritten as:
                                               logp (x(i)) = D      (q (z|x(i))||p (z|x(i))) + L(Œ∏,œÜ;x(i))                  (15)
                                                    Œ∏            KL œÜ             Œ∏
                                                                             12
                             whereagaintheÔ¨ÅrstRHStermistheKLdivergenceoftheapproximatefromthetrueposterior,and
                             L(Œ∏,œÜ;x)isthevariational lower bound of the marginal likelihood of datapoint i:
                                                     (i)    Z                     (i)                                
                                          L(Œ∏,œÜ;x )=           q (z|x) logp (x |z)+logp (z)‚àílogq (z|x) dz                          (16)
                                                                œÜ              Œ∏                 Œ∏            œÜ
                             TheexpectationsontheRHSofeqs (14)and(16)canobviouslybewrittenasasumofthreeseparate
                             expectations, of which the second and third component can sometimes be analytically solved, e.g.
                             when both p (x) and q (z|x) are Gaussian. For generality we will here assume that each of these
                                           Œ∏          œÜ
                             expectations is intractable.
                             Under certain mild conditions outlined in section (see paper) for chosen approximate posteriors
                                                                                                 e
                             q (Œ∏) and q (z|x) we can reparameterize conditional samples z ‚àº q (z|x) as
                               œÜ          œÜ                                                            œÜ
                                                                 e
                                                                 z = g (,x)      with     ‚àº p()                                 (17)
                                                                        œÜ
                             where we choose a prior p() and a function gœÜ(,x) such that the following holds:
                                              (i)    Z                     (i)                                
                                   L(Œ∏,œÜ;x )=           q (z|x) logp (x |z)+logp (z)‚àílogq (z|x) dz
                                                         œÜ              Œ∏                 Œ∏            œÜ
                                                     Z                                                    
                                                                        (i)                                  
                                                  = p() logp (x |z)+logp (z)‚àílogq (z|x)                                 d       (18)
                                                                    Œ∏                 Œ∏            œÜ         
                                                                                                              z=gœÜ(,x(i))
                             Thesamecanbedonefortheapproximateposterior qœÜ(Œ∏):
                                                                  e
                                                                  Œ∏ = h (Œ∂)      with    Œ∂ ‚àº p(Œ∂)                                  (19)
                                                                         œÜ
                             where we, similarly as above, choose a prior p(Œ∂) and a function h (Œ∂) such that the following
                                                                                                        œÜ
                             holds:                       Z
                                             L(œÜ;X)=         q (Œ∏)(logp (X)+logp (Œ∏)‚àílogq (Œ∏)) dŒ∏
                                                              œÜ           Œ∏             Œ±             œÜ
                                                          Z                                                
                                                                                                           
                                                       = p(Œ∂)(logp (X)+logp (Œ∏)‚àílogq (Œ∏))                           dŒ∂            (20)
                                                                         Œ∏             Œ±             œÜ     
                                                                                                            Œ∏=h (Œ∂)
                                                                                                                œÜ
                             For notational conciseness we introduce a shorthand notation fœÜ(x,z,Œ∏):
                                   f (x,z,Œ∏) = N ¬∑(logp (x|z)+logp (z)‚àílogq (z|x))+logp (Œ∏)‚àílogq (Œ∏)                               (21)
                                    œÜ                        Œ∏               Œ∏            œÜ                Œ±            œÜ
                             Using equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given
                             datapoint x(i), is:
                                                                        L
                                                                     1 X         (l)      (l)  (l)       (l)
                                                       L(œÜ;X)'             f (x ,g ( ,x ),h (Œ∂ ))                                 (22)
                                                                    L        œÜ        œÜ              œÜ
                                                                       l=1
                                      (l)                (l)
                             where       ‚àº p() and Œ∂      ‚àº p(Œ∂). The estimator only depends on samples from p() and p(Œ∂)
                             which are obviously not inÔ¨Çuenced by œÜ, therefore the estimator can be differentiated w.r.t. œÜ.
                             The resulting stochastic gradients can be used in conjunction with stochastic optimization methods
                             such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to computing stochastic
                             gradients.
                             F.1   Example
                             Let the prior over the parameters and latent variables be the centered isotropic Gaussian pŒ±(Œ∏) =
                             N(z;0,I) and p (z) = N(z;0,I). Note that in this case, the prior lacks parameters. Let‚Äôs also
                                               Œ∏
                             assume that the true posteriors are approximatily Gaussian with an approximately diagonal covari-
                             ance. In this case, we can let the variational approximate posteriors be multivariate Gaussians with
                             a diagonal covariance structure:
                                                                  logq (Œ∏) = logN(Œ∏;¬µ ,œÉ2I)
                                                                        œÜ                    Œ∏   Œ∏
                                                                logq (z|x) = logN(z;¬µ ,œÉ2I)                                        (23)
                                                                     œÜ                      z    z
                                                                                 13
                           Algorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See text for
                           meaning of the functions f , g  and h .
                                                     œÜ œÜ         œÜ
                           Require: œÜ (Current value of variational parameters)
                             g ‚Üê0
                             for l is 1 to L do
                                 x‚ÜêRandomdrawfromdatasetX
                                  ‚ÜêRandomdrawfrompriorp()
                                 Œ∂ ‚ÜêRandomdrawfrompriorp(Œ∂)
                                 g ‚Üêg+ 1‚àá f (x,g (,x),h (Œ∂))
                             endfor        L œÜ œÜ        œÜ        œÜ
                             return g
                           where ¬µ and œÉz are yet unspeciÔ¨Åed functions of x. Since they are Gaussian, we can parameterize
                                   z
                           the variational approximate posteriors:
                                                        e
                                          qœÜ(Œ∏)    as   Œ∏ = ¬µ +œÉŒ∏Œ∂                      where    Œ∂ ‚àº N(0,I)
                                                              Œ∏
                                                        e
                                        q (z|x)    as   z = ¬µ +œÉ                        where     ‚àº N(0,I)
                                         œÜ                    z     z
                           With  we signify an element-wise product. These can be plugged into the lower bound deÔ¨Åned
                           above (eqs (21) and (22)).
                           In this case it is possible to construct an alternative estimator with a lower variance, since in this
                           modelp (Œ∏),p (z),q (Œ∏)andq (z|x)areGaussian,andthereforefourtermsoff canbesolved
                                   Œ±      Œ∏      œÜ          œÜ                                                œÜ
                           analytically. The resulting estimator is:
                                              L      Ô£´ J                                                             Ô£∂
                                           1 X       Ô£≠1X                 (l) 2      (l) 2     (l) 2             (i) (i) Ô£∏
                              L(œÜ;X)'            N¬∑           1+log((œÉ ) )‚àí(¬µ ) ‚àí(œÉ )                +logp (x z )
                                          L            2                 z,j        z,j       z,j           Œ∏
                                             l=1         j=1
                                             J                                       
                                       +1X 1+log((œÉ(l))2)‚àí(¬µ(l))2‚àí(œÉ(l))2                                               (24)
                                          2                 Œ∏,j        Œ∏,j       Œ∏,j
                                            j=1
                            (i)      (i)                                           (i)      (i)
                           ¬µ   and œÉ    simply denote the j-th element of vectors ¬µ   and œÉ   .
                            j        j
                                                                          14
