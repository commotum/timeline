                                                                Stabilizing Equilibrium Models by Jacobian Regularization
                   Table 4. Hyperparameters, optimizer choices, and model details (at training time) for all tasks reported in Section 5. The arrows in the
                    Jacobian regularization strength (e.g., A ‚Üí B) mean that we dynamically increase from A to B over the course of DEQ training.
                                                                Synthetic Dataset      WikiText-103 language modeling           CIFAR-10classiÔ¨Åcation          ImageNetclassiÔ¨Åcation
                               Architecture of f              2-Layer ReLU block               Transformer layer                  Multiscale DEQ layer           Multiscale DEQ layer
                                                 Œ∏                (see Section 5)              (Pre- and Post-LN)               (residual block + fusion)      (residual block + fusion)
                                  #ofEpochs                             50                             23                                  200                            120
                                   Batch Size                           64                             60                                   96                            112
                                   Optimizer                          Adam                           Adam                                 Adam                           SGD
                               Start Learning rate                    0.001                         0.00025                               0.001                           0.05
                             Learning rate warmup                       No                        Yes, 1 epoch                             No                             No
                             Learning rate schedule                   Cosine                         Cosine                              Cosine                         Cosine
                                 Weight Decay                           0                               0                                   0                           5¬∑10‚àí5
                             Hidden dimensionality                      50                    700 (embedding size)              [28,56,112,224] (4 scales)          [32,64,128,256]
                             Input Sequence Length                     N/A                             150                                 N/A                            N/A
                                Input Image Size                       N/A                            N/A                                32√ó32                        224√ó224
                                 Normalization                        None                LayerNorm(Baetal., 2016)           GroupNorm(Wu&He,2018)                    GroupNorm
                                Recurrent Droput                       N/A                            0.06                                 0.25                           0.02
                             Weight Normalization                       No                             Yes                                 Yes                            Yes
                      #ofInput Injection Downsamplings                 N/A                            N/A                                  N/A                             2
                            Forward NFEs Threshold                      6                              12                                   7                              14
                           BackwardNFEsThreshold                        6                              12                                   8                              14
                              Forward Threshold "                     10‚àí3                            10‚àí3                                10‚àí3                           10‚àí3
                             BackwardThreshold "                      10‚àí4                            10‚àí4                                10‚àí4                           10‚àí4
                            Jacobian Reg. Strength Œ≥                {0,1,2,4}                      1.6 ‚àí2.5                                0.5                         2.0 ‚àí 3.0
                           Jacobian Reg. Frequency p                   0.4                            0.35                                 0.05                           0.1
                          MforHutchinsonEstimator                       1                             1or2                                  1                            1or2
                    A.4. Training Setting and Hardware                                                    mator and backpropagate through it; and 2) it helps reduce
                    Ourexperimental protocols are intentionally set to be max-                            the likelihood of the model overÔ¨Åtting on this auxiliary loss
                    imally consistent with prior work (Bai et al., 2019; 2020).                           term (since, as we noted in Section 5.5, the model could be
                    This includes hyperparameters (see the subsection below),                             sensitive to Œ≥, and M is small), which we generally observe
                    other regularization methods (e.g., recurrent dropout (Gal                            to beneÔ¨Åt the performance, though only slightly. Therefore,
                   &Ghahramani, 2016) & group normalization (Wu & He,                                     during training, the model would still proceed in the actual
                    2018)), and initialization schemes (where all parameters                              stochastic gradient direction, and only use the regularized
                    are initialized at the start of training by sampling from                             direction occasionally.
                    N(0;0:01)). For the multiscale DEQs that were used in                                 Formally, the training objective we highlighted in Sec-
                    the image classiÔ¨Åcation task, we used 4 resolutions, where                            tion 4.2 should be:
                                                                                                                                               P
                    each subsequent resolution is of exactly half the height and                                                                  M      >       ?  2
                                                                                                                   ?               ?              m=1k Jf (z )k2
                                                                                                           L (z )=L (z )+œÑ¬∑Œ≥                                 Œ∏        ;     ‚ààN(0;I )
                   width of the previous resolution. Although Bai et al. (2020)                              total          orig                        Md                m             d
                    highlighted the need to train a ReLU-based network with                               where œÑ = Bernoulli(p) is a random variable and M is the
                    softplus for stability purposes, we found it not necessary                            numberofsamplesusedforHutchinson estimator.
                    in our experiments with regularized DEQs, most likely be-                             All experiments in this paper, including the speed and mem-
                    cause of the role Jacobian regularization plays in stabilizing                        ory benchmarks we provide, were conducted on RTX 2080
                    the network convergence.                                                              Ti GPUs. WikiText-103 language modeling and ImageNet
                    Onething to note is that empirically, rather than applying                            classiÔ¨Åcation models (MDEQ-small) were trained with 4
                    the proposed Jacobian regularization on all training itera-                           GPUsinadata-parallel setting.
                    tions, we only randomly and partially apply this auxiliary
                    loss. For example, when we set the auxiliary loss frequency                           A.5. Hyperparameters
                    p to 0.5, only half of the training iterations (randomly se-
                    lected) are trained with the Jacobian regularization term (see                        We report the hyperparameters used at training time in
                    Table 4). This is motivated by the empirical observation that                         Table 4. Except for those used in the synthetic data and
                    Jacobian-related regularizations usually hurt performance,                            for Jacobian regularization, most of the other hyperpa-
                    e.g., as in its application in robust learning (Hoffman et al.,                       rameters were essentially taken from the original DEQ-
                    2019). Therefore, such partial/random supervision with                                Transformer (Bai et al., 2019) and MDEQ (Bai et al.,
                    the Jacobian regularization brings two beneÔ¨Åts: 1) the rest                           2020) without major modiÔ¨Åcations. For both Anderson
                   (1‚àíp)-portionofthetrainingiterationscanpickupafurther                                  andBroydenÔ¨Åxed-pointsolvers,weusetherelativeresidual
                                                                                                           kf (z;x)‚àízk
                    speedup as we don‚Äôt need to compute the Hutchinson esti-                                  Œ∏            as a measure of convergence quality in forward
                                                                                                             kfŒ∏(z;x)k
