                                                             t                            i−1
                                                      r =X(min(s [i],max(0,1−Xs [j])))·V [i]                                       (5)
                                                       t                t                      t          t
                                                            i=1                           j=1
                             3.3   Neural DeQue
                             Aneural DeQue operates likes a neural Stack, except it takes a push, pop, and value as input for
                             both “ends” of the structure (which we call top and bot), and outputs a read for both ends. We write
                               top       bot                 top        bot
                             u    and u     instead of u , v     and v     instead of v , and so on. The state, V and s are now
                               t         t               t   t         t                t                           t       t
                             a 2t × m-dimensional matrix and a 2t-dimensional vector, respectively. At each timestep, a pop
                             from the top is followed by a pop from the bottom of the DeQue, followed by the pushes and reads.
                             The dynamics of a DeQue, which unlike a neural Stack or Queue “grows” in two directions, are
                             described in Equations 6–11, below. Equations 7–9 decompose the strength vector update into three
                             steps purely for notational clarity.
                                               vbot             if i = 1
                                               t
                                      V [i] =     vtop           if i = 2t                                                         (6)
                                       t       t
                                                  V    [i − 1]   if 1 < i < 2t
                                                   t−1
                                                                                   2(t−1)−1
                                       top                                   top      X
                                      s   [i] = max(0,st−1[i] − max(0,u          −           st−1[j]))   if 1 ≤ i < 2(t − 1)       (7)
                                       t                                     t
                                                                                     j=i+1
                                                                                    i−1
                                       both                top                bot   Xtop
                                      s    [i] = max(0,s      [i] − max(0,u      −      s   [j]))   if 1 ≤ i < 2(t − 1)            (8)
                                       t                   t                  t          t
                                                                                   j=1
                                               sboth[i−1]       if 1 < i < 2t
                                                   t
                                      s [i] =     dbot           if i = 1                                                          (9)
                                       t       t
                                                  dtop           if i = 2t
                                                   t
                                              2t                              2t
                                      rtop = X(min(st[i],max(0,1− X st[j])))·Vt[i]                                                (10)
                                       t
                                              i=1                          j=i+1
                                              2t                            i−1
                                      rbot = X(min(s [i],max(0,1−Xs [j])))·V [i]                                                  (11)
                                       t                  t                      t         t
                                             i=1                           j=1
                             To summarise, a neural DeQue acts like two neural Stacks operated on in tandem, except that the
                             pushes and pops from one end may eventually affect pops and reads on the other, and vice versa.
                             3.4   Interaction with a Controller
                             Whilethethreememorymodulesdescribedcanbeseenasrecurrentlayers,withtheoperationsbeing
                             usedtoproducethenextstateandoutputfromtheinputandpreviousstatebeingfullydifferentiable,
                             they contain no tunable parameters to optimise during training. As such, they need to be attached
                             to a controller in order to be used for any practical purposes. In exchange, they offer an extensible
                             memory,thelogical size of which is unbounded and decoupled from both the nature and parameters
                             of the controller, and from the size of the problem they are applied to. Here, we describe how any
                             RNNcontroller may be enhanced by a neural Stack, Queue or DeQue.
                             Webegin by giving the case where the memory is a neural Stack, as illustrated in Figure 1c. Here
                             we wish to replicate the overall ‘interface’ of a recurrent layer—as seen from outside the dotted
                             lines—which takes the previous recurrent state H          and an input vector i , and transforms them
                                                                                  t−1                        t
                             to return the next recurrent state H and an output vector o . In our setup, the previous state H
                                                                  t                         t                                     t−1
                             of the recurrent layer will be the tuple (h    , r   , (V    , s    )), where h     is the previous state
                                                                        t−1    t−1     t−1   t−1             t−1
                             of the RNN, rt−1 is the previous stack read, and (Vt−1, st−1) is the previous state of the stack
                             as described above. With the exception of h0, which is initialised randomly and optimised during
                             training, all other initial states, r0 and (V0, s0), are set to 0-valued vectors/matrices and not updated
                             during training.
                                                                                 5
