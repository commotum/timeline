                Pan et al.                                                                                                                    3
                bidirectional spatial decay matrix grounded in Manhattan         SOLOFusion (Park et al., 2022) and ViideoBEV (Han et al.,
                distance, enriching the model with dynamic prior knowledge       2024) investigate the application of long-term temporal fusion
                concerning variations in distance. This innovation empowers      for advanced multi-view 3D perception, showcasing the evol-
                the self-attention mechanism to pinpoint spatial relationships   ving landscape of BEV technology in understanding complex
                with heightened precision in the context of global information   environments.
                modeling, thus aligning with the intricate demands of 3D            DETR3D (Wang et al., 2021b) marks the advent of the
                object detection tasks.                                          first transformer-based BEV technique, which establishes
                   In particular, RetentiveBEV employs a decay weight            object queries within a 3D space and leverages a transformer
                matrix, D2d, derived from camera imagery to facilitate the       decoder to extract learnings from features across multiple
                          nm
                computational processes of the attention mechanism, as deli-     image viewpoints. On DETR3D’s foundation, PETR (Liu
                neated in equation (6). This method underscores a refined        et al., 2022) has further refined this approach by incorporating
                approach to embedding spatial awareness within the model,        positional embedding transformations. BEVFormer (Li et al.,
                ensuring a nuanced understanding of spatial relationships,       2022) advances the field by adaptively synthesizing BEV fea-
                pivotal for enhancing the precision and reliability of 3D        tures from the spatiotemporal characteristics captured by
                object detection.                                                cameras from various perspectives, thereby reducing its reli-
                                                                                 ance on explicit depth cues and 3D assumptions. UniAD (Hu
                   2D-to-3D transform. Extracting cues and features from         et al., 2023) expands on BEVFormer’s capabilities, facilitating
                2D images gathered by single or multiple cameras represents      multi-task learning within the BEV spatial context. Despite
                a straightforward strategy for performing environment per-       the impressive efficacy shown by these BEV perception meth-
                ception tasks in autonomous driving. However, feature trans-     odologies, they typically initialize using models either pre-
                formation methods based on views from a single camera fall       trained on ImageNet with single-view images (Russakovsky
                short by not providing a unified space for information repre-    et al., 2015) or through deep pre-training techniques (Park
                sentation. This requires individual processing of the data col-  et al., 2021), highlighting a common foundation in their
                lected by different cameras, which compromises the efficiency    development.
                of both training and inference processes.                           Among the transformer-based BEV techniques, BEV
                   BEVserves as a unified framework for spatial representa-      Former stands out by nearly matching the performance of
                tion. Utilizing BEV requires the reconstruction of depth infor-  previous LiDAR-based methods, setting a reference point for
                mation from camera images, either through direct depth           further innovations. However, the computational demands of
                estimation or supported by categorized depth estimation, to      BEVFormer and its successors inspire ongoing research into
                enable the transformation of image features into BEV fea-        optimizing BEV representation learning for greater efficiency.
                tures. Specifically, the transformation of image features into
                BEVfeatures typically involves several key steps. First, depth   Main contribution
                information is used to project 2D image features into a 3D
                space, creating a point cloud or voxel grid that represents the  To incorporate spatial priors directly within the attention
                scene from multiple perspectives. These 3D representations       mechanism, the image utilize a 2D bidirectional spatial
                are then mapped onto a BEV plane by aggregating features         decay matrix based on the Manhattan distance before for-
                from different views. In many approaches, neural networks        ward propagation process, introducing the concept of
                such as CNNs or transformers are employed to refine the          Manhattan Self-Attention (MaSA): the greater the distance
                BEV features, enhancing spatial coherence and accuracy. In       from a target token, the more significant the decay in atten-
                autonomous driving, visual-input-based 3D perception BEV         tion weight for the other tokens. This feature ensures that
                approaches have drawn significant interest recently (Huang       while global information is processed, varying levels of
                et al., 2022b; Jiang et al., 2023; Zhang et al., 2022). These    attention are allocated to the tokens based on proximity.
                approaches fall into two primary categories: set-based meth-     Addressing the considerable computational load posed by
                ods, which employ geometric relationships for the 2D to 3D       modeling global information with traditional attention
                transformation, and learning-based methods, which leverage       mechanisms, numerous studies (Liu et al., 2021; Tu et al.,
                deep learning networks, such as transformers, for accomplish-    2022; Zhu et al., 2023) have attempted solutions, yet often
                ing the conversion.                                              at the expense of disrupting the spatial decay matrix essen-
                   LSS (Philion and Fidler, 2020), a geometry-based              tial for embedding spatial priors within MaSA. To circum-
                approach, generates spatial point clouds from categorized        vent this, the retentive mechanism employs a decomposition
                depth estimation, transforming each image into feature frus-     approach along the image’s horizontal and vertical axes.
                tums for each camera and merging them into a rasterized          This approach enables MaSA to model global information
                BEV view. Extending LSS’s capabilities, BEVDet (Huang            efficiently with linear computational overhead and retain
                et al., 2022b) integrates techniques for augmenting both image   the original MaSA’s receptive field. Consequently, we intro-
                views and BEV data. BEVDepth (Li et al., 2023b) improves         duce the Retentive BEV, a BEV encoder leveraging the
                the quality of its BEV features by incorporating explicit depth  retentive mechanism. It is designed to explicitly furnish spa-
                cues from LiDAR, underscoring the significance of depth          tial  prior  information while bolstering the attention
                information in BEV perception. BEVStereo (Li et al., 2023a)      mechanism, allowing for global information modeling of
                and STS (Wang et al., 2022c) enhance depth accuracy using        the BEV perspective with linear time complexity. Key fea-
                temporal    multi-view   stereo   techniques.   Furthermore,     tures of our RetentiveBEV include the following:
