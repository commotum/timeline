                            Here P = {P1,...,Pn} is a sequence of n vectors and CP = {C1,...,Cm(P)} is a sequence of
                            m(P)indices, each between 1 and n.
                            The parameters of the model are learnt by maximizing the conditional probabilities for the training
                            set, i.e.                                      X
                                                            θ∗ = argmax         logp(CP|P;θ),                                (2)
                                                                      θ    P,CP
                            where the sum is over training examples.
                            As in [1], we use an Long Short Term Memory (LSTM) [11] to model p (C |C ,...,C              , P;θ).
                                                                                                      θ   i  1        i−1
                            TheRNNisfedPi ateachtimestep,i,untiltheendoftheinputsequenceisreached, at which time
                            a special symbol, ⇒ is input to the model. The model then switches to the generation mode until
                            the network encounters the special symbol ⇐, which represents termination of the output sequence.
                            Note that this model makes no statistical independence assumptions. We use two separate RNNs
                            (one to encode the sequence of vectors P , and another one to produce or decode the output symbols
                                                                     j
                            Ci). We call the former RNN the encoder and the latter the decoder or the generative RNN.
                            During inference, given a sequence P, the learnt parameters θ∗ are used to select the sequence
                             P                                     P                 P      ∗
                            ˆ                                     ˆ                                                            ˆ
                            C with the highest probability, i.e., C   =argmaxp(C |P;θ ). Finding the optimal sequence C
                                                                            CP
                            is computationally impractical because of the combinatorial number of possible output sequences.
                            Instead we use a beam search procedure to ﬁnd the best possible sequence given a beam size.
                            In this sequence-to-sequence model, the output dictionary size for all symbols Ci is ﬁxed and equal
                            to n, since the outputs are chosen from the input. Thus, we need to train a separate model for each
                            n. This prevents us from learning solutions to problems that have an output dictionary with a size
                            that depends on the input sequence length.
                            Under the assumption that the number of outputs is O(n) this model has computational complexity
                            ofO(n). However,exactalgorithmsfortheproblemswearedealingwitharemorecostly. Forexam-
                            ple, the convex hull problem has complexity O(nlogn). The attention mechanism (see Section 2.2)
                            adds more “computational capacity” to this model.
                            2.2   Content Based Input Attention
                            The vanilla sequence-to-sequence model produces the entire output sequence CP using the ﬁxed
                            dimensional state of the recognition RNN at the end of the input sequence P. This constrains
                            the amount of information and computation that can ﬂow through to the generative model. The
                            attention model of [5] ameliorates this problem by augmenting the encoder and decoder RNNs with
                            an additional neural network that uses an attention mechanism over the entire sequence of encoder
                            RNNstates.
                            For notation purposes, let us deﬁne the encoder and decoder hidden states as (e ,...,e ) and
                                                                                                                 1       n
                            (d ,...,d      ), respectively. For the LSTM RNNs, we use the state after the output gate has
                              1       m(P)
                            been component-wise multiplied by the cell activations. We compute the attention vector at each
                            output time i as follows:
                                                      i        T
                                                    u    = v tanh(W1ej +W2di) j ∈(1,...,n)
                                                      j
                                                      i                 i
                                                    a    = softmax(u )                   j ∈ (1,...,n)                       (3)
                                                      j                 j
                                                               n
                                                      0       Xi
                                                    d    =        a e
                                                      i            j j
                                                              j=1
                                                                    i
                            where softmax normalizes the vector u (of length n) to be the “attention” mask over the inputs,
                            and v, W , and W are learnable parameters of the model. In all our experiments, we use the same
                                     1        2
                            hidden dimensionality at the encoder and decoder (typically 512), so v is a vector and W and W
                                                                                                                      1        2
                            are square matrices. Lastly, d0 and d are concatenated and used as the hidden states from which we
                                                         i      i
                            makepredictions and which we feed to the next time step in the recurrent model.
                            Note that for each output we have to perform n operations, so the computational complexity at
                            inference time becomes O(n2).
                                                                             3
