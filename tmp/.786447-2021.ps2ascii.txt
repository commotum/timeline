              ANeurosymbolic Approach to
                Abstraction and Reasoning
                            by
                         Simon Alford
               B.S. Mathematics and Computer Science and Engineering
                  Massachusetts Institute of Technology, 2020
          Submitted to the Department of Electrical Engineering and Computer Science
               in partial fulﬁllment of the requirements for the degree of
               Master of Engineering in Electrical Engineering
                      and Computer Science
                           at the
                  Massachusetts Institute of Technology
                          June 2021
             ©Massachusetts Institute of Technology 2021. All rights reserved.
         Signature of Author: .............................................................
                    Department of Electrical Engineering and Computer Science
         Certiﬁed By: .....................................................................
                                         Tomaso Poggio
                        Eugene McDermott Professor in the Brain Sciences
                                        Thesis Supervisor
         Accepted By: ....................................................................
                                        Katrina LaCurts
                          Chair, Master of Engineering Thesis Committee
                            1
           ANeurosymbolic Approach to Abstraction and
                            Reasoning
                                by
                            Simon Alford
           Submitted to the Department of Electrical Engineering and Computer Science
            on May 20, 2021 in partial fulﬁllment of the requirements for the degree of
              Master of Engineering in Electrical Engineering and Computer Science
          Abstract
          Current deep learning systems are highly specialized to whatever task they are de-
          signed to solve. Their application to more general domains is limited by their inability
          to form explicit, systematic knowledge and reason over it. Such an ability would be
          required for a machine to, for instance, rediscover the scientiﬁc method, and use
          this method to learn new things. This thesis attempts to make progress on this
          front by developing an approach for the Abstraction and Reasoning Corpus (ARC),
          an artiﬁcial intelligence benchmark consisting of a set of few-shot visual reasoning
          tasks which measures the ability for an agent to solve tasks beyond those speciﬁed
          by the developer. We present two approaches that address that challenges posed
          by ARC. First, we give an approach for learning abstractions on ARC. We apply a
          program synthesis system called DreamCoder to create symbolic abstractions out of
          the solutions of tasks solved so far. These abstractions enable the solving of progres-
          sively more diﬃcult ARC tasks. Second, we design a reasoning algorithm for ARC
          motivated by the way humans approach solving ARC tasks. Our algorithm com-
          bines execution-guided program synthesis with deductive reasoning based on inverse
          semantics, enabling a bidirectional, execution-guided program synthesis algorithm
          for solving ARC tasks. Despite diﬃculty ultimately achieving high performance on
          ARC, we believe the approach is a ﬁrm basis for a learning-based search algorithm
          for ARC, especially compared to existing brute-force approaches. We additionally
          evaluate the bidirectional algorithm on a set of “24 Game” style math puzzles. We
          conclude by discussing how these two approaches can be combined as well as future
          research directions relevant to ARC and AI in general.
          Thesis Supervisor: Tomaso Poggio
          Title: Professor
                                2
                    Contents
                    Acknowledgements                                                                          5
                    1 Introduction                                                                            6
                    2 The Abstraction and Reasoning Corpus                                                  10
                        2.1  Challenges of ARC . . . . . . . . . . . . . . . . . . . . . . . . . . . .       11
                        2.2  Solving ARC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .       11
                    3 Abstraction                                                                           12
                        3.1  DreamCoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .      14
                        3.2  Enabling Generalization through Compression . . . . . . . . . . . . .           14
                        3.3  Enabling generalization on ARC symmetry tasks . . . . . . . . . . . .           15
                        3.4  Neural-guided synthesis . . . . . . . . . . . . . . . . . . . . . . . . . .     16
                        3.5  Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    17
                    4 Reasoning                                                                             19
                        4.1  Execution-guided Program Synthesis . . . . . . . . . . . . . . . . . .          21
                        4.2  Deductive reasoning via inverse semantics       . . . . . . . . . . . . . . .   23
                        4.3  Bidirectional, Execution-guided Program Synthesis         . . . . . . . . . .   24
                        4.4  Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   27
                    5 Discussion                                                                            31
                    A Appendix                                                                              33
                        A.1 A bidirectional grammar for ARC tasks . . . . . . . . . . . . . . . . .          33
                             A.1.1 List operations . . . . . . . . . . . . . . . . . . . . . . . . . .       34
                        A.2 Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     36
                                                                 3
                    List of Figures
                        1     Failure of MLP’s to generalize outside training range        . . . . . . . . .   7
                        2     ARCexample task . . . . . . . . . . . . . . . . . . . . . . . . . . . .         10
                        3     ARCabstraction examples . . . . . . . . . . . . . . . . . . . . . . . .         13
                        4     ARCreasoning examples . . . . . . . . . . . . . . . . . . . . . . . . .         13
                        5     Sample tasks involving applying an action towards the left . . . . . .          15
                        6     Learned symmetry abstractions . . . . . . . . . . . . . . . . . . . . .         17
                        7     Challenging four-way mirror task . . . . . . . . . . . . . . . . . . . .        17
                        8     Neural-guided synthesis . . . . . . . . . . . . . . . . . . . . . . . . . .     18
                        9     ARCtasks involving denoising and reasoning . . . . . . . . . . . . . .          19
                        10    Task 173 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    20
                        11    Task 303 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    21
                        12    ARCexecution-guided walkthrough . . . . . . . . . . . . . . . . . . .           22
                        13    Example inverse and conditional inverse        . . . . . . . . . . . . . . . .  23
                        14    ARCsymmetry tasks used in bidirectional experiment             . . . . . . . .  29
                        15    Task 148 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    34
                        16    Task 168 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    35
                        17    Task 10 bidirectional demo . . . . . . . . . . . . . . . . . . . . . . . .      37
                    List of Tables
                        1     Primitives used to solve ARC symmetry tasks. . . . . . . . . . . . . .          28
                        2     Percent of tasks solved for 24 Game . . . . . . . . . . . . . . . . . . .       30
                        3     24 test task result . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   31
                                                                  4
                     Acknowledgements
                     There are many individuals who deserve recognition for their contributions to this
                     thesis.  Thank you to Professor Tomaso Poggio for agreeing to take me on as a
                     master’s student and for providing the vibrant community in which I could pursue
                     research. Thank you to Andrzej Banburski, my amazing direct advisor, who has
                     helped me discover and explore my research interests and is a constant source of
                     animated research conversations.
                        Thank you to Anshula Gandhi, Akshay Rangamani, and Tony Wang for their
                     contributions to the ARC project. Anshula and I worked side-by-side for the majority
                     of the project. She contributed to practically every part of it—working with her was
                     a joy.  Akshay helped with bidirectional project, implementing network training,
                     ﬁnding hyperparameters, and providing much-appreciated organizational guidance.
                     Tonyalso helped with the bidirectional project, implementing reinforcement learning
                     and many general system contributions. His programming and research wisdom had
                     a large impact.
                        ThankyoutothestudentsSyleeDandekar,SubhashKantemneni,FadumaKhalif,
                     and Noa Korne`ev, who helped program various aspects of the project and who I
                     enjoyed working with tremendously. Thank you to Peter Chin, who helped advise
                     and organize the project. Thank you also to Kevin Ellis, whose research is a large
                     inﬂuence and inspiration for the present thesis and who has graciously discussed
                     many research ideas with me. Lastly, thank you to my family, especially my parents
                     for supporting me throughout MIT, and my grandparents for cultivating my scientiﬁc
                     future.
                                                                   5
        1  Introduction
        What would it take to develop generally intelligent machines? The growth and
        tremendous success of deep learning has catapulted us past many benchmarks of ar-
        tiﬁcial intelligence. Reaching human and superhuman performance in object recog-
        nition, language generation and translation, and complex games such as Go and
        Starcraft have pushed the boundaries of what humans can do and machines cannot
        [12, 9, 5, 14, 17, 21]. To continue to make progress, we must isolate and tackle the
        ways in which humans remain superior to machines.
          One feature shared in common among the machine learning systems listed above
        is notably deﬁcient to humans: they all in many aspects remain highly specialized
        for a speciﬁc task, and excel only in the domain they are designed to operate in—a
        sharp contrast to humans. Now, it is not the underlying neural network architectures
        which meaningfully diﬀer from task to task. The feedforward structure of networks
        andtheir functional role is essentially shared between all tasks. Indeed, the successful
        application of language-model architectures to computer vision tasks and vice versa
        is a reassuring sign that task-to-task variance of minute network architectural details
        is not likely to be a roadblock to developing more general-purpose systems down the
        road.
          Instead, task-speciﬁc specialization inhibitive to general applicability occurs in
        what might be designated the symbolic, rule-based behavior of the systems. For ex-
        ample, AlphaZero, DeepMind’s AI for playing chess and go, uses a standard residual
        convolutional network to predict good board positions and combines this with a form
        of Monte Carlo tree search [17]. The network weights are learnable, but the search
        strategy is essentially ﬁxed. As another example, a SAT solver might be provided to
        assist a neural network with solving Sudoku puzzles[22]. This solver knows from the
        start how to work; the only learning takes place in the neural network. Importantly,
        these symbolic parts of the systems diﬀer depending on the task.
          Why not have neural networks directly learn this aspect of the system as well?
        Unfortunately, despite their successes, neural networks have been found to struggle
        at tasks involving systematic, rule-based behavior. While eﬀective at learning to
                          6
        ral language model[2], which astonished the community with its ability to generate
        hyperrealistic text. Despite containing more parameters than the human brain has
        neurons and being trained on a large swath of data from the internet, GPT-3 can
        add two digit numbers with 100% accuracy, but ﬁve digit numbers with only 10%
        accuracy. Simply increasing the amount of training data and the size of the network
        does not appear to feasible if one wishes to solve a large variety of symbolic tasks
        eﬀectively.
          For another example, we can consider Conway’s Game of Life, a cellular automa-
        ton where cells in a grid update their state to black or white based on the state of
        their neighbors. If we provide a network with the “systematic rule” for predicting
        the state of a future cell—that a cell’s state at time t+1 depends only on the neigh-
        boring cell values at time t—the problem is trivial for a neural network to learn, as
        it involves learning a lookup table for possible conﬁgurations of eight Boolean cells.
        However, if instead we task a convolutional network with predicting the next state
        for an entire grid at once, without the rule, then networks fail to converge to an
        accurate prediction that properly generalizes to arbitrary grid sizes[19].
          One way to look at this failing of neural networks is that they are simply mem-
        orizing the training data. Symbolic reasoning, in contrast, consists of rules which
        can be applied outside the distribution: a memorized solution to addition consists of
        a cleverly represented lookup-table, while a symbolic solution consists of an explicit
        procedure which adds digit-by-digit, keeping track of the carry value as it goes—a
        procedure which generalizes to arbitrary input lengths. This is how humans solve
        symbolic tasks, and there is reason to believe AI agents need to acquire this ability
        to learn such procedures if they are to succeed on symbolic tasks.
          Astriking characterization of the interplay between symbolic reasoning and the
        pattern recognition done by neural networks is described in dual-process theories of
        reasoning[18, 8, 11] which suggest that our abilities stem from the interaction between
        a fast, associative system (similar to that of modern deep nets) and a slower symbolic
        system. Neural networks excel at conducting associative reasoning, but are not suited
        to conducting rule-based reasoning in their computations. Current systems fail at
        systematicity and generalization, having an inability to form declarative, systematic
                          8
                  knowledge and reason over it to solve new problems. Such an ability would be
                  required for a machine to, for example, rediscover the scientiﬁc method, and use this
                  method to learn new things. As such, it is an important problem to address in the
                  design of more generally intelligent systems.
                     How might we combine learning of symbolic rules with machine learning to pro-
                  duce more capable artiﬁcial intelligence systems?  This thesis attempts to make
                  progress in this direction. To begin to solve this large challenge, it is helpful to have
                  a benchmark that appropriately measures the ability we are seeking. Importantly, it
                  mustbedesignedinawaytopreventdevelopersfromhard-codingtheknowledgeinto
                  the system 1. The Abstraction and Reasoning Corpus, developed by Fran¸cois Chollet
                  and henceforth referred to as ARC, is an excellent test of this type of learning.
                     WepresenttwoapproachesforsolvingARCtasksdesignedtoaidinbothabstrac-
                  tion and reasoning. In Section 2, we describe the Abstraction and Reasoning Corpus.
                  We discuss the relevance of the corpus of tasks with respect to the challenges put
                  forward above, and outline existing work on the dataset. In Section 3 and Section 4,
                  we develop two approaches and sets of results on ARC. The ﬁrst approach involves
                  learning abstractions of symbolic concepts out of the solutions of tasks solved so far
                  using a program synthesis system called DreamCoder. However, the type of search
                  DreamCoder uses to discover solutions is weak and does not scale to solving general
                  ARC problems. To address this, the second section develops a reasoning approach
                  for ARC motivated by the reasoning humans exhibit when discovering solutions to
                  ARCtasks. Ourapproachcombinesexecution-guided program synthesis with deduc-
                  tive reasoning based on inverse semantics, enabling a bidirectional program synthesis
                  algorithm for ARC. We present preliminary results on ARC problems with this ap-
                  proach, as well as application to a simpler domain of solving tasks from the “24
                  Game” family of puzzles. We conclude in Section 5 by discussing how these two
                  approaches can be combined as well as future research directions relevant to ARC
                  and AI in general.
                    1Thisisnotasmuchofanissueforbenchmarksinvolvingobjectrecognition,languagegeneration,
                  etc. where hard-coding a recognizer yields poor performance. In contrast, systematic tasks are by
                  nature easier to program.
                                                          9
              objectness, simple arithmetic abilities, symmetry, or goal-directedness.
                 Chollet hosted a Kaggle-competition and the winning solution, a hard-coded
              brute force approach, achieved only ∼ 20% performance on the private test set [1].
              At the time of writing, we are not aware of any improvements upon this mark.
              2.1  Challenges of ARC
              The Abstraction and Reasoning Corpus has a number of features which make it
              quite a challenging benchmark. To start, since each task is a few-shot problem
              whose solution is symbolic and mostly rule-based, neural networks are extremely
              poorly equipped for the benchmark. Second, the tasks in the private test set are
              not from the same distribution as the tasks in the training set. This prevents a
              developer from succeeding by hard-coding a program that can solve tasks in the
              training set unless that program learns to do so from the training set itself, or does
              so based on some sort of universal problem-solving technique that is not tailored to
              the observed idiosyncrasies of the training set. Last, the search for a solution to a
              task may be easy for humans, but for any rule-based approach, the search quickly
              explodes exponentially in a way which is infeasible without close supervision.
              2.2  Solving ARC
              As laid out in the paper, ARC is best thought of as a program synthesis benchmark.
              The most straightforward way to approach solving a task is to seek to write a pro-
              gram that converts the input grid to the output grid for each training example, and
              then evaluate this program on the test input grids. This is the general framework
              we will assume. Such an approach starts with a Domain Speciﬁc Language of use-
              ful functions—in this case, the DSL could be based on the Core Knowledge priors
              that form the foundation of ARC. These functions are then combined to produce
              programs. For example, the solution to the task in Figure 2 could be written in
              Pythonic style as
              f(in) = get_first(sort(map(objects(in),
                                             11
                      lambda obj: freq(obj, objects(in))))
           or more simply as f(in) = most_common_element(get_objects(in)). There is
           a trade-oﬀ here: starting with a universal (fundamentally, all we need is Turing-
           completeness) set of functions ensures any task solution is expressible, but solution
           programs may be so long that any search for them is prohibitively expensive. On
           the other hand, introducing specialized functions to reduce the description length of
           solution programs may not generalize well to the test set.
             To ensure the rigor of the benchmark, Chollet organized a Kaggle competition
           around it. The winning solution used a brute-force approach which solves roughly
           60% of the training tasks but only 20% of of the private test tasks[10]. To this day,
           this solution remains state of the art. In the words of the winner, “Unfortunately, I
           don’t feel like my solution itself brings us closer to AGI”.
             As one might expect, two important components underlying a proper solution to
           ARCare the ability to conduct abstraction and reasoning. We brieﬂy showcase this
           through a few examples. Figure 3 demonstrates the way concepts are introduced and
           developed through interrelated tasks. Figure 4 shows a task whose solution requires
           multiple steps of reasoning to uncover.
             In what follows, we will design approaches that address these two necessary com-
           ponents.
           3  Abstraction
           Our ﬁrst approach to ARC addresses the central desire of our system: the ability
           to learn symbolic rules from data. To do so we use DreamCoder, a recent tool for
           program synthesis [7]. What follows consists of essentially applying DreamCoder as
           designed to ARC without any signiﬁcant modiﬁcations. We show that it is well-
           suited to solving ARC tasks, and crucially enables the ability of learning the types
           of abstractions found in ARC.
                                  12
              3.1  DreamCoder
              DreamCoder is a program synthesis engine; it solves tasks by learning to write pro-
              grams which convert, in this case, the input grid into the target grid. As it solves
              tasks, it learns new abstractions through “compression”, an algorithm which distills
              higher-level functions out of existing task solutions. This allows DreamCoder to solve
              new tasks that it would not have been able to solve with its original library. Dream-
              Coder also trains a neural network to learn to recognize which functions are most
              likely to solve a given task. Together, compression and neural-guided synthesis allow
              DreamCoder to gradually acquire expertise in an area. For example, it rediscovers
              laws of classical physics (including Newton’s and Coulomb’s laws) from much simpler
              functions, by compositionally building on concepts from those learned earlier.
                 As a simple example, given only an addition function, DreamCoder can learn to
              solve multiplication tasks through repeated addition. Then, during “compression,”
              it refactors these multiplication programs to express them in terms of a higher-level
              multiplication function. This new function can be used to solve more diﬃcult tasks
              such as calculating exponents or factorials.
              3.2  Enabling Generalization through Compression
              The compression component of DreamCoder is crucial to our program synthesis
              approach. After each iteration of attempting to solve ARC tasks, our agent looks
              at all of the correct programs, notices structures that were similar between diﬀerent
              solved programs, and then re-writes new, higher-level programs based on lower-level
              programs.
                 Compression enables our agent to learn new techniques and behaviors based on
              the tasks it is solving, rather than being limited to the tools the developer provided
              it with. This type of generalization ability is at the heart of the ARC challenge —
              creating a machine that quickly learns to solve problems its developers might not
              have anticipated.
                 Wedemonstrate how the synthesizer can create more abstract concepts from ex-
              isting ones in the following experiment. First, we supply our agent with six synthetic
                                             14
              grids vertically with stack_vertically, and getting the left half of a grid with
              left_half. We then train our agent on a subset of 36 ARC training tasks involving
              symmetryoverﬁveiterationsofenumerationandcompression. Duringeachiteration,
              our agent attempts to solve all 36 tasks by enumerating possible programs for each
              task. It then runs compression to create new abstractions. During the next iteration,
              the agent tries to solve all tasks again within the same amount of time but equipped
              with the new abstractions. In this experiment, our agent solves 16 tasks before any
              training. After one iteration, it solves 17 in the same amount of time. After another,
              it solves 19 tasks, and after the ﬁnal iteration, it solves 22.
                 After each iteration, our agent learns new abstractions which help it solve tasks
              that were previously too diﬃcult. Thus, the DreamCoder compression framework
              enables our agent to learn interpretable, compositional abstractions not provided by
              the developer, such as ﬂipping horizontally, rotating counter-clockwise, and stacking
              grids horizontally. It uses these new abstractions to solve progressively harder tasks.
              Themostdiﬃculttaskssolved involve mirroring the input grid four ways, requiring a
              synthesized program which is extremely long when expressed in terms of the original
              functions. Such a program would be very diﬃcult to discover without compression
              due to its length and the exponential nature of program search.
                 This experiment shows a promising path towards the developer-aware generaliza-
              tion required to succeed on the ARC dataset. In order to solve unknown tasks in the
              test set, our agent will need to learn from the tasks themselves. As shown in this
              experiment, DreamCoder is able to learn new concepts based on tasks given, which
              enable it to solve more diﬃcult tasks.
              3.4  Neural-guided synthesis
              Guiding program enumeration with a neural network is a commonly used program
              synthesis technique to speed up search, and is included in DreamCoder’s synthesis
              approach. We showcase the appropriateness of this approach for the ARC dataset by
              comparing neural-guided synthesis with brute-force enumeration on a set of artiﬁcial
              ARC-like tasks involving sorting blocks of various sizes. Training a neural network
                                             16
                  common situation is when a function is conditionally invertible: given one or more
                  inputs to the function and the output, one can deduce the remaining inputs needed
                  such that when the function is evaluated the output is produced. A surprisingly large
                  number of functions are conditionally invertible; perhaps the most familiar family is
                  arithmetic operators: if we know 1 + x = 5, we can deduce that x = 4. An example
                  relevant to ARC is shown in 13.
                     The deductive reasoning in our motivating examples also takes the form of con-
                  ditionally invertible functions. For task 173, one might write the solution as
                  out = kronecker(in, filter_color(in, most_common_color(in)))
                  In step 1, we determine that the output is the Kronecker product of the input grid
                  with another grid. In step 2, we deduce the second input to kronecker by means
                  of its conditional invertibility 2. Likewise, given the target grid which is the second
                  input to kronecker, we deduce the second argument to filter_color conditioned
                  on the ﬁrst argument being the input grid.
                     For task 303, one might write the solution as
                  out = filter(objects(in), lambda obj: has_vertical_symmetry(obj))
                  The conditional inverse of filter tells us whether objects in the ﬁrst argument
                  must ﬁlter to true or false. To make this ﬁt in our deductive framework, it is con-
                  venient to have filter take a list of Boolean values as the second argument. This
                  list of Booleans can then be produced as an intermediate target by the conditional
                  inverse of filter, and is satisﬁed by mapping has_vertical_symmetry to the list
                  objects(in). More details on designing a conditionally invertible DSL for ARC can
                  be found in Appendix A.1.
                  4.3    Bidirectional, Execution-guided Program Synthesis
                  With the addition of the deductive reasoning powered by inverse and conditional
                  inverse functions, we are ready to present the bidirectional, execution-guided algo-
                  rithm for program synthesis. Like the approach in [6, 23], we approach the synthesis
                     2There are some subtleties with the conditional inverse presented. The conditional inverse of
                  kronecker is unique up to changes in color of the second argument.
                                                          24
        task via reinforcement learning. We assume we have a grammar G of functions, each
        of which has an arity, a type signature, and associated inverse or conditional inverse
        functions. This includes constants of arity zero. For example, the subtraction func-
        tion has two conditional inverses: one which produces the ﬁrst argument conditioned
        on the second, and vice versa.
          The underlying Markov Decision Process has the following components:
          The current state is a graph of nodes. Each node represents either an input
        value, the output value, or an intermediate value resulting from the execution of an
        operation in the forwards or backwards direction, and has a corresponding program
        informing how that node is created. A node is grounded if there is a valid program
        to create that node from the operations applied so far.
          For ARC, each “value” is a set of examples, each of which is a grid. Applying
        an operation implicitly applies the operation to each example in the set to produce
        a new set of examples as a new value.
          An operation is a function from the grammar along with a designation of being
        appliedinforwards, inverse, or as a conditional inverse (and if as a conditional inverse,
        conditioned on which input arguments). There are three types of operations: forward
        operations, inverse operations, and conditional inverse operations.
          For an inverse operation, the sole argument will be the output value. For condi-
        tional inverse operations, the ﬁrst argument is the output, and the other arguments
        are a subset of the inputs.
          Aforward operation applies the function to a set of grounded inputs to produce a
        new grounded node. We restrict our functions to have a single output. An invertible
        operation takes an ungrounded output and produces a new ungrounded target node
        such that grounding the target node will cause the output node to be grounded
        as well. A conditionally invertible operation takes an ungrounded output and one
        or more grounded input nodes, and produces a new ungrounded target node such
        that grounding the target node will cause the output node to be grounded as well.
        All invertible and conditionally invertible operations have a corresponding forward
        operation.
          Multiple examples are placed in the same node. When an operation is applied, it
                          25
                  calculates the result for each example, and equality testing is done based on whether
                  all examples match each other.
                     For a task with n input values per example and a single output value, the initial
                  state consists of n grounded value nodes and one ungrounded target node. In general,
                  grounded nodes correspond to those from the bottom-up side of the search, while
                  ungrounded nodes correspond to those from the top-down side of the search.
                     Solving a given task thus consist of an episode in the MDP where the agent seeks
                  to solve the task by grounding the output value node. Actions in the MDP correspond
                  to a choice of operation and the choice of arguments for that operation. Each action
                  applies a function in either the forward or backward direction.  Intuitively, this
                  executes a bidirectional search to try to connect the grounded nodes on one side
                  with the ungrounded output node on the other.
                     The reward for solving a task is R. A penalty of −1 is applied for attempting
                  to apply any invalid operation.  Invalid operations may be caused by a number
                  of causes, including type errors, using an ungrounded node as input to a forward
                  function, using a grounded node as improper input to an inverse or conditional
                  inverse function, applying an operation which creates a node that already exists,
                  or providing an invalid argument to an operation—for example, providing 3 as the
                  conditioning argument to the inverse of an integer multiplication op when the output
                  is not divisible by three. We can even keep track of the type of values, and give errors
                  for violating the type signature of functions.
                     Unlike [6], we refrain from garbage-collecting nodes used as arguments to a for-
                  ward operation, or correspondingly garbage-collecting nodes that are the designated
                  output of an inverse or conditional inverse operation.
                     Given a sequence of actions that solve a task, only the subset of the actions
                  corresponding to the functions used in the ﬁnal program are important to solve
                  the task. We can give a shaped reward function which gives reward R/|P| to the
                  operations used in the ﬁnal program, where |P| is the length of the ﬁnal program,
                  and zero otherwise (unless a penalty is applicable). Dividing by the length of the
                  program encourages shorter solutions. We expected this reward to work better than
                  standard reward-to-go with exponential decay due to the more targeted feedback.
                                                         26
              Network and training
              Theneuralnetworkreceivesasinputasetof(node_value, node_grounded)tuples,
              chooses one of O operations to apply and selects M arguments for the operation,
              where M is the arity of the operation. Each argument is a value node from those
              provided. We can constrain our network to only choose arguments whose types
              match the type signature of the chosen operation by automatically masking invalid
              options in the same manner as [23].
                 Our network architecture mirrors that used by [6], with a network for embedding
              value nodes, a deep set embedding for the set of value nodes, and a pointer network
              for choosing arguments to an operation.
                 Wefound the combination of supervised pretraining and ﬁne-tuning with REIN-
              FORCE used in [6] to work best for training the model. An important diﬀerence
              is that generating random bidirectional programs is more complicated than random
              forward programs. To do so, we randomly apply forward operations, choose a ran-
              domnodeamongtheoutputs, and recover the sequence of actions used to create this
              node. We then convert this forward program into a bidirectional program by proba-
              bilistically converting operations at the end of the program to inverse or conditional
              inverse operations when applicable.
                 For test-time search, we stick with taking episode rollouts as they are easiest to
              implement. Unlike [6], we did not yet implement a value network for our system.
              4.4  Results
              We evaluate the bidirectional algorithm on a set of 18 ARC symmetry tasks—a
              subset of those used in Section 3. Here, we provide our agent with six primitives
              shown in 1. We use a convolutional neural network to embed grid example sets.
              We trained on a set of randomly generated programs evaluated on random input
              grids from the ARC training set, and ﬁne-tuned with REINFORCE before sampling
              rollouts for ten minutes on all tasks at once. The agent is able to solve 13 of 18
              tasks, shown in Figure 14. Unfortunately, it struggles to scale to tasks requiring
              more than a few operations. The “four-way mirror” tasks solved at the end of the
                                             27
                                Primitive                 Description                   Inverse type
                                  hstack       combine two grids horizontally       two cond. inverses
                                  vstack         combine two grids vertically       two cond. inverses
                                rotate_cw            rotate grid clockwise                 inverse
                               rotate_ccw        rotate grid counterclockwise              inverse
                                  vflip               ﬂip grid vertically                  inverse
                                  hflip              ﬂip grid horizontally                 inverse
                                     Table 1: Primitives used to solve ARC symmetry tasks.
                     experiments in 3.3 remain unsolved, as well as two even harder tasks. For some
                     reason we were not able to get the REINFORCE learning to consistently improve
                     results.  Here, equal performance came from using the supervised model directly,
                     with REINFORCE yielding no improvement.
                         To gain more insight into the performance of a forward-only search with the
                     new bidirectional algorithm, we evaluate the agent in a simpler domain: solving “24
                     Game” problems. A 24 Game consists of four input numbers and a target number
                     24. To solve the task, one must use each number once in an arithmetic expression
                     that creates 24. For example, given 8, 1, 3, and 2, a solution is 24 = (2−1)×3×8.
                     Solving a 24 problem requires similar reasoning to that solving ARC tasks: one
                     executes various operations and tries to get “closer” to 24. We can use the conditional
                     inverses of each arithmetic operator to facilitate a bidirectional search. For example,
                     the conditional inverse of multiplication applied to 24 with 6 as one input yields 4
                     as a new target. The 24 Game serves as a good alternative benchmark of the ability
                     of our system, and lets us measure how important the bidirectional reasoning is to
                     system performance.3
                         For the supervised pretraining of our model, we train for 10,000 epochs on a
                     dataset of randomly generated programs between depth one and four which yields
                     50,000 training examples of actions. Programs may create any number as a target,
                        324 Game is played with the rule that each number must be used exactly once in the expression
                     to create 24. We relax this rule as it is not relevant for the program synthesis our agent is originally
                     designed to carry out.
                                                                   28
                                           Depth         1       2       3      4
                                       Forward-only    0.232   0.801   0.516  0.340
                                        Bidirectional  0.953   0.929   0.877  0.853
                   Table 2: Percent of tasks solved for 24 Game. Forward-only denotes only using
                   forward operations. Bidirectional includes conditional-inverse operations. Accuracy
                   is average of three runs.
                   not just 24, with the maximum allowed integer 100, and no negative or nonintegral
                   numbers allowed. We then ﬁne-tune with REINFORCE for 10,000 epochs of 1000
                   actions taken each. We measure performance at the end of training as a function
                   depth, andcomparethefullbidirectional, execution-guidedalgorithmwithaforward-
                   only baseline that only applies operations in the forwards direction with no inverse
                   operations.
                      Results are shown in Table 2. Accuracy remains fairly high as depth increases.
                   This is because although tasks under depth four are generated through the applica-
                   tion of four operations, as many as 40% remain solvable in fewer than four actions.
                   Another observation is that bidirectional synthesis outperforms forward only across
                   all depths. However, this should be questioned: it’s not clear why forward-only per-
                   formance diﬀers so much between depths. In fact, for depth one tasks, there should
                   be no diﬀerence between forward-only and bidirectional approaches, since either way
                   the solution takes a single action. So the disparity is likely the result of some bug
                   rather than a meaningful diﬀerence, or peculiarities with how REINFORCE is tuning
                   our model.
                      Besides bugs, there are a few reasons why learning to solve tasks with REIN-
                   FORCEis not working as well as one might hope here. First, our hyperparameters
                   may need to be tuned. Second, we may need to train for much longer. Compared
                   to the number of examples seen during supervised training, the amount seen during
                   policy gradient is quite low. This is due to the time taken to “simulate” the search
                   graph environment while playing episodes. Last, our network architecture may need
                   tuning or strengthening as well.
                                                            30
                         Model       Tasks solved
                     SV depth 1 FW       14            Model       Tasks solved
                     SV depth 2 FW       74        PGdepth 1 FW        0
                     SV depth 3 FW       96        PGdepth 2 FW        15
                     SV depth 4 FW       45        PGdepth 3 FW        19
                      SV mixed FW        96        PGdepth 4 FW        12
                     SV depth 1 bidir    66        PGdepth 1 bidir     27
                     SV depth 2 bidir    96        PGdepth 2 bidir     39
                     SV depth 3 bidir    96        PGdepth 3 bidir     39
                     SV depth 4 bidir    46        PGdepth 4 bidir     38
                      SV mixed bidir     96
               Table 3: Number of tasks solved out of 96 by various models. SV denotes a model
               trained in a supervised manner. FW denotes only forward operations, while bidir
               denotes an agent with bidirectional search. PG denotes a model ﬁne-tuned with
               REINFORCE.Fine-tuned models are pretrained on supervised data of mixed depth.
               Depth refers to the number of operations to solve a task; mixed combines all four
               depths into one.
                  Weadditionally measured performance of the agent on a set of 96 24 Game tasks,
               showninTable3. Wesampletasksbytakingrolloutsrepeatedly for 60 seconds on all
               96 tasks at once. Surprisingly, the supervised models — without any REINFORCE
               ﬁne-tuning — performed best, solving all 96 tasks. The ﬁne-tuned models may be
               overﬁtting to a subset of the tasks that the models can solve, preventing proper
               exploration while solving the harder tasks. Incorporating entropy regularization to
               ameliorate this issue would be a good next step.
               5    Discussion
               As one of the ﬁrst documented eﬀorts towards developing a principled learning ap-
               proach to the Abstraction and Reasoning Corpus, we have proposed and tested two
               directions independently. The ﬁrst addresses the abstraction required for ARC; it
               uses the compression algorithm from DreamCoder to learn abstractions of rule-based
                                                31
        behavior to enable further proﬁciency and compositional generalization on progres-
        sively more challenging tasks. The second addresses the reasoning required for ARC;
        it uniﬁes execution-guided program synthesis, a bottom-up synthesis method, with
        top-down deductive reasoning via inverse semantics. This enables a bidirectional
        program synthesis algorithm well-suited to solving tasks like those seen in ARC, and
        addresses the diﬃculties DreamCoder faces in scaling search to the level required for
        performance on ARC.
          The next important step is to combined these two approaches to create a uniﬁed
        system capable of both abstraction and reasoning on ARC. Neither approach is com-
        pletely suﬃcient on its own. Without a strong reasoning approach, DreamCoder’s
        search algorithm is too weak to scale to challenging ARC tasks. But without the abil-
        ity to create new abstractions, the bidirectional, execution-guided synthesis is limited
        to solving tasks via whatever functions the developer provides it with. This requires
        a carefully crafted DSL, which not only goes against the spirit of ARC but inevitably
        leads to limited performance on the hidden test set, which likely builds on functions
        not seen in the training or evaluation sets. Luckily, combining the two approaches
        is very possible: new functions created by DreamCoder’s abstraction algorithm can
        be provided to the bidirectional search algorithm, which can use these functions to
        solve new tasks. In essence, we can replace DreamCoder’s neural-guided search al-
        gorithm with the more powerful bidirectional, execution-guided search. Tasks solved
        by the bidirectional search can be given to DreamCoder’s compression algorithm to
        produce new functions. We can then train the bidirectional algorithm to use these
        newfunctions by sampling random programs in them in a manner similar to the way
        DreamCoder does.
          In addition to learning forward functions, it would be nice if our bidirectional
        agent could learn inverses and conditional inverses of new abstractions. One ap-
        proach to this would be to treat these as synthesis problems of their own, sampling
        random inputs and tasking our system with creating a function which converts out-
        puts into inputs, perhaps conditioned on a subset of inputs. We hope to explore
        these possibilities for future work.
          There are a few other important further directions to consider for those interested
                          32
        in making progress on ARC. An important limit of all systems developed so far is
        their reliance on explicit programs to represent solutions to tasks. As discussed in
        Section 3.5, representing solutions with neural modules rather than programs opens
        the door to systems that are more ﬂexible and rely less on hard-coded reasoning
        systems like that described here. In the introduction, we stressed the importance
        of moving away from systems whose reasoning abilities are task-speciﬁc and imple-
        mented by the developer. Yet the bidirectional algorithm proposed is an example of
        just such a thing. Generalizing the algorithm to something more widely applicable
        is an important step. For example, deductive reasoning could be implemented in the
        form of local causality-propagating systems of neural modules.
          Another direction involves addressing the vast amounts of prior knowledge hu-
        mans bring to a challenge like ARC. Large generative pretrained models [2, 16] oﬀer
        a potential way to incorporate prior knowledge in this manner. It remains to be seen
        the extent to which such models can generalize to the idiosyncratic ARC environ-
        ment of discrete-colored grids, but the impressive improvements shown over the past
        few years indicate that betting against such models is not safe.
          Thediﬃculty of performing well on ARC without resorting to developer-provided
        brute-force solutions is a testament to its design as a measure of intelligence. Not
        only is it diﬃcult, but its design captures the exact challenges facing us in artiﬁ-
        cial intelligence. We hope that the benchmark continues to receive attention and
        motivates new learning algorithms to get us closer to the development of generally
        intelligent machines.
        A Appendix
        A.1 A bidirectional grammar for ARC tasks
        A lot of eﬀort went into designing a set of functions suited to solving ARC tasks
        that facilitates bidirectional reasoning. While not used in the results shown here, we
        record the details to exemplify the potential of inverse semantics for ARC, and in
        the interest of assisting future work.
                          33
          of themselves. We can apply the conditional inverse color_in(grid, color) with
          the input grid supplied as the condiitoning argument to produce a new task: given a
          grid, deduce what color to color it in. If we then apply area in the forwards direction
          to get the area of each grid, then we get an association between integers and colors.
          If this were a synthesis task, the solution would be simple: it is a simple lookup: all
          identical inputs map to the same output. We can include a lookup operation which
          connects the two sides of the bidirectional search this way.
            Onelast example, task 10, exempliﬁes the capabilities of bidirectional, execution-
          guided synthesis. In order, we can:
            • Split the input into a list of objects in the forwards direction
            • Apply “add gridlines” in reverse to the output to create a new target with the
             gridlines removed.
            • Apply “inﬂate” in reverse (conditioned on the inﬂate factor, a constant 3) to
             produce a 3x3 grid target.
            • Apply the “ﬁlter” conditional inverse: given the input list, and the target
             element which is a member of the list, produce a new target list of Booleans
             which is true for the element chosen and false otherwise.
            • Apply “area” in the forwards direction to the list of objects.
            • Apply “lookup” between the list of areas and the list of Booleans. The only
             object chosen is that with area four.
            Avisual demonstration of this search is shown in Figure 17.
          A.2 Code
          For code access, please reach out to Simon at simonalford42@gmail.com.
                                36
                   References
                    [1] top-quarks/arc-solution.   https://github.com/top-quarks/ARC-solution.
                       Accessed: 2020-10-05.
                    [2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Ka-
                       plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
                       Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
                       Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeﬀrey Wu,
                       Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
                       Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
                       Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
                       learners, 2020.
                    [3] Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program
                       synthesis. In International Conference on Learning Representations, 2019.
                    [4] Fran¸cois Chollet. On the measure of intelligence, 2019.
                    [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
                       Pre-training of deep bidirectional transformers for language understanding,
                       2019.
                    [6] Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Ar-
                       mando Solar-Lezama. Write, execute, assess: Program synthesis with a repl,
                       2019.
                    [7] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lu-
                       cas Morales, Luke Hewitt, Armando Solar-Lezama, and Joshua B. Tenenbaum.
                       Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep
                       bayesian program learning, 2020.
                    [8] Jonathan Evans. In two minds: Dual-process accounts of reasoning. Trends in
                       cognitive sciences, 7:454–9, 11 2003.
                                                            38
         [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learn-
          ing for image recognition, 2015.
        [10] “icecuber”. Arc-solution. https://github.com/top-quarks/ARC-solution,
          2020.
        [11] Daniel Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, 2011.
        [12] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. Imagenet classiﬁcation
          with deep convolutional neural networks. In Advances in neural information
          processing systems, pages 1097–1105, 2012.
        [13] Brenden M. Lake and Marco Baroni. Generalization without systematicity: On
          the compositional skills of sequence-to-sequence recurrent networks, 2018.
        [14] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Ve-
          ness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland,
          Georg Ostrovski, et al. Human-level control through deep reinforcement learn-
          ing. nature, 518(7540):529–533, 2015.
        [15] Oleksandr Polozov and Sumit Gulwani. Flashmeta: a framework for inductive
          program synthesis. In Jonathan Aldrich and Patrick Eugster, editors, OOPSLA,
          pages 107–126. ACM, 2015.
        [16] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
          Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation,
          2021.
        [17] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
          Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian
          Bolton, et al. Mastering the game of go without human knowledge. nature,
          550(7676):354–359, 2017.
        [18] Steven A. Sloman. Theempiricalcasefortwosystemsofreasoning. Psychological
          Bulletin, 119:3–22, 1996.
                          39
        [19] Jacob M. Springer and Garrett T. Kenyon. It’s hard for neural networks to
          learn the game of life, 2020.
        [20] Andrew Trask, Felix Hill, Scott E. Reed, Jack W. Rae, Chris Dyer, and Phil
          Blunsom. Neural arithmetic logic units. CoRR, abs/1808.00508, 2018.
        [21] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, An-
          drew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds,
          Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent rein-
          forcement learning. Nature, 575(7782):350–354, 2019.
        [22] Po-Wei Wang, Priya L. Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging
          deep learning and logical reasoning using a diﬀerentiable satisﬁability solver,
          2019.
        [23] Chenghui Zhou, Chun-Liang Li, and Barnabas Poczos. Unsupervised program
          synthesis for images using tree-structured lstm, 2020.
        [24] L  ukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms, 2016.
                          40
