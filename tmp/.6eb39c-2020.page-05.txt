                                                   Agent57: Outperforming the Atari Human Benchmark
                                                                                 4. Experiments
                                                                                 Webeginthissectionbydescribingourexperimentalsetup.
                                                                                 Following NGU, Agent57 uses a family of coefÔ¨Åcients
                                                                                 {(Œ≤ ,Œ≥ )}N‚àí1 of size N = 32. The choice of discounts
                                                                                     j  j   j=0
                                                                                      N‚àí1
                                                                                 {Œ≥ }      differs from that of NGU to allow for higher val-
                                                                                    j j=0
                                                                                 ues, ranging from 0.99 to 0.9999 (see App. G.1 for de-
                                                                                 tails). The meta-controller uses a window size of œÑ = 160
                                                                                 episodes and  = 0.5 for the actors and a window size of
               Figure 3. Capped human normalized score where we observe at       œÑ = 3600 episodes and  = 0.01. All the other hyper-
               whichpoint the agent surpasses the human benchmark on the last    parameters are identical to those of NGU, including the
               6 games.                                                          standard preprocessing of Atari frames. For a complete
                                                                                 description of the hyperparameters and preprocessing we
                                                                                 use, please see App. G.3. For all agents we run (that is,
               posedtoaglobalmeta-controller, is that each actor follows         all agents except MuZero where we report numbers pre-
               a different  -greedy policy which may alter the choice of        sented in Schrittwieser et al. (2019)), we employ a separate
                            l                                                    evaluator process to continuously record scores. We record
               the optimal arm. Each arm j from the N-arm bandit is              the undiscountedepisodereturnsaveragedover3seedsand
               linked to a policy in the family and corresponds to a pair        using a windowed mean over 50 episodes. For our best al-
               (Œ≤ ,Œ≥ ). At the beginning of each episode, say, the k-
                 j   j                                                           gorithm, Agent57, we report the results averaged over 6
               th episode, the meta-controller chooses an arm Jk setting         seeds on all games to strengthen the signiÔ¨Åcance of the re-
               which policy will be executed. Note here that the arm J
                                                                           k     sults. On that average, we report the maximum over train-
               is a random variable. Then the l-th actor acts  -greedily
                                                                  l              ing as their Ô¨Ånal score, as done in Fortunato et al. (2017);
               with respect to the corresponding state-action value func-        Puigdom√®nech Badia et al. (2020). Further details on our
               tion, Q(x,a,J ;Œ∏ ), for the whole episode. The undis-
                               k  l                                              evaluation setup are described in App. E.
               counted extrinsic episode returns, noted Re(Jk), are used
                                                            k                    In addition to using human normalized scores HNS =
               as a reward signal to train the multi-arm bandit algorithm
                                                                                  Agent  ‚àíRandomscore
               of the meta-controller.                                                score         , we report the capped human normal-
                                                                                 Human   ‚àíRandom
                                                                                       score     score
               The reward signal Re(J ) is non-stationary, as the agent          ized scores, CHNS = max{min{HNS,1},0}. This mea-
                                     k   k                                       sure is a better descriptor for evaluating general perfor-
               changes throughout training. Thus, a classical bandit algo-       mance, as it puts an emphasis in the games that are below
               rithm such as Upper ConÔ¨Ådence Bound (UCB; Garivier &              the average human performance benchmark. Furthermore,
               Moulines, 2008) will not be able to adapt to the changes          and avoiding any issues that aggregated metrics may have,
               of the reward through time. Therefore, we employ a sim-           we also provide all the scores that all the ablations obtain
               pliÔ¨Åed sliding-window UCB with UCB-greedy exploration.           in all games we evaluate in App. H.1.
               Withprobability 1‚àíUCB, this algorithm runs a slight mod-
               iÔ¨Åcation of classic UCB on a sliding window of size œÑ and         Westructure the rest of this section in the following way:
               selects a random arm with probability UCB (details of the        Ô¨Årstly, we show an overview of the results that Agent57
               algorithms are provided in App. D).                               achieves. Then we proceed to perform ablations on each
               Note that the beneÔ¨Åt of adjusting the discount factor             one of the improvements we propose for our model.
               through training and at evaluation could be applied even          4.1. Summary of the Results
               in the absence of intrinsic rewards. To show this, we pro-
               pose augmenting a variant of R2D2 with a meta-controller.         Tab. 1 shows a summary of the results we obtain on all 57
               In order to isolate the contribution of this change, we eval-     Atari games when compared to baselines. MuZero obtains
               uate a variant of R2D2 which uses the same RL loss as             the highest uncapped mean and median human normalized
               Agent57. Namely, a transformed Retrace loss as opposed            scores, but also the lowest capped scores. This is due to the
               to a transformed n-step loss as in the original paper. We re-     fact that MuZeroperformsremarkablywellinsomegames,
               fer to this variant as R2D2 (Retrace) throughout the paper.       such as Beam Rider, where it shows an uncapped score of
               Thereasonfor choosing this different loss is that it worked       27469%,butatthesametimecatastrophicallyfailstolearn
               better than the n-step loss for NGU, as described in Puig-        in games such as Venture, achieving a score that is on par
               dom√®nech Badia et al. (2020). In all other aspects, R2D2          with a random policy. We see that the meta-controller im-
               (Retrace)isexactlythesamealgorithmasR2D2. Weincor-                provement successfully transfers to R2D2: the proposed
               porate the joint training of several policies parameterized       variant R2D2 (bandit) shows a mean, median, and CHNS
                       N‚àí1
               by {Œ≥ }       to R2D2 (Retrace). We refer to this algorithm
                     j j=0                                                       that are much higher than R2D2 with the same Retrace
               as R2D2 (bandit).
