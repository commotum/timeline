                                 TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
                                                                     Table 2: Overall Experimental Results with ROUGE-L
                                                         NLU               TU                     TBO                      TCO                      DA                      ADA
                                ModelName                TCoT TCoT         PoT    ICoT    TCoT     PoT    ICoT    TCoT      PoT    ICoT    TCoT     PoT    ICoT    TCoT     PoT     ICoT    Overall
                                                                                                      General LLMs
                       Llama-3.1-8B-Instruct             20.73    23.72   23.41   12.37   15.60    18.04  11.91    13.24   20.06   9.30    16.70   23.33   10.97    9.26    8.84    7.77      15.33
                       Llama-3.1-70B-Instruct            22.06    39.70   42.90    6.88   19.90    23.94   9.70    32.77   25.61   10.51   23.39   26.31   11.56    20.68   17.92   7.39      21.33
                       Qwen2.5-7B-Instruct               19.88    36.87   41.58   57.43   15.09    26.16  27.15    29.76   21.78   36.94   27.05   20.43   23.73    22.09   17.79   35.89     28.73
                       Qwen2.5-72B-Instruct              25.13    43.79   45.08   73.51   25.98    26.00  29.36    37.24   26.74   43.21   29.98   23.00   30.94    23.09   18.63   41.76     33.96
                       Mistral-7B-Instruct-v0.3          19.37    21.12   21.92   26.04   12.82    20.05  18.39    16.05   17.34   12.88   20.09   15.75   13.74    4.03    5.70    8.20      15.84
                                                                                                  CodeOptimizedLLMs
                       Qwen2.5-Coder-7B-Instruct         34.61    36.61   38.10   51.60   14.76    22.06  29.36    25.57   21.76   31.59   23.08   26.86   32.13    23.09   14.35   36.80     28.89
                       Deepseek-Coder-7B-Instruct-v1.5    4.11    6.86    10.39    8.74    2.79    12.40   6.81    2.46     9.50   5.16     3.05    7.33    6.16    0.36    1.77    8.93       6.05
                       Deepseek-Coder-33B-Instruct        9.82    14.14   33.75   28.37   14.78    19.98  25.50    8.11    14.08   15.40    8.47   25.74   16.61    1.68    11.19   10.88     16.15
                       Seed-Coder-8B-Instruct            23.39    25.94   37.95   36.54   15.77    26.53  31.61    19.63   22.30   28.13   24.27   27.63   26.03    9.72    17.75   29.26     25.15
                       Yi-Coder-9B-Chat                  18.02    10.66   32.24   26.02    9.00    21.83  24.83    7.86    19.04   20.60    9.08   26.85   20.42    5.16    12.67   14.19     17.40
                                                                                                   DeepThinkingLLMs
                       Deepseek-R1-Distill-Qwen-7B       16.55    30.05   19.58   37.14   20.61    17.98  21.67    22.99   13.19   23.27   29.65   15.94   28.97    11.79   6.02    29.60     21.56
                       Deepseek-R1-Distill-Qwen-14B      22.66    37.20   41.31   68.88   20.46    21.22  29.38    31.57   18.72   38.16   37.21   16.99   29.95    14.26   14.38   38.48     30.05
                       Deepseek-R1-Distill-Qwen-32B      19.13    41.64   47.25   73.25   29.35    28.07  28.85    38.84   24.07   46.30   38.33   26.91   21.69    21.05   21.90   43.19     34.36
                       Deepseek-R1-Distill-Llama-8B      20.51    26.74   18.51   40.09   20.52    13.02  17.77    20.64   13.04   18.91   32.07    9.50   18.87    5.80    3.86    19.28     18.69
                       Deepseek-R1-Distill-Llama-70B     20.97    40.49   36.28   71.37   24.05    23.86  30.71    36.71   25.80   43.91   34.12   24.70   27.73    21.02   15.34   45.13     32.63
                       QwQ-32B                           20.44    42.91   57.89   75.37   32.75    31.85  30.06    42.39   39.93   48.27   24.69   29.29   16.91    28.21   31.35   50.42     37.67
                       Qwen3-8B                          20.05    30.83   57.32   70.10   33.84    30.14  30.34    36.22   35.14   45.27   23.82   21.33   23.25    21.19   28.75   41.67     34.33
                       Qwen3-14B                         25.49    39.63   44.64   69.84   33.07    30.45  32.77    38.59   26.21   42.34   26.62   27.10   28.61    33.71   27.96   41.85     35.55
                       Qwen3-32B                         21.72    38.19   53.47   75.06   33.37    30.72  32.38    36.81   32.52   47.04   27.40   26.67   23.63    31.65   30.88   46.97     36.78
                                                                                                  MathOptimizedLLMs
                       Kimina-Prover-Preview-Distill-7B   9.66    3.56     0.09    3.10    4.85    0.24    5.87    3.04     0.19   5.28     5.47    0.18    6.45    1.94    0.00    4.72       3.41
                       Qwen2.5-Math-7B-Instruct          15.07    6.70     7.48   29.28    8.15    13.07  19.64    4.65    10.44   22.32    5.19   10.80   12.14    0.56    2.09    18.42     11.62
                       Qwen2.5-Math-72B-Instruct         18.29    15.59   30.04   50.93   13.77    21.41  26.64    12.66   23.47   39.46    7.39   16.40   15.24    1.48    14.19   27.42     20.90
                       Deepseek-Math-7B-Instruct         13.49    9.19     6.65   16.18    6.50    1.96    5.40    5.22     3.28   5.54     8.18    4.55   10.11    2.90    0.82    5.69       6.60
                                                                                             Table Reasoning Optimized LLMs
                       TableGPT2-7B                      21.44    33.14   48.09   38.15   17.73    26.31  35.85    28.56   23.10   29.34   22.33   17.96   16.75    12.55   18.27   26.99     26.03
                       Table-R1-SFT-7B                   21.51    30.21   41.57    7.52   16.38    27.41   2.19    17.81   24.39   6.64    18.61   15.62    7.16    10.31   15.28   2.54      16.57
                       Table-R1-Zero-7B                  19.24    34.99   31.07   62.42   17.70    18.69  27.61    28.13   14.13   35.85   23.95   19.89   27.23    9.59    20.06   35.40     26.62
                                                                 Table 3: Overall Experimental Results with LLM-as-a-judge
                                                          NLU              TU                      TBO                     TCO                      DA                      ADA
                                ModelName                TCoT TCoT         PoT    ICoT    TCoT     PoT     ICoT    TCoT     PoT    ICoT    TCoT     PoT     ICoT    TCoT     PoT    ICoT    Overall
                                                                                                      General LLMs
                       Llama-3.1-8B-Instruct             61.46    49.53   45.04   47.06    39.22   50.11   55.76   41.51   55.12   50.20    39.68   51.97   50.89   31.36   24.01   13.62     44.16
                       Llama-3.1-70B-Instruct            70.75    64.38   66.47   57.93    57.74   72.20   48.04   65.75   77.69   61.46    48.58   61.80   51.91   40.29   42.48   27.50     57.18
                       Qwen2.5-7B-Instruct               64.66    58.79   57.98   68.26    41.46   56.99   61.23   51.61   62.71   73.63    44.88   52.17   61.84   37.13   28.08   44.63     54.13
                       Qwen2.5-72B-Instruct              75.87    72.39   64.46   87.07    66.56   71.05   72.06   73.18   80.93   85.25    54.05   59.69   70.97   42.14   46.49   61.25     67.71
                       Mistral-7B-Instruct-v0.3          47.47    40.22   37.33   25.14    32.57   50.09   37.18   35.28   45.46   35.61    37.20   23.96   27.09   18.89   15.14   16.05     32.79
                                                                                                  CodeOptimizedLLMs
                       Qwen2.5-Coder-7B-Instruct         61.87    55.51   57.92   64.30    44.36   61.03   66.86   47.59   65.17   69.26    39.99   57.73   62.99   35.99   33.97   45.50     54.38
                       Deepseek-Coder-7B-Instruct-v1.5   16.63    21.82   19.36   12.38    26.83   36.69   24.33   19.93   30.11   19.26    14.65   13.46   14.28   13.27    3.62   13.77     18.77
                       Deepseek-Coder-33B-Instruct       26.59    35.09   49.06   24.39    39.40   57.01   54.72   29.16   42.74   30.91    24.77   53.87   33.54   10.94   26.96   19.60     34.92
                       Seed-Coder-8B-Instruct            45.18    50.65   57.21   59.61    44.06   66.58   66.33   42.66   67.57   68.67    38.68   62.94   65.62   32.59   38.05   42.27     53.04
                       Yi-Coder-9B-Chat                  43.64    32.76   50.61   39.70    30.59   56.24   51.56   32.49   58.34   54.58    23.07   48.17   51.50   21.41   28.72   20.62     40.25
                                                                                                   DeepThinkingLLMs
                       Deepseek-R1-Distill-Qwen-7B       51.28    49.28   33.86   54.58    56.33   48.24   52.49   62.01   43.28   56.50    49.19   41.19   57.34   22.86   18.02   36.89     45.83
                       Deepseek-R1-Distill-Qwen-14B      61.78    62.29   66.71   81.18    73.07   65.02   68.66   67.10   51.27   79.11    53.87   46.51   61.98   34.81   31.34   51.26     59.75
                       Deepseek-R1-Distill-Qwen-32B      63.04    69.19   65.33   86.18    78.23   71.92   70.51   68.34   62.82   69.76    56.36   67.90   61.33   32.14   43.22   52.51     63.67
                       Deepseek-R1-Distill-Llama-8B      53.17    56.13   34.86   54.48    55.23   39.07   49.60   57.29   36.86   52.17    49.24   13.14   40.90   20.32    6.40   25.19     40.25
                       Deepseek-R1-Distill-Llama-70B     65.21    67.88   67.51   86.98    74.65   69.16   69.23   80.07   76.14   86.08    59.35   66.64   73.03   37.27   44.68   54.04     67.37
                       QwQ-32B                           71.39    71.51   72.62   91.02    80.57   78.23   74.00   74.47   73.31   75.36    51.79   72.54   60.68   45.06   54.79   64.88     69.51
                       Qwen3-8B                          62.60    67.27   70.68   82.93    80.87   69.06   70.51   71.25   66.31   71.58    58.16   55.51   61.14   52.76   44.34   57.41     65.15
                       Qwen3-14B                         68.07    68.80   64.08   87.33    79.26   76.16   77.51   73.03   69.27   73.63    58.00   66.49   66.18   48.28   50.55   56.90     67.72
                       Qwen3-32B                         67.72    67.84   71.16   90.89    81.19   75.83   77.56   73.32   70.91   75.07    62.23   68.63   67.55   49.63   57.34   62.70     69.97
                                                                                                  MathOptimizedLLMs
                       Kimina-Prover-Preview-Distill-7B  22.69    11.53    0.15    9.85    19.50   0.40    16.58   24.94    0.70   16.31    12.67   0.05    11.43    8.04    0.00    8.66     10.22
                       Qwen2.5-Math-7B-Instruct          40.17    22.35   13.48   41.38    34.68   26.22   38.29   37.61   31.01   51.51    18.68   20.04   39.08   12.09    2.25   19.57     28.03
                       Qwen2.5-Math-72B-Instruct         56.98    53.56   46.57   76.98    58.35   48.73   59.40   61.20   58.62   79.27    35.23   35.51   58.50   17.74   23.70   39.31     50.60
                       Deepseek-Math-7B-Instruct         28.85    21.02    7.77   28.98    15.91   4.25    8.41    20.61   10.91   11.45    20.34   9.50    18.02   12.63    1.30   11.02     14.43
                                                                                             Table Reasoning Optimized LLMs
                       TableGPT2-7B                      60.83    58.97   64.61   73.82    48.38   59.79   65.43   57.05   71.94   75.58    44.14   51.24   66.80   32.06   33.21   50.68     57.16
                       Table-R1-SFT-7B                   71.58    62.04   53.52   25.10    68.21   54.49   15.70   71.25   59.87   13.57    37.08   41.26   33.75   35.64   25.95   19.95     43.06
                       Table-R1-Zero-7B                  66.18    64.36   48.99   77.99    54.56   36.01   58.57   62.06   50.28   76.16    50.32   48.95   63.79   28.21   32.32   45.36     54.01
                                                                                                            9
