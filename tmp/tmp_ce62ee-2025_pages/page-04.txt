                       TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
              Advanced Data Analysis (ADA): This category focuses             bility with standardized text formats (Markdown, HTML,
               on LLMs’ ability to execute high-complexity, multi-step        CSV).Finally, we select 29 representative datasets, such as
              (≥ 3 steps) data analysis tasks, through six subtasks, in-      AIT-QA(Katsisetal., 2022) , ToTTo (Parikh et al., 2020),
               cluding Multi-step Retrieval, Multi-step Fact Checking, and    HybridQA(Chenetal.,2020), and TableBench (Wu et al.,
               Multi-step Operations. This evaluation targets the model’s     2025b). These datasets are derived from diverse sources
               competency in multi-stage information integration, logical     such as Wikipedia, news reports, financial documents, and
               path planning, and cross-task reasoning.                       academic publications.
               In summary, this hierarchical evaluation framework quanti-     2.2.3. COLLECTION OF NON-QA TABULAR DATA
               fies LLMs’ performance across both individual and syner-
               gistic competencies in table reasoning scenarios, providing    Given the limitations of existing TableQA datasets in terms
               a comprehensive assessment of real-world utility.              of source diversity and scale, we further curate non-QA
                                                                              tabular datasets comprising 205,224 entries. This dataset
               2.2. Data Collection                                           is curated through a systematic search using keywords
              Toovercomelimitations in existing table reasoning bench-        such as “table classification”, “time series forecasting”, and
               marks, such as limited task diversity and fragmented dataset   “anomaly detection” across platforms including Web of Sci-
               integration, we adopt a multi-source heterogeneous data        ence, PubMed, Google Scholar, and GitHub. This effort
               collection strategy. The collected data is categorized into    yields a comprehensive tabular dataset covering 300 dis-
               three major types: (1) natural language data, (2) table-based  tinct domains, such as telecommunications, meteorology,
               question answering (QA) data, and (3) non-QA tabular data.     academia, manufacturing, finance, education, and health-
                                                                              care.
               2.2.1. COLLECTION OF NATURAL LANGUAGE DATA                     2.3. Data Generation and Augmentation
               For natural language data, we collect a total of 59,901        Existing table-based QA datasets do not fully cover all 26
               data using keyword-based retrieval from sources includ-        subtasks in our evaluation framework. To fill this gap, we in-
               ing OpenCompass, Google Scholar, and GitHub. The re-           troduce three complementary data augmentation strategies:
               trieval process leverages keywords such as “semantic un-       (1) Rule-based Data Generation, (2) End-to-End Generation
               derstanding”, “instruction following”, “hallucination”, “ro-   withLLMs,and(3)Multi-TurnAdversarialGenerationwith
               bustness”, “code generation”, and “mathematical reason-        LLMs.
               ing”. Based on this process, we select ten representative
               datasets: MMLU (Wang et al., 2024), Winogrande (Sak-           Rule-based Data Generation: This method automatically
               aguchi et al., 2021), MATH (Hendrycks et al., 2021),           generates approximately 1 million question-answer pairs
               GSM8K(Cobbeetal.,2021),MathBench(Liuetal., 2024),              from existing tables based on a set of generic rules. It is
               BigCodeBench (Zhuo et al., 2024), DS-1000 (Lai et al.,         designed to support four subtasks: Table Query, Table Selec-
               2023), UHGEval (Liang et al., 2023), FollowEval (Jing          tion, Table General Operations, and Table Distribution Test-
               et al., 2023), and AdvGLUE (Wang et al., 2021). After the      ing. The process involves: (1) selecting tables with numeric
               data collection phase, we apply a unified post-processing      columns, while excluding those with missing values or sum-
               procedure involving deduplication, format normalization,       maryrows/columns (e.g., those labeled “total” or “mean”);
               and bilingual translation (Chinese–English) to ensure both     (2) designing rule-based templates using common data ma-
               diversity and non-redundancy in the final dataset.             nipulation operations and predefined field combinations to
                                                                              automatically generate QA pairs. These manipulation oper-
               2.2.2. COLLECTION OF TABLE-BASED QUESTION                      ations include statistical computation, value querying at the
                      ANSWERING DATA                                          cell or column level, and single or multi-condition filtering,
              Toconstruct a comprehensive table-based QA dataset, we          amongothers.
               collect a total of 2 million data through a hybrid strategy    End-to-End Generation with LLMs: This method gen-
               combining keyword-based retrieval and manual curation.         erates 12,010 question–answer pairs, primarily targeting
               Specifically, we conduct a extensive literature search span-   the Table General Operations and Table Summary subtasks.
               ning the past two decades across major academic platforms      Thegeneration process adopts two modes: table-based gen-
               suchasWebofScienceandGoogleScholar,usingkeywords               eration and zero-shot generation. The workflow includes (1)
               such as “table summary”, “table understanding”, “table         crafting subtask-specific prompts to guide LLM outputs and
               question answering”, “cell-level table question answering”,    (2) evaluating the generated QA pairs using two independent
              “table fact checking”, “table structure recognition”, and “ta-  discriminator models based on sample accuracy, semantic
               blerelationextraction”. Candidatedatasetsarefilteredbased      relevance, and subtask coverage. Only instances that re-
               on representativeness, recency, redundancy, and compati-       ceive full scores from both discriminators are retained. This
                                                                           4
