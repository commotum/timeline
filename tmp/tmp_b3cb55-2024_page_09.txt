                              • Reordering of input-output example pairs, which may improve out-of-
                                 distribution performance, as evidenced in the literature [7]
                           While we use the same class of augmentations at each part of the pipeline,
                           the role and requirements of these augmentations vary across our approach:
                           During training, augmentations generate additional valid examples which
                           helps preventing the model from overfitting and provides a broader set of
                           tasks. For inference, we use augmentations to generate a more varied set
                           of solution candidates as outlined in Section 3.5. This requires T to be
                           reversible, enabling us to map the model output to the original un-augmented
                           task.  In Section 3.6, we use augmentations to evaluate candidates from
                           different perspectives and pick the best two guesses to be submitted. Here
                           the only requirement is that the transformed tasks and solution candidates
                           can be meaningfully evaluated by our model.
                           3.4   Training the models
                           Choosing the right large language model (LLM) was essential for achiev-
                           ing strong performance. We tested a variety of different models and found
                           that Mistral-NeMo-Minitron-8B-Base [5] performed the best in our ex-
                           periments. The model is rather large, so efÏcient fine-tuning methods were
                           necessary to use it effectively.
                           As a result, we used Low-Rank Adaptation (LoRA)[18], 4-bit quantization
                           and gradient checkpointing, all supported by the unsloth library. We applied
                           the LoRA adaptations to all layers of the network, including the input and
                           output embeddings. We used a learning rate of 1e−4 for all layers, except
                           for embedding which had a reduced learning rate of 1e−5.
                           For each task C = (X ,Y ,X ,Y ,...,Xc,Yc), we computed gradients only
                                                 1   1   2   2
                           on the outputs Y for i > 1 and Yc, so the model never has to predict an
                                             i
                           input grid, and as it is impossible to correctly predict the first output grid
                           without having seen at least on example. During our second finetuning we
                           compute the gradient on all available outputs.
                           Preliminary training: The Preliminary training used a LoRA rank of 256
                           andwasdoneonasingleH100GPU(seeTable3)forvariousdatasets. These
                           included Re-ARC[11], the ARC-AGIpublic evaluation set [1], Concept-ARC
                           [12], and ARC-Heavy [13]. The best-performing model on the private evalu-
                           ation set (Nemo-heavy) was trained on 531.318 training examples: 257.600
                           from Re-Arc, 51.200 from ARC-AGI-pub-eval, 22.528 from Concept-Arc and
                           200.000 from ARC-Heavy. Due to the data augmentations we applied, there
                           were no exact repetitions in the examples.
                           Secondary training: Secondary training was time-constrained and focused
                           solely on the hidden test set, using a LoRA rank of 64 and running for
                                                                  9
