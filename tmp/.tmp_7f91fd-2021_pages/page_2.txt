                                                   Large-scale few-shot program induction and synthesis 
               1.1. Motivation                                                  In a program induction task, input-output examples have 
               One of the distinctive abilities of human intelligence is        to satisfy two more properties beyond satisfying the pro-
               building fexible representations from small amounts of           gram’s pre-conditions. First, in the same way distribution 
               data (Lake et al., 2015). Neural networks provide power-         shift between training and test data affects performance in 
               ful representations, but require substantial amounts of data     a single-task, meta-training examples have to come from 
               to train.  To alleviate these needs, a set of few-shot learn-    the same distribution as meta-test examples. Therefore, if 
               ing challenges has catalyzed progress into building deep         we want our algorithms to generalize at deployment-time, 
               meta-learning systems. These systems generalize from few         they have to be meta-trained on real, non-random input-
               examples by learning powerful priors on large amounts of         output examples to avoid such a (meta-)domain shift. More-
               previous tasks (Hospedales et al., 2020).                        over, for each program, the overall set of input-output tests 
                                                                                must probe potential edge cases to distinguish the desired 
               Programs often provide extreme generalization capabilities       program from possible alternatives, both for specifcation 
               from surprisingly few examples, such as generalizing to          (training examples) and validation(testing) purposes.  In 
               larger arrays or numbers outside the training range. How-        PROGRES input-output examples come from propagating 
               ever, the combinatorial space of programs has proven hard        unit tests designed by humans; thus, we expect them to be 
               to search.  Machine learning can then be used to learn to        natural and cover most edge cases. 
               search more effciently, either by learning to predict (priors 
               over) programs from examples, or directly learning a pre-        1.2. Generating a large program induction benchmark 
               dictor that can answer future queries. To train such systems         from a real code-base 
               we need a dataset of program induction tasks.                    Large repositories of human-generated code are already 
               A program induction task is a supervised learning task           available, many with accompanying examples designed to 
               whose input-output examples come from a program. For             probe the possible ways each program could fail.  These 
               training tasks, we can optionally have access to the cor-        include many internal or open-source code-bases with unit 
               responding implementation of such program, which en-             tests. Programming competitions are another paradigmatic 
               ables learning to predict (priors over) programs from ex-        example: programmers compete to code solutions to a set 
               amples. Sometimes we can also have access to a text de-          of problems, and their solutions are checked on hidden 
               scribing other relevant context.  In this work, we present       input-output tests. PROGRES builds on programs from the 
               PROGRES (Programs from Real Executed Subproblems),               competitive programming website codeforces.com. 
               a large meta-dataset of program induction tasks, enabling        Directly using the programs and their test-cases from the 
               future methods in few-shot program induction and synthesis.      website as a benchmark has a number of downsides. First, 
               You can fnd an example of a task in fgure 2.                     programs written by humans are often signifcantly too com-
               Multiple methods have created large synthetic program in-        plex for current program induction methods to solve. Sec-
               duction datasets by sampling programs from a Domain Spe-         ond, even though CodeForces has millions of programs, they 
               cifc Language (DSL) and feeding them random inputs to            only implement ∼ 5, 500 different programming problems. 
               generate tests.  Because DSLs can generate an unlimited          In this work, we propose to leverage a program with an ac-
               amount of programs, this method provides an easy way             companying test suite (defning a program induction task) to 
               of creating large datasets.  This approach may work well         create a series of input-output examples for its subprograms. 
               for hand-crafted DSLs where the language is highly tuned         Subprograms are subsequences of the overall program that 
               to a specifc application.  There, a reasonable portion of        express an intermediate variable as a function of previous 
               valid programs implement sensible functions and shorter          variables. As illustrated in fgure 1, subprograms in a pur-
               programs tend to be more useful.  In contrast, in general-       poseful program are also likely to be useful. Moreover, by 
               purpose languages like C++, the subset of interesting and        being shorter and simpler, subprograms provide a useful 
               meaningful programs is an extremely small fraction of the        learning curriculum. Finally, since subprogram complexity 
               set of compiling C++ programs. Thus, it is essential to use      ranges from a single line of code to entire programs, they 
               real programs to learn about this set of interesting programs.   provide a relevant benchmark for both current and future 
               In addition to real code, having meaningful input-output         program induction methods. 
               tests has multiple advantages over evaluating random inputs.     In order to generate input-output examples for subprograms, 
               Many programs have implicit pre-conditions for them to run       we use a program interpreter to propagate the inputs of the 
               successfully. For instance, if we use an integer to index an     overall program line by line. Then, at each step in the execu-
               array, it has to be both positive and smaller than the array’s   tion, we capture the value of all the relevant variables. From 
               length; if a program compares two lists element-wise, the        these execution traces, we derive relevant input-output tests 
               input lists must have equal length.                              for each subprogram. For more information, see section 3.2. 
