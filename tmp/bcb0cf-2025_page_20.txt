                     Published as a conference paper at ICLR 2025
                     1×10−5,withamaximumof100iterationsperlayer. Allmodelswerevalidatedusing9-foldcross-validation
                     with 10 different random seeds to ensure stability and robustness of results.
                     Vision Experiments  For vision tasks, we evaluated the MIND model on CIFAR-100 and ImageNet datasets.
                     CIFAR-100consists of 60,000 32×32 images in 100 classes, while ImageNet has 1.28M images in 1,000
                     classes. We applied standard augmentations including random crops, horizontal flips, and normalization.
                     TheMINDmodelwascomparedtoResNet-50andEfficientNet-B7intermsofTop-1andTop-5accuracy,
                     FLOPs,andinference time. During training, we utilized cosine learning rate scheduling and weight decay of
                           −4
                     5×10 .Themodeldynamicallyselectedlayersusingtheintrospection network, with simple inputs using
                     fewer layers (2-3 layers, 2 FPI iterations) and complex inputs requiring deeper processing (up to 6 layers, 4-6
                     FPI iterations). FLOPs were recorded per complexity level to evaluate efficiency in resource usage across
                     different datasets. MIND’s ability to dynamically adjust computation resulted in significant improvements
                     in Top-1 accuracy and inference time, especially for complex inputs, where the fixed depth models showed
                     diminishing returns.
                     LanguageModeling Forlanguagemodeling,weusedtheWikiText-2andWikiText-103datasets. WikiText-
                     2 contains 2 million tokens, while WikiText-103 consists of 103 million tokens, providing a comprehensive
                     benchmarkforlong-rangedependencies. Weusedperplexity(PPL)andbits-per-character(BPC)asevaluation
                     metrics, comparing the MIND model against LSTM and Transformer baselines. Additionally, SQuAD v2.0
                     wasemployedforquestion-answering, where Exact Match (EM) and F1 scores were reported. For language
                     tasks, the introspection network selected layers dynamically based on input sequence complexity. Simpler
                     sequences exited after fewer layers (e.g., 2-3 layers, 2-3 FPI iterations), while more complex sequences
                     utilized deeper layers (up to 6 layers, 5-6 FPI iterations). Dropout of 0.3 was applied during training, and
                     attention layers were regularized with label smoothing to mitigate overfitting. The MIND model consistently
                     demonstrated lower perplexity and higher accuracy across all tasks, with reduced computation per input due
                     to adaptive layer selection.
                     Ablation Study  Weperformedanablation study to evaluate the contribution of the introspection network
                     and fixed-point iterations. We created three variants: MIND-Reduced (fewer FPI layers and simplified
                     introspection network), MIND-Fixed (static introspection after training), and MIND-Uniform (all layers used
                     without adaptive selection). Results showed that MIND-Reduced reduced FLOPs by 15% at the cost of 2-3%
                     accuracy, while MIND-Fixed and MIND-Uniform led to significantly higher FLOPs without matching the full
                     model’s performance. The ablation highlights the critical role of dynamic introspection and FPI in achieving
                     efficient computation and superior accuracy.
                     Evaluation Metrics  The primary metrics used for the comparison in all tasks included the precision of
                     Top-1 and Top-5, the perplexity (PPL), the F1 score, the EM score, and inference time. Computational
                     efficiency was measured in FLOPs, and the average number of layers and FPI iterations was recorded per
                     input complexity level.
                     D HOWARETHEACTIVATIONMAPSCALCULATED?
                     Theintrospection model A is trained to encapsulate classification accuracy (Schmidhuber, 2015) and compu-
                     tational cost within a objective function R. This introspection capability allows the network to optimize the
                     trade-off between performance and computational efficiency, dynamically adjusting the network’s complexity
                     according to the individual characteristics of each sample x(i).
                     While many established architectures (MacKay et al., 2018; Vaswani et al., 2017; Hochreiter & Schmidhuber,
                     1995; He et al., 2016; Marblestone et al., 2020) utilize mechanisms such as skip connections or attention
                     to optimize performance, these techniques are often task-dependent and do not adapt to individual samples
                     within a dataset. Our approach diverges from this. Our core innovation lies in the model’s introspective
                     ability to recompute the results on more difficult inputs during inference and adaptively select layers
                     for each sample based on its unique activation profile, optimizing both accuracy and computational
                                                               20
