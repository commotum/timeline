          arrays. Attention between latent arrays and input arrays reduces the space
          and time complexity compared to the self-attention for the input arrays only.
          The attention complexity of the Perceiver is O(NM) while the complexity
          of the self-attention for input-image is O(N2). In summary, the Perceiver
          is composed of the multiple blocks of cross-attention and the latent Trans-
          former that adopts the self-attention for the latent arrays. The input arrays
          are fed into the Transformer iteratively across the layers. After the multiple
          blocks, the Perceiver adopts the average pooling over the index dimension
          and utilizes it to predict the target label.
          3.2. Perceiver IO
           The Perceiver is a general model that handles diverse datasets with flex-
          ible input structures. However, the output structure of the Perceiver is not
          flexible, and the Perceiver is limited to the classification tasks. To extend
          the Perceiver into diverse tasks such as language modeling and autoencoding,
          flexible output structures are necessary. Perceiver IO proposes an additional
          output query array components y ∈ RM×E for the flexible output structures.
          For classification task on image, the M denotes the number of images and E
          denotes the number of classes, respectively. Perceiver IO adds the additional
          query-key-value attention block, i.e. decoder, on the top of the Perceiver.
          The additional attention block computes the attention between the output
          query array and the latent arrays extracted by the Perceiver. The shape
          of the output query array is a controllable hyper-parameter, and it induces
          flexible output structures that produce any size of outputs.
           The Perceiver and Perceiver IO attend to the relationship between the
          latent arrays and input arrays. They adopt query-key-value attention that
                          8
