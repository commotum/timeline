           Published as a conference paper at ICLR 2022
           Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom
            Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien
            de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven
            Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson,
            Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level
            code generation with alphacode. DeepMind, 2022.
           Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.
            CoRR,abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.
           MarkusNormanRabe,DennisLee,KshitijBansal, and Christian Szegedy. Mathematical reasoning
            via self-supervised skip-tree training. In ICLR, 2021.
           Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap.
            Compressive transformers for long-range sequence modelling. In ICLR, 2020.
           Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
            Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text
            transformer. Journal of Machine Learning Research, 2020.
           HongyuRen,HanjunDai,ZihangDai,MengjiaoYang,JureLeskovec,DaleSchuurmans,andBoDai.
            Combiner: Full attention transformer with sparse computation cost. CoRR, abs/2107.05768, 2021.
            URLhttps://arxiv.org/abs/2107.05768.
           AurkoRoy,MohammadSaffar,AshishVaswani,andDavidGrangier. Efﬁcient content-based sparse
            attention with routing transformers. Transactions of the Association for Computational Linguistics,
            9:53–68, 2021.
           NoamShazeerandMitchellStern. Adafactor: Adaptive learning rates with sublinear memory cost.
            In ICML, 2018.
           Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. Aug-
            menting self-attention with persistent memory. arXiv preprint arXiv:1907.01470, 2019.
           Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela
            Fan. Not all memories are created equal: Learning to forget by expiring. In ICML, 2021.
           SimengSun,KalpeshKrishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language
            models actually use long-range context? In EMNLP, 2021.
           Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv
            preprint arXiv:2009.06732, 2020.
           Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
            Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient
            transformers. In ICLR, 2021.
           Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
            Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.
           Qingxiang Wang, Chad Brown, Cezary Kaliszyk, and Josef Urban. Exploration of neural machine
            translation in autoformalization of mathematics in mizar. In International Conference on Certiﬁed
            Programs and Proofs, 2020a.
           Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
            linear complexity. arXiv preprint arXiv:2006.04768, 2020b.
           XunWang,HaozhiZhang,WeilinHuang,andMatthewR.Scott. Cross-batchmemoryforembedding
            learning. In CVPR, 2020c.
           Ronald J. Williams and Jing Peng. An efﬁcient gradient-based algorithm for on-line training of
            recurrent network trajectories. Neural Computation, 1990.
           Dani Yogatama, Cyprien de Masson d’Autume, and Lingpeng Kong. Adaptive semiparametric
            language models. ACL, 9:362–373, 2021.
                               12
