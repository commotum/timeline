                                         Training data-efﬁcient image transformers & distillation through attention
               Initialization and hyper-parameters.       Transformers are       Fine-tuning at different resolution.      Weadopt the ﬁne-
               relatively sensitive to initialization. After testing several     tuning procedure from Touvron et al. (2020): our schedule,
               options, some of them not converging, we follow Hanin &           regularization and optimization procedure are identical to
               Rolnick (2018) and initialize the weights with a truncated        that of FixEfﬁcientNet but we keep the training-time data
               normal distribution. Table 9 indicates the hyper-parameters       augmentation, unlike the dampened data augmentation of
               that we use by default at training time for all our experi-       Touvron et al. (2020). We also interpolate the positional
               ments,unlessstatedotherwise. Fordistillationwefollowthe           embeddings: In principle any classical image scaling tech-
               recommendations from Cho & Hariharan (2019) to select             nique, like bilinear interpolation, could be used. However, a
               the parameters τ and λ. We take the typical values τ = 3.0        bilinear interpolation of a vector from its neighbors reduces
               or τ = 1.0 and λ = 0.1 for the usual (soft) distillation.         its ℓ2-norm compared to its neighbors. These low-norm
                                                                                 vectors are not adapted to the pre-trained transformers and
               Data-Augmentation.       Comparedtomodelsthatintegrate            we observe a signiﬁcant drop in accuracy if we employ
               morepriors (such as convolutions), transformers require a         use directly without any form of ﬁne-tuning. Therefore we
               larger amount of data. Thus, in order to train with datasets      adopt a bicubic interpolation that approximately preserves
               of the same size, we rely on extensive data augmentation.         the norm of the vectors, before ﬁne-tuning the network with
               We evaluate different types of strong data augmentation,          either AdamW (Loshchilov & Hutter, 2017) or SGD. These
               with the objective to reach a data-efﬁcient training regime.      optimizers have a similar performance for the ﬁne-tuning
                                                                                 stage, see Table 7.
               Auto-Augment(Cubuketal.,2018),Rand-Augment(Cubuk
               et al., 2019), and random erasing (Zhong et al., 2020) im-        Bydefault and similar to ViT we train DeiT models with
               prove the results. For the two latter we use the timm (Wight-     at resolution 224 and ﬁne-tune at resolution 384. We detail
               man, 2019) customizations, and after ablation we choose           howtodothisinterpolation in Section 3.
               Rand-AugmentinsteadofAutoAugment. Overallourexper-
               iments conﬁrm that transformers require a strong data aug-        Training time.    Atypical training of 300 epochs takes 37
               mentation: almost all the data-augmentation methods that          hours with 2 nodes or 53 hours on a single 8-GPU node
               weevaluate prove to be useful. One exception is dropout,          for the DeiT-B. As a comparison point, a similar training
               which we exclude from our training procedure.                     with a RegNetY-16GF (Radosavovic et al., 2020) (84M
                                                                                 parameters) is 20% slower. DeiT-S and DeiT-Ti are trained
               Regularization & Optimizers.        Wehaveconsidered dif-         in less than 3 days on 4 GPU. Then, optionally we ﬁne-tune
               ferent optimizers and cross-validated different learning rates    the model at a larger resolution. This takes 20 hours on
               and weight decays. Transformers are sensitive to the set-         8GPUstoﬁne-tuneaDeiT-Bmodelatresolution384×384,
               ting of optimization hyper-parameters.       Therefore, dur-      whichcorrespondsto25epochs. Nothavingtorelyonbatch-
               ing cross-validation, we tried 3 different learning rates         normallows one to reduce the batch size without impacting
                    −4       −4       −5                                         performance, which makes it easier to train larger models.
               (5.10   , 3.10   , 5.10  ) and 3 weight decay (0.03, 0.04,
               0.05). We scale the learning rate according to the batch size     Note that, since we use repeated augmentation (Berman
               with the formula: lr       = lr ×batchsize, similarly to          et al., 2019; Hoffer et al., 2020) with 3 repetitions, we only
                                    scaled    512
               Goyal et al. (2017) except that we use 512 instead of 256 as      see one third of the images during a single epoch.
               the base value. The best results use the AdamW optimizer
               with a much smaller weight decay than in ViT.                     7. Conclusion
               Wehave employed stochastic depth (Huang et al., 2016),            Wehaveintroducedadata-efﬁcient training procedure for
               whichfacilitates the convergence of transformers, especially      image transformers so that do not require very large amount
               deep ones (Fan et al., 2019; 2020). For vision transform-         of data to be trained, thanks to improved training and in par-
               ers, they were ﬁrst adopted in the training procedure by          ticular a novel distillation procedure. Convolutional neural
               Wightman(2019). Regularization like Mixup (Zhang et al.,          networks have been optimized, both in terms of architecture
               2017) and Cutmix (Yun et al., 2019) improve performance.          andoptimization, during almost a decade, including through
               Wealso use repeated augmentation (Berman et al., 2019;            extensive architecture search prone to overﬁting.
               Hoffer et al., 2020), which is one of the key ingredients of
               our proposed training procedure.                                  For DeiT we relied on existing data augmentation and regu-
                                                                                 larization strategies pre-existing for convnets, not introduc-
               Exponential Moving Average (EMA).           Weevaluate the        ing any signiﬁcant architectural change beyond our novel
               EMAofournetworkobtainedaftertraining. Therearesmall               distillation token. Therefore we expect that further research
               gains, which vanish after ﬁne-tuning: the EMA model has           onimagetransformers will bring further gains.
               an edge of is 0.1 accuracy points, but when ﬁne-tuned the
               two models reach the same (improved) performance.
