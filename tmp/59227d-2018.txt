                               ListOps: A Diagnostic Dataset for Latent Tree Learning
                                                         1                                                1,2,3
                                        Nikita Nangia                             SamuelR.Bowman
                                    nikitanangia@nyu.edu                               bowman@nyu.edu
                      1Center for Data Science              2Dept. of Linguistics           3Dept. of Computer Science
                        NewYorkUniversity                   NewYorkUniversity                   NewYorkUniversity
                           60 Fifth Avenue                  10 Washington Place                    60 Fifth Avenue
                        NewYork,NY10011                     NewYork,NY10003                     NewYork,NY10011
                                       Abstract
                      Latent tree learning models learn to parse a
                      sentence without syntactic supervision, and                    [MAX29[MIN47]0]
                      use that parse to build the sentence representa-
                      tion. Existing work on such models has shown        Figure 1: Example of a parsed ListOps sequence.
                      that, while they perform well on tasks like sen-    The parse is left-branching within each list, and
                      tence classiﬁcation, they do not learn gram-        each constituent is either a partial list, an integer,
                      marsthatconformtoanyplausiblesemanticor
                      syntactic formalism (Williams et al., 2018a).       or the ﬁnal closing bracket.
                      Studying the parsing ability of such models in
                      natural language can be challenging due to the
                      inherent complexities of natural language, like     Penn Treebank (PTB; Marcus et al., 1999). The-
                      having several valid parses for a single sen-       oretically then, they have the freedom to learn
                      tence. In this paper we introduce ListOps, a        whichever grammar is best suited for the task at
                      toy dataset created to study the parsing ability    hand. However, Williams et al. (2018a) show that
                      of latent tree models. ListOps sequences are in     current latent tree learning models do not learn
                      thestyleofpreﬁxarithmetic. Thedatasetisde-          grammars that follow recognizable semantic or
                      signed to have a single correct parsing strategy
                      that a system needs to learn to succeed at the      syntactic principles when trained on natural lan-
                      task. We show that the current leading latent       guage inference. Additionally, the learned gram-
                      tree models are unable to learn to parse and        mars are not consistent across random restarts.
                      succeed at ListOps. These models achieve ac-        This begs the question, do these models fail to
                      curacies worse than purely sequential RNNs.         learn useful grammars because it is unnecessary
                  1   Introduction                                        for the task? Or do they fail because they are in-
                                                                          capable of learning to parse? In this paper we in-
                  Recent work on latent tree learning models (Yo-         troduce the ListOps datasets which is designed to
                  gatama et al., 2017; Maillard et al., 2017; Choi        address this second question.
                  et al., 2018; Williams et al., 2018a) has introduced       Since natural language is complex, there are
                  new methods of training tree-structured recurrent       often multiple valid parses for a single sentence.
                  neural networks (TreeRNNs; Socher et al., 2011)         Furthermore, as was shown in Williams et al.
                  without ground-truth parses.      These latent tree     (2018a), using sensible grammars is not neces-
                  models learn to parse with indirect supervision         sary to do well at some existing natural language
                  from a downstream semantic task, like sentence          datasets. Since our primary objective is to study
                  classiﬁcation. They have been shown to perform          a system’s ability to learn a correct parsing strat-
                  well at sentence understanding tasks, like textual      egy, we build a toy dataset, ListOps, that primar-
                  entailment and sentiment analysis, and they gen-        ily tests a system’s parsing ability. ListOps is in
                  erally outperform their TreeRNNcounterpartsthat         the style of preﬁx arithmetic; it is comprised of
                  use parses from conventional parsers.                   deeply nested lists of mathematical operations and
                    Latent tree learning models lack direct syntac-       a list of single-digit integers.
                  tic supervision, so they are not being pushed to           TheListOpssequencesaregeneratedwitharef-
                  conform to expert-designed grammars, like the           erence parse, and this parse corresponds to the
                                                                       92
                                       Proceedings of NAACL-HLT 2018: Student Research Workshop, pages 92–99
                                                                     c
                                NewOrleans, Louisiana, June 2 - 4, 2018. 2017 Association for Computational Linguistics
                        [MAX[MED[MED1[SM313]9]6]5]                         [MAX[MED[MED1[SM313]9]6]5]
                                     Truth: 6; Pred: 5                                     Truth: 6; Pred: 5
                         [SM [SM [SM [MAX 56 ] 2 ] 0 ] 5 0 8 6 ]          [SM [SM [SM [MAX 56 ] 2 ] 0 ] 5 0 8 6 ]
                                      Truth: 7; Pred: 7                                     Truth: 7; Pred: 2
                        [MED6[MED322] 85[MED862] ]                          [MED6[MED322]85[MED862]]
                                       Truth: 6; Pred: 6                                     Truth: 6; Pred: 5
                 Figure 2: Left: Parses from RL-SPINN model. Right: Parses from ST-Gumbel model. For the ﬁrst set of
                 examplesinthetoprow,botheachmodelspredictthewrongvalue(truth:6,pred:5). Inthesecondrow,
                 RL-SPINN predicts the correct value (truth: 7) while ST-Gumbel does not (pred: 2). In the third row,
                 RL-SPINN predicts the correct value (truth: 6) and generates the same parse as the ground-truth tree;
                 ST-Gumbelpredicts the wrong value (pred: 5).
                 simplest available strategy for interpretation. We      2   Related Work
                 are unaware of reasonably effective strategies that
                 differ dramatically from our reference parses. If a     To the best of our knowledge, all existing work
                 system is given the ground-truth parses, it is triv-    on latent tree models studies them in a natural
                 ially easy to succeed at the task. However, if the      language setting. Williams et al. (2018a) experi-
                 system does not have the reference parses, or is        ment with two leading latent tree models on the
                 unable to learn to parse, doing well on ListOps         textual entailment task, using the SNLI (Bow-
                 becomes dramatically more difﬁcult. Therefore,          manetal., 2015) and MultiNLI corpora (Williams
                 we can use ListOps as a litmus test and diagnos-        et al., 2018b). The Williams et al. (2018a) anal-
                 tic tool for studying latent tree learning models.      ysis studies the models proposed by Yogatama
                 ListOps is an environment where parsing is essen-       et al. (2017) (which they call RL-SPINN) and
                 tial to success. So if a latent tree model is able to   Choietal.(2018)(whichtheycallST-Gumbel). A
                 achieve high accuracy in this rigid environment, it     third latent tree learning model, which is closely
                 indicates that the model is able to learn a sensible    related to ST-Gumbel, is presented by Maillard
                 parsing strategy. Conversely, if it fails on ListOps,   et al. (2017).
                 it may suggest that the model is simply incapable         All three models make use of TreeLSTMs (Tai
                 of learning to parse.                                   et al., 2015) and learn to parse with distant super-
                                                                         vision fromadownstreamsemanticobjective. The
                                                                         RL-SPINN model uses the REINFORCE algo-
                                                                         rithm (Williams, 1992) to train the model’s parser.
                                                                      93
                  Theparsermakesdiscretedecisionsandcannotbe                  • MAX: the largest value of the given list. For
                  trained with backpropogation.                                  the list {8,12,6,3}, 12 is the MAX.
                     The model Maillard et al. (2017) present uses a          • MIN: the smallest value of the given list. For
                  CYK-style(Cocke,1969;Younger,1967;Kasami,                      the list {8,12,6,3}, 3 is the MIN.
                  1965) chart parser to compute a soft combination
                  of all valid binary parse trees. This model com-            • MED: the median value of the given list. For
                  putes O(N2) possible tree nodes for N words,
                  making it computationally intensive, particularly              the list {8,12,6,3}, 7 is the MED.
                  on ListOps which has very long sequences.                   • SUM MOD (SM): the sum of the items in
                     The ST-Gumbel model uses a similar data                     the list, constrained to a single digit by the
                  structure to Maillard et al., but instead utilizes             use of the modulo-10 operator. For the list
                  the Straight-Through Gumbel-Softmax estimator                  {8,12,6,3}, 9 is the SM.
                  (Jang et al., 2016) to make discrete decisions in
                  the forward pass and select a single binary parse.          ListOps is constructed such that it is trivially
                     Ourwork, while on latent tree learning models,        easy to solve if a model has access to the ground-
                  is with a toy dataset designed to study parsing abil-    truth parses. However, if a model does not have
                  ity. There has been some previous work on the use        the parses, or is unable to learn to parse correctly,
                  of toy datasets to closely study the performance         it may have to maintain a large stack of informa-
                  of systems on natural language processing tasks.         tion to arrive at the correct solution. This is partic-
                  For instance, Weston et al. (2015) present bAbI, a       ularly true as the sequences become long and have
                  set of toy tasks for to testing Question-Answering       manynestedlists.
                  systems. The tasks are designed to be prerequi-          Efﬁcacy     We take an empirical approach to de-
                  sites for any system that aims to succeed at lan-        termine the efﬁcacy of the ListOps dataset to test
                  guage understanding. The bAbI tasks have inﬂu-           parsing capability.    ListOps should be trivial to
                  encedthedevelopmentofnewlearningalgorithms               solve if a model is given the ground-truth parses.
                  (Sukhbaatar et al., 2015; Kumar et al., 2016; Peng       Therefore, a tree-structured model that is provided
                  et al., 2015).                                           with the parses should be able to achieve near
                  3   Dataset                                              100% accuracy on the task. So, to establish the
                                                                           upper-bound and solvability of the dataset, we use
                  Description     The ListOps examples are com-            a TreeLSTMasoneofourbaselines.
                  prised of summary operations on lists of single-            Conversely, if the ListOps dataset is adequately
                  digit integers, written in preﬁx notation. The full      difﬁcult, then a strong sequential model should not
                  sequence has a corresponding solution which is           perform well on the dataset. We use an LSTM
                  also a single-digit integer, thus making it a ten-       (Hochreiter and Schmidhuber, 1997) as our se-
                  way balanced classiﬁcation problem. For exam-            quential baseline.
                  ple, [MAX 2 9 [MIN4 7 ] 0 ] has the solution 9.             We run extensive experiments on the ListOps
                  Eachoperationhasacorrespondingclosingsquare              dataset to ensure that the TreeLSTM does consis-
                  bracket that deﬁnes the list of numbers for the op-      tently succeed while the LSTM fails. We tune the
                  eration. In this example, MIN operates on {4,7},         model size, learning rate, L2 regularization, and
                  while MAX operates on {2,9,4,0}. The correct             decay of learning rate (the learning rate is low-
                  parse for this example is shown in Figure 1. As          ered at every epoch when there has been no gain).
                  with this example, the reference parses in ListOps       We require that the TreeLSTM model does well
                  are left-branching within each list. If they were        at a relatively low model size. We further ensure
                  right-branching, the model would always have to          that the LSTM, at an order of magnitude greater
                  maintain the entire list in memory. This is because      modelsize, is still unable to solve ListOps. There-
                  the summary statistic for each list is dependent on      fore, we build the dataset and establish its effec-
                  the type of operation, and the operation token ap-       tiveness as a diagnostic task by maximizing this
                  pears ﬁrst in preﬁx notation.                            RNN–TreeRNNgap.
                     Furthermore, we select a small and easy opera-           Theoretically, this RNN–TreeRNN gap arises
                  tion space to lower output set difﬁculty. The oper-      becauseanRNNofﬁxedsizedoesnothavetheca-
                  ations that appear in ListOps are:                       pacity to store all the necessary information. More
                                                                        94
                  concretely, we know that each of the operations
                  in ListOps can be computed by passing over the
                  list of integers with a constant amount of mem-
                  ory. For example, to compute the MAX, the system
                  only needs to remember the largest number it has
                  seen in the operation’s list. As an RNN reads a se-
                  quence,ifitisinthemiddleofthesequence,itwill
                  have read many operations without closed paren-
                  theses, i.e. without terminating the lists. There-
                  fore, it has to maintain the state of all the open
                  operations it has read. So the amount of informa-
                  tion the RNN has to maintain grows linearly with
                  tree depth. As a result, once the trees are deep
                  enough, an RNN with a ﬁxed-size memory can-             Figure 3: Distribution of average tree depth in the
                  not effectively store and retrieve all the necessary    ListOps training dataset.
                  information.
                    For a TreeRNN, every constituent in ListOps is
                  either a partial list, an integer, or the ﬁnal clos-    dataset. We wrote a simple Python script to gen-
                  ing bracket. For example, in Figure 1, the ﬁrst         erate the ListOps data. Variables such as maxi-
                  constituent, ([MAX,2,9), is a partial list. So, the     mum tree-depth, as well as number and kind of
                  amountofinformation the TreeLSTM has to store           operations, can be changed to generate variations
                  at any given node is no greater than the small          on ListOps. One might want to increase the aver-
                  amount needed to process one list. Unlike with          age tree depth if a model with much larger hidden
                  an RNN,this small amount of information at each         states is being tested. With a very large model size,
                  nodedoesnotgrowwithtreedepth. Consequently,             an RNN, in principle, can succeed at the ListOps
                  TreeRNNs can achieve high accuracy at ListOps           dataset presented in this paper. The dataset and
                                                                                                                          1
                  with very low model size, while RNNs require            data generation script are available on GitHub.
                  higher capacity to do well.                             4   Models
                  Generation     The two primary variables that de-       WeuseanLSTMforoursequential baseline, and
                  termine the difﬁculty of the ListOps dataset are        a TreeLSTM for our tree-structured baseline. For
                  tree depth and the function space of mathematical       the latent tree learning models, we use two leading
                  operations. We found tree depth to be an essen-         models discussed in Section 2: RL-SPINN (Yo-
                  tial variable in stressing model performance, and       gatama et al., 2017) and ST-Gumbel (Choi et al.,
                  in maximizing the RNN–TreeRNN gap. While                2018). We are borrowing the model names from
                  creating ListOps, we clearly observe that with in-      Williams et al. (2018a).
                  creasing recursion in the dataset the performance
                  of sequential models falls. Figure 3 shows the dis-     Trainingdetails     All models are implemented in
                  tribution of tree depths in the ListOps dataset; the    a shared codebase in PyTorch 0.3, and the code
                  average tree depth is 9.6.                              is available on GitHub.1 We do extensive hyper-
                    As discussed previously, since we are con-            parameter tuning for all baselines and latent tree
                  cerned with a model’s ability to learn to parse, and    models. We tune the learning rate, L2 regular-
                  not its ability to approximate mathematical opera-      ization, and rate of learning rate decay. We tune
                  tions, we choose a minimal number of operations         the model size for the baselines in our endeavor
                  (MAX, MIN, MED, SM). In our explorations, we            to establish the RNN–TreeRNN gap, wanting to
                  ﬁnd that these easy-to-compute operations yield         ensure that the TreeLSTM, with reference parses,
                  bigger RNN–TreeRNN gaps than operations like            can solve ListOps at a low hidden dimension size,
                  multiplication.                                         while the LSTM can not solve the dataset at sig-
                    The ListOps dataset used in this paper has 90k        niﬁcantly larger hidden sizes. We test model sizes
                  training examples and 10k test examples. During         from 32D to 1024D for the baselines. The model
                  data generation, the operations are selected at ran-       1https://github.com/NYU-MLL/spinn/
                  dom, and their frequency is balanced in the ﬁnal        tree/listops-release
                                                                       95
                      Model                      ListOps  SNLI          large performance gap (∼25%) between our tree-
                                 Prior Work: Baselines                  based and sequential baselines, we conclude that
                      100DLSTM(Yogatama)            –      80.2         ListOps is an ideal setting to test the parsing abil-
                      300DBiLSTM(Williams)          –      81.5         ity of latent-tree learning models that are deprived
                      300DTreeLSTM(Bowman)          –      80.9         of syntactic supervision.
                            Prior Work: Latent Tree Learning
                      300DRL-SPINN(Williams)        –      83.3         Latenttreemodels      Prior work(Yogatamaetal.,
                      300DST-Gumbel(Choi)           –      84.6         2017; Choi et al., 2018; Maillard et al., 2017;
                      100DSoft-Gating (Maillard)    –      81.6         Williams et al., 2018a) has established that la-
                                  This Work: Baselines                  tent tree learning models often outperform stan-
                      128D LSTM                   73.3      –           dard TreeLSTMs at natural language tasks.       In
                      1024DLSTM                   74.4      –           Table 1 we summarize results for baseline mod-
                      48D   TreeLSTM              94.7      –           els and latent tree models on SNLI, a textual en-
                      128D TreeLSTM               98.7      –           tailment corpus. We see that all latent tree mod-
                             This Work: Latent Tree Learning            els outperform the TreeLSTM baseline, and ST-
                      48D RL-SPINN                62.3      –           Gumbel does so with a sizable margin. However,
                      128DRL-SPINN                64.8      –           the same models do very poorly on the ListOps
                      48D ST-Gumbel               58.5      –
                      128DST-Gumbel               61.0      –           dataset. A TreeLSTM model, with its access to
                 Table 1:   SNLI shows test set results of models       groundtruth parses, can essentially solve ListOps,
                 on the Stanford Natural Language Inference Cor-        achieving an accuracy of 98.7% with 128D model
                 pus, a sentence classiﬁcation task. We see that the    size. The RL-SPINN and ST-Gumbel models ex-
                 latent tree learning models outperform the super-      hibit very poor performance, achieving 64.8% and
                 vised TreeLSTM model. However, on ListOps,             61.0% accuracy with 128D model size. These la-
                 RL-SPINN and ST-Gumbel have worse perfor-              tent tree models are designed to learn to parse, and
                 manceaccuracy than the LSTM baseline.                  use the generated parses to build sentence repre-
                                                                        sentations. Theoretically then, they should be able
                                                                        to ﬁnd a parsing strategy that enables them to suc-
                 size for latent tree models is tuned to a lesser ex-   ceed at ListOps. However, their poor performance
                 tent, since a model with parsing ability should        in this setting indicates that they can not learn a
                 have adequate representational power at lower di-      sensible parsing strategy.
                 mensions. Wechoosethenarrowerrangeofmodel                Interestingly, the latent tree models perform
                 sizes based on how well the TreeLSTM baseline          substantially worse than the LSTM baseline. We
                 performs at those sizes. We consider latent tree       theorize that this may be because the latent tree
                 model sizes from 32D to 256D. Note that the la-        models do not settle on a single parsing strategy.
                 tent tree models we train with sizes greater than      TheLSTMcanthoroughlyoptimizegivenitsfully
                 128Ddonotshowsigniﬁcantimprovementinper-               sequential approach. If the latent tree models keep
                 formance accuracy.                                     changing their parsing strategy, they will not be
                    For all models, we pass the representation          able to optimize nearly as well as the LSTM.
                 through a 2-layer MLP, followed by a ten-way             To test repeatability and each model’s robust-
                 softmax classiﬁer. We use the Adam optimizer           ness to random initializations, we do four runs of
                 (KingmaandBa,2014)withdefaultvaluesforthe              each 128D model (using the best hyperparameter
                 beta and epsilon parameters.                           settings); we report the results in Table 2.   We
                                                                        ﬁndthattheLSTMmaintainsthehighestaccuracy
                 5   ListOps Results                                    with an average of 71.5. Both latent tree learning
                                                                        modelshaverelativelyhighstandarddeviation, in-
                 Baseline models     The results for the LSTM and       dicating that they may be more susceptible to bad
                 TreeLSTMbaseline models are shown in Table 1.          initializations.
                 We clearly see the RNN–TreeRNN gap.            The       Ultimately, ListOpsisasettinginwhichparsing
                 TreeLSTMmodeldoeswellonListOpsatembed-                 correctly is strongly encouraged, and doing so en-
                 ding dimensions as low as 48D, while the LSTM          sures success. The failure of both latent tree mod-
                 model shows low performance even at 1024D,             els suggests that, in-spite their architectures, they
                 and with heavy hyperparameter tuning. With this        maybeincapable of learning to parse.
                                                                     96
                                                                  Accuracy                  Self                high F1 score of 71.1 with ground-truth trees. The
                                                                µ(σ)                                            128D model also produces parses with an aver-
                                 Model                          µµ((σσ))        max          F1
                                 LSTM                       71.5 (1.5)          74.4           -                age tree depth (10.4) closer to that of ground-truth
                                 RL-SPINN                   60.7 (2.6)          64.8        30.8                trees (9.6).
                                 ST-Gumbel                  57.6 (2.9)          61.0        32.3                   The parses from the 128D ST-Gumbel have
                                                                                                                a signiﬁcantly lower F1 score with ground-truth
                                 RandomTrees                        -              -        30.1                trees than the parses from RL-SPINN. This result
                                                                                                                corresponds with the performance on the ListOps
                           Table2: Accuracyshowsaccuracyacrossfourruns                                          task where RL-SPINN outperforms ST-Gumbel
                           of the models (expressed as mean, standard devia-                                    by∼4%.EventhoughthetreesST-Gumbelgener-
                           tion, and maximum). Self F1showshowwelleach                                          ates are of a worse quality than RL-SPINN’s, the
                           of these four model runs agrees in its parsing de-                                   trees are consistently better than random trees on
                           cisions with the other three.                                                        F1withground-truth trees.
                                                                                                                   It’s important to note that the F1 scores have
                                                                      F1wrt.                  Avg.              very high variance from one run to the next. Ta-
                               Model                          LB        RB        GT         Depth              ble 2 shows the self F1 scores across random
                               48D RL-SPINN                  64.5      16.0       32.1        14.6              restarts of both models.                   Both have very poor
                               128DRL-SPINN                  43.5      13.0       71.1        10.4              agreement in parsing decisions across restarts,
                               48D ST-Gumbel                 52.2      15.3       55.3        11.1              their self F1 is comparable to that of randomly
                               128DST-Gumbel                 56.5       9.8       57.3        12.7
                               Ground-Truth Trees            41.6       8.8      100.0         9.6              generated trees.              For RL-SPINN, the F1 with
                               RandomTrees                   24.0      24.0       24.2         5.2              ground-truth trees ranges from 18.5 to 71.1, with
                                                                                                                an average of 39.8 and standard deviation of 19.4.
                           Table 3: F1 wrt. shows F1 scores on ListOps with                                     While ST-Gumbel has an average of 44.5, and a
                           respect to left-branching (LB), right-branching                                      standard deviation of 11.8. This high variance in
                           (RB), and ground-truth (GT) trees. Avg. Depth                                        F1 scores is reﬂective of the high variance in ac-
                           shows the average across sentences of the average                                    curacy across random restarts, and it supports our
                           depth of each token in its tree.                                                     hypothesis that these latent tree models do not ﬁnd
                                                                                                                and settle on a single parsing strategy.
                           6     Analysis                                                                       Parse trees           In Figure 2, we show some exam-
                           Given that the latent tree models perform poorly                                     ples of trees generated by both models. We use
                           onListOps, we take a look at what kinds of parses                                    the best runs for the 128D versions of the models.
                           these models produce.                                                                Parses generated by RL-SPINN are in the left col-
                                                                                                                umn, and those generated by ST-Gumbel are on
                           F1 scores           In Table 3, we show the F1 scores                                the right.
                           between each model’s predicted parses and fully                                         For the pair of examples in the top row of Fig-
                           left-branching, right-branching, and ground-truth                                    ure 2, both models incorrectly predict 5 as the so-
                           trees. We use the best run for each model in the                                     lution. Both parses compose the ﬁrst three opera-
                           reported statistics.                                                                 tions together, and it is not clear how these models
                              Overall, the RL-SPINN model produces parses                                       arrive at that solutions given their chosen parses.
                           that are most consistent with the ground-truth                                          In the second pair of examples, RL-SPINN pre-
                           trees. The ListOps ground-truth trees have a high                                    dicts the correct value of 7, while ST-Gumbel
                           F1 of 41.6 with left-branching trees, compared                                       wrongly predicts 2. The parse generated by RL-
                           to 9.8 with right-branching trees. Williams et al.                                   SPINNisnotthesameastheground-truthtreebut
                           (2018a) show that RL-SPINN tends to settle on a                                      it ﬁnds some of the correct constituent boundaries:
                                                                                                                                 
                           left-branching strategy when trained on MultiNLI.                                     [MAX 5 6 are composed with a right-branching
                           We observe a similar phenomena here at 48D.                                          tree, and  2 ] are composed together.                                Since
                           Since ListOps is more left-branching, this ten-                                      the ﬁrst three operations are all SUM MOD, their
                           dency of RL-SPINN’s could offer it an advan-                                         strange composition does not prevent the model
                           tage. Furthermore, as might be expected, increas-                                    from correctly predicting 7.
                           ing model size from 48D to 128D helps improve                                           For the third pair of examples, the RL-SPINN
                           RL-SPINN’s parsing quality. At 128D, it has a                                        model generates the same parse as the ground-
                                                                                                          97
                                                                         keeping these latent tree models from success.
                                                                         7   Conclusion
                                                                         In this paper we introduce ListOps, a new toy
                                                                         dataset that can be used as a diagnostic tool to
                                                                         study the parsing ability of latent tree learning
                                                                         models. ListOps is an ideal setting for testing a
                                                                         system’s parsing ability since it is explicitly de-
                                                                         signed to have a large RNN–TreeRNN perfor-
                                                                         mance gap. While ListOps may not be the sim-
                                                                         plest type of dataset to test a system’s parsing ca-
                                                                         pability, it is certainly simpler than natural lan-
                                                                         guage, and it ﬁts our criteria.
                 Figure 4: Model accuracy on ListOps test set by           The experiments conducted on ListOps with
                 size of training dataset.                               leading latent tree learning models show that these
                                                                         models are unable to learn to parse, even in a
                 truth reference and rightly predicts 6.       While     setting that strongly encourages it. We only test
                 ST-Gumbel gets some of the correct constituent          two latent tree models, and are unable to train and
                 boundaries, it produces a fairly balanced tree, and     analyse some other leading models, like Maillard
                 falters by predicting 5.    Overall, the generated      et al.’s (2017) due to its high computational com-
                 parses are not always interpretable, particularly       plexity. In the future, we would like to develop a
                 when the model composes several operations to-          version of ListOps with shorter sequence lengths,
                 gether.                                                 while maintaining the RNN–TreeRNN gap. With
                                                                         suchaversion,wecanexperimentwithmorecom-
                 Dataset size    ListOps is intended to be a simple      putationally intensive models.
                 dataset that can be easily solved with the correct        Ultimately, we aim to develop a latent tree
                 parsing strategy. One constraint on ListOps is the      learning model that is able to succeed at ListOps.
                 dataset size. With a large enough dataset, in prin-     If the model can succeed in this setting, then
                 ciple an RNNwithenoughcapacityshouldbeable              perhaps it will discover interesting grammars in
                 to solve ListOps. As we stated in Section 3, a          natural language that differ from expert designed
                 requirement for ListOps is having a large RNN–          grammars. If those discovered grammars are prin-
                 TreeRNNgaptoensuretheefﬁcacyofthedataset.               cipled and systematic, they may lead to improved
                    However, it is possible that the latent tree mod-    sentence representations. We hope that this work
                 els we discuss in this paper could greatly beneﬁt       will inspire more research on latent tree learning
                 from a larger dataset size, and may indeed be able      and lead to rigorous testing of such models’ pars-
                 to learn to parse given more data. To test this hy-     ing abilities.
                 pothesis, and to ensure that data volume is not crit-   Acknowledgments
                 ical to solving ListOps, we generate three expan-
                 sions on the training data, keeping the original test   This project has beneﬁted from ﬁnancial support
                 set. The new training datasets have 240k, 540k,         to Sam Bowman by Google, Tencent Holdings,
                 and990kexamples,witheachdatasetbeingasub-               and Samsung Research. We thank Andrew Droz-
                 set of the next larger one. We train and tune the       dov, who contributed to early discussions that mo-
                 128DLSTM,RL-SPINN,andST-Gumbelmodels                    tivated this work.
                 on these datasets. Model accuracies for all train-
                 ing sets are plotted in Figure 4. We see that while     References
                 accuracy does go up for the latent tree models, it’s
                 not at a rate comparable to the LSTM. Even with         Samuel R. Bowman, Gabor Angeli, Christopher Potts,
                 an order of magnitude more data, the two mod-             and Christopher D. Manning. 2015.     A large an-
                 els are unable to learn how to parse successfully,        notated corpus for learning natural language infer-
                 and remain thoroughly outstripped by the LSTM.            ence.  In Proceedings of the 2015 Conference on
                                                                           Empirical MethodsinNaturalLanguageProcessing
                 Clearly then, data volume is not a critical issue         (EMNLP).
                                                                      98
                  Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018.           andthe7thInternational Joint Conference on Natu-
                     Learning to compose task-speciﬁc tree structures.        ral Language Processing (ACL-IJCNLP).
                     In Proceedings of the 2018 Association for the Ad-    Jason Weston, Antoine Bordes, Sumit Chopra, and
                     vancement of Artiﬁcial Intelligence (AAAI).              Tomas Mikolov. 2015. Towards AI-complete ques-
                  JohnCocke.1969. ProgrammingLanguagesandTheir                tion answering: A set of prerequisite toy tasks.
                     Compilers: Preliminary Notes. Courant Institute of       arXiv preprint 1502.05698.
                     Mathematical Sciences, New York University.
                                                                           Adina Williams, Andrew Drozdov, and Samuel R.
                                           ¨
                  Sepp Hochreiter and Jurgen Schmidhuber. 1997.               Bowman. 2018a. Learning to parse from a seman-
                     Long short-term memory.      Neural computation,         tic objective: It works. is it syntax? In Proceedings
                     9(8):1735–1780.                                          of the Transactions of the Association for Computa-
                  Eric Jang, Shixiang Gu, and Ben Poole. 2016. Cat-           tional Linguistics (TACL).
                     egorical reparameterization with gumbel-softmax.      Adina Williams, Nikita Nangia, and Samuel R Bow-
                     In Proceedings of the International Conference on        man.2018b. Abroad-coveragechallengecorpusfor
                     Learning Representations (ICLR).                         sentence understanding through inference. In Pro-
                  Tadao Kasami. 1965. An efﬁcient recognition and syn-        ceedings of the 2018 Conference of the North Amer-
                     tax analysis algorithm for context-free languages.       ican Chapter of the Association for Computational
                     Air Force Cambridge Research Laboratory, Bed-            Linguistics (NAACL).
                     ford, MA.                                             Ronald J. Williams. 1992. Simple statistical gradient-
                  Diederik P. Kingma and Jimmy Ba. 2014. Adam: A              following algorithms for connectionist reinforce-
                     method for stochastic optimization. In Proceedings       mentlearning. Machine learning, 8(3-4):229–256.
                     of the International Conference for Learning Repre-   Dani Yogatama, Phil Blunsom, Chris Dyer, Edward
                     sentations (ICLR).                                       Grefenstette, and Wang Ling. 2017.    Learning to
                  Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit              compose words into sentences with reinforcement
                     Iyyer, James Bradbury, Ishaan Gulrajani, Victor          learning. In Proceedings of the International Con-
                     Zhong, Romain Paulus, and Richard Socher. 2016.          ference on Learning Representations (ICLR).
                     Ask me anything: Dynamic memory networks for          Daniel H. Younger. 1967. Recognition and parsing of
                     natural language processing. In Proceedings of the       context-free languages in time n3. Information and
                     33rdInternationalConferenceonMachineLearning             Control, 10:10:189–208.
                     (ICML).
                  Jean Maillard, Stephen Clark, and Dani Yogatama.
                     2017.  Jointly learning sentence embeddings and
                     syntax with unsupervised tree-lstms. arXiv preprint
                     1705.09189.
                  Mitchell P. Marcus, Beatrice Santorini, Mary Ann
                     Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
                     LDC99T42. Linguistic Data Consortium.
                  Baolin Peng, Zhengdong Lu, Hang Li, and Kam-Fai
                     Wong.2015. Towardsneuralnetwork-basedreason-
                     ing. arXiv preprint 1508.05508.
                  Richard Socher, Jeffrey Pennington, Eric H. Huang,
                     Andrew Y. Ng, and Christopher D. Manning. 2011.
                     Semi-supervised recursive autoencoders for predict-
                     ing sentiment distributions. In Proceedings of the
                     2011 Conference on Empirical Methods in Natural
                     Language Processing (EMNLP).
                  Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston,
                     and Rob Fergus. 2015.    End-to-end memory net-
                     works. In Proceedings of the 2015 Conference on
                     AdvancesinNeuralInformationProcessingSystems
                     (NIPS).
                  Kai Sheng Tai, Richard Socher, and Christopher D.
                     Manning. 2015. Improved semantic representations
                     from tree-structured long short-term memory net-
                     works. In Proceedings of the 53rd Annual Meet-
                     ing of the Association for Computational Linguistics
                                                                       99
