                               D Ablationresults
                               D.1   Curating the small sample
                               In two-set training, the examples in the small set are chosen at random from the overall training set.
                               In this section, we experiment with curating the small set, by selecting the examples that will be
                               repeated during training. As in curriculum learning, selecting easier or more informative examples
                               may help improve performance. Perhaps when increasing the frequency of our small random set,
                               whatreally matters is the repetition of some particular examples, rather than all? The GCD problem
                               is particularly well suited for this type of investigation, due to the inverse polynomial distribution
                               of outcomes (Prob(GCD = k) ∼ 1 ). On this problem, we leverage the findings of Charton
                                                                        k2
                               (2024), who observes that ∞-models trained from log-uniform distributions of inputs and/or out-
                               comes(Prob(GCD = k) ∼ 1)learn better.
                                                              k
                               Weexperiment with four settings of |S| and p, which correspond to the best results in our previous
                               experiments (Section 4): 50,000 and 150,000 with p = 0.25 and 150,000 and 500,000 with p =
                               0.5, for a data budget of 100 million and training budget of 600M. For every setting, we train
                               5 models with the following three choices for S: log-uniform inputs, uniform GCD or both log-
                               uniform inputs and GCD. We use two-sample training with a random small set S as our baseline.
                               Table 5 shows that the performance of models using log-uniform inputs, or uniform GCD, is slightly
                               lower than the baseline. Models trained on log-uniform inputs and GCD achieve slightly better
                               performance, but we note that models trained on the small set distribution only (p = 1) would
                               predict 91 GCD . On these three distributions, curating the small set proves disappointing.
                               In curriculum learning fashion, we also experiment with small sets S of a few “easier cases”: small
                               inputs (from 1 to 1000), GCD that are products of 2 and 5, the easiest to learn in base 1000 (Charton,
                               2024), and GCD between 1 and 10 (the most common outcomes). We observe that while models
                               trained with small inputs in S perform on par with the baseline, models trained on “easy GCD”
                               perform slightly worse.
                               Finally, inspired by arguments that rare tail outcomes might require particular attention for learning
                               (Dohmatob et al., 2024), we experiment with small sets composed of examples from the tail of the
                               training distribution, namely, large GCD. Charton(2024)observesthatthesearebothhardertolearn,
                               and less common in the training set. Specifically, we create S with examples with GCD larger than
                               k (for k ranging from 1 to 5). While experiments achieve the best accuracies compared to the other
                               curation schemes we proposed, and values of k equal to 2 and 3 train slightly faster, they remain a
                               little below the baseline both in accuracy and learning speed.
                               Table 5: GCD problem: cherry-picking the small set. (Left) Number of (test) GCD predicted for training
                               budget of 600 million examples, average of 5 models (3 models for baseline). bold: more than 65 GCD
                               predicted. (Right) Training budget needed to predict 60 GCD, fastest of 20 models (of 12 models for baseline).
                                                                                                                          Training budget
                                                                   50k / 0.25   150k / 0.25    150k / 0.5   500K/0.5      for 60 GCD (M)
                                 Log-uniform inputs                   55.9         59.4          57.9          62.0             332
                                 Uniform GCD                          55.9         54.5          41.9          54.9               -
                                 Log-uniform inputs and GCD           62.2         71.7          66.5          72.6              88
                                 Small inputs (1-1000)                61.2         67.5          62.6          62.9             247
                                 GCD1-10                              59.9         63.8          55.8          62.3             401
                                 GCDproductsof2and5                   54.2         39.8          40.7          30.1             548
                                 All GCDbut1                          65.4         63.7          56.7          58.1             405
                                 All GCDbut1,2                        66.8         60.0          62.8          56.9             326
                                 All GCDbut1,2,3                      66.7         58.4          62.8          58.2             327
                                 All GCDbut1,2,3,4                    65.5         60.3          62.8          56.9             379
                                 All GCDbut1,2,3,4,5                  66.5         60.6          64.9          56.3             376
                                 GCDproductof2,3,and5                 66.1         59.4          59.8          47.3             359
                                 Prime GCD                            64.9         62.5          58.8          64.7             422
                                 GCDdivisible by primes ≥ 11          60.1         54.4          35.7          42.7             569
                                 Baseline (two-set training)          69.4         61.9          65.9          59.4             373
                                                                                    10
