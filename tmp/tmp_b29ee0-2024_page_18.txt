                18 S. He et al.
                67. Wu, Y., Cheng, X., Zhang, R., Cheng, Z., Zhang, J.: Eda: Explicit text-decoupling
                  and dense alignment for 3d visual grounding. In: CVPR (2023)
                68. Xiao, Z., Zhang, W., Wang, T., Loy, C.C., Lin, D., Pang, J.: Position-guided point
                  cloud panoptic segmentation transformer. arXiv preprint arXiv:2303.13509 (2023)
                69. Xu, J., Liu, S., Vahdat, A., Byeon, W., Wang, X., De Mello, S.: Open-vocabulary
                  panoptic segmentation with text-to-image diffusion models. In: CVPR (2023)
                70. Xu, R., Wang, X., Wang, T., Chen, Y., Pang, J., Lin, D.: Pointllm: Empowering
                  large language models to understand point clouds. In: ECCV (2024)
                71. Yang, J., Ding, R., Deng, W., Wang, Z., Qi, X.: Regionplc: Regional point-language
                  contrastive learning for open-world 3d scene understanding. In: CVPR (2024)
                72. Yang, Y.Q., Guo, Y.X., Xiong, J.Y., Liu, Y., Pan, H., Wang, P.S., Tong, X., Guo,
                  B.: Swin3d: A pretrained transformer backbone for 3d indoor scene understanding.
                  arXiv preprint arXiv:2304.06906 (2023)
                73. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P.,
                  Shi, Y., et al.: mplug-owl: Modularization empowers large language models with
                  multimodality. arXiv:2304.14178 (2023)
                74. Yeshwanth, C., Liu, Y.C., Nie√üner, M., Dai, A.: Scannet++: A high-fidelity dataset
                  of 3d indoor scenes. In: ICCV (2023)
                75. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,
                  Yang,Y.:Ferret: Refer and ground anything anywhere at any granularity. In: ICLR
                  (2024)
                76. Zhang, H., Li, H., Li, F., Ren, T., Zou, X., Liu, S., Huang, S., Gao, J., Zhang,
                  L., Li, C., et al.: Llava-grounding: Grounded visual chat with large multimodal
                  models. arXiv preprint arXiv:2312.02949 (2023)
                77. Zhang, H., Ding, H.: Prototypical matching and open set rejection for zero-shot
                  semantic segmentation. In: ICCV (2021)
                78. Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K.,
                  Luo, P.: Gpt4roi: Instruction tuning large language model on region-of-interest.
                  arXiv:2307.03601 (2023)
                79. Zhang, Y., Gong, Z., Chang, A.X.: Multi3drefer: Grounding text description to
                  multiple 3d objects. In: ICCV (2023)
                80. Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V.: Point transformer. In: ICCV
                  (2021)
                81. Zhou, J., Wang, J., Ma, B., Liu, Y.S., Huang, T., Wang, X.: Uni3d: Exploring
                  unified 3d representation at scale. In: ICLR (2024)
                82. Zhou,Z.,Zhang,Y.,Foroosh,H.:Panoptic-polarnet:Proposal-freelidarpointcloud
                  panoptic segmentation. In: CVPR (2021)
                83. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
                  language understanding with advanced large language models. In: ICLR (2024)
                84. Zhu, Z., Ma, X., Chen, Y., Deng, Z., Huang, S., Li, Q.: 3d-vista: Pre-trained trans-
                  former for 3d vision and text alignment. In: ICCV (2023)
