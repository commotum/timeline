                         Preprint, Under Review.
                         5.3  RLFINE-TUNING AND INSTRUCTION FOLLOWING
                         Following the supervised fine-tuning stage, we apply RL to further refine the agents’ capabilities.
                         The results (Fig. 1e-f and Fig. 3) from this phase highlight two main findings: (i) SFT priming
                         is essential prerequisite for learning effective dynamic planning under RL; and (ii) the two-stage
                         training process yields agents that can be steered by human-written plans to accomplish tasks beyond
                         their autonomous performance.
                         Ourexperiments reveal that the SFT priming stage is crucial for enabling agents to develop dynamic
                         planning skills through RL. The evidence for this is not just that SFT-primed agents outperform
                         their non-primed counterparts, but is most starkly demonstrated by the fact that the Base+RL plan
                         dynamically agent performed worse than the Base+RL no plan agent. This suggests that without the
                         structured demonstrations provided by SFT, the base model cannot learn to plan effectively through
                         RLalone; the attempt to generate plans is not only unhelpful but actively degrades performance. SFT
                         priming is what provides the essential scaffolding, turning planning from a detrimental behavior into
                         a highly effective strategy.
                         Theagent primed with explicit plans (SFT+RL plan dynamically) significantly outperforms the agent
                         primed only on actions (SFT+RL no plan). When planning is layered on top of SFT foundation, we
                         observeimprovedsampleefficiencyandtaskprogression. Qualitativeanalysisconfirmsthatthisagent
                         successfully learns to generate and execute plans at varying levels of abstraction, demonstrating the
                         ability to plan when necessary and replan in response to changing circumstances. We further observe
                         that the planning cost penalty Ctokens introduced in Section 3.3 effectively adjusts agent behavior
                         in proportion to the penalty magnitude, with higher costs leading to reduced planning frequency
                         and length during training while preserving task performance, indicating that agents can flexibly
                         adapt their planning strategies to different computational constraints (see Appendix C.3 for detailed
                         results).
                         Steerability and Human-Agent Collaboration While the training methodology produces significant
                         performance improvements, computational constraints prevent any of the autonomous agents from
                         fully solving the Crafter environment. To better understand the potential upper bound of our approach,
                         wealsoevaluate agents in a human-in-the-loop setup.
                         TheSFT+RLplandynamicallyagentprovestobethemostreliable at instruction following, making
                         fewer mistakes during combat and adhering more consistently to the plans than the base and SFT-only
                         models. Under human-provided plans, it successfully completes Crafter by collecting diamonds—an
                         achievement not observed in autonomous runs (see Fig. 3 for a representative sequence). Additional
                         qualitative examples and a Best-of-N analysis are provided in the Appendix D.
                         6   DISCUSSION, LIMITATIONS & CONCLUSION
                         Theability for LLMs to leverage test-time compute has been transformative, yet efficiently allocating
                         these resources in agentic settings stands as a critical, largely unexplored challenge. To the best of our
                         knowledge, this work presents the first systematic investigation and learned solution enabling LLM
                         agents to effectively allocate test-time compute in sequential decision-making tasks. Our findings
                         reveal that prevailing always-plan (e.g. ReAct) and never-plan approaches are suboptimal. Instead,
                         a “Goldilocks” effect emerges whereby intermediate frequencies outperform both extremes, likely
                         avoiding instability (Cnoise in our framework) induced by excessive replanning. This highlights the
                         need for LLM agents to strategically allocate, rather than naively scale, deliberation resources. Our
                         two-stage SFT+RL methodology demonstrates that agents can learn this meta-cognitive skill, moving
                         beyond fixed heuristics towards the adaptive, efficient behaviour essential for sustained autonomy.
                         Moreover, the resulting agents become sufficiently adept at planning and execution to be effectively
                         steered by human-written plans towards remarkable feats, including the full completion of Crafter
                         through diamond collection, significantly impacting human-AI collaboration and safety.
                         While our results establish the value of dynamic test-time compute allocation, several limitations
                         suggest directions for future work. Our experiments focused on specific models at certain scales
                         (Llama-3.1-8B-Instruct for fine-tuning, Llama-3.3-70B-Instruct for evaluation) due to computational
                         constraints. Investigating how optimal compute allocation strategies scale with model parameters
                         would provide valuable insights. Additionally, extending this work beyond our current environments
                                                                    9
