                              Latent Execution for Neural Program Synthesis
                                   XinyunChen                      DawnSong                  YuandongTian
                                   UCBerkeley                      UCBerkeley             Facebook AI Research
                           xinyun.chen@berkeley.edu       dawnsong@cs.berkeley.edu         yuandong@fb.com
                                                                Abstract
                                Program synthesis from input-output (IO) examples has been a long-standing
                                challenge. While recent works demonstrated limited success on domain-speciÔ¨Åc
                                languages (DSL), it remains highly challenging to apply them to real-world pro-
                                gramminglanguages, such as C. Due to complicated syntax and token variation,
                                there are three major challenges: (1) unlike many DSLs, programs in languages
                                like C need to compile Ô¨Årst and are not executed via interpreters; (2) the program
                                search space grows exponentially when the syntax and semantics of the program-
                                minglanguage become more complex; and (3) collecting a large-scale dataset of
                                real-world programs is non-trivial. As a Ô¨Årst step to address these challenges, we
                                propose LaSynth and show its efÔ¨Åcacy in a restricted-C domain (i.e., C code with
                                tens of tokens, with sequential, branching, loop and simple arithmetic operations
                                but no library call). More speciÔ¨Åcally, LaSynth learns the latent representation
                                to approximate the execution of partially generated programs, even if they are in-
                                complete in syntax (addressing (1)). The learned execution signiÔ¨Åcantly improves
                                the performance of next token prediction over existing approaches, facilitating
                                search (addressing (2)). Finally, once trained with randomly generated ground-
                                truth programs and their IO pairs, LaSynth can synthesize more concise programs
                                that resemble human-written code. Furthermore, retraining our model with these
                                synthesized programs yields better performance with fewer samples for both Karel
                                and C program synthesis, indicating the promise of leveraging the learned program
                                synthesizer to improve the dataset quality for input-output program synthesis (ad-
                                dressing (3)). When evaluating on whether the program execution outputs match
                                the IO pairs, LaSynth achieves 55.2% accuracy on generating simple C code with
                                tens of tokens including loops and branches, outperforming existing approaches
                                without executors by around 20%. 1
                        1   Introduction
                        Programsynthesis from input-output (IO) pairs, also called programming by example (PBE), requires
                        high-level reasoning and remains a challenging problem for deep models. Unlike Natural Language
                        Processing (NLP) [5, 16] and perceptual tasks such as Computer Vision (CV) [14, 22], the mapping
                        from IO pairs to the program itself is hard to model. Many works attempt to learn a direct mapping
                        from training samples, but often found that it is already difÔ¨Åcult to achieve a low training error, and
                        generalization to new problems is even harder. Alternatively, one might choose to formulate program
                        synthesis as a search problem: to Ô¨Ånd the program that satisÔ¨Åes IO pairs. Unfortunately, the search
                        space of programs is often vast and highly non-smooth, i.e., a small perturbation of the program often
                        leads to a complete change of the output.
                        While there are many previous works on programming by example tasks [6, 17, 9], they mainly focus
                        on Domain SpeciÔ¨Åc Languages (DSLs), and cannot be easily applied to popular general-purpose
                        programminglanguages. Forexample,tosynthesizeCprograms,weneedtodealwithbothhigh-level
                           1The code is available at https://github.com/Jungyhuk/latent-execution.
                        35th Conference on Neural Information Processing Systems (NeurIPS 2021).
                            control Ô¨Çows (e.g., branching and loop) and low-level operations (e.g., which variable is the target of
                            assignment). Moreover, unlike DSLs (e.g., Karel) for which it is feasible to implement a per-line
                            interpreter, C programs need compilation and a partial C program cannot execute. On the other
                            hand, some recent works investigate natural language descriptions as the auxiliary information of the
                            programspeciÔ¨Åcation,andtheyevaluateneuralprogramsynthesismodelsonconstrainedorsimpliÔ¨Åed
                            competitive programming problems [27, 3, 23, 11, 4]. Although some of these works demonstrate
                            promising results for synthesizing Python or C code, they require manual annotations of natural
                            language speciÔ¨Åcations [27] or large-scale pre-training on human-written programs [11, 4], and the
                            performance signiÔ¨Åcantly degrades when only input-output examples are fed into the synthesizer [3].
                            Tosynthesize C programs from input-output examples only, we propose LaSynth, which generates
                            the programinarecurrentandtoken-by-tokenmanner. AstheÔ¨Årstcontributiononmodelarchitectures
                            for program synthesis, we propose to use two latent parallel representations in the recurrent model.
                            Onerepresentationislearnedfromregularrecurrentmodelsasinautoregressivelanguagemodels[24],
                            with the double attention mechanism over IO pairs proposed in RobustFill [17] and an operation
                            predictor that models the arithmetic relationship between the program input and output. The second
                            representation, named Latent Execution Trace (LaET), models the hypothetical input signal for the
                            remaining partial program to execute to get to the desired output. Motivated by the line of work
                            on execution-guided program synthesis [47, 18, 57, 12], we learn a latent representation for C
                            programs which are not executed via interpreters, and train the model given only IO pairs without the
                            intermediate program execution states. The two parallel representations are trained end-to-end.
                            Asthesecondcontribution on dataset construction, we demonstrate that it is possible to automatically
                            construct a C codebase that is of high quality, controllable and concise through our proposed program
                            synthesis procedure. SpeciÔ¨Åcally, starting from randomly generated C programs that might contain
                            a lot of redundant statements, we show that via iterative retraining, the subsequent generated code
                            from our learned model becomes more concise and similar to human-written ones. Moreover,
                            learning directly from the generated code leads to better performance given the same amount of
                            samples, and improves the sample efÔ¨Åciency. We observe similar results when applying our iterative
                            retraining technique to Karel [9], another programmingbyexamplebenchmarkconsistingofrandomly
                            generated programs. Although the initial Karel dataset includes a large proportion of complicated
                            programs with different control Ô¨Çow constructs, we demonstrate that nearly half of the problems can
                            be solved by straight-line programs, which again conÔ¨Årms that randomly generated programs tend
                            to be unnecessarily complicated. We envision that the iterative retraining procedure could greatly
                            reduce laborious efforts in human codebase collection in future research.
                            Asthe third contribution, we show for the Ô¨Årst time that short C code in a restricted domain (tens
                            of tokens, no library call) with sequential, branching, loop and simple arithmetic operations can be
                            effectively synthesized from IO pairs only. In particular, while LaSynth tends to generate more
                            concise programs (and does not have exact token match with random generated ground truth code),
                            whenmeasuringwhethertheprogramexecutionoutputsmatchtheIOpairs,LaSynthachieves55.2%
                            accuracy, and outperforms existing neural program synthesis models by around 20%. These results
                            demonstrate the effectiveness of learning latent execution traces.
                            2   Neural ProgramSynthesis from Input-Output Examples
                            In programming by example tasks, the program speciÔ¨Åcation is a set of input-output examples [17, 9].
                            SpeciÔ¨Åcally, we provide the synthesizer with a set of K input-output pairs {(I(k),O(k))}K   ({IO}K
                                                                                                                   k=1
                            in short). These input-output pairs are annotated with a ground truth program P?, so that P?(I(k)) =
                            O(k) for any k ‚àà {1,2,...,K}. To measure the program correctness, we include another set of held-
                            out test cases {IO}Ktest that differs from {IO}K. The goal of the program synthesizer is to predict
                                                test
                                                     K                    ?                                K          K
                            a program P from {IO} , so that P(I) = P (I) = O for any (I,O) ‚àà {IO}             +{IO} test.
                                                                                                                      test
                            CProgramSynthesis. In this work, we make the Ô¨Årst attempt of synthesizing C code in a restricted
                            domain from input-output examples only, and we focus on programs for list processing. List
                            processing tasks have been studied in some prior works on input-output program synthesis, but
                            they synthesize programs in restricted domain-speciÔ¨Åc languages instead of full-Ô¨Çedged popular
                            programming languages [6, 39, 38].
                            OurCcodesynthesisproblembrings new challenges for programming by example. Compared to
                            domain-speciÔ¨Åc languages, the syntax and semantics of C are much more complicated, which signiÔ¨Å-
                            cantly enlarges the program search space. Meanwhile, learning good representations for partially
                                                                             2
                                                     Random Input                                    Random Program 
                                                        Generator                                        Generator
                                                   I : [-4, 3, 1, 2, 1]                                                          O : [-4, 3, 3, 3, 3]
                                                    1                                                                              1
                                                   I : [3, 4, 3, 2, 4]                                                           O : [3, 4, 3, 3, 3]
                                                    2                                                                              2                                Neural 
                                                   I : [2, 1, 4, 1, 2]                                                           O : [2, 1, 3, 3, 3]
                                                    3                                                                              3                               Program 
                                                   I : [1, 4, 1, 1, -2]                                                          O : [1, 4, 3, 3, 3]
                                                    4                                                                              4                              Synthesizer
                                                   I : [2, 4, 4, -1, 4]                                                          O : [2, 4, 3, 3, 3]
                                                    5                                                                              5
                                                  Figure 1: Illustration of the C program synthesis pipeline. For dataset construction, we develop a random
                                                  program generator to sample random C programs, then execute the program over randomly generated inputs and
                                                  obtain the outputs. The input-output pairs are fed into the neural program synthesizer to predict the programs.
                                                  Note that the synthesized program can be more concise than the original random program.
                                                  (a) Model Overview                                                                     (b) Program Decoder                                                             	Ì†µ„åµ
                                                                                                                                               Ì†µ„åµ ,	Ì†µ„åµ ,	‚Ä¶,			Ì†µ„åµ             Program	                                       "
                                                   Ì†µ„åµ                                    Ì†µ„åµ                                 Ì†µ„åµ                   /     $           "#$        Context
                                                     "#$                                   "                                  "?$            	‚Ñé                                      Ì†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµ            	‚Ñé            Softmax
                                                   ‚Ñé                Program	             ‚Ñé               Program	           ‚Ñé                   "#$                                           -              "
                                                     "#$            Decoder                "             Decoder              "?$
                                                                     Latent	                              Latent	                             '                     IO	                 1     2         Max 	Ì†µ„åµ           	Ì†µ„åµ
                                                                                                                                             	Ì†µ„åµ       Ì†µ„åµ                            	Ì†µ„åµ   Ì†µ„åµ                      "        "
                                                    '                                     '                                  '                 "#$              Encoder                 "    "          Pool
                                                   	Ì†µ„åµ              Executor             	Ì†µ„åµ             Executor           	Ì†µ„åµ
                                                     "#$                                   "                                  "?$                                                        Ì†µ„åµ4Ì†µ„åµ
                                                                                                                                                                                             "
                                                  (c) Latent Executor                                                                    (d)                                                    Operation Predictor
                                                    ‚Ñé                                                                          Ì†µ„åµ                           Attention
                                                      "                                                                                                                    I    O        Op
                                                                                                                     Training	                       1($)       1(=)
                                                                                                                                                  	Ì†µ„åµ     ‚Ä¶ Ì†µ„åµ            2     4     O=2+I
                                                                                                                      Target                         "          "
                                                                                Ì†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµ                                                                                  2     0     O=2-I Retrieve
                                                    '            Ì†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµ              "   Softmax            '                 '                   2($)       2(=)                                               ($)         (=)
                                                   	Ì†µ„åµ                     7                                	Ì†µ„åµ                	Ì†µ„åµ                 Ì†µ„åµ     ‚Ä¶Ì†µ„åµ                                                 	Ì†µ„åµ4Ì†µ„åµ  ‚Ä¶	Ì†µ„åµ4Ì†µ„åµ
                                                     "#$                                                       "                 >                  "          "          ‚Ä¶ ‚Ä¶                                     "          "
                                                  Figure 2: (a) An overview of LaSynth model architecture. (b), (c), and (d) present the details of the program
                                                  decoder, latent executor, and the operation predictor. Note that the operation predictor is specialized for numerical
                                                  calculation, and thus is not used for the Karel domain.
                                                  decoded programs also becomes more difÔ¨Åcult. In particular, prior neural program synthesizers that
                                                  utilize per-line interpreters for the programming language to guide the synthesis and representation
                                                  learning [12, 44, 37, 18, 38] are not directly applicable to C. Although it is possible to dump some
                                                  intermediate variable states during C code execution [10], since partial C programs are not executable,
                                                 weareable to obtain all the execution states only until a full C code is generated, which is too late to
                                                  include them in the program decoding process. In particular, the intermediate execution state is not
                                                  available when the partial program is syntactically invalid, and this happens more frequently for C
                                                  due to its syntax design.
                                                  3       ProgramSynthesiswithLearnedExecution
                                                  In this section, we present LaSynth which learns to represent the execution of partial programs to
                                                  guide the synthesis process. Fig. 2(a) provides an overview of LaSynth model architecture which
                                                  consists of two components, the program decoder and the latent executor. We present the core design
                                                  below, and defer more details to Appendix B and Appendix C.
                                                  3.1       ModelOverview
                                                 Atahighlevel,theprogramdecoder(Fig.2(b))takesalatentvectorh                                                                       that represents the generated
                                                                                                                                                                            t‚àí1
                                                  partial program, the previous (generated) program token p                                                       , and outputs the latent vector h and
                                                  the next program token p to be generated at time step t:                                                  t‚àí1                                                             t
                                                                                               t
                                                                                               (h ,p ) = ProgramDecoder(h                                     , p        ; IO          )                                        (1)
                                                                                                   t     t                                              t‚àí1       t‚àí1           t‚àí1
                                                  Here the recurrent model is conditioned on the IO pair IOt‚àí1. When IOt = IO := (I,O) for
                                                  every t, i.e., IOt remains constant over the entire recurrent generation process, Eqn. 1 represents the
                                                  standard recurrent architecture used in most autoregressive natural language models [24, 49], and is
                                                  also used in prior works on program synthesis from input-output examples [17, 9].
                                                                                                                                          3
                                                                                             I      O
                          For program decoding, the decoder Ô¨Årst takes two attention vectors s and s   computed from IO
                                                                                             t      t
                          pairs and latent vector h  via double attention [17], and utilizes a max pooling layer to compute an
                                                 t‚àí1
                          aggregated embedding mt for all IO pairs (Fig. 2(b)):
                                                                                      I(j)  O(j)
                                                 mt =MaxPool               (tanh(W[s      ; s   ]))                   (2)
                                                                j‚àà{1,2,...,K}         t     t
                          Here the superscript (j) indicates that the representation is for the j-th IO pair, [a;b] is vector
                          concatenation of a and b, and W is a trainable matrix. To facilitate the prediction of long programs,
                          we compute an attention vector d over previously generated program tokens using the standard
                                                           t
                          attention mechanism [5, 30]:
                                                        d =Attention(m ,{p ,...,p        })                           (3)
                                                          t                t   0     t‚àí1
                          Finally, the next token p is sampled from P[p ] = Softmax(V d )  where V is a trainable matrix.
                                                 t                    t                t pt
                          3.2   Latent Executor Design
                          Asshowninourexperiments(Sec.5), the standard program decoder architecture may not be able
                          to achieve strong performance in program synthesis when the program complexity increases. One
                          main reason is that the standard program decoder only takes the initial IO pairs as the input without
                          considering the program execution, thus the learned representation for the partial program does
                          not effectively guide the synthesis process. Motivated by prior works that utilize execution traces
                          for Karel program synthesis [12, 44, 47], in this paper, we introduce latent executor (Fig. 2(c))
                                                                  ÀÜ                                       ÀÜ
                          which maintains a second representation I during program decoding. Intuitively, I   models the
                                                                   t                                      t‚àí1
                          hypothetical input of the partial program pt...T so that its output becomes O. Given the estimated
                                ÀÜ                                                     ÀÜ
                          input I    and the latent vector h , the latent executor returns I at the next time step t:
                                 t‚àí1                      t                           t
                                                          ÀÜ                      ÀÜ
                                                          I =LatentExecutor(I        , h )                            (4)
                                                           t                     t‚àí1   t
                                             ÀÜ T
                          Thecollection of {It}t=0 is the latent execution trace (LaET). With the help of latent executor, we
                                                          ÀÜ
                          nowusetheIOpairsIOt‚àí1 := (It‚àí1,O)instead of (I,O) for the program decoder (Eqn. 1).
                          3.3   End-to-end Training
                          Wetrain our model with supervised learning, by minimizing the sum of token prediction loss LProg,
                          and the latent executor loss LExec:
                                                 P              L=LProg+LExec                                         (5)
                                                    T            ?
                          SpeciÔ¨Åcally, LProg :=        Loss(pt,p ) is the step-by-step cross-entropy loss between the pre-
                                                    t=1          t             ?
                          dicted programs p1...T and the ground truth programs p1...T.
                          For latent executor, since the semantics of partial programs (e.g., partial C programs) are not always
                          well-deÔ¨Åned, there is no step-by-step training supervision. However, the output of the executor should
                          be consistent with the program speciÔ¨Åcation after taking the annotated ground truth program as the
                                                   ÀÜ                                                      ÀÜ
                          input. Therefore, we set I  = I (true input) and minimize the distance between I   and O (true
                                                    0                                                      T
                          output) after the program Ô¨Ånishes:
                                                                              ÀÜ
                                                               LExec = Loss(IT,O)                                     (6)
                          Note that LExec does not rely on any assumptions of the partial program semantics, and thus is
                          applicable to both domain-speciÔ¨Åc languages and general-purpose programming languages such as C.
                          In our evaluation, equipping with the latent executor signiÔ¨Åcantly improves the program prediction
                          performance, where each program could include up to 256 tokens.
                          3.4   DataRegenerationandIterative Retraining
                          Interestingly, once our model is trained on the initial random generated programs D , the predicted
                                                                                                          0
                          program becomes more concise and resembles human-written code. While the exact token match
                          accuracy is low even on the training set, the model still satisÔ¨Åes the IO pairs for many problems. We
                          leverage such a phenomenon to construct a new dataset D with higher-quality programs from D .
                                                                                  1                                     0
                          SpeciÔ¨Åcally, we run beam search on the trained model to predict program p0...T given input-output
                          pairs in the training set. If model prediction p0...T satisÔ¨Åes all the input-output examples and held-out
                          cases, we replace the original program p?  with p0...T in D1, and keep p?  otherwise. Afterward,
                                                                0...T                           0...T
                          we re-train the model on D . In Sec. 5, we will demonstrate that the retraining process further
                                                     1
                          improves the model performance, especially with smaller training datasets.
                                                                         4
                          Table 1: The comparison between our restricted C domain and existing programming by example tasks.
                                                  Control Ô¨Çow  Variables  Arithmetics  Nohelper functions
                              Restricted C (Ours)     3            3          3                3
                                   Karel [9]          3            ‚àí          ‚àí                ‚àí
                                DeepCoder[6]          ‚àí            3          3                ‚àí
                                 FlashFill [19]       ‚àí            ‚àí          ‚àí                ‚àí
                        4   Restricted C Program Synthesis Domain
                        In this section, we discuss our restricted C program synthesis domain, and our operation predictor
                        design for improving the numerical reasoning ability of program synthesis models.
                        4.1  DataGeneration
                        Collecting large-scale high-quality datasets for program synthesis requires a lot of human efforts, and
                        weaimtoreducethemanualworkfordatasetconstruction.
                        Ourdatagenerator is built upon Csmith [54], a random C code generation tool originally designed for
                        Ô¨Ånding bugs in compilers. Following the common practice of generating input-output pairs, for each
                        program, we randomly sample 5 numerical lists as the program inputs, and execute the program to
                        obtain the corresponding output lists. This is similar to existing works on PBE problems that sample
                        programs based on a probabilistic context-free grammar, randomly generate valid inputs for the
                        programs and obtain the outputs [40, 15, 6]. This creates inÔ¨Ånite samples for synthesizing programs
                        in domain-speciÔ¨Åc languages. While the programs sampled in this way differ from human-written
                        code, Sec. 3.4 shows that they can be converted to be more concise and human-like.
                        The subset of language features used. Our generated program has variable declaration, variable
                        assignment, and expressions with addition or subtraction operations. The programs also have non-
                        sequential statements, including If statements, For loops, Continue and Break statements. Except
                        for the input argument which is a list, all variables declared are integers, and all program statements
                        are integer manipulation. Each expression has at most 2 mathematical operations, and chaining the
                        full C program could perform multi-step numerical calculation (e.g., p0 = p0 - p1 + p2; p0 =
                        p0 - 1;). Looping statements other than For (i.e., While or Do-While loops) are not supported.
                        Note that we only constrain the Ô¨Ånal program length (‚â§ 256 tokens) and the program can have nested
                        for-loops and complicated if-conditions.
                        Post-processing. Weperformafewpost-processingstepstoobtainourÔ¨Ånalprogramsfromprograms
                        generated by Csmith (see Fig. 1 for an example). We resample different components of the program,
                        so that (1) each constant numerical value lies in [‚àí4,4], (2) mathematical operators only contain
                        addition and subtraction, and (3) upper/lower limits of For loops are positive and within the length
                        of the list. Programs are discarded if they are trivial (e.g., constant or identity mappings), or the
                        input-output examples include values out of the range [‚àí4,4].
                        Final dataset. We reweight the program distribution so that at least half of them include For loops.
                        Ourfull dataset includes 500K samples in the training set, 1K samples in the validation set, and 1K
                        samples in the test set. As shown in Fig. 1, the randomly sampled program may contain redundant
                        statements, which can be easily avoided by human programmers. We compare our restricted C
                        domain to prior datasets of programming by example in Table 1.
                        4.2  ProgramDecodingwiththeOperationPredictor
                        For program decoder, predicting the next program token pt is non-trivial, especially when mathe-
                        matical reasoning is required [43, 29]. To improve the program synthesis performance for domains
                        involving numerical calculation, such as our restricted C domain, we design an associative memory
                        structure named operation predictor (Fig. 2(d)), based on the following intuition: given the input
                        I = 2 and output O = 4, human would infer that ‚ÄúO = I + 2‚Äù might be the desired operation and
                        write down the code accordingly. To materialize such an intuition, we create a pre-computed table that
                        covers all possible integer addition and subtraction operations for valid input and output list values.
                        Wedefer the details of the model architecture to Appendix B.2. The program decoding process
                        remains similar to the one described in Sec. 3, and we highlight the key differences as follows.
                                                                   5
                                  Table 2: The comparison between LaSynth and baseline neural program synthesis models in our evaluation.
                                                               LaSynth      Exec     Shin et al.   Bunel et al.    RobustFill     Property Signatures
                                                                             [12]       [44]            [9]            [17]                [39]
                                    +Programexecution              3          3           3              ‚àí              ‚àí                   ‚àí
                                    Nointerpreter needed           3          ‚àí           ‚àí              3              3                   3
                                                                                                  I       O
                                 Theoperation predictor takes two attention vectors s and s                  as the representations of input-output
                                                                                                  t       t
                                 examples, and yields an operator embedding opÀÜ . To compute the aggregated embedding vector for
                                                                                            t
                                 all input-output examples, we modify Eqn. 2 to also take opÀÜ t as an input of the max pooling layer:
                                                                                                           I(j)   O(j)     (j)
                                                          mt =MaxPool                       (tanh(W[s          ; s     ; opÀÜ  ]))                      (7)
                                                                              j‚àà{1,2,...,K}                t      t        t
                                 Totrain the operation predictor, we add an additional loss LOp:
                                                                             L=LProg+LExec+LOp                                                         (8)
                                 LOp is designed to ensure that the operation predictor predicts operations related to IO pairs, and we
                                 defer the details to Appendix B.2.
                                 Limitations. In our current implementation of the operation predictor, the operation table is only
                                 able to enumerate the arithmetic operations over a pre-deÔ¨Åned constant set, thus it requires that the
                                 set of possible numerical values in input-output pairs is Ô¨Ånite. One way of extending our operation
                                 predictor to support potentially unbounded numerical calculation is to combine it with the subword
                                 tokenizer, which has been commonly used in recent language models [16, 11, 4]. We consider
                                 designing general-purpose number representation for better mathematical reasoning as future work.
                                 5     Experiments
                                 In this section, we discuss our results on synthesizing programs in Karel and C languages. We Ô¨Årst
                                 show that LaSynth achieves competitive performance on Karel benchmark. Then we present the
                                 results on our restricted C benchmark, and demonstrate that our approach signiÔ¨Åcantly outperforms
                                 existing neural program synthesis models. Finally, we discuss the effect of iterative retraining.
                                 5.1    Karel Program Synthesis
                                 5.1.1     Evaluation Setup
                                 Karel domain. Karel is an educational programming language [41], and has been studied in recent
                                 works on neural program synthesis from input-output examples [15, 9, 12, 44]. A Karel program
                                 controls a robot in a 2D grid world. There are instructions that control the robot, e.g., move, turnLeft
                                 and PutMarker, as well as conditionals and loops, i.e., if, repeat and while. See Appendix A for
                                 grammarspeciÔ¨Åcation and the state representation.
                                 WetrainandevaluateallmodelsontheKareldatasetintroducedin[9]. Thedatasetcontainsrandomly
                                 sampled programs from the Karel DSL (1.1M training samples, 2.5K samples in the validation set
                                 and2.5K samplesinthetestset). Eachprogramincludes5input-outputpairsasthespeciÔ¨Åcation, and
                                 the sixth pair as the held-out test case. Following the prior work, we evaluate two metrics: (1) Exact
                                 Match: the predicted program is the same as the ground truth; (2) Generalization: the predicted
                                 program satisÔ¨Åes both the input-output pairs and the held-out input-output test case.
                                 Baselines. Bunel et al. [9] designed the Ô¨Årst program synthesis model for the Karel benchmark with a
                                 similar high-level design as RobustFill, but they use convolutional neural networks (CNN) to encode
                                 the Karel grid maps. Compared to LaSynth, this model does not utilize any program execution
                                 information, and does not include our latent executor. Instead of directly synthesizing the program
                                 frominput-output examples, the model in Shin et al. [44] Ô¨Årst predicts the execution traces containing
                                 the robot actions from the input-output pairs, then decodes the program based on the execution traces.
                                 This model improves the prediction performance over Bunel et al., but it requires the full execution
                                 traces for model training and an interpreter for execution. Exec [12] leverages the execution states of
                                 partial generated programs to guide the subsequent synthesis process, but the execution states are
                                 obtained from the Karel interpreter rather than learned by the model, thus this approach represents
                                 the ideal scenario where the partial programs could be executable.
                                                                                             6
                             Table 3: Results on Karel dataset. Gen and Exact
                             denote generalization and exact match accuracies.
                               Approach             Gen        Exact
                               LaSynth            83.68%      41.12%
                               Exec [12]          86.04%      39.40%
                               Bunel et al. [9]   77.12%      32.17%          Figure 3: Generalization accuracies with different train-
                               Shin et al. [44]   81.30%      42.80%          ing data sizes on Karel. With the full training set, the
                                                                              accuracies are 86.04%, 89.28% and 89.36% for train-
                                                                              ingonrandomprograms,retrainingfor1and2iterations.
                                                         (a)                                             (b)
                             Figure 4: Program distributions after iterative retraining on Karel. (a) The distributions of different program
                             types. Seq-only: no control Ô¨Çows. If-only: the program includes If statements but no loops. Repeat/While-only:
                             the program includes Repeat/While loops, but no other control Ô¨Çow constructs. Mixture: the program includes at
                             least two types of control Ô¨Çow constructs. (b) The distributions of programs with different token lengths.
                             Our model architecture for Karel is largely similar to the model for C code synthesis, except that
                             weemploythe CNN encoder in Bunel et al. [9] in our program decoder and latent executor. The
                             comparison with baseline models is shown in the middle block of Table 2. All models use the beam
                             search for decoding programs, with the beam size of 64.
                             5.1.2   Results
                             Wepresent the results of LaSynth and baseline model architectures in Table 3. First, LaSynth
                             outperforms all baselines that do not incorporate the partial program execution information, and
                             achieves competitive performance compared with the Exec algorithm that requires an interpreter to
                             obtain the partial program execution states. In particular, LaSynth achieves a higher generalization
                             accuracy than Shin et al. with lower exact match accuracy, showing that decoded programs by
                             LaSyntharemoredifferent from randomly generated programs. Although Shin et al. also model the
                             program execution by predicting the robot actions, the prediction of the action traces does not take
                             the program structure into account, resulting in the inferior performance.
                             5.2   CCodeSynthesis
                             5.2.1   Evaluation Setup
                             Given the variety of C programs, we observe that the exact match accuracies of models are mostly
                             nearly 0. Therefore, we focus on evaluating the generalization accuracy, and we consider the predicted
                             program to be correct when it satisÔ¨Åes both the 5 input-output examples and 5 held-out test cases.
                             Baselines. We compare the full LaSynth with its multiple ablated versions:
                                     ‚Ä¢ NoExecutor. The program decoder (Eqn. 1) always takes the initial input-output pairs as
                                                       ÀÜ
                                       the input; i.e,. It = I0 for every t.
                                                                                  7
                                                                                             int * func_1(int a[])
                                                                                             {
                                                         int * func_1(int a[])                   int p_0 = 0;
                                                         {                                       int l_7 = 3;
                              int * func_1(int a[])          int p_0 = 2;                        int l_8 = 1;
                              {                              int l_12 = 3;                       a[l_8] = (a[l_7] - a[p_0]);
                                  int p_0 = 0;               for (p_0 = 1; p_0 <= 2; p_0++)      for (p_0 = 3; p_0 <= 4; p_0++)
                                  int l_25 = 4;              {                                   {
                                  a[p_0] = 1;                    a[p_0]--;                           for (int p_1 = 1; p_1 <= 2; p_1++)
                                  --a[l_25];                 }                                       {
                                  return a;                  a[l_12] = a[l_12] + 4;                       a[p_1] = a[p_1] + a[p_0];
                              }                              return a;                                    a[p_1] = a[p_1] + 2;
                                                         }                                           }
                                                                                                 }
                                                                                                 return a;
                                                                                             }
                              Figure 5: Sample programs that could be correctly predicted by LaSynth, but wrongly predicted by models
                              without the latent executor. These programs require multiple different operations for different input list elements.
                                                                  ÀÜ
                                     ‚Ä¢ NoPartialExecutor. I = I = I for every t and additionally h is regularized so that
                                                                  t     0                                       T
                                        LatentExecutor(I ,h ) matches the output O under loss L                 . Therefore, no partial
                                                            0   T                                          Exec
                                        latent execution.
                                     ‚Ä¢ NoOpPredictor. The max pooling layer only takes the vectors computed by the double
                                        attention as the input (Eqn. 2).
                                     ‚Ä¢ NoAttentionInDecoding. There is no attention over decoded program tokens, and the
                                        output of the max pooling layer is directly fed into the output softmax layer; i.e., P[pt] =
                                        Softmax(Vm ) (comparedtoEqn.3).
                                                       t pt
                              Wealsocomparewithexisting neural program synthesis models with good performance on related
                              tasks, as shown in the rightmost block of Table 2. RobustFill [17] is the state-of-the-art neural
                              network architecture on FlashFill benchmark, which synthesizes string manipulation programs in a
                              domain-speciÔ¨Åc language. As described in Sec. 3, the input-output encoder and the program decoder
                              architectures in RobustFill are similar to LaSynth, except that it does not include the latent executor,
                              operation predictor, and the attention on the decoded program sequence.
                              Property Signatures [39] was designed for synthesizing list manipulation programs in domain-speciÔ¨Åc
                              languages, but instead of taking the raw input and output lists as the neural network input, they design
                              someproperties that distinguish different programs, then take the values of these properties as the
                              modelinput. A sample property could be whether the program output is the same as the input, and
                              the property values could be ‚ÄúAll True‚Äù, ‚ÄúAll False‚Äù, or ‚ÄúMixed‚Äù, indicating that the property always
                              holds for any input-output pair in the speciÔ¨Åcation, never holds, and holds for some pairs but not
                              others, respectively. We customize the original design [39] for our setting. First, our property set
                              takes the format of O = C + I? and O = C ‚àí I?, where C ‚àà [‚àí4,4]. For example, O = 2 + I?
                              meanswhethertheoutput O could be calculated by adding 2 to the input I. These properties focus
                              more on numerical calculation, similar to our operation predictor. Second, different from the task
                              in [39], our C programs sometimes manipulate only a subset of the input lists, thus encoding the list
                              with a single property value is inappropriate. Instead, we compute the property value per element in
                              input-output pairs, use a bi-directional LSTM to encode the property values as a sequence, then take
                              the outputs of the bi-LSTM for program prediction.
                              5.2.2   Results
                              Table 4 presents the results, where all models are trained on the initial random programs. The full
                              LaSynth outperforms other variants, and improves the performance of RobustFill and Property
                              Signatures by around 20%. We also increase the model size of RobustFill to see if the improvement
                              comes from larger model size, but the results are not better. In particular, the latent executor
                              signiÔ¨Åcantly increases the prediction accuracy, and achieves better results than NoPartialExecutor,
                              which shows that learning latent execution traces leads to better partial program representations. In
                              Fig. 5, we present sample programs that could be correctly synthesized by LaSynth, but models
                              without the latent executor provide the wrong prediction. We observe that the latent executor is
                              beneÔ¨Åcial when the program involves different manipulations for different list elements, e.g., more
                              than one For loop and different mathematical calculations. Our breakdown results on programs
                              of different complexity also justify this observation. We Ô¨Årst present the results on programs with
                              different control Ô¨Çow constructs in Fig. 6. SpeciÔ¨Åcally, Seq-only includes programs with no control
                                                                                   8
                                      Table 4: Results on C dataset.
                                Approach                     Accuracy
                                LaSynth                        55.2%
                                NoAttentionInDecoding          53.5%
                                NoOpPredictor                  53.7%
                                NoPartialExecutor              42.9%
                                NoExecutor                     38.6%
                                RobustFill [17]                37.6%
                                Property Signatures [39]       34.5%        Figure 6: Accuracies of different program types
                                                                            on C dataset.
                                                    (a)                                         (b)
                           Figure 7: Results of iterative retraining on the C dataset. (a) Accuracies with different training data sizes. With
                           the full training set, the accuracies are 55.2%, 56.0% and 56.5% for training on random programs, retraining
                           for 1 and 2 iterations, respectively. (b) The program distributions after each retraining iteration.
                           Ô¨Çowconstructs, For-only includes programs with For loops but no If statements, and Mixture includes
                           programswithbothForloopsandIfstatements. Thenwedemonstratetheresultsondifferentprogram
                           lengths in Fig. 8b. We show that LaSynth achieves decent performance on long and complicated
                           programs, while the accuracies of baseline models drop dramatically.
                           5.3  Discussion of Iterative Retraining
                           In Fig. 3, we show the effectiveness of retraining on decoded Karel programs (Sec. 3.4). We observe
                           that retraining for one iteration is sufÔ¨Åcient, and it signiÔ¨Åcantly improves the generalization accuracy
                           by over 3%. To understand the differences between predicted programs and randomly generated
                           programs, we demonstrate the changes of dataset distributions after each retraining iteration in Fig. 4a
                           and 4b. We observe that the model learns to predict more concise programs than the ground truth for
                           a large proportion of input-output examples, and considerably alters the dataset distribution so that it
                           becomesmoreconcentrated on short programs with simpliÔ¨Åed control Ô¨Çow structures. SpeciÔ¨Åcally,
                           from Fig. 4a, although the initial Karel dataset seems to include a large proportion of complicated
                           programs with different control Ô¨Çow constructs, our model synthesizes straight-line programs for
                           nearly half of the samples, which means that many loops and branches in the annotated ground
                           truth programs are unnecessary. This distribution shift also explains the gap between the exact
                           match and generalization accuracies. The program distribution after the second retraining iteration is
                           largely similar to the Ô¨Årst iteration, thus retraining for more iterations does not considerably improve
                           the performance. Note that in the second iteration, the synthesizer tends to generate slightly more
                           complicated programs than the Ô¨Årst iteration, in order to deal with the cases when the input-output
                           examples oversimplify the intended program functionality. For example, sometimes the input-output
                           examplesdonotcovertheedgecasesthattherobotmayencounter,thusaddingadditionalIfbranches
                           could avoid the crashes when testing on held-out cases.
                           Fig. 7a presents the results of retraining on decoded C programs. Similarly, retraining improves the
                           prediction accuracy, especially when the training set is small. From Fig. 7b and 8a, we again observe
                           that the model tends to predict shorter programs than the random code, and it eliminates unnecessary
                           control Ô¨Çows to simplify the programs. We present more examples in Appendix D.
                                                                           9
                        (a)                 (b)
             Figure 8: Results on programs of different token lengths on the C dataset. (a) The program token length
             distributions after each retraining iteration. (b) The accuracies on programs of different token lengths.
             6 Related Work
             Programming by example. Programming by example problems have been widely studied with
             various applications, and recent works have developed deep neural networks as program synthesiz-
             ers [20, 40, 17, 9]. Most prior works focus on synthesizing programs in domain-speciÔ¨Åc languages,
             such as FlashFill [40, 17, 51] for string transformation, Karel [9, 44, 12, 21] for simulated robot
             navigation, and LISP-style languages for list manipulation [6, 25, 57, 36]. In this work, we make the
             Ô¨Årst attempt of synthesizing C code in a restricted domain from input-output examples only, and we
             focus on the list manipulation domain.
             Somerecent works investigate the limitations of synthetic datasets and the ambiguity in program
             speciÔ¨Åcations for neural program synthesis [45, 13, 46, 28]. These works focus on reducing the bias
             of data distributions and generating more diverse input-output pairs, while our data regeneration
             aims to improve the quality of programs. We consider incorporating both lines of work to further
             improve the dataset quality as future work. In addition, drawing the inspiration from self-training
             and bootstrapping techniques developed for other applications [35, 1, 32, 52] to extend our iterative
             retraining scheme is also another future direction.
             Execution-guided program synthesis. To learn better program representations, some recent works
             incorporate the execution information to guide the synthesis process [47, 57, 44, 12, 18, 48, 7, 21, 38,
             37,31]. Inparticular, leveragingpartialprogramexecutionstatesimprovestheperformanceforseveral
             programsynthesistasks[12,57,18,37]. However,existingapproachesrelyonprograminterpretersto
             provide the intermediate execution results whenever applicable. In contrast, we demonstrate that our
             latent executor learns the latent execution traces (LaET) without such a requirement. Besides program
             synthesis, execution traces have also been utilized for other software engineering applications [2, 33].
             Neuralexecution. Ourlatent executor is related to prior works on learning to execute algorithms [55,
             50, 53] and programs [8]. They focus on predicting execution results for full algorithms and programs,
             but do not utilize them for program synthesis. Latent state prediction has also been studied in other
             applications such as task-oriented dialogue systems [34, 56] and robotics [42].
             7 Conclusion
             Inthiswork,weproposeLaSynth,whichlearnsthelatentrepresentationtoapproximatetheexecution
             of partial programs, even if their semantics are not well-deÔ¨Åned. We demonstrate the possibility of
             synthesizing elementary C code from input-output examples only, and leveraging learned execution
             signiÔ¨Åcantly improves the prediction performance by around 20%. Meanwhile, compared to the
             randomly generated programs, LaSynth synthesizes more concise programs that resemble human-
             written code, and training on these synthesized programs further improves the prediction performance
             for both Karel and C program synthesis. Our results indicate the promise of leveraging the learned
             program synthesizer to improve the dataset quality for programming by example tasks.
             Weconsider extending our approach to synthesize more complicated real-world code as future work.
             Forexample,wewillintegrateourlatentexecutorintolarge-scale pre-trained language models, which
             could further improve the performance of those program synthesis models taking natural language
             speciÔ¨Åcations. We will also study program synthesis problems with unbounded input ranges and
             different type signatures, which could be approached with the usage of subword tokenizers.
                                  10
           References
            [1] S. Abney. Bootstrapping. In Proceedings of the 40th annual meeting of the Association for
              Computational Linguistics, pages 360‚Äì367, 2002.
            [2] M. Alam, J. Gottschlich, N. Tatbul, J. S. Turek, T. Mattson, and A. Muzahid. A zero-positive
              learning approach for diagnosing software performance regressions. Advances in Neural
              Information Processing Systems, 32:11627‚Äì11639, 2019.
            [3] F. Alet, J. Lopez-Contreras, J. Koppel, M. Nye, A. Solar-Lezama, T. Lozano-Perez, L. Kaelbling,
              and J. Tenenbaum. A large-scale benchmark for few-shot program induction and synthesis. In
              International Conference on Machine Learning, pages 175‚Äì186. PMLR, 2021.
            [4] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,
              Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732,
              2021.
            [5] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align
              and translate. In International Conference on Learning Representations, 2015.
            [6] M. Balog, A. L. Gaunt, M. Brockschmidt, S. Nowozin, and D. Tarlow. Deepcoder: Learning to
              write programs. In International Conference on Learning Representations, 2017.
            [7] M. Balog, R. Singh, P. Maniatis, and C. Sutton. Neural program synthesis with a differentiable
              Ô¨Åxer. arXiv preprint arXiv:2006.10924, 2020.
            [8] D. Bieber, C. Sutton, H. Larochelle, and D. Tarlow. Learning to execute programs with instruc-
              tion pointer attention graph neural networks. In Advances in Neural Information Processing
              Systems, 2020.
            [9] R. Bunel, M. Hausknecht, J. Devlin, R. Singh, and P. Kohli. Leveraging grammar and rein-
              forcement learning for neural program synthesis. In International Conference on Learning
              Representations, 2018.
           [10] B. Campbell. An executable semantics for compcert c. In International Conference on CertiÔ¨Åed
              Programs and Proofs, pages 60‚Äì75. Springer, 2012.
           [11] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. Ponde, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,
              G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint
              arXiv:2107.03374, 2021.
           [12] X. Chen, C. Liu, and D. Song. Execution-guided neural program synthesis. In International
              Conference on Learning Representations, 2019.
                                 ¬¥
           [13] J. Clymo, H. Manukian, N. Fijalkow, A. Gascon, and B. Paige. Data generation for neural
              programming by example. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics,
              pages 3450‚Äì3459. PMLR, 2020.
           [14] J. Deng, W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei. Imagenet: Alarge-scalehierarchical
              image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE
              Conference on, pages 248‚Äì255. IEEE, 2009.
           [15] J. Devlin, R. Bunel, R. Singh, M. Hausknecht, and P. Kohli. Neural program meta-induction. In
              Advances in Neural Information Processing Systems, 2017.
           [16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional
              transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
           [17] J. Devlin, J. Uesato, S. Bhupatiraju, R. Singh, A. Mohamed, and P. Kohli. RobustÔ¨Åll: Neural
              program learning under noisy I/O. In International Conference on Machine Learning, 2017.
           [18] K. Ellis, M. I. Nye, Y. Pu, F. Sosa, J. B. Tenenbaum, and A. Solar-Lezama. Write, execute,
              assess: Program synthesis with a repl. In Advances in Neural Information Processing Systems,
              2019.
           [19] S. Gulwani. Automating string processing in spreadsheets using input-output examples. In
              ACMSIGPLANNotices,volume46,pages317‚Äì330.ACM,2011.
           [20] S. Gulwani, W. R. Harris, and R. Singh. Spreadsheet data manipulation using examples.
              Communications of the ACM, 55(8):97‚Äì105, 2012.
                               11
           [21] K. Gupta, P. E. Christensen, X. Chen, and D. Song. Synthesize, execute and debug: Learning to
              repair for neural program synthesis. In Advances in Neural Information Processing Systems,
              2020.
           [22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
              Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770‚Äì
              778, 2016.
           [23] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik,
              H. He, D. Song, et al. Measuring coding challenge competence with apps. arXiv preprint
              arXiv:2105.09938, 2021.
           [24] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735‚Äì
              1780, 1997.
           [25] A. S. Illia Polosukhin. Neural program search: Solving data processing tasks from description
              and examples, 2018.
           [26] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International
              Conference on Learning Representations, 2015.
           [27] S. Kulal, P. Pasupat, K. Chandra, M. Lee, O. Padon, A. Aiken, and P. Liang. Spoc: Search-based
              pseudocode to code. In Advances in Neural Information Processing Systems, 2019.
           [28] L. Laich, P. Bielik, and M. Vechev. Guiding program synthesis by learning to generate examples.
              In International Conference on Learning Representations, 2019.
           [29] G.LampleandF.Charton. Deeplearningforsymbolicmathematics. InInternationalConference
              on Learning Representations, 2020.
           [30] M.-T. Luong, H. Pham, and C. D. Manning. Effective approaches to attention-based neural
              machine translation. arXiv preprint arXiv:1508.04025, 2015.
           [31] S. Mandal, T. Anderson, J. Turek, J. Gottschlich, S. Zhou, and A. Muzahid. Learning Ô¨Åtness
              functions for machine programming. Proceedings of Machine Learning and Systems, 3, 2021.
           [32] D.McClosky,E.Charniak,andM.Johnson. Effectiveself-trainingforparsing. InProceedingsof
              the Human Language Technology Conference of the NAACL, Main Conference, pages 152‚Äì159,
              2006.
           [33] C. Mendis, A. Renda, S. Amarasinghe, and M. Carbin. Ithemal: Accurate, portable and fast
              basic block throughput estimation using deep neural networks. In International Conference on
              machine learning, pages 4505‚Äì4515. PMLR, 2019.
           [34] Q. Min, L. Qin, Z. Teng, X. Liu, and Y. Zhang. Dialogue state induction using neural latent
              variable models. In International Joint Conferences on ArtiÔ¨Åcial Intelligence, 2020.
           [35] C. Z. Mooney, C. F. Mooney, C. L. Mooney, R. D. Duval, and R. Duvall. Bootstrapping: A
              nonparametric approach to statistical inference. Number 95. sage, 1993.
           [36] M. Nye, L. Hewitt, J. Tenenbaum, and A. Solar-Lezama. Learning to infer program sketches.
              In International Conference on Machine Learning, pages 4861‚Äì4870. PMLR, 2019.
           [37] M. Nye, Y. Pu, M. Bowers, J. Andreas, J. B. Tenenbaum, and A. Solar-Lezama. Representing
              partial programs with blended abstract semantics. In International Conference on Learning
              Representations, 2021.
           [38] A. Odena, K. Shi, D. Bieber, R. Singh, and C. Sutton. Bustle: Bottom-up program-synthesis
              through learning-guided exploration. In International Conference on Learning Representations,
              2021.
           [39] A. Odena and C. Sutton. Learning to represent programs with property signatures. In Interna-
              tional Conference on Learning Representations, 2020.
           [40] E. Parisotto, A.-r. Mohamed, R. Singh, L. Li, D. Zhou, and P. Kohli. Neuro-symbolic program
              synthesis. In International Conference on Learning Representations, 2017.
           [41] R. E. Pattis. Karel the robot: a gentle introduction to the art of programming. John Wiley &
              Sons, Inc., 1981.
           [42] C. Paxton, Y. Bisk, J. Thomason, A. Byravan, and D. Foxl. Prospection: Interpretable plans
              from language by predicting the future. In 2019 International Conference on Robotics and
              Automation (ICRA), pages 6942‚Äì6948. IEEE, 2019.
                               12
           [43] D. Saxton, E. Grefenstette, F. Hill, and P. Kohli. Analysing mathematical reasoning abilities of
              neural models. 2019.
           [44] E. C. Shin, I. Polosukhin, and D. Song. Improving neural program synthesis with inferred
              execution traces. In Advances in Neural Information Processing Systems, pages 8917‚Äì8926,
              2018.
           [45] R. Shin, N. Kant, K. Gupta, C. Bender, B. Trabucco, R. Singh, and D. Song. Synthetic datasets
              for neural program synthesis. In International Conference on Learning Representations, 2019.
           [46] A. Suh and Y. Timen. Creating synthetic datasets via evolution for neural program synthesis.
              arXiv preprint arXiv:2003.10485, 2020.
           [47] S.-H. Sun, H. Noh, S. Somasundaram, and J. Lim. Neural program synthesis from diverse
              demonstration videos. In Proceedings of the 35th International Conference on Machine Learn-
              ing, 2018.
           [48] Y. Tian, A. Luo, X. Sun, K. Ellis, W. T. Freeman, J. B. Tenenbaum, and J. Wu. Learning to
              infer and execute 3d shape programs. In International Conference on Learning Representations,
              2019.
           [49] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
              I. Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.
                Àá  ¬¥
           [50] P. Velickovic, R. Ying, M. Padovano, R. Hadsell, and C. Blundell. Neural execution of graph
              algorithms. In International Conference on Learning Representations, 2020.
           [51] A. J. Vijayakumar, A. Mohta, O. Polozov, D. Batra, P. Jain, and S. Gulwani. Neural-guided
              deductive search for real-time program synthesis from examples. In International Conference
              on Learning Representations, 2018.
           [52] Q.Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training with noisy student improves imagenet
              classiÔ¨Åcation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
              Recognition, pages 10687‚Äì10698, 2020.
           [53] Y. Yan, K. Swersky, D. Koutra, P. Ranganathan, and M. Heshemi. Neural execution engines:
              Learning to execute subroutines. In Advances in Neural Information Processing Systems, 2020.
           [54] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding and understanding bugs in c compilers. In
              Proceedings of the 32nd ACM SIGPLAN conference on Programming language design and
              implementation, pages 283‚Äì294, 2011.
           [55] W. Zaremba and I. Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.
           [56] Y. Zhang, Z. Ou, M. Hu, and J. Feng. A probabilistic end-to-end task-oriented dialog model with
              latent belief states towards semi-supervised learning. In Proceedings of the 2020 Conference on
              Empirical Methods in Natural Language Processing (EMNLP), 2020.
           [57] A. Zohar and L. Wolf. Automatic program synthesis of long programs with a learned garbage
              collector. In Advances in Neural Information Processing Systems, 2018.
                               13
