                 A DataCollectionDetails
                 A.1   DataPreprocessing
                 We used both dump7 and online version of Wikipedia and Wikidata. We downloaded the dump of
                 English Wikipedia on January 1, 2020, and the dump of English Wikidata on December 31, 2019. From
                 Wikidata and Wikipedia, we obtained 5,950,475 entities. Based on the value of the property instance
                 of in Wikidata, we categorized all entities into 23,763 groups. In this dataset, we focused on the most
                 popular entities (top-50 for comparison questions). When checking the requirements to ensure the multi-
                 hop reasoning of the dataset, several entities in the multi-path are not present in the dump version; in
                 such situations, we used the online version of Wikipedia and Wikidata.
                   We observed that the quality of the dataset depends on the quality of the intersection information
                 between Wikipedia and Wikidata. Speciﬁcally, for the property related to date information, such as
                 publication date and date of birth, information between Wikipedia and Wikidata is quite consistent.
                 Meanwhile, for the property occupation, information between Wikipedia and Wikidata is inconsistent.
                 Forinstance, the Wikipedia of the entity Ebenezer Adam is as follows: “Ebenezer Adam was a Ghanaian
                 educationist and politician.”; meanwhile, the value from Wikidata of the property occupation is politi-
                 cian. In such situations, we manually check all samples related to the property to ensure dataset quality.
                 For the property related to the country name, we handled many different similar names by using the
                 aliases of the entity and the set of demonyms. Moreover, to guarantee the quality of the dataset, we only
                 focused on the set of properties with high consistency between Wikipedia and Wikidata.
                   WeusedbothStanfordCoreNLP(Manningetal.,2014)andSpacytoperformsentencesegmentation
                 for the context.
                 A.2   ComparisonQuestions
                 Table 9 presents all information of our comparison question. We can use more entities and properties
                 fromWikidatatocreateadataset. Inthisversionofthedataset, wefocusedonthetop-50popularentities
                 in Wikipedia and Wikidata. To ensure dataset quality, we used the set of properties as described in the
                 table. For each combination between the entity and the property, we have various templates for asking
                 questions to ensure diversity in the questions.
                 A.3   Inference Questions
                 We argued that logical rules are difﬁcult to apply to multi-hop questions. We obtained a set of 50
                 inference relations, but we cannot use all of it into the dataset.  For instance, the logical rule is
                 placeofbirth(a,b) ∧ country(b,c) ⇒ nationality(a,c); this rule easily fails after checking the re-
                 quirements. To guarantee the multi-hop reasoning of the question, the document describing a person
                 a having a place of birth b should not contain the information about the country c. However, most
                 paragraphs describing humans often contain information on their nationality.
                   Theother issue is ensuring that each sample has only one correct answer on the two gold paragraphs.
                 With the logical rule being child(a,b) ∧ child(b,c) ⇒ grandchild(a,c), if a has more than one child,
                 for instance a has three children b , b and b , then each b has their own children. Therefore, for the
                                                   1  2      3
                 question “Who is the grandchild of a?”, there are several possible answers to this question. To address
                 this issue in our dataset, we only utilized the relation that has only one value in the triple on Wikidata.
                 Thatisthereasonwhythenumberofinferencequestionsinourdatasetisquitesmall. Table10describes
                 all inference relations used in our dataset.
                   In most cases, this rule will be correct. However, several rules can be false in some cases. In such
                 situations, based on the Wikidata information, we double-checked the new triple before deciding whether
                 to use it. For instance, the rule is doctoral advisor(a,b) ∧ employer(b,c) ⇒ educated at(a,c), a has
                 an advisor is b, b works at c, and we can infer that a studies at c. There can be exceptions that b works
                 at many places, and c is one of them, but a does not study at c. We used Wikidata to check whether a
                 studies at c before deciding to use it.
                   Toobtain the question, we used the set of templates in Table 11.
                   7https://dumps.wikimedia.org/
                                                                 6621
