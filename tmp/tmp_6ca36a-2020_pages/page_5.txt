                  requirement here is that the value o must appear in the Wikipedia article for the entity e , which is the
                                                      1                                                       1
                  samecondition for the second entity e .
                                                         2
                    When all information passed the requirements, we generated a question–answer pair that includes a
                  question Q, a context C, the sentence-level SFs SF, the evidence E, and an answer A. Q is obtained by
                  replacing the two tokens #name in the template by the two entity labels. C is a concatenation of the two
                  Wikipedia articles that describe the two entities. E is the two triples (e ,r ,o ) and (e ,r ,o ). SF is
                                                                                            1  1   1        2   1  2
                  a set of sentence indices where the values o and o are extracted. Based on the type of questions, we
                                                               1       2
                  undertake comparisons and obtain the ﬁnal answer A.
                    Wegenerated bridge questions as described in Algorithm 2 (Appendix A.6). For each entity group,
                  we randomly selected an entity e and then obtained a set of statements of the entity from Wikidata.
                  Subsequently, based on the ﬁrst relation information in R (the set of predeﬁned relations), we ﬁltered
                  the set of statements to obtain a set of 1-hop H . Next, for each element in H , we performed the same
                                                                  1                               1
                  process to obtain a set of 2-hop H , each element in H is a tuple (e,r ,e ,r ,e ). For each tuple in
                                                      2                     2                1  1   2  2
                  H , we obtained the Wikipedia articles for two entities e and e . Then, we checked the requirements
                    2                                                               1
                  to ensure that this sample can become a multi-hop dataset. For instance, the two paragraphs p and p1
                  describe for e and e , respectively (see Figure 2). The bridge entity requirement is that p must mention
                                      1
                  e . The span extraction answer requirement is that p must mention e . The 2-hop requirements are that
                   1                                                    1                2
                  p must not contain e and p must not contain e. Finally, we obtained Q, C, SF, E, and A similarly to
                                       2      1
                  the process in comparison questions.
                                      Figure 2: The Requirements for bridge questions in our dataset.
                  (3) Post-process Generated Data:       Werandomly selected two entities to create a question when gen-
                  erating the data; therefore, a large number of no questions exist in the yes/no questions. We performed
                  post-processing to ﬁnalize the dataset that balances the number of yes and no questions. Questions could
                  have several true answers in the real world. To ensure one sample has only one answer, we discarded all
                  ambiguous cases in the dataset (Appendix A.7).
                  Collect Distractor Paragraphs:      Following Yang et al. (2018) and Min et al. (2019), we used bigram
                  tf-idf (Chen et al., 2017) to retrieve the top-50 paragraphs from Wikipedia that are most similar to the
                  question. Then, we used the entity type of the two gold paragraphs (four gold paragraphs for bridge-
                  comparison question) to select the top-8 paragraphs (top-6 for bridge-comparison question) and consid-
                  ered it as a set of distractor paragraphs. We shufﬂed the 10 paragraphs (including gold and distractor
                  paragraphs) and obtained a context.
                  Dataset Statistics (A Benchmark Setting):        We used a single-hop model (Section 5.1) to split the
                  train, dev, and test sets. We conducted ﬁve-fold cross-validation on all data. The average F1 score of the
                  model is 86.7%. All questions solved by the single-hop model are considered as a train-medium subset.
                  Therest was split into three subsets: train-hard, dev, and test (balancing the number of different types of
                  questions in each subset). Statistics of the data split can be found in Table 1. We used train-medium and
                  train-hard as the training data in our dataset.
                  4   DataAnalysis
                  Question and Answer Lengths         We quantitatively analyze the properties of questions and answers
                  for each type of question in our dataset. The statistics of the dataset are presented in Table 2. The
                  compositional question has the greatest number of examples, and the inference question has the least
                                                                     6613
