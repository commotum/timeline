                  Setting                   Answer        SpFact       Evidence       Joint
                                           EM      F1    EM      F1   EM      F1    EM      F1
                  Model                   50.00  58.48 29.00  69.90   0.00  16.74  0.00   9.79
                  Human(average)          80.67  82.34 85.33  92.63  57.67  75.63 53.00  66.69
                  HumanUpperBound(UB) 91.00 91.79 88.00 93.75 64.00 78.81 62.00 75.25
              Table 7: Comparing baseline model performance with human performance (%) on 100 random samples.
              (see Section 5.4 for an analysis). The human performance of the answer prediction task on our dataset
              (91.8 F1 UB) shows a relatively small gap against that on HotpotQA (98.8 F1 UB; borrowed from their
              paper). Although the baseline model is able to predict the answer and sentence-level SFs, it is not very
              effective at ﬁnding the evidence. We also observe that there is a large gap between the performance of
              human and the model in the evidence generation task (78.8 and 16.7 F1). Therefore, this could be a
              new challenging task for explaining multi-hop reasoning. We conjecture that the main reason why the
              score of the evidence generation task was low is the ambiguity in the names of Wikidata. For example,
              in Wikidata, one person can have multiple names. We use only one name in the ground truth, while the
              workers can use other names. Future research might explore these issues to ensure the quality of the
              dataset. Overall, our baseline results are far behind human performance. This shows that our dataset is
              challenging and there is ample room for improvement in the future.
              5.4 Analysis of Mismatched Examples between Wikipedia and Wikidata
              As mentioned in Section 5.3, there are unanswerable questions in our dataset due to the mismatch in-
              formation between Wikipedia articles and Wikidata knowledge. In the dataset generation process, for a
              triple (s,r,o), we ﬁrst checked whether the object entity o appears or not in the Wikipedia article that
              describes the entity s. Our assumption is that the ﬁrst sentence in the article in which the object entity
              o appears is the most important, which we decided to use for the QA pair generation. For instance, we
              obtainedatriple: (Lord William Beauclerk, mother, Lady Diana de Vere) from Wikidata, and we obtained
              a paragraph p from the Wikipedia article that describes “Lord William Beauclerk”. We used the object
              entity “Lady Diana de Vere” to obtain the ﬁrst sentence in p “Beauclerk was the second son of Charles
              Beauclerk, 1st Duke of St Albans, and his wife Lady Diana de Vere, ....” From this sentence, we can
              infer that the mother of “Lord William Beauclerk” is “Lady Diana de Vere”. However, because we only
              checked whether the object entity o appears in the sentence or not, there could be a semantic mismatch
              betweenthesentenceandthetriple. For instance, we obtained a triple: (Rakel Dink, spouse, Hrant Dink)
              from Wikidata, while we obtained the ﬁrst sentence from Wikipedia article: “Rakel Dink (born 1959)
              is a Turkish Armenian human rights activist and head of the Hrant Dink Foundation.” Obviously, from
              this sentence, we cannot infer that “Hrant Dink” is the spouse of “Rakel Dink”. Therefore, we deﬁned
              heuristics to exclude these mismatched cases as much as possible. In particular, we found that some
              exampleshavesubjectentities that are similar/equal to their object entities and are likely to become mis-
              matchedcases. For such cases, we manually checked the samples and decided to use or remove them for
              our ﬁnal dataset. Nonetheless, there are still cases that our heuristics cannot capture. To estimate how
              manymismatched cases our heuristics cannot capture in the dataset, we randomly selected 100 samples
              in the training set and manually checked them. We obtained eight out of 100 samples that have a mis-
              match between Wikipedia article and Wikidata triple. For the next version of the dataset, we plan to
              improve our heuristics by building a list of keywords for each relation to check the correspondence be-
              tween Wikipedia sentence and Wikidata triple. For instance, we observed that for the relation “mother”,
              the sentences often contain phrases: “son of”, “daughter of”, “his mother”, and “her mother”.
              6  Related Work
              Multi-hop questions in MRC domain  Currently, four multi-hop MRC datasets proposed for tex-
              tual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), Hot-
              potQA (Yang et al., 2018), and R4C (Inoue et al., 2020). Recently, Chen et al. (2020) introduced the
                                                      6617
