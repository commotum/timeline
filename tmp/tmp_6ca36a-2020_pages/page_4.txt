                   4. Bridge-comparison question is a type of question that combines the bridge question with the
                     comparison question. It requires both ﬁnding the bridge entities and doing comparisons to obtain
                     the answer. For instance, instead of directly compare two ﬁlms, we compare the information of
                     the directors of the two ﬁlms, e.g., Which movie has the director born ﬁrst, La La Land or Tenet?
                     To answer this type of question, the model needs to ﬁnd the bridge entity that connects the two
                     paragraphs, one about the ﬁlm and one about the director, to get the date of birth information. Then,
                     makingacomparisontoobtain the ﬁnal answer.
                 3   DataCollection
                 3.1  Wikipedia and Wikidata
                 In this study, we utilized both text descriptions from Wikipedia4 and a set of statements from Wikidata to
                 construct our dataset. We used only a summary from each Wikipedia article as a paragraph that describes
                 an entity. Wikidata5 is a collaborative KB that stores data in a structured format. Wikidata contains a
                 set of statements (each statement includes property and an object entity) to describe the entity. There is
                 a connection between Wikipedia and Wikidata for each entity. From Wikidata, we can extract a triple
                 (s,r,o), where s is a subject entity, r is a property or relation, and o is an object entity. A statement for
                 the entity s is (r,o). An object entity can be another entity or the date value. We categorized all entities
                 based on the value of the property instance of in Wikidata (Appendix A.1).
                 3.2  Dataset Generation Process
                 Generating a multi-hop dataset in our framework involves three main steps: (1) create a set of templates,
                 (2) generate data, and (3) post-process generated data. After obtaining the generated data, we used a
                 modeltosplit the data into train, dev, and test sets.
                 (1) Create a Set of Templates:   For the comparison question, ﬁrst, we used Spacy6 to extract named
                 entity recognition (NER) tags and labels for all comparison questions in the train data of HotpotQA
                 (17,456 questions). Then, we obtained a set of templates L by replacing the words in the questions
                 with the labels obtained from the NER tagger. We manually created a set of templates based on L for
                 entities in the top-50 most popular entities in Wikipedia. We focused on a set of speciﬁc properties of
                 each entity type (Appendix A.2) in the KB. We also discarded all templates that made questions become
                 single-hop or context-dependent multi-hop as discussed in Min et al. (2019). Based on the templates
                 of the comparison question, we manually enhanced it to create the templates for bridge-comparison
                 questions (Appendix A.5). We manually created all templates for inference and compositional questions
                 (Appendix A.3 and A.4).
                   Fortheinferencequestion, weutilizedlogical rules in the knowledge graph to create a simple question
                 butstill require multi-hop reasoning. Extracting logical rules is a task in the knowledge graph wherein the
                 target makes the graph complete. We observe that logical rules, such as spouse(a,b) ∧ mother(b,c) ⇒
                 mother in law(a,c), can be used to test the reasoning skill of the model. Based on the results of the
                                   ´
                 AMIE model (Galarraga et al., 2013), we manually checked and veriﬁed all logical rules to make it
                 suitable for the Wikidata relations. We obtained 28 logical rules (Appendix A.3).
                 (2) Generate Data:    From the set of templates and all entities’ information, we generated comparison
                 questions as described in Algorithm 1 (Appendix A.6). For each entity group, we randomly selected two
                 entities: e1 and e2. Subsequently, we obtained the set of statements of each entity from Wikidata. Then,
                 weprocessedthetwosetsofstatementstoobtainasetofmutualrelations(M)betweentwoentities. We
                 then acquired the Wikipedia information for each entity. For each relation in M, for example, a relation
                 r , we checked whether we can use this relation. Because our dataset is a span extraction dataset, the
                  1
                 answer is extracted from the Wikipedia article of each entity. With relation r , we obtained the two
                                                                                               1
                 values o and o from the two triples (e ,r ,o ) and (e ,r ,o ) of the two entities, respectively. The
                         1      2                       1   1  1        2  1   2
                   4https://www.wikipedia.org
                   5https://www.wikidata.org
                   6https://spacy.io/
                                                                 6612
