                                                                             Agent57: Outperforming the Atari Human Benchmark
                       Table 1. Number of games above human, mean capped, mean and median human normalized scores for the 57 Atari games.
                                                                Statistics               Agent57        R2D2(bandit)          NGU          R2D2(Retrace)          R2D2         MuZero
                                                             Cappedmean                   100.00             96.93            95.07             94.20              94.33        89.92
                                                     Numberofgames>human                     57               54                51                52                52            51
                                                                  Mean                    4766.25          5461.66           3421.80           3518.36           4622.09       4998.51
                                                                 Median                   1933.49          2357.92           1359.78           1457.63           1935.86       2041.12
                                                            40th Percentile               1091.07          1298.80           610.44             817.77           1176.05       1172.90
                                                            30th Percentile               614.65            648.17           267.10             420.67            529.23        503.05
                                                            20th Percentile               324.78            303.61           226.43             267.25            215.31        171.39
                                                            10th Percentile               184.35            116.82           107.78             116.03            115.33        75.74
                                                             5th Percentile               116.67             93.25            64.10             48.32              50.27         0.03
                       loss. Finally, Agent57 achieves a median and mean that is
                       greater than NGU and R2D2, but also its CHNS is 100%.
                       This shows the generality of Agent57: not only it obtains
                       a strong mean and median, but also it is able to obtain
                       strong performance on the tail of games in which MuZero
                       and R2D2 catastrophically fail. This is more clearly ob-
                       served when looking at different percentiles: up to the
                       20thpercentile, Agent57 showsmuchgreaterperformance,
                       only slightly surpassed by R2D2 (bandit) when we exam-
                       ine higher percentiles. In Fig. 3 we report the performance
                       of Agent57 in isolation on the 57 games. We show the last
                       6 games (in terms of number of frames collected by the
                       agents) in which the algorithm surpasses the human perfor-
                       mance benchmark. As shown, the benchmark over games                                                 Figure 4. Performance progression on the 10-game challenging
                       is beaten in a long-tailed fashion, where Agent57 uses the                                          set obtained from incorporating each one of the improvements.
                       ﬁrst 5 billion frames to surpass the human benchmark on
                       51games. Afterthat, we ﬁnd hard exploration games, such
                       as Montezuma’s Revenge, Pitfall!, and Private Eye. Lastly,                                          equivalent to Agent57. We observe that each one of the
                       Agent57 surpasses the human benchmark on Skiing after                                               improvementsresults in an increment in ﬁnal performance.
                       78 billion frames. To be able to achieve such performance                                           Further, we see that each one of the improvements that is
                       on Skiing, Agent57 uses a high discount (as we show in                                              part of Agent57 is necessary in order to obtain the consis-
                       Sec. 4.4). This naturally leads to high variance in the re-                                         tent ﬁnal performance of 100% CHNS.
                       turns, which leads to needing more data in order to learn
                       to play the game. One thing to note is that, in the game                                            4.2. State-Action Value Function Parameterization
                       of Skiing, the human baseline is very competitive, with a
                       score of −4336.9, where −17098.1 is random and −3272                                                We begin by evaluating the inﬂuence of the state-action
                       is the optimal score one can achieve.                                                               value function parametrization on a minimalistic gridworld
                       In general, as performance in Atari keeps improving, it                                             environment, called “random coin”. It consists of an empty
                       seems natural to concentrate on the tail of the distribution,                                       room of size 15 × 15 where a coin and an agent are
                       i.e., pay attention to those games for which progress in the                                        randomly placed at the start of each episode. The agent
                       literature has been historically much slower than average.                                          can take four possible actions (up, down, left right) and
                       Wenowpresentresultsforasubsetof10gamesthatwecall                                                    episodes are at most 200 steps long. If the agent steps over
                       the challenging set. It consists of the six hard exploration                                        the coin, it receives a reward of 1 and the episode termi-
                       games as deﬁned in (Bellemare et al., 2016), plus games                                             nates. In Fig. 5 we see the results of NGU with and with-
                       that require long-term credit assignment. More concretely,                                          out the new parameterization of its state-action value func-
                       the games we use are: Beam Rider, Freeway, Montezuma’s                                              tions. We report performance after 150 million frames. We
                       Revenge, Pitfall!, Pong, Private Eye, Skiing, Solaris, Sur-                                         compare the extrinsic returns for the policies that are the
                                                                                                                           exploitative (β           = 0) and the most exploratory (with the
                       round, and Venture.                                                                                                        j
                                                                                                                           largest β        in the family).             Even for small values of the
                                                                                                                                         j
                       In Fig. 4 we can see the performance progression obtained                                           exploration rates (max β ), this setting induces very dif-
                                                                                                                                                               j    j
                       fromincorporating each one of the improvements we make                                              ferent exploratory and exploitative policies. Maximizing
                       on top of NGU. Such performance is reported on the se-                                              the discounted extrinsic returns is achieved by taking the
                       lection of 10 games mentioned above. In the notation of                                             shortest path towards the coin (obtaining an extrinsic re-
                       the legend, NGU + bandit + separate nets + long trace is                                            turn of one), whereas maximizing the augmented returns
