                                              LPNcanperformfastsystem1-like reasoning using the encoder and then narrow down the search
                                              space during latent optimization, simulating system 2 reasoning.
                                              5.4       Analyzing the Latent Space
                                                                                                                               1.5
                                                                                                                               0.9
                                                                                                                               0.6
                                                                                                                               0.3
                                                                                                                              y0.0
                                                                                                                               -0.2
                                                                                                                               -0.5
                                                                                                                               -0.8
                                                                                                                               -1.3
                                                                                                                               -3.1
                                                                                                                                 -3.1    -1.3     -0.8    -0.5    -0.2     0.0     0.3     0.6     0.9      1.5
                                                                                                                                                                           x
                                              Figure 9: (Left) The 2D pattern task with inputs containing marker points where patterns should
                                              be placed, with patterns varying for each different program. (Right) The latent traversal visualizes
                                              the effect of traversing the latent space, on the predicted pattern by the decoder at marker points.
                                              Visualization in higher resolution in fig. 17
                                              To validate that the encoder is learning programs in its latent space, we design an even simpler
                                              task with small 4x4 grids that have 2x2 patterns. We train an LPN model until convergence with
                                              a latent space of dimension 2 to easily visualize it in figure 9. Due to the simplicity of the task
                                              we train the model with mean training, i.e. no latent optimization. Because we are using a 2D
                                              Gaussian prior for the latent space, we can convert R2 to the unit square using the normal cumulative
                                              distribution function (CDF), and then plot on the unit square at coordinates (x,y) the decoderâ€™s output
                                              when conditioned by the latent z = (CDF(x),CDF(y)). These results demonstrate the diversity
                                              and smoothness of the latent space, showing structure in terms of color patterns, which motivates
                                              performing gradient ascent latent optimization in more complex tasks. This shows that the latent
                                              space can encode a wide range of diversity in its latent space which is especially important for
                                              adapting to unseen patterns.
                                              5.5       AdaptingOut-Of-Distribution
                                              Westudythe out-of-distribution (OOD) behavior of different training methods in table 2. We use
                                              the same Pattern task as in section 5.3 but with different levels of color density for the patterns
                                              to reproduce. Specifically, we train on a density of 50% (which means half the patterns are black
                                              cells), and evaluate on 50%, 75% (weakly OOD), and 100% (strongly OOD). We observe that LPN
                                              equipped with gradient ascent at inference time can recover strong performance in the OOD setting
                                              (88% accuracy) whereas the prediction from the mean latent essentially gets an accuracy of 0%.
                                              Moreover, all methods suffer from a performance drop when increasing the OOD setting, but training
                                              with gradient ascent gets the lowest drop in accuracy, demonstrating a latent space better behaved for
                                              search at inference time. This motivates training LPN on some distribution of programs and then
                                              using the model equipped with gradient ascent at inference time given a novel task.
                                              5.6       ARC-AGI2024
                                              DataGeneration TotestLPNontheARC-AGIbenchmark,wedesignatrainingphasewherewe
                                              aimat injecting the Core Knowledge priors into our LPN system. In this work, we use the re-arc
                                                                                                                                13
