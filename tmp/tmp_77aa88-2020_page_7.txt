                               Published as a conference paper at ICLR 2021
                               As seen in Table 3, SAM uniformly improves performance relative to ﬁnetuning without SAM.
                               Furthermore, in many cases, SAM yields novel state-of-the-art performance, including 0.30% error
                               onCIFAR-10,3.92%erroronCIFAR-100,and11.39%erroronImageNet.
                                 Dataset            EffNet-b7    EffNet-b7      Prev. SOTA        EffNet-L2    EffNet-L2    Prev. SOTA
                                                      +SAM                    (ImageNet only)      +SAM
                                 FGVC Aircraft       6.80         8.15        5.3 (TBMSL-Net)     4.82          5.80        5.3 (TBMSL-Net)
                                                         ±0.06        ±0.08                           ±0.08         ±0.1
                                 Flowers             0.63         1.16        0.7 (BiT-M)         0.35         0.40         0.37 (EffNet)
                                                         ±0.02        ±0.05                           ±0.01        ±0.02
                                 Oxford IIIT Pets    3.97         4.24        4.1 (Gpipe)         2.90         3.08         4.1 (Gpipe)
                                                         ±0.04        ±0.09                           ±0.04        ±0.04
                                 Stanford Cars       5.18         5.94        5.0 (TBMSL-Net)     4.04         4.93         3.8 (DAT)
                                 CIFAR-10            0.88±0.02    0.95±0.06   1 (Gpipe)           0.30±0.03    0.34±0.04    0.63 (BiT-L)
                                                         ±0.02        ±0.03                           ±0.01        ±0.02
                                 CIFAR-100           7.44         7.68        7.83 (BiT-M)        3.92         4.07         6.49 (BiT-L)
                                                         ±0.06        ±0.06                           ±0.06        ±0.08
                                 Birdsnap           13.64        14.30        15.7 (EffNet)       9.93         10.31        14.5 (DAT)
                                                         ±0.15        ±0.18                           ±0.15         ±0.15
                                 Food101             7.02         7.17        7.0 (Gpipe)         3.82         3.97         4.7 (DAT)
                                                         ±0.02        ±0.03                           ±0.01        ±0.03
                                 ImageNet           15.14±0.03      15.3      14.2 (KDforAA)      11.39±0.02      11.8      11.45 (ViT)
                               Table 3: Top-1 error rates for ﬁnetuning EfﬁcientNet-b7 (left; ImageNet pretraining only) and
                               EfﬁcientNet-L2 (right; pretraining on ImageNet plus additional data, such as JFT) on various down-
                               stream tasks. Previous state-of-the-art (SOTA) includes EfﬁcientNet (EffNet) (Tan & Le, 2019),
                               Gpipe (Huang et al., 2018), DAT (Ngiam et al., 2018), BiT-M/L (Kolesnikov et al., 2020), KD-
                               forAA(Weietal., 2020), TBMSL-Net (Zhang et al., 2020), and ViT (Dosovitskiy et al., 2020).
                               3.3    ROBUSTNESS TO LABEL NOISE
                               ThefactthatSAMseeksoutmodelparametersthat
                               are robust to perturbations suggests SAM’s poten-                    Method                 Noise rate (%)
                               tial to provide robustness to noise in the training set                                20    40     60     80
                               (which would perturb the training loss landscape).             Sanchez et al. (2019)   94.0  92.8   90.3   74.1
                               Thus, we assess here the degree of robustness that           Zhang&Sabuncu(2018)       89.7  87.6   82.7   67.9
                               SAMprovidestolabelnoise.                                         Lee et al. (2019)     87.1  81.8   75.4    -
                                                                                                Chenetal. (2019)      89.7    -      -    52.3
                               In particular, we measure the effect of apply-                  Huangetal. (2019)      92.6  90.3   43.4    -
                                                                                               MentorNet (2017)       92.0  91.2   74.2   60.0
                               ing SAM in the classical noisy-label setting for                  Mixup(2017)          94.0  91.5   86.8   76.9
                               CIFAR-10, in which a fraction of the training set’s             MentorMix(2019)        95.6  94.2   91.3   81.0
                               labels are randomly ﬂipped; the test set remains                      SGD              84.8  68.8   48.2   26.2
                               unmodiﬁed (i.e., clean). To ensure valid compar-                      Mixup            93.0  90.0   83.8   70.2
                               ison to prior work, which often utilizes architec-              Bootstrap + Mixup      93.3  92.0   87.6   72.0
                                                                                                     SAM              95.1  93.4   90.5   77.9
                               tures specialized to the noisy-label setting, we train           Bootstrap + SAM       95.4  94.2   91.8   79.9
                               a simple model of similar size (ResNet-32) for 200         Table 4: Test accuracy on the clean test set
                               epochs, following Jiang et al. (2019). We evalu-           for modelstrainedonCIFAR-10withnoisyla-
                               ate ﬁve variants of model training: standard SGD,          bels. Lower block is our implementation, up-
                               SGDwith Mixup (Zhang et al., 2017), SAM, and               per block gives scores from the literature, per
                               ”bootstrapped” variants of SGD with Mixup and              Jiang et al. (2019).
                               SAM(wherein the model is ﬁrst trained as usual
                               and then retrained from scratch on the labels pre-
                               dicted by the initially trained model). When apply-
                               ing SAM,weuseρ = 0.1forallnoiselevelsexcept80%,forwhichweuseρ = 0.05formorestable
                               convergence. For the Mixup baselines, we tried all values of α ∈ {1,8,16,32} and conservatively
                               report the best score for each noise level.
                               As seen in Table 4, SAM provides a high degree of robustness to label noise, on par with that
                               provided by state-of-the art procedures that speciﬁcally target learning with noisy labels. Indeed,
                               simply training a model with SAM outperforms all prior methods speciﬁcally targeting label noise
                               robustness, with the exception of MentorMix (Jiang et al., 2019). However, simply bootstrapping
                               SAMyieldsperformancecomparabletothat of MentorMix (which is substantially more complex).
                                                                                      7
