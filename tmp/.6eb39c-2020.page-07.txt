                                                    Agent57: Outperforming the Atari Human Benchmark
                                                                                  Figure 6. Solaris learning curves with small and long backprop
                                                                                  through time window sizes for both R2D2 and Agent57.
                                                                                  round the opponent snake, receive a reward, and start from
               Figure 5. Extrinsic returns for the exploitative (β = 0) and most
                                                              0                   the initial state, or keep wandering around without captur-
               exploratory (β31 = β) on “random coin” for different values of     ing the opponent, and thus visiting new states in the world.
               the intrinsic reward weight, β. (Top) NGU(Bottom) NGU with
               Separate networks for intrinsic and extrinsic values.              4.3. Backprop Through Time Window Size
               is achieved by avoiding the coin and visiting all remaining        In this section we analyze the impact of having a backprop
               states (obtaining an extrinsic return of zero). In principle,      through time window size. More concretely, we analyze its
               NGUshould be able to learn these policies jointly. How-            impact on the base algorithm R2D2 to see its effect with-
               ever, we observe that the exploitative policy in NGU strug-        out NGUoranyoftheimprovementswepropose. Further,
               gles to solve the task as intrinsic motivation reward scale        we also analyze its effect on Agent57, to see if any of the
               increases. As we increase the scale of the intrinsic reward,       improvements on NGU overlap with this change. In both
               its value becomes muchgreaterthanthatoftheextrinsicre-             cases, we compare using backprop through time window
               ward. As a consequence, the conditional state-action value         sizes of 80 (default in R2D2) versus 160, higher values en-
               networkofNGUisrequiredtorepresentverydifferentval-                 able credit assignment further back.
               ues depending on the β we condition on. This implies that
                                        j                                         In aggregated terms over the challenging set, its effect
               the network is increasingly required to have more ﬂexible
               representations. Using separate networks dramatically in-          seems to be the same for both R2D2 and Agent57: us-
               creases its robustness to the intrinsic reward weight that is      ing a longer backprop through time window appears to be
               used. Note that this effect would not occur if the episode         initially slower, but results in better overall stability and
               did not terminate after collecting the coin. In such case,         slightly higher ﬁnal score.     A detailed comparison over
               exploratory and exploitative policies would be allowed to          those 10 games is shown in App. H.2. This effect can be
               be very similar: both could start by collecting the coin as        seen clearly in the game of Solaris, as observed in Fig. 6.
               quickly as possible. In Fig. 4 we can see that this improve-       This is also the game showing the largest improvement in
               mentalsotranslatestothechallengingset. NGUachievesa                terms of ﬁnal score. This is again general improvement, as
               muchloweraverageCHNSthanitsseparatenetworkcoun-                    it enhances performance on all the challenging set games.
               terpart. We also observe this phenomenon when we incor-            For further details we report the scores in App. H.1.
               porate the meta-controller. Agent57 suffers a drop of per-
               formance that is greater than 20% when the separate net-           4.4. Adaptive Exploration
               workimprovementisremoved.                                          In this section, we analyze the effect of using the meta-
               We can also see that it is a general improvement: it does          controller described in Sec. 3.1 in both the actors and the
               not show worse performance on any of the 10 games of               evaluator. To isolate the contribution of this improvement,
               the challenging set. More concretely, the largest improve-         we evaluate two settings: R2D2 and NGU with separate
               ment is seen in the case of Surround, where NGU obtains            networks, with and without meta-controller. Results are
               a score on par with a random policy, whereas with the new          shown in Fig. 7. Again, we observe that this is a general
               parametrization it reaches a score that is nearly optimal.         improvement in both comparisons. Firstly, we observe that
               ThisisbecauseSurroundisacasethatissimilartothe“ran-                there is a great value in this improvement on its own, en-
               dom coin” environment mentioned above: as the player               hancing the ﬁnal performance of R2D2 by close to 20%
               makes progress in the game, they have the choice to sur-           CHNS.Secondly,weobservethatthebeneﬁtonNGUwith
