                                         For the purpose of solving the above problems, let us introduce a recognition model qœÜ(z|x): an
                                         approximation to the intractable true posterior p (z|x). Note that in contrast with the approximate
                                                                                                                 Œ∏
                                         posterior in mean-Ô¨Åeld variational inference, it is not necessarily factorial and its parameters œÜ are
                                         not computed from some closed-form expectation. Instead, we‚Äôll introduce a method for learning
                                         the recognition model parameters œÜ jointly with the generative model parameters Œ∏.
                                         From a coding theory perspective, the unobserved variables z have an interpretation as a latent
                                         representation or code. In this paper we will therefore also refer to the recognition model q (z|x)
                                                                                                                                                                                  œÜ
                                         as a probabilistic encoder, since given a datapoint x it produces a distribution (e.g. a Gaussian)
                                         over the possible values of the code z from which the datapoint x could have been generated. In a
                                         similar vein we will refer to p (x|z) as a probabilistic decoder, since given a code z it produces a
                                                                                      Œ∏
                                         distribution over the possible corresponding values of x.
                                         2.2     Thevariational bound
                                         Themarginallikelihoodiscomposedofasumoverthemarginallikelihoodsofindividualdatapoints
                                         logp (x(1),¬∑¬∑¬∑ ,x(N)) = PN logp (x(i)), which can each be rewritten as:
                                                Œ∏                                   i=1          Œ∏
                                                                     logp (x(i)) = D                (q (z|x(i))||p (z|x(i))) + L(Œ∏,œÜ;x(i))                                              (1)
                                                                            Œ∏                  KL œÜ                      Œ∏
                                         The Ô¨Årst RHS term is the KL divergence of the approximate from the true posterior. Since this
                                         KL-divergence is non-negative, the second RHS term L(Œ∏,œÜ;x(i)) is called the (variational) lower
                                         bound on the marginal likelihood of datapoint i, and can be written as:
                                                              logp (x(i)) ‚â• L(Œ∏,œÜ;x(i)) = E                               [‚àílogq (z|x)+logp (x,z)]                                      (2)
                                                                     Œ∏                                         qœÜ(z|x)                œÜ                     Œ∏
                                         which can also be written as:                                                                           h                      i
                                                            L(Œ∏,œÜ;x(i)) = ‚àíD                   (q (z|x(i))||p (z)) + E                             logp (x(i)|z)                        (3)
                                                                                          KL œÜ                      Œ∏              qœÜ(z|x(i))             Œ∏
                                         We want to differentiate and optimize the lower bound L(Œ∏,œÜ;x(i)) w.r.t. both the variational
                                         parameters œÜ and generative parameters Œ∏. However, the gradient of the lower bound w.r.t. œÜ
                                                                                               ¬®
                                         is a bit problematic. The usual (naƒ±ve) Monte Carlo gradient estimator for this type of problem
                                                                                                                                P
                                         is: ‚àá E             [f(z)] = E                 f(z)‚àá             logq (z) ' 1                L f(z)‚àá                    logq (z(l)) where
                                                 œÜ qœÜ(z)                      qœÜ(z)               qœÜ(z)          œÜ             L      l=1             qœÜ(z(l))          œÜ
                                         z(l) ‚àº q (z|x(i)). This gradient estimator exhibits exhibits very high variance (see e.g. [BJP12])
                                                     œÜ
                                         and is impractical for our purposes.
                                         2.3     TheSGVBestimatorandAEVBalgorithm
                                         In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the
                                         parameters. We assume an approximate posterior in the form q (z|x), but please note that the
                                                                                                                                           œÜ
                                         technique can be applied to the case q (z), i.e. where we do not condition on x, as well. The fully
                                                                                                 œÜ
                                         variational Bayesian method for inferring a posterior over the parameters is given in the appendix.
                                         Undercertainmildconditionsoutlinedinsection2.4forachosenapproximateposteriorq (z|x)we
                                                                                                                                                                             œÜ
                                                                                                    e
                                         can reparameterize the random variable z ‚àº q (z|x) using a differentiable transformation g (,x)
                                                                                                             œÜ                                                                   œÜ
                                         of an (auxiliary) noise variable :
                                                                                           e
                                                                                           z = g (,x)            with        ‚àº p()                                                   (4)
                                                                                                   œÜ
                                         See section 2.4 for general strategies for chosing such an approriate distribution p() and function
                                         gœÜ(,x). We can now form Monte Carlo estimates of expectations of some function f(z) w.r.t.
                                         q (z|x) as follows:
                                          œÜ
                                                                                  h                      i            L
                                                                                                   (i)           1 X                 (l)    (i)                       (l)
                                            E          (i)  [f(z)] = E              f(gœÜ(,x )) '                         f(gœÜ( ,x )) where                              ‚àºp() (5)
                                              qœÜ(z|x      )                 p()                                 L
                                                                                                                    l=1
                                         We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic
                                                                                                                A              (i)                     (i)
                                                                                                               e
                                         Gradient Variational Bayes (SGVB) estimator L (Œ∏,œÜ;x                                     ) ' L(Œ∏,œÜ;x ):
                                                                                                    L
                                                                      A             (i)        1 X                   (i)    (i,l)                   (i,l)    (i)
                                                                    e
                                                                   L (Œ∏,œÜ;x )=                          logp (x ,z               ) ‚àílogq (z              |x     )
                                                                                              L                Œ∏                              œÜ
                                                                                                  l=1
                                                                                    (i,l)            (i,l)     (i)                 (l)
                                                                    where         z       =gœÜ(            , x    )    and             ‚àºp()                                           (6)
                                                                                                                  3
