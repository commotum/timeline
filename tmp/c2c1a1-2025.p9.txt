                        shows promising results on three different modalities including text, image
                        and speech. [5] propose a general framework Perceiver to handle arbitrary
                        configurations of different modalities. Without making domain-specific as-
                        sumptions, Perceiver encodes the data point through iterative self-attention
                        and cross-attention. And they mitigate the quadratic scaling problem of
                        self-attention blocks in Transformer by processing a small set of latent units
                        instead of high-dimensional inputs. Recently, Perceiver IO [6] appeared to
                        extend the capable task that the original Perceiver can not perform, and
                        Hierarchical Perceiver [26] improves the performance and efÏciency of the
                        Perceiver by introducing locality into the model architecture while preserv-
                        ing its modality-independence property. Our proposed model, GPIO, further
                        extends the accommodatable modality of Perceiver IO to the graph struc-
                        tured dataset.
                        2.3. Graph Neural Networks
                           The representative method of Graph Neural Networks is GCN [27] which
                        provides spatial domain graph convolution by approximating a spectral graph
                        convolution. GCN propagates a node representation by using a normalized
                        adjacency matrix. APPNP proposes propagation method based on person-
                        alized PageRank, which is one of the GCN variants. The memory efÏcient
                        RevGAT [28] for the deeper and wider models, Dir-GNN [29] which adopts
                        directionality for effective homophily and HiGCN [30] for the higher-order
                        interaction have improved classification ability on node-level or graph-level
                        tasks.  Furthermore, various modified models of GNNs in link prediction
                        tasks ensure reliable performance. [31] predict a link using an inner product
                        between the latent embeddings of target nodes encoded based on a varia-
                                                               6
