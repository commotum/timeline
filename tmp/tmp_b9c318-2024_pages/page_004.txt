                    iScience                                                                                                                                                  ll
                    Article                                                                                                                                             OPENACCESS
                          C
                                                                                                       Conv
                                                                                                       layer
                                                                         MaxPool
                                                                                                                                   Add&Relu
                                                                                                             Channel
                                                                                                                                           Spatial
                                                                                                            attention
                                                                                                                                         attention
                                                                                                                                                                   output
                                     input
                                                                            MaxPool
                                                                                                           Shared
                                                                                                            MLP
                                                                                                                                         Add&Relu
                                                                            AvgPool
                                                                                                  PST-FPN
                          A
                                                                                                                                             Cls
                                                 Voxelization
                                                                                     BEV
                                                                                                                                             Reg
                                                                                                                                    Head module
                                                                                                  2D backbone module
                                                               3D Backbone module
                                                                                                                                                                 Prediction
                               Raw Point Cloud
                                                                                               Overall framework
                          B                                                                                                                        D
                                                                                                                             Projection layer
                                                    Projection layer
                                                                                       (64,64,1)
                                                                                                                                  Add &
                                                         Add &
                                                                                 Attention-1
                                                                                                                                  Batch 
                                                         Batch 
                                                                                                                              Normalization
                                                     Normalization
                                                                                       (64,64,1)
                                                                                 Attention-1
                                                                                                                                   Feed
                                                          Feed
                                                                                                                                 Forward
                                                                                       (64,64,2)
                                                        Forward
                                                                                                                                  Layer
                                                         Layer
                                                                                 Attention-2
                                                                                                                                                                 Center point
                                                                                       (64,64,1)
                                                                                                                                 ReLU
                                                         ReLU
                                                                                 Attention-1
                                                                                                                                  Add &
                                                                                       (64,64,1)
                                                         Add &
                                                                                                                                  Batch 
                                                                                 Attention-1
                                                         Batch 
                                                                                                                              Normalization
                                                     Normalization
                                                                                       (32,64,2)
                                                                                                                                                                  Regression
                                                                                 Attention-2
                                                                                       (32,32,1)
                                                                                                                                                                             Output
                                                                                                                                                         Feature 
                                                                                 Attention-1
                                                                                                                                                           map
                                                                                       (32,32,1)
                                                                                 Attention-1
                                                                                                                              Ripple-spread 
                                                                                                                                                                    Offset
                                                     Ripple-spread 
                                                                                                                            center-emanating 
                                                                                       (16,32,2)
                                                    center-emanating 
                                                                                                                               attention-2
                                                       attention-1
                                                                                 Attention-2
                                                                             (input channel,output channel,stride)
                                                                                                           Attention-2
                                     Attention-1
                                                                                                                                                                Center-point
                                                                          Voxel self-attention network
                                                                                                                                                                    head
                    Figure 1. Outline of VSAC
                                                                                                                                         iScience 27, 110759, September 20, 2024            3
