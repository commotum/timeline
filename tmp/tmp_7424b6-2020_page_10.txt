                  2.4  Evaluation
                  For few-shot learning, we evaluate each example in the evaluation set by randomly drawing K examples from that
                  task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. For LAMBADA and Storycloze
                  there is no supervised training set available so we draw conditioning examples from the development set and evaluate
                  on the test set. For Winograd (the original, not SuperGLUE version) there is only one dataset, so we draw conditioning
                  examples directly from it.
                  Kcanbeanyvaluefrom0tothemaximumamountallowedbythemodel’scontextwindow,whichisnctx = 2048
                  for all models and typically ﬁts 10 to 100 examples. Larger values of K are usually but not always better, so when a
                  separate development and test set are available, we experiment with a few values of K on the development set and then
                  run the best value on the test set. For some tasks (see Appendix G) we also use a natural language prompt in addition to
                  (or for K = 0, instead of) demonstrations.
                  Ontasks that involve choosing one correct completion from several options (multiple choice), we provide K examples
                  of context plus correct completion, followed by one example of context only, and compare the LM likelihood of
                  each completion. For most tasks we compare the per-token likelihood (to normalize for length), however on a small
                  numberofdatasets (ARC, OpenBookQA, and RACE) we gain additional beneﬁt as measured on the development set
                  by normalizing by the unconditional probability of each completion, by computing   P(completion|context)  , where
                                                                                                  P(completion|answer context)
                  answer context is the string "Answer:    "or"A: "andisusedtopromptthatthecompletionshouldbeananswer
                  but is otherwise generic.
                  Ontasksthat involve binary classiﬁcation, we give the options more semantically meaningful names (e.g. “True” or
                  “False” rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what
                                  +
                  is done by [RSR 19] (see Appendix G) for details.
                                                                                                           +
                  Ontaskswithfree-form completion, we use beam search with the same parameters as [RSR 19]: a beam width of 4
                  and a length penalty of α = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on
                  what is standard for the dataset at hand.
                  Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-,
                  and few-shot). When the test set is private, our model is often too large to ﬁt on the test server, so we report results on
                  the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa)
                  where we were able to make submission work, and we submit only the 200B few-shot results, and report development
                  set results for everything else.
                  3   Results
                  In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6
                  additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling
                  performance follows a power-law when making efﬁcient use of training compute. After extending this trend by two
                  moreorders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these
                  improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will
                  see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a
                  broad spectrum of natural language tasks.
                  Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller
                  models) on a wide range of datasets. We group the datasets into 9 categories representing roughly similar tasks.
                  In Section 3.1 we evaluate on traditional language modeling tasks and tasks that are similar to language modeling,
                  such as Cloze tasks and sentence/paragraph completion tasks. In Section 3.2 we evaluate on “closed book” question
                  answering tasks: tasks which require using the information stored in the model’s parameters to answer general
                  knowledge questions. In Section 3.3 we evaluate the model’s ability to translate between languages (especially one-shot
                  and few-shot). In Section 3.4 we evaluate the model’s performance on Winograd Schema-like tasks. In Section 3.5 we
                  evaluate on datasets that involve commonsense reasoning or question answering. In Section 3.6 we evaluate on reading
                  comprehension tasks, in Section 3.7 we evaluate on the SuperGLUE benchmark suite, and in 3.8 we brieﬂy explore
                  NLI. Finally, in Section 3.9, we invent some additional tasks designed especially to probe in-context learning abilities –
                  these tasks focus on on-the-ﬂy reasoning, adaptation skills, or open-ended text synthesis. We evaluate all tasks in the
                  few-shot, one-shot, and zero-shot settings.
                                                                         10
