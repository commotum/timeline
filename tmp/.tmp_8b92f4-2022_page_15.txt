                          Under review as a conference paper at ICLR 2025
                  756     Jean Raven. Raven progressive matrices. In Handbook of nonverbal assessment, pp. 223–237.
                  757       Springer, 2003.
                  758
                  759     Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position represen-
                  760       tations. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Con-
                  761       ference of the North American Chapter of the Association for Computational Linguistics: Hu-
                  762       man Language Technologies, Volume 2 (Short Papers), pp. 464–468, New Orleans, Louisiana,
                  763       June 2018. Association for Computational Linguistics.   doi: 10.18653/v1/N18-2074.    URL
                  764       https://aclanthology.org/N18-2074.
                  765     Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-
                  766       hanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.
                  767     Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
                  768            ´  ´
                  769       Herve Jegou. Training data-efficient image transformers & distillation through attention. In
                  770       International conference on machine learning, pp. 10347–10357. PMLR, 2021.
                  771     Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
                  772       Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing
                  773       Systems, 2017. URL https://api.semanticscholar.org/CorpusID:13756489.
                  774     Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. Hy-
                  775       pothesis search: Inductive reasoning with language models. In The Twelfth International Confer-
                  776       ence on Learning Representations, 2024. URL https://openreview.net/forum?id=
                  777       G7UtIGQmjm.
                  778     Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li.
                  779       Uformer: Ageneralu-shapedtransformerforimagerestoration. InProceedingsoftheIEEE/CVF
                  780       conference on computer vision and pattern recognition, pp. 17683–17693, 2022.
                  781
                  782     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
                  783       Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
                  784       neural information processing systems, 35:24824–24837, 2022.
                  785     J S Wind. 1st place solution + code and official documentation.   https://www.kaggle.
                  786       com/competitions/abstraction-and-reasoning-challenge/discussion/
                  787       154597,2020.
                  788
                  789     KanWu,HouwenPeng,MinghaoChen,JianlongFu,andHongyangChao. Rethinkingandimprov-
                  790       ing relative position encoding for vision transformer. In Proceedings of the IEEE/CVF Interna-
                  791       tional Conference on Computer Vision (ICCV), pp. 10033–10041, October 2021.
                  792     YudongXu,EliasBKhalil,andScottSanner. Graphs,constraints,andsearchfortheabstractionand
                  793       reasoning corpus. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37,
                  794       pp. 4115–4122, 2023.
                  795
                  796     Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, and Elias Boutros Khalil. LLMs and
                  797       the abstraction and reasoning corpus: Successes, failures, and the importance of object-based
                  798       representations. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL
                  799       https://openreview.net/forum?id=E8m8oySvPJ.
                  800     Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual
                  801       commonsense reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
                  802       Pattern Recognition (CVPR), June 2019.
                  803     QiqiZhouandYichenZhu. Makealongimageshort: Adaptivetokenlengthforvisiontransformers.
                  804       In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp.
                  805       69–85. Springer, 2023.
                  806
                  807     Yongchao Zhou, Uri Alon, Xinyun Chen, Xuezhi Wang, Rishabh Agarwal, and Denny Zhou.
                  808       Transformers can achieve length generalization but not robustly.  In ICLR 2024 Workshop
                  809       on Mathematical and Empirical Understanding of Foundation Models, 2024. URL https:
                            //openreview.net/forum?id=DWkWIh3vFJ.
                                                                       15
