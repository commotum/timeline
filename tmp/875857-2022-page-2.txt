              Three immediate questions arise from (3):                         network. However, implicit networks do not need to store
              I Is the deÔ¨Ånition in (3) well-posed?                             intermediate quantities of the forward pass for backpropaga-
              I HowisN (d)evaluated?                                            tion. Consequently, implicit networks are trained using con-
                           Œò                                                    stant memory costs with respect to depth ‚Äì relieving a major
              I HowaretheweightsŒòofNŒò updatedduringtraining?                    bottleneck of training deep networks.
              Since the Ô¨Årst two points are well-established (Winston and       Noloss of expressiveness    Implicit networks as deÔ¨Åned in
              Kolter 2020; Bai, Kolter, and Koltun 2019), we brieÔ¨Çy re-         (3) are at least as expressive as feedforward networks. This
              view these in Section and focus on the third point. Us-           can easily be observed by setting R   to simply return Q ;
              ing gradient-based methods for training requires comput-                                              Œò                    Œò
              ing dNŒòdŒò, and in particular, du?dŒò. Hitherto, previ-           in this case, the implicit NŒò reduces to the feedforward EŒò
                                                 d                             in (1). More interestingly, the class of implicit networks
              ous works computed du? dŒò by solving a Jacobian-based
                                      d                                         in which SŒò and QŒò are constrained to be afÔ¨Åne maps
              equation (see Section ). Solving this linear system is com-       contains all feedforward networks, and is thus at least as
              putationally expensive and prone to instability, particularly     expressive (Ghaoui et al. 2019), (Bai, Kolter, and Koltun
              when the dimension of the latent space is large and/or in-        2019, Theorem 3). Universal approximation properties
              cludes certain structures (e.g. batch normalization and/or        of implicit networks then follow immediately from such
              dropout) (Bai, Kolter, and Koltun 2019; Bai, Koltun, and          properties of conventional deep neural models (e.g. see
              Kolter 2020).                                                        ¬¥
                Our primary contribution is a new and simple Jacobian-          (Csaji et al. 2001; Lu et al. 2017; Kidger and Lyons 2020)).
              Free Backpropagation (JFB) technique for training im-             Wealsomentionacouplelimitations of implicit networks.
              plicit networks that avoids any linear system solves. Instead,
              our scheme backpropagates by omitting the Jacobian term,          Architectural limitations    As discussed above, in theory
              resulting in a form of preconditioned gradient descent. JFB       given any feedforward network one may write down an im-
              yields much faster training of implicit networks and allows       plicit network yielding the same output (for all inputs). In
                                              1
              for a wider array of architectures .                              practice, evaluating the implicit network requires Ô¨Ånding a
                                                                                Ô¨Åxed point of RŒò. The Ô¨Åxed point Ô¨Ånding algorithm then
                            WhyImplicitNetworks?                                places constraints on RŒò (e.g. Assumption 0.1). Guaran-
              Below, we discuss several advantages of implicit networks         teeing the existence and computability of dNŒòdŒò places
              over explicit, feedforward networks.                              further constraints on RŒò. For example, if Jacobian-based
              Implicit networks for implicitly deÔ¨Åned outputs          In       backpropagation is used, RŒò cannot contain batch normal-
                                                                                ization (Bai, Kolter, and Koltun 2019).
              some applications, the desired network output is most aptly       Slowerinference     Oncetrained, inference with an implicit
              described implicitly as a Ô¨Åxed point, not via an explicit
              function. As a toy example, consider predicting the variable      network requires solving for a Ô¨Åxed point of RŒò. Finding
              y ‚àà Rgivend ‚àà [‚àí1/2,1/2]when(d,y)isknowntosatisfy                 this Ô¨Åxed point using an iterative algorithm requires evaluat-
                                                5                     (4)       ing RŒò repeatedly and, thus, is often slower than inference
                                      y = d+y .                                 with a feedforward network.
              Using y1 = 0 and the iteration                                              Implicit Network Formulation
                      y     =T(y ;d),d+y5, forallk ‚ààN,                (5)
                       k+1        k             k                               All terms presented in this section are provided in a general
              oneobtains yk ‚Üí y. In this setting, y is exactly (and implic-     context, which is later made concrete for each application.
              itly) characterized by y = T(y,d). On the other hand, an          Weincludeasubscript Œò on various terms to emphasize the
              explicit solution to (4) requires an inÔ¨Ånite series representa-   indicated mapping will ultimately be parameterized in terms
                                                               5                                  2
              tion, unlike the simple formula T(y,d) = d + y . See ap-          oftunableweights Œò.Atthehighestlevel,weareinterested
              pendix for further details. Thus, it can be simpler and more      in constructing a neural network NŒò : D ‚Üí Y that maps
                                                                                                 3
              appropriate to model a relationship implicitly. For example,      from a data space D to an inference space Y. The implicit
              in areas as diverse as game theory and inverse problems, the      portion of the network uses a latent space U, and data is
              output of interest may naturally be characterized as the Ô¨Åxed     mapped to this latent space by QŒò: D ‚Üí U. We deÔ¨Åne the
              point to an operator parameterized by the input data d. Since     network operator TŒò : U √ó D ‚Üí U by
              implicit networks Ô¨Ånd Ô¨Åxed points by design, they are well-                       T (u;d) , R (u,Q (d)).                  (6)
              suited to such problems as shown by recent works (Heaton                           Œò            Œò      Œò
              et al. 2021a,b; Gilton, Ongie, and Willett 2021).                 Provided input data d, our aim is to Ô¨Ånd the unique Ô¨Åxed
              ‚ÄúInÔ¨Ånite depth‚Äù with constant memory training           As        point u? of TŒò(¬∑ ;d) and then map u? to the inference space
              mentioned, solving for the Ô¨Åxed point of R (¬∑ ;Q (d)) is                 d                            d
                                                          Œò      Œò                 2
              analogous to a forward pass through an ‚ÄúinÔ¨Ånite depth‚Äù (in            Weusethe same subscript for all terms, noting each operator
              practice, very deep) weight-tied, input injected feedforward      typically depends on a portion of the weights.
                                                                                   3Each space is assumed to be a real-valued Ô¨Ånite dimensional
                 1All codes can be found on Github:                             Hilbert space (e.g. Rn) endowedwithaproducth¬∑,¬∑iandnormk¬∑k.
              github.com/howardheaton/jacobian free backprop                    It will be clear from context which space is being used.
                                                                          6649
