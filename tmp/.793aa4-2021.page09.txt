                                                                              Krzysztof Choromanski, Valerii Likhosherstov, David
                                                                                Dohan, Xingyou Song, Andreea Gane, Tamas Sar-
                                                                                los, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
                                                                                Lukasz Kaiser, David Belanger, Lucy Colwell, and
                                                                                Adrian Weller. 2021. Rethinking attention with per-
                                                                                formers.
                                                                              Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-
                                                                                ina Williams, Samuel R. Bowman, Holger Schwenk,
                                                                                and Veselin Stoyanov. 2018. Xnli: Evaluating cross-
                                                                                lingual sentence representations. In Proceedings of
                                                                                the 2018 Conference on Empirical Methods in Natu-
                  Figure 4: Visualization of learned positional attention       ral Language Processing. Association for Computa-
                  patterns of DIET-ABS. Note that in addition to captur-        tional Linguistics.
                  ing the the relative positional relations, the model also
                  learn to attend to [CLS] at index 0, suggesting the ded-    Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V.
                  icated [CLS] untying design in Ke et al. (2020) is not        Le. 2020. Funnel-transformer: Filtering out sequen-
                  necessary with DIET-ABS.                                      tial redundancy for efﬁcient language processing.
                  cial pattern. This pattern cannot be solely modeled         Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
                  by existing relative positional embedding methods,            Kristina Toutanova. 2018.     BERT: Pre-training of
                  and some existing works (Ke et al., 2020) handled             deep bidirectional Transformers for language under-
                                                                                standing. arXiv preprint arXiv:1810.04805.
                  this case speciﬁcally by introducing new parame-
                  ters. This shows the beneﬁt of DIET-ABS in not              Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham
                  requiring any carefully designed inductive biases             Neubig, Orhan Firat, and Melvin Johnson. 2020.
                  as in existing approaches( Shaw et al. (2018); Raf-           Xtreme: A massively multilingual multi-task bench-
                                                                                mark for evaluating cross-lingual generalisation. In
                  fel et al. (2020)), which may not generalize across           ProceedingsofMachineLearningandSystems2020,
                  tasks.                                                        pages 7449–7459.
                  5    Conclusion                                             Guolin Ke, Di He, and Tie-Yan Liu. 2020. Rethink-
                                                                                ing the positional encoding in language pre-training.
                  In this paper we theoretically and empirically ex-            arXiv preprint arXiv:2006.15595.
                  amined the limitation of additive position embed-           TakuKudoandJohnRichardson.2018. Sentencepiece:
                  ding at input and showed that having per-head posi-           A simple and language independent subword tok-
                  tion embeddings results in better performance. We             enizer and detokenizer for neural text processing.
                  argued that the superior performance of some of             Guillaume Lample and Alexis Conneau. 2019. Cross-
                  the relative position encoding methods come from              lingual language model pretraining.
                  their per-head addition to attention matrix rather
                  than the position information being relative vs ab-         Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
                  solute. Indeed we show that using absolute position           Kevin Gimpel, Piyush Sharma, and Radu Soricut.
                  encodings per-head results in better performance.             2020. Albert: A lite bert for self-supervised learn-
                                                                                ing of language representations.
                  Motivated by this we propose a simple per-head po-
                  sition and segment attention method that achieves           Xiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng
                  the state-of-the-art performance on multiple NLP              Gao. 2020. Very deep transformers for neural ma-
                  tasks and is more computationally efﬁcient than               chine translation.
                  existing approaches.                                        Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
                                                                                dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
                                                                                Luke Zettlemoyer, and Veselin Stoyanov. 2019.
                  References                                                    RoBERTa: A robustly optimized BERT pretraining
                                                                                approach. arXiv preprint arXiv:1907.11692.
                       ˇ
                  Ondrej Bojar, Christian Federmann, Mark Fishel,
                     Yvette Graham, Barry Haddow, Philipp Koehn, and          Matt Post. 2018. A call for clarity in reporting bleu
                     Christof Monz. 2018.     Findings of the 2018 con-         scores. In WMT.
                     ference on machine translation (WMT18). In Pro-
                     ceedings of the Third Conference on Machine Trans-       AlecRadford,KarthikNarasimhan,TimSalimans,and
                     lation: Shared Task Papers, pages 272–303, Bel-            Ilya Sutskever. 2018.    Improving language under-
                     gium, Brussels. Association for Computational Lin-         standing by generative pre-training. Technical Re-
                     guistics.                                                  port, OpenAI.
                                                                         2982
