              www.nature.com/scientificdata/                                                                                           www.nature.com/scientificdata
                                                      Metric                           Training Set    Evaluation Set
                                                      Number of tasks                  400             400
                                                      Total number of participants     783             946
                                                      Incomplete participants          94              242
                                                      Average participants per task    11.8            10.3
                                                      Average attempts to solution     1.3             1.4
                                                      Total attempts                   7,916           7,820
                                                      Unique number of visited states  127,146         208,214
                                                      Total action traces              241,697         344,569
                                                     Table 1.  Human ARC Descriptives. Here we report numerical values summarizing our behavioral dataset. 
                                                     “Total attempts” is the number of individual submissions across all tasks and participants, and “total action 
                                                     traces” is the number of individual actions recorded across all tasks/participants.
                                                     descriptions collected with each problem attempt expose how people use and create on-the-f㘶y abstractions to 
                                                     solve novel problems, af㘶ording the possibility of informative, natural-language analyses.
                                                     Methods
                                                     We collected human data on each of the 400 training tasks and 400 evaluation tasks from ARC in two separate 
                                                     phases (extending the subset of 40 training tasks previously collected and described in Johnson et al.9. Each 
                                                     task has 1–10 training examples and 1–3 test examples, with each example consisting of an input-output pair. 
                                                     Because only a few tasks had more than 1 test example (14 and 19 tasks in the training and evaluation sets 
                                                     respectively), we opted to evaluate humans using only the f㘶rst test example for each of these tasks. On average, 
                                                     11.8 participants completed each of the 400 training tasks, while 10.3 participants completed each of the 400 
                                                     evaluation tasks.
                                                     participants.  We recruited 783 participants (59.6% male, 37.8% female, 2.6% other) on the training set tasks 
                                                     and 946 participants (49.5% male, 48.0% female, 2.5% other) on the evaluation set tasks from Amazon Mechanical 
                                                     Turk using the CloudResearch (cloudresearch.com) platform to ensure high quality data13. Participants were 
                                                     between 18 and 78 years old (M = 40.4, SD = 10.8). T㔴ey were compensated $10 and were also given a bonus 
                                                     of $1 if they succeeded at a randomly selected task and its written solution description was judged adequate 
                                                     by the experimenters. Best judgement was used: if a description was at least one complete sentence and was 
                                                     relevant to the task, it was counted as adequate. T㔴e study was approved by the local institutional review board 
                                                     (NYU’s Committee on Activities Involving Human Subjects; IRB-FY2016-231). Participants were informed about 
                                                     the general purpose of the study, the kinds of content they would be shown, and that they would be required 
                                                     to interact with our interface using their computer. T㔴ey were informed about compensation amounts, the 
                                                     anonymization of their data, their right to withdraw at any time, and asked to consent before proceeding with the 
                                                     experiment.
                                                     Data collection.  We evaluated humans using the same evaluation procedure proposed in the original paper 
                                                                                          1
                                                     describing the ARC benchmark . In particular, human participants were allowed three attempts per task to gen-
                                                     erate a correct solution, and were only given minimal feedback on each attempt, with the interface labeling each 
                                                     submission as correct or incorrect af㘶er each attempt.
                                                     User Interface.     Participants were f㘶rst given instructions about the experiment and explanations about the 
                                                                                                                                          9
                                                     dif㘶erent aspects of the ARC user interface. As in previous experiments , the user interface closely mirrored the 
                                                                                                   1
                                                     original interface provided by Chollet  (see Fig. 2). T㔴e interface allowed participants to select dif㘶erent colors 
                                                     and either edit one cell at a time or multiple selected cells at once. More sophisticated tools allowed the partic-
                                                     ipant to copy and paste a selection from the test input to the test output grid, or use the f㘶ood f㘶ll tool to change 
                                                     the color of all neighboring cells of the same color to a new color. Participants could resize the grid height and 
                                                     width, as well as copy the full test input grid to the test output grid. A reset button allowed participants to revert 
                                                     the output grid back to the initial state, a 3  × 3 black grid. Finally, unlike in previous iterations of the interface, 
                                                     we added another tool allowing participants to undo actions and revert the state of the output grid to the pre-
                                                     vious state before the last action was taken. At any point in time, the participant could click the help button to 
                                                     display the full set of instructions.
                                                     Tutorial.    At the beginning of the experiment, participants were provided with animated instructions outlining 
                                                     the user interface with an example task, and then asked to solve the same task to familiarize themselves with the 
                                                     interface. A relatively simple task was given to participants for the tutorial, and they were required to generate 
                                                     the correct test output to proceed (see Fig. 2 for an example). For the training set experiment, we chose task 
                                                     21f83797.json from the evaluation set, whereas for the evaluation set experiment, we chose task e9af-
                                                     cf9a.json from the training set. Af㘶er the tutorial, participants were asked to answer several basic compre-
                                                     hension questions to make sure they understood the instructions. T㔴e experiment started immediately af㘶er 
                                                     successful completion of the quiz. If one of the questions was answered incorrectly, participants were told that 
                                                     one or more of the questions were incorrect. Participants were given unlimited attempts at the quiz, but could 
                                                     only continue to the main part of the experiment if they successfully answered each question.
              Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                                             3
