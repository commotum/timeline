                    LOC         LOC     testing LOC error classiﬁcation top-5 LOC error                       method                     top-5 localization err
                   method      network         on GTCLS     network   on predicted CLS                                                    val           test
                 VGG’s[41]    VGG-16    1-crop  33.1 [41]                                         OverFeat [40] (ILSVRC’13)              30.0           29.9
                    RPN      ResNet-101 1-crop    13.3                                            GoogLeNet[44](ILSVRC’14)                 -            26.7
                    RPN      ResNet-101 dense     11.7
                    RPN      ResNet-101 dense              ResNet-101       14.4                  VGG[41](ILSVRC’14)                     26.9           25.3
                 RPN+RCNN ResNet-101 dense                 ResNet-101       10.6                  ours (ILSVRC’15)                        8.9           9.0
                 RPN+RCNN ensemble       dense              ensemble         8.9
                Table 13. Localization error (%) on the ImageNet validation. In              Table 14. Comparisons of localization error (%) on the ImageNet
                the column of “LOC error on GT class” ([41]), the ground truth               dataset with state-of-the-art methods.
                class is used. In the “testing” column, “1-crop” denotes testing
                on a center crop of 224×224 pixels, “dense” denotes dense (fully             ports a center-crop error of 33.1% (Table 13) using ground
                convolutional) and multi-scale testing.                                      truth classes. Under the same setting, our RPN method us-
                                                                                             ing ResNet-101 net signiﬁcantly reduces the center-crop er-
                58.8%mAPandourensembleof3modelshas62.1%mAP                                   ror to 13.3%. This comparison demonstrates the excellent
                ontheDETtestset(Table12). Thisresultwonthe1stplace                           performance of our framework. With dense (fully convolu-
                in the ImageNet detection task in ILSVRC 2015, surpassing                    tional) and multi-scale testing, our ResNet-101 has an error
                the second place by 8.5 points (absolute).                                   of 11.7% using ground truth classes. Using ResNet-101 for
                                                                                             predicting classes (4.6% top-5 classiﬁcation error, Table 4),
                C. ImageNet Localization                                                     the top-5 localization error is 14.4%.
                                                                                                 Theaboveresultsareonlybasedontheproposalnetwork
                   The ImageNet Localization (LOC) task [36] requires to                     (RPN) in Faster R-CNN [32]. One may use the detection
                classify and localize the objects. Following [40, 41], we                    network (Fast R-CNN [7]) in Faster R-CNN to improve the
                assume that the image-level classiﬁers are ﬁrst adopted for                  results. But wenoticethatonthisdataset,oneimageusually
                predicting the class labels of an image, and the localiza-                   contains a single dominate object, and the proposal regions
                tion algorithm only accounts for predicting bounding boxes                   highly overlap with each other and thus have very similar
                based on the predicted classes. We adopt the “per-class re-                  RoI-pooled features. As a result, the image-centric training
                gression” (PCR) strategy [40, 41], learning a bounding box                   of Fast R-CNN [7] generates samples of small variations,
                regressor for each class. We pre-train the networks for Im-                  whichmaynotbedesiredforstochastictraining. Motivated
                ageNet classiﬁcation and then ﬁne-tune them for localiza-                    by this, in our current experiment we use the original R-
                tion. We train networks on the provided 1000-class Ima-                      CNN[8]thatisRoI-centric, in place of Fast R-CNN.
                geNet training set.                                                              OurR-CNNimplementationisasfollows. We apply the
                   Our localization algorithm is based on the RPN frame-                     per-class RPN trained as above on the training images to
                work of [32] with a few modiﬁcations. Unlike the way in                      predict bounding boxes for the ground truth class. These
                [32] that is category-agnostic, our RPN for localization is                  predicted boxes play a role of class-dependent proposals.
                designed in a per-class form. This RPN ends with two sib-                    For each training image, the highest scored 200 proposals
                ling 1×1 convolutional layers for binary classiﬁcation (cls)                 are extracted as training samples to train an R-CNN classi-
                and box regression (reg), as in [32]. The cls and reg layers                 ﬁer. The image region is cropped from a proposal, warped
                are both in a per-class from, in contrast to [32]. Speciﬁ-                   to 224×224 pixels, and fed into the classiﬁcation network
                cally, the cls layer has a 1000-d output, and each dimension                 as in R-CNN[8]. Theoutputsofthisnetworkconsistoftwo
                is binary logistic regression for predicting being or not be-                sibling fc layers for cls and reg, also in a per-class form.
                ing an object class; the reg layer has a 1000×4-d output                     This R-CNN network is ﬁne-tuned on the training set us-
                consisting of box regressors for 1000 classes. As in [32],                   ing a mini-batch size of 256 in the RoI-centric fashion. For
                our bounding box regression is with reference to multiple                    testing, the RPN generates the highest scored 200 proposals
                translation-invariant “anchor” boxes at each position.                       for each predicted class, and the R-CNN network is used to
                   AsinourImageNetclassiﬁcation training (Sec. 3.4), we                      update these proposals’ scores and box positions.
                randomly sample 224×224 crops for data augmentation.                             This method reduces the top-5 localization error to
                Weuseamini-batch size of 256 images for ﬁne-tuning. To                       10.6% (Table 13). This is our single-model result on the
                avoid negative samples being dominate, 8 anchors are ran-                    validation set. Using an ensemble of networks for both clas-
                domlysampledforeachimage,wherethesampledpositive                             siﬁcation and localization, we achieve a top-5 localization
                and negative anchors have a ratio of 1:1 [32]. For testing,                  error of 9.0% on the test set. This number signiﬁcantly out-
                the network is applied on the image fully-convolutionally.                   performstheILSVRC14results(Table14),showinga64%
                   Table 13 compares the localization results. Following                     relative reduction of error. This result won the 1st place in
                [41], weﬁrstperform“oracle”testingusingthegroundtruth                        the ImageNet localization task in ILSVRC 2015.
                class as the classiﬁcation prediction. VGG’s paper [41] re-
                                                                                        12
