                 Of the other implications the least ridiculous (though still ridiculous) of these is the imaginary
                 mass, or negative mass squared. Coincidentally or not, negative values for the mass squared of
                 the neutrino were measured multiple independent times over a decade ago! In fact it seemed
                 like everyone that tried measuring the square mass of the neutrino found it to be negative. The
                 ﬁrst of these cases had a lot of noise, but I believe some of the results eventual got to a few
                 sigma. For some elusive reason that no one seems to know, no fuss was ever made over the
                 results.  It  sparked  some theoretical papers on superluminal neutrino models, but overall it
                 faded away. Even the critics of the Opera result note these old results and admit that they can’t
                 come up with a dismissal any better than “well we stopped hearing about it so it was probably
                 nothing”.
                 On top of all this, their have been level headed people arguing that neutrinos are tachyonic for
                 over 20 years, and there are a number of reasons why they did so. Furthermore, there is
                 nothing in any law of physics directly forbidding tachyons as far as I know. The only things that
                 do are the result of a number of dearly held intuitions being forced upon the laws of physics.
                 And if the last century has taught us anything, it is that we shouldn’t get too cozy with our
                 deepest intuitions.
                 Keep your hand on your wallet, but be ready to take your credit card out if the time comes.
                 Chris W. Says:
                 Comment #31 September 23rd, 2011 at 10:03 pm
                 To the extent that this is a “why” question, and not merely a matter of formally characterizing
                 the  complexity  of  the  “middle”  state,  what  about  the  role  of  gravity?  After  all,  isn’t  it  the
                 universal attraction of gravity, including the expansion of the universe as described in general
                 relativity,  that  drives  primordial matter towards this complicated non-equilibrium state? I’m
                 surprised Sean Carroll didn’t bring this up.
                 Scott Says:
                 Comment #32 September 23rd, 2011 at 10:03 pm
                 rrtucci: Thanks for the interesting suggestion! However,
                 (1) Won’t μ(range(P(c|r))) always be 0 in a discrete system like the one we’re considering, since
                 there are only ﬁnitely many c’s and r’s, hence ﬁnitely many P(c|r)’s?
                 (I agree that one could easily ﬁx this problem, for example by coarse-graining over the unit
                 interval.)
                 (2) Your measure only makes sense given a probability measure over the microstates. However,
                 I  want  a  complextropy  measure  that  (at  least  in  principle)  can  be  calculated  for  a  speciﬁc
                 microstate, and (a related requirement) makes sense even for deterministic dynamical systems.
                 That was my motivation for bringing in Kolmogorov complexity. (Sorry I didn’t make that more
                 explicit in the post!)
                 (3)  Your  measure  is  (of  course)  rather  tailored  to  the  cof㘶ee  cup  example,  and  could  be
                 invalidated even by small changes to that example. For example, suppose I told you that there
                 were equal amounts of cof㘶ee and milk, and that with probability 1/2 the milk started out on
                 top of the cof㘶ee, and with probability 1/2 the cof㘶ee started out on top of the milk. In that
                 case, symmetry considerations imply that P(c|r) would always be 1/2, for every c and r and at
                 every time step. Yet it still seems intuitively like the complextropy starts out small, increases,
                 and then decreases again.
                 Chris W. Says:
                 Comment #33 September 23rd, 2011 at 10:11 pm
                 PS: I should have said “…, along with the expansion of the universe as described in general
                 relativity, …”.
                 Scott Says:
                 Comment #34 September 23rd, 2011 at 10:25 pm
                 Justin  #30:  I  agree  with  almost  everything  you  say  (as  usual,  Sean  Carroll  did  a  great  job
                 summarizing the issues). But of course, during the Deolalikar af㘶air, there were also lots of
                 serious  people  making serious arguments for taking that claim seriously! It was only long
                 experience with wrong P≠NP proofs that emboldened me to bet against.
                 As for the neutrinos, I’m obviously far from an expert, but am moved by the following two
                 points:
                 (1) Closed timelike curves seem to me to be a dif㘶erent order of strangeness from anything
                 thus far discovered in physics—like maybe 1000 times stranger than relativity, QM, virtual
                 particles, and black holes put together. And I don’t understand how one could have tachyonic
                 neutrinos without getting CTCs as well—would anyone who accepts that possibility be kind
                 enough to explain it to me?
                 (2) As I understand it, the possibility of systematic errors in an experiment of this sort are
                 legion, no matter how careful the experimenters are. And if there is a systematic error, then
                 presumably there’s a 50% chance that it’s going to be in the direction that was reported! In
                                                                                         -5
                 other words: once someone decides to search for particles going ~1+10  times faster than the
                 speed of light, the prior probability that they’ll ﬁnd what look like such particles seems to me
                 to be quite high, even under the assumption that no such particles exist.
                 IThinkImClever Says:
                 Comment #35 September 23rd, 2011 at 10:48 pm
                 “In answering Sean’s provocative question (whether there’s some “law of complexodynamics”
                 that would explain his graph)”
                 I’m gonna side with [Complexodynamics: Comment #2] here, as in:
                 Yeah, it’s called “Rolle’s Theorem”:
                 In calculus, Rolle’s theorem essentially states that a dif㘶erentiable function which attains equal
                 values  at  two  distinct  points  must  have  a  point  somewhere  between  them  where  the  ﬁrst
                 derivative (the slope of the tangent line to the graph of the function) is zero.
                 Scott Says:
                 Comment #36 September 23rd, 2011 at 10:57 pm
                 IThinkImClever: Your argument is “clever” but wrong! The constant zero function would also
                 satisfy Rolle’s Theorem. Therefore, that theorem is manifestly irrelevant to the problem that I
                 stated:  explaining  mathematically  why  the  complextropy  becomes  large  and  positive at
                 intermediate times.
                 Henry Y Says:
                 Comment #37 September 24th, 2011 at 12:12 am
                 Justin: I’m no physicist and I haven’t read the results about negative squared mass of neutrinos,
                 so I’m quite underqualiﬁed to comment, but:
                 I’m quite skeptical about the negative squared mass claims, because how would one even set
                 up an experiment to measure imaginary mass? The caricatured method that comes to mind is
                 this: measure the velocity of neutrino, plug in velocity into relativistic Lorentz equations, and
                 “observe” imaginary mass! Please let me know if this resembles the actual experiments at all.
                 If  the  actual  experiment  were  anything  like  this,  then  I  believe  we  would  have  the  same
                 questions about the method of measuring velocities of neutrinos.
                 Then, it would be hard to say that the results of the imaginary mass experiment support the
                 results of FTL neutrino experiment, because the former relies on the latter!
                 (Apologies for diverting comment thread from “complextropy”!)
                 Snif㘶noy Says:
                 Comment #38 September 24th, 2011 at 1:44 am
                 Seems at least one physicist has had the same idea as you!
                 Chang Kee Jung, a neutrino physicist at Stony Brook University in New York, says he’d wager
                 that the result is the product of a systematic error. “I wouldn’t bet my wife and kids because
                 they’d get mad,” he says. “But I’d bet my house.”
                 (Hat tip to hegemonicon on LessWrong.)
                 Snif㘶noy Says:
                 Comment #39 September 24th, 2011 at 1:46 am
                 (I guess not really, as I don’t see anywhere where he’s announced that he actually *is* betting
                 his house, and stated terms. But someone was going to say it.     )
                 IThinkImClever Says:
                 Comment #40 September 24th, 2011 at 2:41 am
                 @Scott: Comment #36
                 OK, ﬁne.
                 But note that I claimed only to ever be clever, and not right. 
                 Also,  I  won’t  maintain  here  that  the  constant  zero  function  case  would  actually  cause  the
                 question to be trivial and uninteresting. 
                 Anyhow, after thinking about “complextropy” a bit more, I am now wondering:
                 1) Why are we assuming that “equilibrium is simple”, when the length of the *minimum boolean
                 expression* required to describe the ‘mixing’ system *exactly* at time step t grows rapidly as
                 entropy increases?
                 (BTW, how about these minimum boolean expressions as an objective, calculable criterion for
                 “complextropy”, on say, your 2D array of black and white pixels?)
                 2) In terms of universality (of whose threshold of acquirement is extremely low), once acquired
                 by, or present in a system, where is there left to go? What is more ‘complex’ than a ‘universal
                 computer’ on which these ‘particles’ are interacting on? Aren’t we ‘maxed out’ pretty early on
                 in the game?
                 3)  Perhaps  we  need  better  deﬁnitions  of  the  ‘players’  involved,  especially  physical
                 ‘randomness’? Maybe, in the end, only pseudorandomness really exists, if indeed the universe
                 is ultimately discrete at the lowest level, and everything has low Kolmogorov complexity.
                 *As an aside, on the Neutrino Debacle: Yeah, again, time to again ﬁrst invoke the KISS Principle
                 and/or Murphy’s Law. It’s most likely a simple error.
                 IThinkImClever Says:
                 Comment #41 September 24th, 2011 at 2:57 am
                 w.r.t  the  Neutrino  Debacle:  I  would  now  be  cautious  to  taking  a  betting  position  against
                 “superluminal neutrinos”, but ONLY because Nikola Tesla somewhat foresaw it, and Tesla was
                 NEVER wrong. 
                 “In 1901 Nikola Tesla was one the ﬁrst to identify “radiant energy.” Tesla says that the source
                 of this energy is our Sun. He concluded that the Sun emits small particles, each carrying so
                 small of a charge, that they move with great velocity, exceeding that of light. Tesla further
                 states that these particles are the neutron particles. Tesla believed that these neutron particles
                 were responsible for all radioactive reactions. Radiant matter is in tune with these neutron
                 particles. Radiant matter is simply a re-transmitter of energy from one state to another.”
                 “All of my investigations seem to point to the conclusion that they are small particles, each
                 carrying so small a charge that we are justiﬁed in calling them neutrons. They move with great
                 velocity, exceeding that of light. More than 25 years ago I began my ef㘶orts to harness the
                 cosmic rays and I can now state that I have succeeded in operating a motive device by means of
                 them. I will tell you in the most general way, the cosmic ray ionizes the air, setting free many
                 charges  ions  and  electrons.  These  charges  are  captured  in  a  condenser  which  is  made  to
                 discharge through the circuit of the motor. I have hopes of building my motor on a large scale,
                 but circumstances have not been favorable to carrying out my plan.”
                 MattF Says:
                 Comment #42 September 24th, 2011 at 9:00 am
                 Just a sentence or two about neutrinos. Even if neutrinos are tachyonic, you still have to explain
                 why they aren’t traveling at speeds closer to the speed of light, given their low mass. To me,
                 the really unlikely thing about this measurement is the magnitude of (v_neutrino – speed of
                 light), though I’ll admit that the sign is weird too.
                 Jef Allbright Says:
                 Comment #43 September 24th, 2011 at 2:35 pm
                 This topic has fascinated me for years, and I’m convinced (intuitively) that it will remain an
                 open-ended problem.
                 I was going to suggest the evolutionary, and then the anthropological aspect, but I see others
                 here already have.
                 This leaves the “Edge of Chaos” aspect, which may help round out your research.
                 1993—Melanie Mitchell & James Crutchﬁeld & Peter Hraber—Dynamics, Computation, and the
                 “Edge     of    Chaos”:    A    Re-Examination      http://www.santafe.edu/research/working-
                 papers/abstract/e5b0ef2ae9887b454ea8501f4a9568a7/
                 2010—Thierry  Mora  &  William  Bialek—Are  biological  systems  poised  at  criticality?
                 http://arxiv.org/abs/1012.2242
                 I believe the Kolmogorov-based approach is a dead end, as we inherently lack the necessary
                 context  within  the  vast  space  of  possible  evolutionary  trajectories  to  predict  which
                 mathematical  combinations  will  represent  synergistic  and  persistent,  thus  “interestingly
                 complex” novel structures.
                 As to the anthropological question of why *we* ﬁnd certain structures “interestingly complex”, I
                 believe this is doable in theory, and to the extent it is achievable it could provide a useful
                 building-block for comparative modeling of systems of values and ethics.
                 Sean Carroll Says:
                 Comment #44 September 24th, 2011 at 2:57 pm
                 I started to comment on Scott’s CTC worry, but it turned into a blog post:
                 http://blogs.discovermagazine.com/cosmicvariance/2011/09/24/can-neutrinos-kill-their-
                 own-grandfathers/
                 Sid Says:
                 Comment #45 September 24th, 2011 at 4:07 pm
                 @Justin Dove: Despite the fact that superluminal signalling may not be possible for certain
                 classes of tachyons, from what I understand, if the result is correct, superluminal signalling
                 shouldn’t be that hard – the 0 bit could be represented as a small number of neutrinos being
                 sent (and detected) while the 1 bit could be a much larger number of neutrinos being sent (and
                 detected). Varying this, one should be able to send superluminal messages.
                 Scott Says:
                 Comment #46 September 24th, 2011 at 4:12 pm
                 Jef Allbright #43:
                 I believe the Kolmogorov-based approach is a dead end, as we inherently lack the necessary
                 context  within  the  vast  space  of  possible  evolutionary  trajectories  to  predict  which
                 mathematical  combinations  will  represent  synergistic  and  persistent,  thus  “interestingly
                 complex” novel structures.
                 Look, there’s clearly the issue that, while the second cof㘶ee cup in Sean’s picture is more
                 “interesting” than the ﬁrst or third cups, it’s still not very interesting! (No of㘶ense, Sean.)
                 So it would be nice to have a complextropy/interestingness measure that assigned an even
                 higher score to, for example, the cof㘶ee being drunk by a baboon wearing a top-hat.
                 However, I took at as obvious that the goal here was not to construct an “ultimate theory of
                 interestingness”  in  one  go—yes,  I  agree,  that  sounds  pretty  hopeless!—but  merely  to  do
                 somewhat better than entropy as an interestingness criterion, by matching our intuition that
                 the second cup is more interesting than either the ﬁrst or the third. I should have made that
                 clearer in the post.
                 Abram Demski Says:
                 Comment #47 September 24th, 2011 at 4:35 pm
                 (Why are people talking about neutrinos in response to a post that’s not at all about neutrinos?)
                 I’ve decided to cast my vote for the “logical depth” version. Assume we have some ﬁnite system
                 evolving according to some rules. If the system has a simple initial state, and if the rules are
                 “interesting”, and if the system’s rules don’t discard information, then the logical depth will
                 tend to increase linearly at ﬁrst: the shortest description of the current state will be to give the
                 initial state and the amount of time that’s passed. However, after a long time (on the order of
                 the number of possible states of the system, in the worst case), it will no longer be worth it to
                 describe the state via the entire history. At this point the logical depth will at least hit a ceiling.
                 Again assuming the rules are su㘠陦ciently “interesting”, the logical depth will drop back down to
                 near 0, because at this stage the system state will look random (and that’ll be it’s shortest
                 description).
                 A word on my assumptions.
                 First, the assumption that we don’t discard information. I make this assumption because it’s
                 needed for thermodynamics in the ﬁrst place: if information ever went away, entropy could
                 decrease. I think it’s also needed for my result here.
                 The relevant deﬁnition of “interesting” may sound like a sticking point. Intuitively, I mean that
                 as the state evolves, it doesn’t stay simple. (The cream doesn’t stay at the top of the cof㘶ee, nor
                 does it fall to the bottom in a straight line.) A decent deﬁnition for my purposes is that any
                 state can reach a large portion of other states. (IE, for any initial state we choose, if we keep
                 running  the  simulation  for  long  enough,  it  will  pass  through  at  least  1/4  of  the  possible
                 conﬁgurations–  1/4  being  chosen  arbitrarily).  Since  the  vast  majority  of  states  have  high
                 kolmogorov  complexity,  this  is  enough  to  guarantee  that  the  Kolmogorov  complexity  will
                 increase over time and eventually hit the upper bound for the system, ie, the complexity of a
                 randomly chosen conﬁguration for that system. At this point the logical depth of a state will
                 almost always be on the order of the log of the number of system states.
                 Will the logical depth exceed the log of the number of system states before that, though? I
                 suspect so, because I suspect the best way to describe the system will be its history for a long
                 time (on the order of the number of system states), and for as long as that’s the case, depth
                 increases linearly. However, I’m not totally sure I can prove this based on my current deﬁnition
                 of “interesting rules”. (They may not be interesting enough!)
                 wolfgang Says:
                 Comment #48 September 24th, 2011 at 4:48 pm
                 This seems to me to be about coarse-graining, the picture of the cup on the left consists
                 mostly of black and white pixels, neatly ordered.
                 The one in the middle is more interesting because it consists of mixed pixels of dif㘶erent
                 colors, while the one on the right again has a simple description (all pixels are ‘brown’).
                 But of course the brown pixels hide the many possible microstates of black and white little
                 drops.  So  the  simple  description  (but  with  high  entropy)  is  due  to  coarse-graining  and
                 ‘averaging’ of microstates.
                 So the question is why coarse-grained descriptions play such an important role and I would say
                 the answer is that we human beings don’t care about the statistics of microstates but instead
                 pay most attention to what our eyes report and they are coarse-graining in a particular way –
                 optimized by evolution to make ‘important’ stuf㘶 interesting to us and boring stuf㘶 (=high
                 entropy stuf㘶) uninteresting to us.
                 Abram Demski Says:
                 Comment #49 September 24th, 2011 at 6:41 pm
                 @ comment #8 (& #48):
                 I think we don’t have to take the terms macro/micro too literally. Course-graining, to me, is
                 about partial information. We can’t fully observe the state of the cof㘶ee, and a “macro-state” is
                 just  a  set  of  states  that  we  can’t  distinguish  from  one  another.  So,  to  deﬁne  entropy  for
                 deterministic  systems,  we  make  them  partially  observable.  This  is  functionally  similar  to
                 making them stochastic, since it means that we have uncertainty about the system.
                 David Schwab Says:
                 Comment #50 September 24th, 2011 at 7:42 pm
                 There’s  a  really  nice  collection  of  work  by  Bill  Bialek  and  collaborators  on  “predictive
                 information” deﬁned as “the mutual information between the past and the future of a time
                 series.” In it they “argue that the divergent part of [the predictive information] provides the
                 unique measure for the complexity of dynamics underlying a time series.”
                 This paper is very well written and, with a thorough discussion of its relation to previous work,
                 is also a useful reference for those interested in these issues:
                 http://www.princeton.edu/~wbialek/our_papers/bnt_01a.pdf
                 Justin Dove Says:
                 Comment #51 September 24th, 2011 at 9:05 pm
                 @Henry The old experiments did *not* measure the mass in that way. They measured the mass
                 squared  “directly”  using  analysis  of  the  beta  spectrum  of  tritium  decay.  I’m  not  going  to
                 pretend to understand it completely, but there’s a large amount of literature and references at
                 http://arxiv.org/pdf/0909.2104v1.  The  results  were  almost  all  (with  a  few  exceptions)
                 consistent with the possibility of positive mass squared values (within an error bar), but they
                 nearly unanimously favored the negative side.
                 As time has gone on they have gotten closer to c, but oddly the absolute measurements are
                 still often coming up negative. Now these results are not very interesting given the uncertainty,
                 but the coincidence of the superluminal claims makes them fun to toy with at least.
                 @Scott I agree. The main dif㘶erence I ﬁnd with the Deolalikar situation is that this is a claim
                 coming from a large group of people that *weren’t* looking for this, as far as I can tell (as
                 opposed to one person that was). Furthermore, I’ve also read that they have spent months
                 shaking their heads in disbelief and looking for systematic errors with the help of metrology
                 folks. I get the impression that they had no intention of publicly announcing this, but ultimately
                 it came to the point where they couldn’t ﬁnd the error (if it exists) and needed to open it up to
                 the peer community.
                 As  far  as  CTC’s  go,  you  could  always  go  with  Lorentz  violations,  but  honestly  I  ﬁnd  that
                 probably  even  more  unappetizing.  The  other  thing  is  to  envoke  some  sort  of  consistency
                 principle that forces ctc’s to only exist in consistent ways.
                 Again, I’m by no means claiming to believe this claim is true. But I just think its worth much
                 more serious care and considerations then many other such incidents.
                 Kaleberg Says:
                 Comment #52 September 24th, 2011 at 10:31 pm
                 @Wolfgang may be on to something. If you look at the JPEG encoding (using DCT) of the image
                 of the cof㘶ee, the less interesting cups have higher coe㘠陦cients in the upper left and lower right
                 of the DCT matrix. The interesting middle cup has lots more going on in the middle of the
                 matrix. (You don’t get this ef㘶ect with PNG or GIF images of cof㘶ee, so it may just be serendipity
                 that Sean used JPEG.)
                 JPEG works by breaking an image into 8×8 pixel squares and doing a transformation that
                 measures the visual complexity in the horizontal and vertical directions. The top left coe㘠陦cient
                 of the matrix is basically the average value of all the pixels and the bottom right a more or less
                 checkerboard mix. From a distance, these two extremes both correspond to something that
                 looks like gray. If you want interesting structure, you need stuf㘶 in the middle of the matrix.
                 SpeakerToManagers Says:
                 Comment #53 September 25th, 2011 at 2:15 am
                 Scott @ 22, Sean Carroll @ 25:
                 I’d  conjecture  the  opposite,  that  higher-dimensional  (more  complex  rules  because  larger
                 neighborhoods) spaces of CA rule sets have smaller subspaces containing “interesting” rules. If
                 the “edge of chaos” is a real phenomenon (and there was some controversy about that the last
                 time I looked), then it represents a hypersurface in the rule space around which “interesting”
                 rules  cluster,  and  that’s  going  to  be  a  smaller  percentage  of  the  total  space  as  the
                 dimensionality increases.
                 In any case I rather doubt we’ll ever have a way to determine the complexity of a CA rule in less
                 time than it takes to execute it; that would seem to violate the undecidability of the Halting
                 Problem.
                 Gil Kalai Says:
                 Comment #54 September 25th, 2011 at 2:49 am
                 Dear Scott, these are nice thoughts. Here are a few comments.
                 1)  The  idea  that  the  complexity  of  a  system  increases  in  intermediate  states  is  nice.
                 Kolgomorov complexity appears to be much too complex notion of complexity to express this
                 idea.  Maybe  some  notions  from  ergodic  theory  (according  to  which  both  deterministic
                 sequences and very simple random sequemces are “simple”) can be more relevant. (So perhaps
                 “complex” should mean that the system does not behave deterministically; where by laws of
                 large numbers simple random processes do behave deterministically.)
                 2) A related idea is that in physical systems, complex behavior is witnessed near criticality.
                 Namely, if the system depending on some parameter t witnesses a phase transition for t=T_0
                 then it is considerably more “complex” when t is close to T_0.
                 3) There is some di㘠陦culty with the assertion that equilibrium states are “simple” with what we
                 know about possible complexity of equilibrium states, be it in the context of quantum ground
                 states, or in the context of protein folding.
                 It is not clear that we can usiversally claim that the equilibrium states must be “simple”.
                 4)  One idea that I found appealing is that for certain purposes clocks of physical systems
                 should be scaled in a way that the rate of time at a time T is inversly proportional to the
                 “complexity” of the system at time T. So time passes slowly at interesting times and quickly at
                 boring times.
                 mkatkov Says:
                 Comment #55 September 25th, 2011 at 3:06 am
                 The dif㘶erence between pictures 1,3 and 2 is the ability to predict color of the “cap”, based on
                 their neighbors. The color distribution is almost everywhere uniform. In the “middle cap”, (I
                 mean  most  interesting  one)  the  knowledge  of  the  “color”  (say,  some  coding  of  possible
                 conﬁgurations) for the “box” at any scale have no/minimal predictive power at distant “box” of
                 the same scale, leading to situation similar to fractal images.
                 jonas Says:
                 Comment #56 September 25th, 2011 at 4:57 am
                 There is, however, a dif㘶erence between the Deolalikar af㘶air and the current ftl neutrinos issue.
                 The ﬁrst one was original research claiming something groundbreaking, and attempting to
                 supporting it in a scientiﬁc manner, even if the proof is not actually correct. The second is the
                 case of the media trying to make news from a strange experimental result where the physicists
                 apparently  don’t  claim  to  have  found  anything  groundbreaking,  which  phenomenon  is
                 illustrated in the strip “http://www.phdcomics.com/comics/archive.php?comicid=1174”.
                 lylebot Says:
                 Comment #57 September 25th, 2011 at 11:01 am
                      you can’t ignore the fact that it is a post ﬁve sigma result, hell it’s post six sigma
                 I feel free to ignore the fact that it’s a “post six sigma result” because the number of sigmas is
                 not a measure of reliability. It’s closer to a measure of how bad your initial assumptions are—
                 not just the null hypothesis, but also anything regarding the experimental design. If it’s true
                 that there are many possible systemic errors in the experiment (I’m nowhere near a physicist,
                 so I can’t say), then that’s most likely what all those sigmas reﬂect.
                 Craig Says:
                 Comment #58 September 25th, 2011 at 1:55 pm
                 For  the  record,  I’ve  long  believed  that  there’s  a  similar  argument  to  be  made  for  an
                 informational (or, in your language, complexodynamical) sweet spot in art. We don’t seem to
                 derive a strong aesthetic response to paintings that are either too simple or too complex,
                 because the former are boring and the latter are noise. There does seem to be a middle ground
                 in which a painting contains the “right” amount of information.
                 Of course, any such measurement is tricky. If I turn the Mona Lisa upside down, or break it into
                 1″x1″ tiles and permute them, I have a design that’s very close in terms of information, but
                 which may produce a quite dif㘶erent aesthetic response.
                 Of  course,  this  is  all  pseudoscientiﬁc  daydreaming  until  someone  (not  me)  can  produce  a
                 suitable mathematical deﬁnition, combined with a psychological validation. I will note that Jim
                 Bumgardner did say something very similar in a short essay on information theory and art
                 (http://www.mungbeing.com/issue_3.html?page=9#235),  though  using  the  language  of
                 Shannon entropy rather than Kolmogorov complexity.
                 Raoul Ohio Says:
                 Comment #59 September 25th, 2011 at 2:16 pm
                 Complexodynamics: While this is clearly an interesting direction, there is a major problem
                 likely(?) to to prevent any useful results:
                 There is no reason to think there is ANY scalar (as opposed to multidimensional) function that
                 meaningfully captures the notion of complexity (let along “interesting-ness”).
                 The  situation  is  very  dif㘶erent  in  thermodynamics,  where  entropy  S  has  a  reasonable,  not
                 arbitrary,  deﬁnition.  Key  thermodynamic  concepts  are  deﬁned  as  derivatives  of  S,  or  with
                 respect to S, and they agree with measurable quantities deﬁning heat engines, etc.
                 Grasping for a workable deﬁnition of complexity seems rather like trying to deﬁne IQ.
                 Why discuss neutrinos right now? Well, because it would be a once in a century event. My
                 (purely intuitive) estimate on the likelihood of this holding up: one in a million.
                 IThinkImClever Says:
                 Comment #60 September 25th, 2011 at 5:11 pm
                 “Grasping for a workable deﬁnition of complexity seems rather like trying to deﬁne IQ.”
                 Yeah, I’m pretty sure Wolfram would have accomplished this, if at all possible, in his 20+ years
                 and ‘million mouse miles’ of researching complexity for his tome, but I’m still rather happy
                 with his book as an overall informal deﬁnition of this “Complexodynamics”.
                 Scott Says:
                 Comment #61 September 25th, 2011 at 6:01 pm
                 Raoul #59: Once again, the key point is that I had no pretensions to deﬁne “complexity” for
                 every possible purpose—only for the speciﬁc purpose of answering Sean’s question about the
                 three cof㘶ee cups.
                 For some reason, many, many commenters seem to have missed that, and assumed that I was
                 trying to do something much broader than I was.
                 Speaking  of  which,  IThinkImClever  #60:  I’ve  read  Wolfram’s  entire  tome,  and  as  far  as  I
                 remember, he never once discusses the question of Sean that this post is about.
                 (More  generally,  Wolfram  never  of㘶ers  any  formal  deﬁnition  of  complexity—his  notion  of
                 complexity  is  analogous  to  Justice  Potter  Stewart’s  “I  know  it  when  I  see  it”  deﬁnition  of
                 pornography. His attitude toward people who try to formalize and prove things, as opposed
                 just to eyeballing thousands of CA printouts, is one of open disdain.)
                 IThinkImClever Says:
                 Comment #62 September 25th, 2011 at 7:13 pm
                 Well,  as  a  quick  response,  I’d  say  that  while  Wolfram  never  focused  on  any  *one*  speciﬁc
                 question in NKS, I think he did succeed in his overall general goal of elucidating the origins of
                 complexity, and why it occurs at all.
                 And you’re right, in sections where he mentions deﬁnitions (e.g. Deﬁning Complexity, pg.557),
                 he only considers how it *might* be formally deﬁned, given our limited processes of perception
                 and analysis.
                 And while I wouldn’t ever have a disdain at formalizing and proving things, I am with Wolfram
                 for now in just accepting as axiom that you never need to go beyond the elementary CA’s, as
                 processes will only ever evolve behaviours that can be ‘captured’ by the 4 atomic/primitive
                 patterns they exhibit: simple, repetitive, complex, random.
                 But that being said, I think your attempt at formalizing Sean’s question is quite clever, using
                 the formally deﬁned concept of Sophistication within Kolmogorov Complexity.
                 I think we all see what you are trying to get at, but of course we are naturally gonna go of㘶 on
                 tangents. 
                 Mike Says:
                 Comment #63 September 25th, 2011 at 8:06 pm
                 IThinkI’mClever,
                 “Yeah, I’m pretty sure Wolfram would have accomplished this, if at all possible, in his 20+ years
                 and ‘million mouse miles’ of researching complexity for his tome, . . .”
                 So,  you’re  saying  it’s  impossible  because  Wolfram  hasn’t  accomplished  it,  because  it’s
                 “impossible” — really, your “pretty sure” about all of this?
                 What is it do think went wrong; do the the laws of physics say it’s impossible? Or, is it that
                 humans just aren’t bright enough to “really” ﬁgure it out?
                 And, if we haven’t, why is that, because if we had it would have been this guy Wolfram who
                 would have manged to ﬁgured it out already?
                 What is it exactly you’re trying to say — other than being just a little bit of㘶-putting? 
                 Maybe, just maybe, notwithstanding his past accomplishments, Wolfram simply has the wrong
                 approach.
                 As for me, I think we will accomplish deﬁning complexity — around about the same time as we
                 get to a good working deﬁnition of creativity and create AI Programs.
                 I know, this doesn’t seem to be close at hand, but I’m hoping for a surprise.
                 IThinkImClever Says:
                 Comment #64 September 25th, 2011 at 8:35 pm
                 Well, it’s not impossible because Wolfram hasn’t accomplished it. Anything
                 “So,  you’re  saying  it’s  impossible  because  Wolfram  hasn’t  accomplished  it,  because  it’s
                 “impossible” — really, your “pretty sure” about all of this?”
                 No. Anything’s possible, right? I think I was just giving my informal and humble views on the
                 topic.
                 “What is it do think went wrong; do the the laws of physics say it’s impossible? Or, is it that
                 humans just aren’t bright enough to “really” ﬁgure it out?”
                 Well, I think there exists uncomputable and irreducible things, as well as concepts that can’t be
                 contained in a few symbols.
                 “And, if we haven’t, why is that, because if we had it would have been this guy Wolfram who
                 would have manged to ﬁgured it out already?”
                 Although he can be a bit too ‘proud’ at times, I recognize and respect Wolfram’s genius on the
                 topic of complexity in general.
                 “What is it exactly you’re trying to say — other than being just a little bit of㘶-putting?”
                 Really? I was just putting forth my limited views on the matter. I guess I’m not as clever as I
                 once thought.
                 “Maybe, just maybe, notwithstanding his past accomplishments, Wolfram simply has the wrong
                 approach.”
                 Maybe. But as he puts it, the bits are the bits. You can’t argue with all of his results, at least.
                 “As for me, I think we will accomplish deﬁning complexity — around about the same time as we
                 get to a good working deﬁnition of creativity and create AI Programs.”
                 Yeah, me too. But for now, it’s between simple and random. 
                 Terry Bollinger Says:
                 Comment #65 September 25th, 2011 at 10:17 pm
                 Scott,
                 Have we met?… I ask as I sit absently scribbling expanded-dimensionality SR diagrams on what
                 I’ve belatedly realize is the backside of CSAIL memo paper I picked up when I last visited the
                 Seussaplex (yes, that’s my term, but I did get laughs with it and I don’t _think_ anyone was
                 of㘶ended… 
                 (Do you know Russ T? I always love hearing updates on the work he’s doing on on expanding
                 regions of stability. It’s a mathematical approach that’s giving results intriguingly similar to
                 biological  systems,  even  though  the  underlying  mechanism  are  entirely  dif㘶erent,  as  Russ
                 always notes. That kind of unexpected parallelism is simply intriguing!)
                 I thought I understood Kolmogorov complexity. Actually, I _still_ think I understand it, since I
                 used to like to give the example of how an arbitrary string pulled deep from within pi can in
                 principle be generated by a very short program, but good luck on ﬁguring that out if you don’t
                 know it’s from pi ahead of time!
                 The trouble is that on ﬁrst whack, I just don’t get where you are heading in trying to link that
                 particular idea with the emergence of “interesting” complexity in our universe.
                 If your main point is that there is likely a deep connection between seemingly mundane data
                 compression and the workings of the physics of our universe as it stands, and that this link
                 also ties in to how intelligent systems work, I’m absolutely with you, you’re preachin’ to the
                 choir [SC you never read that line], etc.
                 In fact, I would go so far as to say that I think a deeper understanding of multilevel information
                 factoring — data compression — needs to play a major role in the science of intelligence if we
                 ever want to build robotic systems as small, fast, and energy-e㘠陦cient as say insects.
                 So, I clearly need to read your ideas more closely and will try to do so soon. I’ve only skimmed,
                 and I know it.
                 My blog question to you would be this: If you had to give a two-line pitch of your most critical
                 insight or postulate to an expert — not just to some random person on an elevator! — what
                 would it be? You could use equations, big words, small words, cryptic, or simple, just as long
                 as it captures what you think is most important.
                 Maybe it’s already in there, in which case your best reply to my query may be “Go back and
                 read paragraph x, you skxawng!” I do believe in and try to practice ego-free paper reading
                 whenever possible… 
                 Cheers,
                 Terry Bollinger
                 IThinkImClever Says:
                 Comment #66 September 25th, 2011 at 11:17 pm
                 OK, since I apparently came of㘶 as unproductive to some readers, let me try to make up for it by
                 giving my ‘more formal’ take on ‘complexodynamics”:
                 So it would seem that any formal deﬁnition of “complexodynamics” would necessarily require
                 that  *localized  structures*  can  exist,  that  is,  eventually  appear  and  evolve  and  die,  giving
                 “complextropy”, within the underlying representation, say, bits here.
                 And it would appear that Scott’s Sophistication measure magically “picks out” these families of
                 bit-strings that are not too simple, not too random, but just right: they have provable clusters
                 of *localized structure*. (I say ‘magically’ here because we don’t know their origins. How were
                 they ultimately constructed?)
                 So what’s going on in the cof㘶ee cups? Brownian motion. Random motion. But why should this
                 random motion produce ‘interestingness’, or rather, temporary localized structures, at all?
                 Well, as time passes, the partitioned 0’s and 1’s randomly interact and trespass in each other’s
                 initial territories. And since this interaction is indeed a random process, we *should* expect
                 that  localized  structures  will  eventually  be  produced,  otherwise  the  ‘mixing’  will  occur
                 uniformly, and hence not ﬁt the deﬁnition of randomness, thus contradicting the brownian ﬂuid
                 ﬂow under which the particles operate!
                 So the middle cup is ‘sophisticated’ simply because of how random processes work: ﬂipping a
                 fair coin does not go 0,1,0,1,…,0,1. It eventually produces temporary streaks of repetitions, or
                 in the cof㘶ee case, temporary localized structures of ‘milk tendrils’.
                 But alas, as more time passes, the localized structures are forced to ultimately interact with
                 each other, becoming highly interconnected and “smeared”, and thus disappear from ef㘶ective
                 measure.
                 Hence, I conclude that randomness is one origin of complexity, as it ef㘶ectively takes simplicity
                 from one end of the spectrum to the other: from partitioned, ordered elements to mixed,
                 disordered elements.
                 Bee Says:
                 Comment #67 September 26th, 2011 at 12:40 am
                 I’ll go and disagree with commenter #2 and your reply. Change plays a role of course, but it’s
                 not the main point. If it were, you could deﬁne complexity as the time derivative of entropy,
                 which doesn’t make much sense. Of course you can deﬁne what you want but you can come up
                 with a lot of systems that have a steep increase in entropy without being complex in any way
                 that we’d naively think of as complex. On that note, I also have an issue with Sean’s image of
                 the cafe au lait, as to me it looks more like chaos than complexity.
                 In any case, I think Sean is right when he points out that coarse graining is important. I think
                 the  issue  of  change  is  to  be  found  more  in  spatial  heterogeneity.  What  you  want  is  local
                 deviations from a straight evolution towards increasing entropy, something that maintains a
                 low entropy on the expense of entropy increase elsewhere. Again the problem is that there are
                 systems that might have that without actually being complex, so that won’t do either, but I
                 think that’s where one would start from.
                 Peter Says:
                 Comment #68 September 26th, 2011 at 1:22 am
                 Nice work. But please don’t corrupt it with cosmological theories that might be wrong.
                 My  favorite  cosmological  model,  not  requiring  any  new  physics  and  resulting  in  huge
                 conceptual  simpliﬁcations,  is  one  where  the  universe  is  inﬁnite  and  matter  is  fractally
                 distributed with all momenta equally represented at large enough scales. An eternal smash up
                 because there is always a bigger and faster structure bearing down on us. Perhaps our pocket
                 of the universe is the result of such a collision. I do believe that entropy is hard to deal with
                 when the system is inﬁnite. But it seems to me that if you take our local universe out to about
                 15  billion  light  years  in  radius  and  collide  it  with  another  similar  or  bigger  pocket  at
                 0.9999999c, the result might look like what we call the big bang. The situation would not
                 change much even if both pockets had previously entered heat-death.
                 Dr. Ajit R. Jadhav Says:
                 Comment #69 September 26th, 2011 at 3:17 am
                 Dear Scott:
                 You said: “First, some background: we all know the Second Law, which says that the entropy of
                 any closed system tends to increase with time until it reaches a maximum value.”
                 Two points:
                 (i) The Second Law applies to the _thermodynamic_ _isolated_ systems, not to _all_ _closed_
                 systems.
                 Inasmuch as an information-theoretical view of entropy might perhaps dif㘶er in some manner
                 from the real universe (i.e., from the inductive context and scope of the observations on which
                 the Second Law was originally based), one might expect the law not to hold in an exactly
                 similar manner in all the respects.
                 (ii)  Of㘶-hand, I take it that the Second Law says only that the entropy of an isolated system
                 either remains constant or inﬁnitesimally increases with an inﬁnitesimal passage of time. _If_
                 an isolated system initially has such a level (or quantity) of entropy that it can increase with the
                 passage of time, _then_ it will increase. The entropy’s actually reaching a maximum value,
                 starting from a lower value, is not a part of the statement of the Second Law itself. The Second
                 Law is equally applicable to the case wherein you have an asymptotically increasing entropy
                 that never in fact reaches “the” (limiting) maximum value.
                 I have yet to read the rest of your (to me somewhat) interesting post as also all the previous
                 comments,  and  so,  might  come  back  later  on  once  again  if  I  have  any  ((at  least  to  me)
                 interesting) comments to make. What, however, caught the eye was your inclusion of a graph
                 which seems remarkably similar to the one which is widely studied in the chaos and nonlinear
                 systems theory.
                 Best,
                 –Ajit
                 [E&OE]
                 Shir Says:
                 Comment #70 September 26th, 2011 at 3:58 am
                 I would just like to comment that mkatkov (#55) and Kaleberg (#52) are pointining in directions
                 that we know to be related.
                 When discussing boolean functions on the Hamming cube, the sensitivity of a function g is
                 deﬁned to be $\max_{x\in\left\{ 0,1\right\} ^{n}}|\{y\sim x:g(x)\neq g(y)\}|$
                 When the sensitivity of a function is low, we can anticipate the value of g on a point give its
