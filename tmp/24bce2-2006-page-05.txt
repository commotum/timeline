              AFastLearningAlgorithmforDeepBeliefNets                           1531
              weights, w , on the directed connections from the ancestors:
                        ij
                    p(s = 1) =           1          ,                         (2.1)
                      i        1+exp −b − s w
                                          i    j j  ij
              where bi is the bias of unit i. If a logistic belief net has only one hidden
              layer, the prior distribution over the hidden variables is factorial because
              their binary states are chosen independently when the model is used to
              generatedata.Thenonindependenceintheposteriordistributioniscreated
              by the likelihood term coming from the data. Perhaps we could eliminate
              explaining away in the ﬁrst hidden layer by using extra hidden layers to
              createa“complementary”priorthathasexactlytheoppositecorrelationsto
              those in the likelihood term. Then, when the likelihood term is multiplied
              by the prior, we will get a posterior that is exactly factorial. It is not at
              all obvious that complementary priors exist, but Figure 3 shows a simple
              exampleofaninﬁnitelogisticbeliefnetwithtiedweightsinwhichthepriors
              arecomplementaryateveryhiddenlayer(seeappendixAforamoregeneral
              treatment of the conditions under which complementary priors exist). The
              useoftiedweightstoconstructcomplementarypriorsmayseemlikeamere
              trickformakingdirectedmodelsequivalenttoundirectedones.Asweshall
              see, however, it leads to a novel and very efﬁcient learning algorithm that
              worksbyprogressivelyuntyingtheweightsineachlayerfromtheweights
              in higher layers.
                 2.1 An Inﬁnite Directed Model with Tied Weights. We can generate
              data from the inﬁnite directed net in Figure 3 by starting with a random
                                                            1
              conﬁguration at an inﬁnitely deep hidden layer and then performing a
              top-down “ancestral” pass in which the binary state of each variable in a
              layerischosenfromtheBernoullidistributiondeterminedbythetop-down
              input coming from its active parents in the layer above. In this respect, it
              is just like any other directed acyclic belief net. Unlike other directed nets,
              however,wecansamplefromthetrueposteriordistributionoverallofthe
              hidden layers by starting with a data vector on the visible units and then
              using the transposed weight matrices to infer the factorial distributions
              over each hidden layer in turn. At each hidden layer, we sample from the
              factorial posterior before computing the factorial posterior for the layer
              above.2 Appendix A shows that this procedure gives unbiased samples
                 1 ThegenerationprocessconvergestothestationarydistributionoftheMarkovchain,
              so weneedtostartatalayerthatisdeepcomparedwiththetimeittakesforthechainto
              reach equilibrium.
                 2 This is exactly the same as the inference procedure used in the wake-sleep algorithm
              (Hintonetal.,1995)butforthemodelsdescribedinthisletternovariationalapproximation
              is required because the inference procedure gives unbiased samples.
