                                                   Reflection System for the Abstraction and Reasoning Corpus
        055     reflection. We introduce a new augmented ARC (AugARC)                          Dataset Size          MaxPermutations
        056     benchmark tailored towards LLMs, which shows consis-                           2 000 tasks                     -
        057     tently improved performance across all tested LLMs com-                        4 000 tasks                     2
        058     paredtothenormalARC.Weshowthebenefitoffine-tuning                              5 715 tasks                     3
        059     LLMsonaugmentedARCdata. Finally,webuildaReflec-                                7 430 tasks                     4
        060     tion System which solves 5 more evaluation tasks than a pre-                   9 145 tasks                     5
        061     vious system that combines multiple ARC solvers (Bober-                        18668610tasks                  All
        062     Irizar & Banerjee, 2024).
        063                                                                          Table 1. Size of the augmented ARC training datasets according
        064     2. AugARC:AugmentedARCforLLMs                                        to the maximum number of permutations. All datasets include
        065                                                                          90° and 270° rotations, and horizontal and vertical flipping. The
        066     ARCtraining data can be utilised to fine-tune LLMs and               augmented datasets range in size from 2000 to 18 million tasks.
        067     improve their performance in evaluation and test sets. One
        068     potential issue with this approach is the size of the training
        069     set - it contains only 400 samples. Since LLMs have billions
        070     of parameters, they usually cannot be effectively trained on         tive at evaluating broad generalization abilities due to its
        071     smaller datasets and instead require more samples. There-            hand-crafted abstract logic.
        072     fore, due to its small size, the ARC training dataset limits the     Identifying that the lack of a textual ARC benchmark is a
        073     ability to fine-tune LLMs for improved broad generalization          significant barrier for evaluating LLMs, we create the Au-
        074     and reasoning.                                                       gARCBenchmark. TheAugARCBenchmarkprovidesan
        075                                                                          easy and unified way to evaluate LLMs on 3-shot accuracy
        076     2.1. Augmented Training Data                                         on reasoning tasks. In AugARC, each ARC task starts with
        077                                                                          a textual description explaining the format of the problem.
        078     Toovercomethelimited number of ARC training tasks, we                Each ARCgridisrepresented as a 2D matrix of numbers.
        079     implement an augmentation procedure that can significantly
        080     extend the training dataset. Our approach expands the ARC            2.2.1. AUGARC INPUT TO LLMS
        081     training set by applying the following transformations:
        082                                                                          Thefirst prediction is based on a normal ARC task, whereas
        083        - Rotation: clockwise rotation of each ARC grid for a             the second and the third ones are 90° and 270° clockwise
        084          given task by 90° or 270°.                                      rotated versions of the same task. The AugARC Benchmark
        085                                                                          is tailored for LLMs, as these models process inputs in an
        086        - Flipping: flips each ARC grid of a task horizontally            auto-regressive, sequential manner. By rotating the ARC
        087          (along the y-axis) and vertically (along the x-axis).           tasks, LLMs are presented with a different sequence of
        088                                                                          numbers (2D matrices) which contain the same abstract
        089        - Permutations: rearranges the sequence of demonstra-             logic.
        090          tion input-output pairs before the test input grid. We set
        091          a threshold for the maximum number of permutations              2.2.2. REPRODUCING ARC SOLUTIONS FROM
        092          per task to produce datasets of various sizes.                          AUGARCOUTPUTS
        093                                                                          Although the second and third shot in AugARC are based
        094     Depending on the transformations applied and the maxi-               on rotated ARC tasks, the output of the LLMs can easily be
        095     mumnumberofpermutationsapplied, the augmented ARC                    transformed back to a solution to the original ARC problem.
        096     training datasets vary from 2000 up to over 18 million tasks.        OnceanoutputisgeneratedbytheLLM,itissimplyrotated
        097                                                                          back in an anticlockwise direction. In this way, AugARC
        098                                                                          only changes the input representation of the ARC problems:
        099     2.2. 3-Shot AugARC Benchmark                                         the outputs by the models are then rotated to valid ARC
        100     A key reason for the relatively scarce ARC research on               solutions. This process ensures that the results with the
        101     LLMsisthelackofatextualversionofthebenchmark. The                    AugARCapproacharedirectlycomparabletopreviousARC
        102     only benchmark suitable for LLMs that resembles Chollet’s            attempts.
        103     visual ARC (Chollet, 2019) is the AI2 Reasoning Chal-
        104                                     ˘
        105     lenge (Clark et al., 2018; Patras et al., 2022). AI2 is a            3. Fine-tuning LLMs on ARC tasks
        106     multi-choice question answering benchmark that focuses on
        107     assessing reasoning. Although AI2 is a more popular and              Although LLMs have shown impressive capabilities, they
        108     well-established reasoning benchmark for LLMs compared               can sometimes hallucinate and are therefore regarded as
        109     to Chollet’s ARC (Chollet, 2019), the latter is more effec-          unreliable in reasoning tasks. One potential way to reduce
                                                                                  2
