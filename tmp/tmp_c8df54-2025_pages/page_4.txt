                                               Reflection System for the Abstraction and Reasoning Corpus
        165
        166
        167
        168
        169
        170
        171
        172
        173
        174
        175
        176
        177
        178
        179
        180
        181
        182
        183    Figure 3. Reflection System - execution on two ARC evaluation tasks. Initially, multiple solvers make independent predictions on the
        184    task. Then, the task and the prediction are presented to the reflection model, which chooses the correct final prediction. In the example,
        185    solver 1 is based on program synthesis (DSL Search) and solver 2 is an LLM (Claude 3 Opus). The reflection model is an LLM (GPT-4o).
        186    Both task flows are actual demonstrations of how the Reflection System performs on ARC evaluation tasks. In both cases, the Reflection
        187    System produces a correct final solution.
        188
        189    4.2. Reflection over all Prediction                             let, emphasised that the performance of intelligent systems
        190                                                                    should be measured by the fraction of tasks solved on the
        191    The second stage of our approach is inspired by previous        evaluation set (Chollet, 2019). Therefore, we perform our
        192    studies on self-reflection (Lee et al., 2024a; Renze & Gu-      experiments on the evaluation set and use 3 shots per task,
        193    ven, 2024), in which LLMs refine their responses based on       as set out in the ARC design (Chollet, 2019).
        194    feedback against previous outputs and, in this way, achieve
        195    more accurate predictions. In our system, the reflection        Topresentfullyreproducibleresults, allexperimentsareexe-
        196    model processes all generated predictions from all the ARC      cuted on the complete evaluation set. Some previous solvers
        197    solvers. Conditioning on the given ARC task, the reflection     have been evaluated on a subset of the ARC evaluation data,
        198    model chooses the prediction from the solver that is most       making it difficult to understand the true performance of
        199    likely to be correct.                                           the solver. Our testing approach ensures that future studies
        200                                                                    could easily use our results for direct comparison with new
        201    4.3. Flexibility of the Reflection System                       ARCsolvers.
        202
        203    In the Reflection System, an ARC solver can be any model        5.1. Performance on base ARC and AugARC
        204    including LLMs, program synthesis approaches or neuro-
        205    symbolicmodels. Anynumberofsolverscanbeused,asthe               We start our experiments with LLMs on the base ARC
        206    reflection model can easily process the outputs of various      benchmark, shown in Table 2. The ARC accuracy across
        207    solvers. This makes our approach customisable, as each of       7-13billion modelsrangesfrom5to9correctlysolvedARC
        208    its components - the ARC solvers and the reflection model,      evaluation tasks. Bigger LLMs solve slightly more ARC
        209    can easily be changed. This architectural design allows the     tasks, from 7 to 20 solved tasks, with Gemini Pro achieving
        210    Reflection System to be easily tested with various ARC          the highest accuracy (20).
        211    solvers for finding the optimal ARC configuration.              Using the same LLMs, we evaluate the performance on Au-
        212                                                                    gARC.ForallLLMs,thereisaclearaccuracyimprovement
        213    5. Experiments                                                  on AugARC compared to base ARC. The increase varies
        214                                                                    from 29% for Llama-2 7B up to 125% for Mixtral 8x7B,
        215    We perform all experiments on the ARC evaluation set            with the majority of models achieving at least 60%.
        216    which consist of 400 tasks. By design, the ARC evalua-
        217    tion set is significantly more challenging than the training    Thesignificant improvement in all LLMs on AugARC com-
        218    set (Chollet, 2019). The creator of ARC, FrancÂ¸ois Chol-        pared to ARC suggests that changing the grid structure of
        219
                                                                            4
