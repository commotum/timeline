                                                                                         Reflection System for the Abstraction and Reasoning Corpus
              220                         Model                          ARC             AugARC                 Increase                               5.3. Performance of Fine-tuned LLMs on ARC
              221                         Llama-2 7B                    5/400                7/400                  29%                                To observe whether we can reduce the performance gap
              222                         Mistral 7B                    9/400               15/400                  67%                                between smaller and bigger LLMs on ARC, we fine-tune
              223                         Llama-2 13B                   5/400                8/400                 100%                                the 7 and 13B models. The models are fine-tuned on the
              224                         Llama-2 70B                   7/400               14/400                 100%
              225                         Mixtral 8x7B                  9/400               18/400                 125%                                training set only using a single Nvidia A100 80GB GPU.
              226                         GeminiPro                    20/400               33/400                  65%                                Theresults in Table 3 show that the fine-tuned LLMs solve
              227          Table 2. Performance of LLMs on ARC and AugARC (on the                                                                      between 18 and 34 ARC evaluation tasks. Training ben-
              228           evaluation set). There is a consistent increase of the accuracy of                                                         efited all the models substantially - the small fine-tuned
              229           LLMswhenusingtheAugARCinputscomparedtousingthebase                                                                         Llama-2 7B and 13B achieved a performance on par with
              230          ARCones(29-125%).                                                                                                           the base versions of the significantly bigger models such as
              231                                                                                                                                      Llama-2 70B. After fine-tuning, Mistral 7B outperforms the
              232                                                                                                                                      standard Mixtral 8x7B by 5 correct tasks. The highest result
              233           the tasks for the second and third shots leads to increased                                                                of 34 correct solutions after fine-tuning by Llama-3 8B is
              234           accuracy. LLMs process the ARC tasks sequentially, and                                                                     impressive, as it outperforms Gemini Pro.
              235           thus are directly influenced by the exact order of the grids.
              236           Based on the results, we conclude that the proposed Au-                                                                                Model                          Base            Fine-tuned                 Increase
              237           gARCbenchmarkiswellsuitedforLLMs.                                                                                                      Llama-2 7B                    7/400                 21/400                  200%
              238                                                                                                                                                  Mistral 7B                   15/400                 23/400                    53%
              239           Since AugARCresults are directly comparable to ARC, we                                                                                 Llama-2 13B                   8/400                 18/400                  125%
              240           proceed to use AugARC for the remainder of our experi-                                                                                 Llama-3 8B                   21/400                 34/400                    62%
              241           ments.
              242                                                                                                                                     Table 3. Results of base and fine-tuned LLMs on the ARC evalua-
              243           5.2. ARC accuracy across LLMs                                                                                              tion set. The increase column shows the improvement in accuracy
              244                                                                                                                                      from a base LLM compared to its fine-tuned version. All LLMs
              245                                                                                                                                      consistently show improved ARC performance after fine-tuning,
              246                                                                                                                                      ranging from 62% to 200%.
                                         80
              247                        70                                                                                                            Theresults in Table 3 demonstrate a significant increase in
              248                        60                                                                                                            ARCperformance across all fine-tuned LLMs compared
              249                      asks50                                                                                                          to their base versions. The accuracy improvement after
              250                      C Eval T40                                                                                                      training varies between 53% in Mistral 7B up to 200% in
              251                        30                                                                                                            Llama-2 7B. While Llama-2 7B and 13B both achieve more
              252                      AugAR20                                                                                                         than 100% improvement - 125% and 200% respectively,
              253                        10                                                                                                            Mistral 7B and Llama-3 8B improved in the range of 50%
              254                         0
                                                                                         o
              255                                                                       r                                                              to 65%.
                                                 Mistral 7B                     Gemini P
                                        Llama-2 7B                    Mixtral 8x7B     Llama-3 8B
                                                       Llama-2 13BLlama-2 70B                                Llama-3 70B
                                                                                                     dLM-2-8x22B
              256                                                                           Mixtral-8x22B          Claude 3 Opus
                                                                                                 izar                                                  Our results suggest that training small LLMs on the Au-
              257                                                                              W
                                                                                                                                                       gARCdatasetconsistently improves their performance. In
              258          Figure 4. ARC evaluation tasks solved by LLMs. Claude 3 Opus                                                                particular, fine-tuning smaller LLMs (7-13B parameters) is
              259           solves the most ARC tasks (74).                                                                                            so effective that it can lead to better ARC performance than
              260                                                                                                                                      significantly larger base LLMs.
              261
              262                                                                                                                                      5.4. Solution Overlap and Gain Measure
              263          The ARC accuracy of LLMs ranges between 7 up to 74
              264           solvedtasks, asvisualisedinFigure4. Thebestperformance                                                                     Tomotivate our reflection approach to ARC, we show the
              265           of a small model is achieved by Llama-3 8B (21). Some                                                                      benefit of combining ARC solutions from base and fine-
              266           bigger open-source LLMs can solve more than 30 ARC                                                                         tuned LLMs together with Program Synthesis solvers.
              267           tasks, with Llama-3 70B achieving 36. The highest number                                                                   Theratio of overlapping solutions between different ARC
              268           of solved ARC task, 74, is by Claude 3 Opus.                                                                               solvers is visualised in Figure 5a. The numbers in Figure
              269          The ARCresults demonstrated some variability in perfor-                                                                     5a refer to the proportion of overlapping tasks solved by
              270           mance across LLMs. Bigger models appear to be more                                                                         the systems on the left and on the bottom. For each pair
              271           accurate on ARC compared to smaller ones. Most LLMs                                                                        of LLMs, there is an overlap between 0.5 and 0.9 in their
              272           achieve an accuracy in the range of 10-35 tasks, with the                                                                  correct ARC solutions. A lower overlap can be observed
              273           only exception being Claude 3 Opus with 74.                                                                                between the base LLMs and the fine-tuned ones. For ex-
              274
                                                                                                                                                 5
