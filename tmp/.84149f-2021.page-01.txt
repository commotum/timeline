                                                  ViViT: A Video Vision Transformer
                                 ∗                           ∗                                                      ˇ ´†                        †
               Anurag Arnab          Mostafa Dehghani           Georg Heigold          ChenSun         Mario Lucic         Cordelia Schmid
                                                                     Google Research
                                  {aarnab, dehghani, heigold, chensun, lucic, cordelias}@google.com
                                       Abstract                                   that a pure-transformer based architecture has outperformed
                                                                                  its convolutionalcounterpartsinimageclassiﬁcation. Doso-
                 We present pure-transformer based models for video               vitskiy et al. [17] closely followed the original transformer
              classiﬁcation, drawingupontherecentsuccessofsuchmod-                architecture of [67], and noticed that its main beneﬁts
              els in image classiﬁcation.     Our model extracts spatio-          were observed at large scale – as transformers lack some
              temporal tokens from the input video, which are then en-            of the inductive biases of convolutions (such as transla-
              coded by a series of transformer layers. In order to han-           tional equivariance), they seem to require more data [17]
              dle the long sequences of tokens encountered in video, we           or stronger regularisation [63].
              propose several, efﬁcient variants of our model which fac-             Inspired by ViT, and the fact that attention-based ar-
              torise the spatial- and temporal-dimensionsoftheinput. Al-          chitectures are an intuitive choice for modelling long-
              though transformer-based models are known to only be ef-            range contextual relationships in video, we develop sev-
              fective when large training datasets are available, we show         eral transformer-based models for video classiﬁcation. Cur-
              howwecaneffectivelyregularise the model during training             rently, the most performant models are based on deep 3D
              andleveragepretrainedimagemodelstobeabletotrainon                   convolutional architectures [8, 19, 20] which were a natu-
              comparatively small datasets. We conduct thorough abla-             ral extension of image classiﬁcation CNNs [26, 59]. Re-
              tion studies, and achieve state-of-the-art results on multiple      cently, these models were augmented by incorporating self-
              video classiﬁcation benchmarks including Kinetics 400 and           attention into their later layers to better capture long-range
              600, Epic Kitchens, Something-Something v2 and Moments              dependencies [74, 22, 78, 1].
              in Time, outperforming prior methods based on deep 3D
              convolutional networks.                                                As shown in Fig. 1, we propose pure-transformer mod-
                                                                                  els for video classiﬁcation. The main operation performed
              1. Introduction                                                     in this architecture is self-attention, and it is computed on
                                                                                  a sequence of spatio-temporal tokens that we extract from
                 Approaches based on deep convolutional neural net-               the input video. To effectively process the large number of
              works have advanced the state-of-the-art across many stan-          spatio-temporal tokens that may be encountered in video,
              dard datasets for vision problems since AlexNet [37]. At            wepresent several methods of factorising our model along
              the same time, the most prominent architecture of choice in         spatial and temporal dimensions to increase efﬁciency and
              sequence-to-sequence modelling (e.g. in natural language            scalability. Furthermore, to train our model effectively on
              processing) is the transformer [67], which does not use con-        smaller datasets, we show how to reguliarise our model dur-
              volutions, but is based on multi-headed self-attention. This        ing training and leverage pretrained image models.
              operation is particularly effective at modelling long-range            We also note that convolutional models have been de-
              dependencies and allows the model to attend over all ele-           veloped by the community for several years, and there are
              ments in the input sequence. This is in stark contrast to           thus many “best practices” associated with such models.
              convolutions where the corresponding “receptive ﬁeld” is            As pure-transformer models present different characteris-
              limited, and grows linearly with the depth of the network.          tics, we need to determine the best design choices for such
                 The success of attention-based models in NLP has re-             architectures. We conduct a thorough ablation analysis of
              cently inspired approaches in computer vision to integrate          tokenisation strategies, model architecture and regularisa-
              transformers into CNNs [74, 7], as well as some attempts to         tion methods. Informed by this analysis, we achieve state-
              replace convolutions completely [48, 3, 52]. However, it is         of-the-art results on multiple standard video classiﬁcation
              only very recently with the Vision Transformer (ViT) [17],          benchmarks, including Kinetics 400 and 600 [34], Epic
                ∗Equal contribution                                               Kitchens 100 [13], Something-Something v2 [25] and Mo-
                †Equal advising                                                   ments in Time [44].
                                                                               6836
