                              Setting           CoQA DROP QuAC SQuADv2 RACE-h RACE-m
                                                    a       b       c       d           e        e
                              Fine-tuned SOTA   90.7    89.1    74.4    93.0        90.0      93.1
                              GPT-3Zero-Shot    81.5    23.6    41.5    59.5        45.5      58.4
                              GPT-3One-Shot     84.0    34.3    43.3    65.4        45.9      57.4
                              GPT-3Few-Shot     85.0    36.5    44.3    69.8        46.8      58.1
                Table 3.7: Results on reading comprehension tasks. All scores are F1 except results for RACE which report accuracy.
                a    +    b      c      d        e    +
                 [JZC 19] [JN20] [AI19] [QIA20] [SPP 19]
                ﬁne-tuned RoBERTa. PIQA shows relatively shallow scaling with model size and is still over 10% worse than human
                performance, but GPT-3’s few-shot and even zero-shot result outperform the current state-of-the-art. Our analysis
                ﬂagged PIQAforapotential data contamination issue (despite hidden test labels), and we therefore conservatively mark
                the result with an asterisk. See Section 4 for details.
                          +
                ARC[CCE 18] is a dataset of multiple-choice questions collected from 3rd to 9th grade science exams. On the
                “Challenge” version of the dataset which has been ﬁltered to questions which simple statistical or information retrieval
                methods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot
                setting, and 51.5% in the few-shot setting. This is approaching the performance of a ﬁne-tuned RoBERTa baseline
                (55.9%) from UniﬁedQA [KKS+20]. On the “Easy” version of the dataset (questions which either of the mentioned
                baseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a ﬁne-tuned
                                           +
                RoBERTa baseline from [KKS 20]. However, both of these results are still much worse than the overall SOTAs
                achieved by the UniﬁedQA which exceeds GPT-3’s few-shot results by 27% on the challenge set and 22% on the easy
                set.
                OnOpenBookQA[MCKS18],GPT-3improvessigniﬁcantlyfromzerotofewshotsettingsbutisstillover20points
                short of the overall SOTA. GPT-3’s few-shot performance is similar to a ﬁne-tuned BERT Large baseline on the
                leaderboard.
                Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and
                inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a signiﬁcant
                improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings.
                3.6  ReadingComprehension
                Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive,
                multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread
                in GPT-3’s performance across these datasets suggestive of varying capability with different answer formats. In general
                weobserve GPT-3 is on par with initial baselines and early results trained using contextual representations on each
                respective dataset.
                GPT-3performsbest(within 3 points of the human baseline) on CoQA [RCM19] a free-form conversational dataset
                and performs worst (13 F1 below an ELMo baseline) on QuAC [CHI+18] a dataset which requires modeling structured
                                                                                          +
                dialog acts and answer span selections of teacher-student interactions. On DROP [DWD 19], a dataset testing discrete
                reasoningandnumeracyinthecontextofreadingcomprehension,GPT-3inafew-shotsettingoutperformstheﬁne-tuned
                BERTbaseline from the original paper but is still well below both human performance and state-of-the-art approaches
                which augment neural networks with symbolic systems [RLL+19]. On SQuAD 2.0 [RJL18], GPT-3 demonstrates its
                few-shot learning capabilities, improving by almost 10 F1 (to 69.8) compared to a zero-shot setting. This allows it to
                                                                                       +
                slightly outperform the best ﬁne-tuned result in the original paper. On RACE [LXL 17], a multiple choice dataset of
                middle school and high school english examinations, GPT-3 performs relatively weakly and is only competitive with
                the earliest work utilizing contextual representations and is still 45% behind SOTA.
                3.7  SuperGLUE
                In order to better aggregate results on NLP tasks and compare to popular models such as BERT and RoBERTa in a
                more systematic way, we also evaluate GPT-3 on a standardized collection of datasets, the SuperGLUE benchmark
                     +          +         +                            +        +                    +
                [WPN 19] [WPN 19] [CLC 19] [DMST19] [RBG11] [KCR 18] [ZLL 18] [DGM06] [BHDD 06] [GMDD07]
                     +                 +
                [BDD 09][PCC18][PHR 18]. GPT-3’stest-set performance on the SuperGLUE dataset is shown in Table 3.8. In the
                few-shot setting, we used 32 examples for all tasks, sampled randomly from the training set. For all tasks except WSC
                                                                 18
