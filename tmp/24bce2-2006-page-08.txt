                             1534                                                                       G.Hinton,S.Osindero,andY.-W.Teh
                                                               t = 0                 t = 1                  t = 2                       t = infinity
                                                                   j                       j                      j                             j
                                            <v0h0>                                                                                         <v∞h∞>
                                                   i     j                                                                                        i     j
                                                       i                      i                      i                              i
                                                                                                                                 t = infinity
                             Figure 4: This depicts a Markov chain that uses alternating Gibbs sampling.
                             In one full step of Gibbs sampling, the hidden units in the top layer are all
                             updatedinparallelbyapplyingequation2.1totheinputsreceivedfromthethe
                             current states of the visible units in the bottom layer; then the visible units are
                             all updated in parallel given the current hidden states. The chain is initialized
                             by setting the binary states of the visible units to be the same as a data vector.
                             The correlations in the activities of a visible and a hidden unit are measured
                             after the ﬁrst update of the hidden units and again at the end of the chain. The
                             difference of these two correlations provides the learning signal for updating
                             the weight on the connection.
                             This learning rule is the same as the maximum likelihood learning rule
                             for the inﬁnite logistic belief net with tied weights, and each step of Gibbs
                             sampling corresponds to computing the exact posterior distribution in a
                             layer of the inﬁnite logistic belief net.
                                   Maximizingthelogprobabilityofthedataisexactlythesameasminimiz-
                             ingtheKullback-Leiblerdivergence, KL(P0||P∞),betweenthedistribution
                                                                                                                       θ
                             of the data, P0, and the equilibrium distribution deﬁned by the model, P∞.
                                                                                                                                                                         θ
                             Incontrastivedivergencelearning(Hinton,2002),weruntheMarkovchain
                             foronlynfullstepsbeforemeasuringthesecondcorrelation.3 Thisisequiv-
                             alent to ignoring the derivatives that come from the higher layers of the
                             inﬁnite net. The sum of all these ignored derivatives is the derivative of the
                             log probability of the posterior distribution in layer V , which is also the
                                                                                                                                        n
                             derivative of the Kullback-Leibler divergence between the posterior dis-
                             tribution in layer V , Pn, and the equilibrium distribution deﬁned by the
                                                                   n      θ
                             model. So contrastive divergence learning minimizes the difference of two
                             Kullback-Leibler divergences:
                                                0 ∞                        n ∞
                                        KLP P                    −KLP P .                                                                                          (3.2)
                                                           θ                      θ      θ
                                   Ignoringsamplingnoise,thisdifferenceisnevernegativebecauseGibbs
                             sampling is used to produce Pn from P0, and Gibbs sampling always re-
                                                                                          θ
                             ducestheKullback-Leiblerdivergencewiththeequilibriumdistribution.It
                                   3 Each full step consists of updating h given v,thenupdatingv given h.
