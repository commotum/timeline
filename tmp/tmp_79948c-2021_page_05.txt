                                           (a) ImageNet-1K                                    (b) JFT
                          Figure 1: Comparison for model generalization and capacity under different data size. For fair
                          comparison, all models have similar parameter size and computational cost.
                            Particularly, VITREL is signiﬁcantly worse than variants by a large margin, which we conjecture
                            is related to the lack of proper low-level information processing in its aggressive down-sampling
                            Stem. Among the multi-stage variants, the overall trend is that the more convolution stages the
                            model has, the smaller the generalization gap is.
                          • As for model capacity, from the JFT comparison, both the train and evaluation metrics at the end
                            of the training suggest the following ranking:
                                              C-C-T-T⇡ C-T-T-T> VITREL > C-C-C-T> C-C-C-C.
                            Importantly, this suggests that simply having more Transformer blocks does NOT necessarily
                            meanhigher capacity for visual processing. On one hand, while initially worse, VITREL ultimately
                            catch up with the two variants with more MBConv stages, indicating the capacity advantage of
                            Transformer blocks. On the other hand, both C-C-T-Tand C-T-T-Tclearly outperforming VITREL
                            suggest that the ViT stem with an aggressive stride may have lost too much information and hence
                            limit the model capacity. More interestingly, the fact that C-C-T-T ⇡ C-T-T-T indicates the for
                            processing low-level information, static local operations like convolution could be as capable as
                            adaptive global attention mechanism, while saving computation and memory usage substantially.
                          Finally, to decide between C-C-T-T and C-T-T-T, we conduct another transferability test3 — we
                          ﬁnetune the two JFT pre-trained models above on ImageNet-1K for 30 epochs and compare their
                          transfer performances. From Table 2, it turns out that C-C-T-T achieves a clearly better transfer
                          accuracy than C-T-T-T, despite the same pre-training performance.
                                                         Table 2: Transferability test results.
                                                          Metric               C-C-T-T     C-T-T-T
                                              Pre-training Precision@1 (JFT)     34.40       34.36
                                                Transfer Accuracy 224x224        82.39       81.78
                                                Transfer Accuracy 384x384        84.23       84.02
                          Taking generalization, model capacity, transferability and efﬁciency into consideration, we adapt the
                          C-C-T-T multi-stage layout for CoAtNet. More model details are included in Appendix A.1.
                          3    Related Work
                          Convolutionalnetworkbuildingblocks.       Convolutional Networks (ConvNets) have been the dom-
                          inating neural architectures for many computer vision tasks. Traditionally, regular convolutions, such
                          as ResNet blocks [3], are popular in large-scale ConvNets; in contrast, depthwise convolutions [28]
                          are popular in mobile platforms due to its lower computational cost and smaller parameter size [27].
                          Recent works show that an improved inverted residual bottlenecks (MBConv [27, 35]), which is
                          built upon depthwise convolutions, can achieve both high accuracy and better efﬁciency [5, 19]. As
                          discussed in Section 2, due to the strong connection between MBConv and Transformer blocks , this
                          paper mostly employs MBConv as convolution building blocks.
                             3Rigorously speaking, this test examines not only the transferability but also the generalization.
                                                                         5
