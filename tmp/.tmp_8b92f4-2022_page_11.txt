              Under review as a conference paper at ICLR 2025
          540 to positional encoding can significantly improve performance on ARC tasks. Furthermore, we show
          541 that incorporating object indices as additional positional information via OPEs provides a meaning-
          542 ful improvement in handling complex spatial relationships in ARC tasks.
          543 Additionally, we introduced 2D padding and border tokens to handle variable-sized images requir-
          544 ing high precision in visual reasoning. Given ARC’s pixel-level precision and abstract reasoning
          545 requirements (e.g., 1x1 pixel tasks in ARC, but potentially n x n pixels in more generalized visual
          546 reasoning), resizing or cropping—commonly used in standard vision tasks—is infeasible. VITARC
          547 reveals limitations in current ViT structures under these conditions and suggests necessary adapta-
          548 tions for such tasks.
          549 Moreover,webelievethatourinsightsintotheimportanceofpositionalencodingsforvisualreason-
          550 ing tasks have implications beyond ARC, particularly for applications such as physical reasoning in
          551 vision generation tasks. In these contexts, accurate spatial relationships are equally critical, and our
          552 findings provide a foundation for further exploration of how Vision Transformers can be adapted to
          553 meet these challenges.
          554 It is important to note that VITARC solves task-specific instances of ARC in a data-driven approach,
          555 treating each ARC task independently. This method does not fully solve ARC, which requires the
          556 ability to generalize across different tasks—a challenge that remains open for future research. How-
          557 ever, since the current state-of-the-art (SOTA) in ARC relies on LLM-based transduction models
          558 that handle tasks through supervised input-output transformations (arcprize, 2024), integrating the
          559 2Dinductive bias from ViTARC could provide an orthogonal benefit. This is especially relevant as
          560 prior studies indicate that the sequential nature of 1D methods in LLMs can limit ARC performance;
          561 for example, because the input grid is processed in raster order, LLMs experience a significant drop
          562 in success rates when horizontal movement/filling tasks are rotated 90 degrees (Xu et al., 2024).
          563 In summary, this work highlights the importance of 2D positional information and object-based en-
          564 codingsinabstractvisualreasoningthatleadstoournovelcontributionofthe VITARCarchitecture.
          565 VITARC advances the application of Vision Transformers for pixel-level reasoning and suggests
          566 further avenues for improving generalization capabilities in models tackling visual reasoning tasks.
          567
          568
          569
          570
          571
          572
          573
          574
          575
          576
          577
          578
          579
          580
          581
          582
          583
          584
          585
          586
          587
          588
          589
          590
          591
          592
          593
                                      11
