                  6.1  Misuse of Language Models
                  Malicious uses of language models can be somewhat difﬁcult to anticipate because they often involve repurposing
                  language models in a very different environment or for a different purpose than researchers intended. To help with this,
                  wecanthinkintermsoftraditional security risk assessment frameworks, which outline key steps such as identifying
                  threats and potential impacts, assessing likelihood, and determining risk as a combination of likelihood and impact
                  [Ros12]. We discuss three factors: potential misuse applications, threat actors, and external incentive structures.
                  6.1.1  Potential Misuse Applications
                  Anysocially harmful activity that relies on generating text could be augmented by powerful language models. Examples
                  include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing
                  and social engineering pretexting. Many of these applications bottleneck on human beings to write sufﬁciently high
                  quality text. Language models that produce high quality text generation could lower existing barriers to carrying out
                  these activities and increase their efﬁcacy.
                  Themisusepotential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to
                  generate several paragraphs of synthetic content that people ﬁnd difﬁcult to distinguish from human-written text in
                  3.9.4 represents a concerning milestone in this regard.
                  6.1.2  Threat Actor Analysis
                  Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors
                  whomaybeabletobuildamaliciousproductto‘advancedpersistent threats’ (APTs): highly skilled and well-resourced
                                                                           +
                  (e.g. state-sponsored) groups with long-term agendas [SBC 19].
                  Tounderstand how low and mid-skill actors think about language models, we have been monitoring forums and chat
                  groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did
                  ﬁndsigniﬁcant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances
                  of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated
                  with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is
                  not immediate, but signiﬁcant improvements in reliability could change this.
                  Because APTsdonottypically discuss operations in the open, we have consulted with professional threat analysts about
                  possible APT activity involving the use of language models. Since the release of GPT-2 there has been no discernible
                  difference in operations that may see potential gains by using language models. The assessment was that language
                  models may not be worth investing signiﬁcant resources in because there has been no convincing demonstration that
                  current language models are signiﬁcantly better than current methods for generating text, and because methods for
                  “targeting” or “controlling” the content of language models are still at a very early stage.
                  6.1.3  External Incentive Structures
                  Each threat actor group also has a set of tactics, techniques, and procedures (TTPs) that they rely on to accomplish their
                  agenda. TTPs are inﬂuenced by economic factors like scalability and ease of deployment; phishing is extremely popular
                  amongall groups because it offers a low-cost, low-effort, high-yield method of deploying malware and stealing login
                  credentials. Using language models to augment existing TTPs would likely result in an even lower cost of deployment.
                  Ease of use is another signiﬁcant incentive. Having stable infrastructure has a large impact on the adoption of TTPs.
                  Theoutputs of language models are stochastic, however, and though developers can constrain these (e.g. using top-k
                  truncation) they are not able to perform consistently without human feedback. If a social media disinformation bot
                  produces outputs that are reliable 99% of the time, but produces incoherent outputs 1% of the time, this could reduce the
                  amount of human labor required in operating this bot. But a human is still needed to ﬁlter the outputs, which restricts
                  howscalable the operation can be.
                  Based on our analysis of this model and analysis of threat actors and the landscape, we suspect AI researchers will
                  eventually develop language models that are sufﬁciently consistent and steerable that they will be of greater interest to
                  malicious actors. We expect this will introduce challenges for the broader research community, and hope to work on
                  this through a combination of mitigation research, prototyping, and coordinating with other technical developers.
                                                                         35
