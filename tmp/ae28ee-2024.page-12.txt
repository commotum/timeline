                         Published as a conference paper at ICLR 2024
                         Maurice H. Halstead. Elements of Software Science (Operating and programming systems series).
                           Elsevier Science Inc., USA, 1977. ISBN 0444002057.
                         DanHendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin
                           Burns,SamirPuranik,HoraceHe,DawnSong,andJacobSteinhardt. Measuringcodingchallenge
                           competence with apps, 2021.
                         Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John
                           Grundy, and Haoyu Wang. Large language models for software engineering: A systematic litera-
                           ture review, 2023.
                         Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
                           and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Con-
                           ference on Learning Representations, 2022. URL https://openreview.net/forum?
                           id=nZeVKeeFYf9.
                         Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam Ra-
                           jbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of
                           extreme long sequence transformer models, 2023.
                         Jiajun Jiang, Yingfei Xiong, Hongyu Zhang, Qing Gao, and Xiangqun Chen. Shaping program
                           repair space with existing patches and similar code. In Proceedings of the 27th ACM SIGSOFT
                           international symposium on software testing and analysis, pp. 298–309, 2018.
                         Tae-HwanJung.Commitbert: Commitmessagegenerationusingpre-trainedprogramminglanguage
                           model, 2021.
                             ´
                         ReneJust, Darioush Jalali, and Michael D. Ernst. Defects4J: A Database of existing faults to enable
                           controlledtestingstudiesforJavaprograms. InISSTA2014,Proceedingsofthe2014International
                           SymposiumonSoftwareTesting and Analysis, pp. 437–440, San Jose, CA, USA, July 2014. Tool
                           demo.
                         SungminKang,JuyeonYoon,andShinYoo. Largelanguagemodelsarefew-shottesters: Exploring
                           llm-based general bug reproduction, 2023.
                         Rafael-Michael Karampatsis and Charles Sutton.  How often do single-statement bugs occur?
                           the manysstubs4j dataset. 2020 IEEE/ACM 17th International Conference on Mining Software
                           Repositories (MSR), pp. 573–577, 2019. URL https://api.semanticscholar.org/
                           CorpusID:173188438.
                         Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, and Zhengxuan Wu et.
                           al. Dynabench: Rethinking benchmarking in nlp, 2021.
                         YunhoKim,SeokhyeonMun,ShinYoo,andMoonzooKim. Preciselearn-to-rankfaultlocalization
                           usingdynamicandstaticfeaturesoftargetprograms. ACMTransactionsonSoftwareEngineering
                           andMethodology(TOSEM),28(4):1–34, 2019.
                         Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen
                           tau Yih, Daniel Fried, Sida Wang, and Tao Yu. Ds-1000: A natural and reliable benchmark for
                           data science code generation, 2022.
                                                                                                       ´
                         Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, and Re mi Leblond
                           et. al. Competition-level code generation with AlphaCode. Science, 378(6624):1092–1097, dec
                           2022a. doi: 10.1126/science.abq1158. URL https://doi.org/10.1126%2Fscience.
                           abq1158.
                         Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared
                           Green, Alexey Svyatkovskiy, Shengyu Fu, and Neel Sundaresan. Automating code review activi-
                           ties by large-scale pre-training, 2022b.
                         Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, and Michihiro Yasunaga
                           et. al. Holistic evaluation of language models, 2022.
                                                                     12
