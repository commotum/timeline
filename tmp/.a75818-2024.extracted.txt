

===== Page 1 =====
                                                LengthExtrapolation of Transformers:
                                    ASurveyfromthePerspectiveofPositionalEncoding
                                                 1                      2                       1,3*                       1
                                 LiangZhao ,XiachongFeng ,XiaochengFeng                             , Weihong Zhong ,
                                                      4                4                   4              1,3             1
                                   Dongliang Xu , Qing Yang , Hongtao Liu , Bing Qin , Ting Liu
                                      1Harbin Institute of Technology 2The University of Hong Kong
                                         3Peng Cheng Laboratory 4Du Xiaoman Financial (Beijing)
                             {lzhao, xcfeng, whzhong, qinb, tliu}@ir.hit.edu.cn fengxc@hku.hk
                                       {xudongliang, yangqing, liuhongtao01}@duxiaoman.com
                                          Abstract                               Transformer architecture (Vaswani et al., 2017),
                        Built upon the Transformer, large language               though Transformer-based large language models
                        models (LLMs) have captured worldwide at-                (LLMs) (Touvron et al., 2023a; OpenAI, 2023)
                        tention due to their remarkable abilities. Never-        have drastically advanced the NLP field.
                        theless, all Transformer-based models includ-               Transformer-based models are trained on se-
                        ing LLMs suffer from a preset length limit and           quences with a maximum length (Raffel et al.,
                        can hardly generalize from short training se-            2020; Zhang et al., 2020; Brown et al., 2020), as a
                        quences to longer inference ones, namely, they           result of the quadratic memory and computational
                        cannot perform length extrapolation to handle            complexity with regard to input length. To make
                        long sequences, which severely hinders their             matters worse, some research reveals that Trans-
                        application in scenarios demanding long input            formers might have gained their performance from
                        sequences such as legal or scientific documents.         surface-level memorization instead of abstract, gen-
                        Thus, numerous methods have emerged to en-               eralizable skills (Razeghi et al., 2022; Wu et al.,
                        hance the length extrapolation of Transformers.
                        Despite the great research efforts, a systematic         2024), which means they can hardly break through
                        survey is still lacking. To fill this gap, we delve      the maximumtraining length and perform poorly
                        into these advances in a unified notation from           on sequences with length beyond it (Dai et al.,
                        the perspective of positional encoding (PE), as          2019; Neishi and Yoshinaga, 2019), i.e., they can-
                        it has been considered the primary factor on             not perform length extrapolation (Mitchell et al.,
                        length extrapolation. Specifically, we begin             2018; Press et al., 2021). To offer a more compre-
                        with extrapolatable PEs that have dominated              hensive understanding of the challenges in length
                        this research field. Then, we dive into extrap-
                        olation methods based on them, covering po-              extrapolation, we present comparison results of
                        sition interpolation and randomized position             three state-of-the-art models with different context
                        methods. Finally, several challenges and future          sizes on several generation tasks in Appendix A.1.
                        directions in this area are highlighted. Through            The length limit together with poor length ex-
                        this survey, we aim to enable the reader to gain         trapolation prevents LLMs from handling long
                        a deep understanding of existing methods and             sequences, such as DNA and protein sequences
                        provide stimuli for future research.                     (Abramson et al., 2024), high-resolution images
                   1 Introduction                                                (Liu et al., 2023a), and even videos (Lin et al.,
                   It has been suggested that with limited learning re-          2023). Moreover, existing approaches for harness-
                   sources, humans can potentially comprehend utter-             ing the full potential of LLMs also demand a larger
                   ances of infinite length by understanding their com-          context window, to incorporate elaborate prompts
                   ponents and structures (Chomsky, 1957; MON-                   (Liu et al., 2023c), sufficient in-context demonstra-
                   TAGUE, 1970). In natural language processing                  tions (Brown et al., 2020) and long-term mem-
                   (NLP), given the limited training data (Kazem-                ory of agents (Park et al., 2023). Hence, there is
                   nejad et al., 2023) and compute, models cannot                a growing body of research trying to strengthen
                   learn from large-scale long sequences and thus are            length extrapolation of LLMs (Press et al., 2021;
                   also expected to possess such generalization ability          Ontanon et al., 2022; Anil et al., 2022; Chi et al.,
                   to process long sequences (Shaham et al., 2023).              2023b; Sun et al., 2023), mostly from the perspec-
                   However, it is a challenging task for the de facto            tive of positional encoding (PE).
                                                                                    Despite the prosperity in this area, a systematic
                       *Corresponding Author                                     survey is still lacking. We aim to fill this blank
                                                                            9959
                                   Findings of the Association for Computational Linguistics: EMNLP 2024, pages 9959–9977
                                            November12-16,2024©2024AssociationforComputational Linguistics


===== Page 2 =====
                                                                                                                        Integrating                             SHAPE (Kiyono et al., 2021); CAPE (Likhomanenko et al., 2021)
                                                                        APEs (Vaswani et al., 2017) §3.1          Shift Invariance §3.1.1
                                                                                                               Enhancing Smoothness §3.1.2                          Complex (Wang et al., 2019); FLOATER (Liu et al., 2020)
                                            Extrapolatable PEs §3                                                   RoPE Family §3.2.1                                    RoPE (Su et al., 2024); xPos (Sun et al., 2023)
                                                                          RPEs (Shaw et al., 2018) §3.2                                                         T5-Bias (Raffel et al., 2020); TISA (Wennberg and Henter, 2021);
                                                                                                                   T5-bias Family §3.2.2                              ALiBi (Press et al., 2021); KERPLE (Chi et al., 2022);
                                      Extrapolation                                                                                                      Sandwich (Chi et al., 2023b); FIRE (Li et al., 2023b); CAPE (Zheng et al., 2024)
                                                                           Position Interpolation §4.1                        Linear Positional Interpolation (Chen et al., 2023b); NTK-Aware Interpolation (bloc97, 2023b);
                                                                                                               Dynamic-NTK Interpolation (emozilla, 2023); NTK-by-parts Interpolation (bloc97, 2023a); Truncated Basis (Pal et al., 2023)
                                      LengthPE-based Methods §4
                                                                              Randomized PE §4.2                                                Randomized PE (Ruoss et al., 2023); PoSE (Zhu et al., 2023)
                                                                                Figure 1: Taxonomy for length extrapolation of Transformers.
                                                                                                                                                                            1        √
                                 by investigating existing approaches that enable                                                           a scaling factor 1/ d (Equation 1). Then, the
                                 and enhance length extrapolation of Transform-                                                             row-wise softmax function converts compatibility
                                 ers. Specifically, a brief formal introduction to                                                          scores into weights, and the weighted sum of the
                                 Transformer is given in §2 as a solid foundation for                                                       values is the output of the attention layer (Equa-
                                 further discussion. Then, we comprehensively sum-                                                          tion 2).            The fully connected feed-forward net-
                                 marize extrapolatable PEs proposed from the birth                                                          workconsists of two linear transformations with a
                                 of Transformer to the prevalence of LLMs in §3.                                                            ReLUactivationbetween(Equation4),withparam-
                                                                                                                                                             (f )               d×d               (f )              d ×d          f(1)
                                 Note that we focus exclusively on PEs proposed                                                             eters W 1 ∈ R                              f,W 2 ∈ R f ,b                                       ∈
                                                                                                                                                df       (f2)             d
                                 for better extrapolation and omit others, since there                                                      R ,b                 ∈ R , where df is the intermediate di-
                                 is already an insightful survey on PEs of Trans-                                                           mension. Besides, residual connection (He et al.,
                                 former (Dufter et al., 2022). Based on these PEs,                                                          2016) and layer normalization (Ba et al., 2016) are
                                 many novel methods emerge in the era of LLMs                                                               leveraged (Equation 3 and 5) to enhance scalability.
                                 to further enhance extrapolation, which we inten-                                                               Note that in the above descriptions, we have not
                                 tionally centralize in §4, covering popular position                                                       imposed any limit on input length n, which means
                                 interpolation methods and randomized methods.                                                              the Transformer is naturally equipped with a notion
                                 These advancements demonstrate the vibrancy and                                                            of length extrapolation. Theoretically, a fixed set-
                                 vastness of this area, from which we distill future                                                        ting of Transformer weights defines a sequence-to-
                                 directions and insights, represented in §5 and §6.                                                         sequence function on sequences of arbitrary length
                                                                                                                                            (Yun et al., 2019). If the function applies the cor-
                                 2 Preliminary                                                                                              rect transformation for inputs of any length, it is
                                                                                                                                            expected to length extrapolate (Zhou et al., 2023).
                                 In this section, we follow Dufter et al. (2022) to                                                              However, we have to break this nature by in-
                                 present a formal description of the encoder layer                                                          tegrating PE with Transformers to inject position
                                 of the Transformer, as the decoder layer is almost                                                         information into them. Otherwise, they are per-
                                 the same except for the cross-attention mechanism.                                                                                                                                            2
                                                                                            n×d                                             mutation equivalent or order invariant . Thus,
                                 Given an input matrix X ∈ R                                        as a sequence of                        PEs are central to length extrapolation and form
                                 nembeddingswithdimensiond,anencoderlayer                                                                   the core focus of this survey.
                                           n×d                n×d
                                 f : R              →− R               with f(X) = Z is defined by:                                         3 Extrapolatable Positional Encodings
                                               QKT
                                   C= √d                                                                                       (1)          Sinusoidal position embeddings are proposed with
                                   A=Softmax(C)V                                                                               (2)          Transformer as it may help extrapolate to longer
                                                                                                                                            sequences beyond training (Vaswani et al., 2017).
                                   O=LayerNorm (A+X)                                                                           (3)          The idea behind this claim, that length extrapola-
                                                                      1
                                                                       (f )            (f )           (f )           (f )
                                    F =ReLU(OW 1 +b 1 )W 2 +b 2 (4)                                                                         tion can be enabled by simply changing PE, has
                                    Z=LayerNorm (O+F)                                                                          (5)          been widely supported and demonstrated (Neishi
                                                                      2                                                                     and Yoshinaga, 2019; Press et al., 2021; Ruoss
                                 where Q = XW ,K =XW ,V =XW are                                                                             et al., 2023). Hence, developing better PEs has
                                                                    q                           k                          v
                                 queries, keys and values, with Wq,Wk,Wv ∈                                                                        1Wewill omit this scaling factor in the following for sim-
                                    d×d                                                                                                     plicity and clarity.
                                 R           being the projection matrices.                                                                       2Notethatsomeexistingresearchsuggestscausallanguage
                                      Firstly, the compatibility scores C are computed                                                      models can learn position information without PE (Tsai et al.,
                                 as the dot product between queries and keys with                                                           2019; Haviv et al., 2022; Chi et al., 2023a).
                                                                                                                                    9960


===== Page 3 =====
                               PE                            Manifestation Learnable Integration Injection       is fed into Transformer, so the compatibility score
                                                                                                   Layer         between query q and key k can be formalized as
                               Sinusoidal(Vaswanietal., 2017) Embedding        ✕         Add       Initial                                  i                 j
                               with Shift Invariance                                                                       T                                                          T
                               SHAPE(Kiyonoetal.,2021)        Embedding        ✕         Add       Initial           q k =((x +p)W )((x +p )W ) . (7)
                                                                                                                       i   j            i        i       q        j        j       k
                             APECAPE(Likhomanenkoetal.,2021)  Embedding        ✕         Add       Initial
                               with Smoothness                                                                   This equation is the basis of many other PEs.
                               Complex(Wangetal.,2020)        Embedding        ✓       Multiply    Initial           However, researchers subsequently found that
                               FLOATER(Liuetal.,2020)         Embedding        ✓         Add       Initial
                               Shawetal. (2018)               Embedding        ✓         Add       Every         sinusoidal APE is hard to extrapolate (Dai et al.,
                               T5Family                                                                          2019; Neishi and Yoshinaga, 2019). Hence, a wide
                               T5Bias(Raffeletal., 2020)          Bias         ✓         Add       Every
                               ALiBi(Pressetal., 2021)            Bias         ✕         Add       Every         variety of APEs have been proposed to enhance
                               KERPLE(Chietal.,2022)              Bias         ✓         Add       Every
                             RPESANDWICH(Chietal.,2023b)      Embedding        ✕         Add       Every         sinusoidal APE and extrapolation of Transformers
                               FIRE(Lietal.,2023b)                Bias         ✓         Add       Every         from different perspectives, either trying to inte-
                               CAPE(Zhengetal.,2024)              Bias         ✓         Add       Every
                               RoPEFamily                                                                        grate shift invariance in sinusoidal APE (§3.1.1) or
                               RoPE(Suetal.,2024)             Embedding        ✕       Multiply    Every         aiming to generate position embeddings varying
                               xPOS(Sunetal.,2023)            Embedding        ✕       Multiply    Every
                                                                                                                 smoothly with position indices (§3.1.2).
                          Table 1: A list of extrapolatable PEs. Bolded methods
                           are proposed or widely adopted for LLMs. Manifesta-                                   3.1.1       Integrating Shift Invariance
                           tion shows how the position infomation is introduced.                                 Taking inspiration from the three properties of PEs
                          Learnable shows whether it can adjust based on the                                     proposed by Wang et al. (2020), Kiyono et al.
                           input. Integration shows how the position representa-                                 (2021) speculated superior extrapolation perfor-
                           tions are integrated with token representations. Injection                            mance comes from shift invariance, the property
                          Layer shows the injecting position PE.                                                 of a function to not change its output even if its
                                                                                                                 input is shifted. Aiming to incorporate the benefit
                           been the predominant avenue to enhance length                                         of shift invariance in sinusoidal APE, they simply
                           extrapolation of Transformers. Table 1 presents a                                     shift every position index of a sequence by a ran-
                           characterization of these extrapolatable PEs.                                         domoffset k during training, which prevents the
                               Basically, absolute positional encodings (APEs)                                   model from using absolute positions and instead
                           mapeachposition to a unique representation and                                        encourages the use of relative positions.
                           integrate it with corresponding word embedding,                                           Following a similar idea, Likhomanenko et al.
                          while relative positional encodings (RPEs) encode                                      (2021) took it a step further by leveraging continu-
                           the relative distance between tokens and directly                                     ous signals. In addition to shifting every position
                           inject it into the attention module. Besides, RPEs                                    index of APE by an identical random offset, which
                           usually keep modifications independent of value                                       they call global shift, they also introduced local
                          vectors and leaves them not entangled with posi-                                       shift, i.e., shifting each position index by a differ-
                           tion information. Hence, position information of                                      ent random shift, and global scaling, i.e., scaling
                           RPEs can be scalars and usually recurs at each                                        every position index by an identical random scalar,
                           layer.      Figure 2 illustrates these general differ-                                to further prevent capturing spontaneous correla-
                           ences. We divide Table 1 and this section based on                                    tions and memorizing distances.
                          whether the PE is absolute or relative, as existing                                    3.1.2       EnhancingSmoothness
                           research suggests this distinction significantly im-                                  Apart from above relatively straightforward meth-
                           pacts length extrapolation (Neishi and Yoshinaga,                                     ods based on sinusoidal APE, there are several
                           2019; Likhomanenko et al., 2021; Chi et al., 2022).                                   APEstakingquite different theoretical avenues to
                           3.1      Absolute Positional Encodings                                                enhance length extrapolation, aiming to improve
                           Specifically, for a token in position pos, the sinu-                                  the smoothness of the position representations.
                           soidal position embedding is defined as:                                                    Wang et al. (2019) proposed to extend each
                                                                                                                 wordembeddingasacontinuousfunction over an
                              [. . . , sin(       pos        ),cos(          pos         ), . . . ],   (6)       independent variable, i.e., position, so that word
                                                       2i/d                       2i/d                           representations vary smoothly with increasing po-
                                             10000                      10000
                          where i ∈ [0,d/2 − 1] is the dimension of the                                          sitions. Through mathematically sound derivation,
                           position embedding and d denotes model dimen-                                         their general complex-valued embedding f(j,pos)
                           sion. Then, each position embedding is added to                                       of a word wj in position pos is
                                                                                                                             i(ω     pos+θ      )                  i(ω     pos+θ       )
                           the corresponding token embedding and the sum                                            [r     e     j,1         j,1 , · · · , r     e     j,d         j,d ],    (8)
                                                                                                                       j,1                                   j,d
                                                                                                          9961


===== Page 4 =====
                                                                     ���㌵                                         ���㌵                       3.2        Relative Positional Encodings
                                                  Attention                                  Modified
                                                             ���㌵                              Attention���㌵                                 Albeit for the efforts in extrapolatable APEs, it
                                                                                                                                         is believed that RPEs are theoretically capable of
                                                       ���㌵         ���㌵         ���㌵                   ���㌵        ���㌵         ���㌵                running on unseen lengths and are more robust to
                                                                                                                                         input length change (Neishi and Yoshinaga, 2019;
                                                          Input Projection                          Input Projection                     Likhomanenko et al., 2021; Chi et al., 2022), as
                                    Position        ���㌵       ���㌵      ���㌵        ���㌵                                                        RPEs only rely on relative position information,
                                 Embeddings           !        "       #        $
                                                    +        +        +        +               ���㌵       ���㌵       ���㌵       ���㌵             which means they encode the idea of shift invari-
                                                    ���㌵       ���㌵       ���㌵       ���㌵                !       "        #        $
                                                      !        "       #         $                                                       ance naturally and are not subject to a maximum
                                                     Word Embedding Layer                       Word Embedding Layer                     position value. Besides, there is a consensus that
                                     Words          ���㌵       ���㌵       ���㌵       ���㌵              ���㌵       ���㌵       ���㌵       ���㌵
                                                      !        "        #        $               !        "        #        $            in natural language, it is not absolute but relative
                                Figure 2: General differences between APE (left part)                                                    position that matters (Huang et al., 2020; Sinha
                                and RPE (right part), where orange denotes elements                                                      et al., 2022). Thus, RPEs become the dominant
                                holding position information.                                                                            way to encode positions, which we detail in this
                                                                                                                                         section. Before that, we reformulate Equation 7 as
                                                                                                                                         follows to clarify the perspective of RPEs:
                                where amplitude r = [r                                 , . . . , r       ], frequency                          q kT = (x W )(x W )T ⊕p(j −i),                                                     (10)
                                                                                   j,1              j,d                                           i   j              i      q        j       k
                                ω = [ω ,...,ω ] and initial phrase θ =
                                                    j,1               j,d                                                                where p(j −i) encodes the relative position infor-
                                [θj,1, . . . , θj,d] are all trainable. In addition to rep-                                              mation, ⊕ denotes any approach of integrating the
                                resenting positions in complex plane for the first                                                       position information into the compatibility score.
                                time, multiplying position embeddings with word                                                               Among the first,                        Shaw et al. (2018) intro-
                                embeddings is another of their innovations.                                                              duced the idea of RPE based on above formulation.
                                     Analternative approach is to directly capture the                                                   Specifically, they concretized Equation 10 as
                                dynamics between position representations. Liu                                                                                  T                                                     T
                                                                                                                                                         q k =(xiWq)(xjWk+p ) ,                                                   (11)
                                et al. (2020) introduced a dynamical system to                                                                             i    j                                                 r
                                                                                                                  d
                                model position representations {pi ∈ R                                                : i =                                         d
                                1,...,n}, which can be characterized as                                                                  where pr ∈ R is a trainable relative position em-
                                                                                                                                         bedding and r = clip(j − i,r                                       , r        ) denotes
                                                                                                                                                                                                     min        max
                                                                                                                                         the clipped relative position. By clipping the rela-
                                  p(t) = p(s)+Z th(τ,p(τ);θ )dτ,0 ≤ s ≤ t < ∞                                                            tive positions to a determined range, the number of
                                                                                         h                                               position embeddings to be learned is reduced and
                                                                s                                                           (9)          length extrapolation is enhanced as unseen position
                                                                                                                               d         embeddings are avoided. This RPE can also be
                                with an initial vector p(0), where p(t) : R                                          7→ R
                                                                                                                 +                       regarded as a derivation of sinusoidal APE. Fol-
                                is the continuous version of the discrete sequences
                                {p }. h(τ,p(τ);θh), which is the "latent force"                                                          lowing this line, more RPEs have been proposed
                                     i                                                                                                   to better model position information, such as Dai
                                that drives the changes from p to p                                        , is actually
                                                                                          i          i+1                                 et al. (2019), Huang et al. (2020) and TUPE (Ke
                                a neural network parameterized by θh and takes in
                                the previous state (τ,p(τ)).                                                                             et al., 2020). We omit them here since they are not
                                     Highlights: As the first PE for Transformer, si-                                                    proposed for stronger length extrapolation.
                                nusoidal APEhasasignificantimpactonPEsthere-                                                             3.2.1         RoPEFamily
                                after, despite its poor extrapolation. To improve                                                        Also inspired by sinusoidal APE, Su et al. (2024)
                                this, researchers either leverage random shift to                                                        proposed to multiply keys and queries by rotation
                                incorporate shift invariance in sinusoidal APE or                                                        matrices, leaving compatibility scores as
                                generate position embeddings varying smoothly                                                                          T                    d                   T        d
                                with position. Among them, simple random shift-                                                                     qi kj = (RΘ,ixiWq) (RΘ,jxjWk)
                                ing is like a small patch for sinusoidal APE and                                                                                 =WTxTRd                           x W ,                          (12)
                                                                                                                                                                            q     i      Θ,j−i j              k
                                has limited benefits for extrapolation, at the cost of                                                                    d                      d      T       d                    d
                                possible semantic confusion in position encoding,                                                        where RΘ,j−i = (RΘ,i) RΘ,j with RΘ,i being a
                                while the latter can hopefully lead to better extrapo-                                                   block-diagonal matrix with rotation matrices
                                lation, coming with a much higher parameter- and                                                                                   cosiθ                 −siniθ 
                                                                                                                                                                                  m                      m                        (13)
                                computation-complexity.                                                                                                                siniθ                cosiθ
                                                                                                                                                                                 m                     m
                                                                                                                                 9962


===== Page 5 =====
                  on its diagonal, given the parameters Θ            = wherescalar m is a head-specific slope fixed be-
                                                            −2(m−1)/d
                 (θm)               where θm = 10000                   .   fore training. It is worth noting that there is no
                       m=1,2,...,d/2
                  Here the base is 10000, and λ     =2π/θ iswave-          additional learnable parameter, which leads to su-
                                                 m          m
                  length. This method is called Rotary Position Em-        perior efficiency and may also contribute to better
                  bedding (RoPE) as intuitively it rotates key/value       extrapolation of ALiBi. Empirical experiments on
                  embeddings according to their position index:            language modeling demonstrated its superiority.
                            f     (x ,i) = Rd x W           .      (14)       From the perspective of kernel methods, Chi
                             {q,k}   i        Θ,i i    {q,k}               et al. (2022) considered ALiBi as a triangle ker-
                  It is noteworthy that despite the absolute nature of     nel and extended it to KERPLE, a framework that
                  this rotary process, the compatibility score and thus    generalizes relative position embeddings for ex-
                  attention depend only on relative distance. This         trapolation by kernelizing positional differences
                  property together with long-term decay for inter-        using conditionally positive definite kernels. In this
                  token product benefit length extrapolation.              framework, various RPEs can be derived from dif-
                    AsRoPEhasbeenwidelyusedinpopularLLMs                   ferent conditionally positive definite kernels in a
                  (Touvronetal.,2023a;Jiangetal.,2023;Aniletal.,           principled way, among which the logarithmic vari-
                  2023), there are some variants proposed to improve       ant achieves preferred extrapolation performance,
                  it. Sun et al. (2023) defined attention score expecta-   bycalculating the compatibility score as follows:
                  tion between two tokens at a specific distance and       qTk =(xW )T(x W )−r ·log(1+r |i−j|),
                  further attributed the poor extrapolation of RoPE          i  j      i   q      j   k     1          2
                  to the dramatic oscillation of their attention expec-                                                     (18)
                                                                           where r ,r are positive scalar parameters.
                  tations. They proposed to fix this issue by incorpo-             1   2
                  rating a balancing term to punish the oscillation of        Awareoftheoverfitting issue of sinusoidal APE,
                  unstable dimensions and keep the distribution of         Chi et al. (2023b) proposed to overcome it by sim-
                  stable ones, which can be simplified to:                 plifying sinusoidal APE to a new RPE, Sandwich.
                                                                           Specifically, they dropped the cross terms in Equa-
                         T        i−j    T T d                             tion 7 and kept the inner product of two position
                       q k =γ W x R                  x W ,         (15)
                         i  j            q  i   Θ,j−i j     k
                  where γ ∈ (0,1) is a scalar hyperparameter.              embeddings as position information:
                                                                                  T               T               T
                                                                                q kj = (xiwq) (xjWk)+p p .                  (19)
                  3.2.2   T5-Bias Family                                          i                               i  j
                  Different from complex embedding form, some re-                                                       T
                                                                           It is worth noting that in this formula, p p be-
                  searchers reduce position information p(j −i) to a                                                    i  j
                  simplerform. Raffeletal.(2020)utilizedlearnable          comes a temporal bias term with the same decay-
                  scalars to represent relative position information:      with-distance pattern as ALiBi, which is exactly
                                                                           what the authors want to achieve as they suggested
                            T                      T                       this pattern is likely to be the secret to success-
                        qikj = (xiWq)(xjWk) +βi,j.                 (16)    ful length extrapolation. Besides, since position
                  In addition, they extended the clipping mechanism        embeddings here only need to interact with them-
                  by a logarithmic bucket assignment to achieve pre-       selves, the authors make the dimension of them a
                  cise discrimination of nearby positions and less         hyperparameter to further improve performance.
                  precise discrimination of further positions (e.g.,          FIRE (Li et al., 2023b) integrates positional
                  mapping the position indices 1-4 to themselves, 5-       information into Transformers following T5 bias:
                  6 to 5, 7-8 to 6, 9-12 to 7, and so forth.), which
                                                                                    T                      T
                  further reduces the parameters to be learned and              qikj = (xiWq)(xjWk) +b(i,j),                (20)
                  is beneficial for extrapolation (Chi et al., 2022).      where the bias b(i,j) is mapped from positions us-
                  Moreover,WennbergandHenter(2021)introduced               ing a learnable continuous function f : R →− R,
                  TISE, which leverages a radial-basis function of                                                  θ
                  relative distance with multiple trainable parameters     e.g., MLP. To avoid the generalization issue when
                  to add a bias to attention scores.                       the inputs are outside the training domain of the
                    Asthefirst PE aiming mainly for length extrap-         function, they proposed progressive interpolation
                  olation, ALiBi (Press et al., 2021) takes an even        by normalizing the distance by query position in-
                                                                           dex, namely b(i,j) = fθ(i−j).          Note that in
                  simpler way to represent relative position:                                               i
                                                                           causal attention, the normalized distance is always
                         T                      T
                     q k =(xW )(x W ) +m(j−i), (17)                        bounded between [0,1], which aligns the inference
                      i  j       i   q    j   k
                                                                       9963


===== Page 6 =====
                  domain with the training domain for any sequence         distance-attention functions based on Fourier basis
                  lengths, leading to better length extrapolation.         like RoPE. Therefore, RoPE become the de facto
                    However, the above methods separate positional         PE of recent LLMs due to its advanced general
                  bias from semantics completely, which may cause          performance, in spite of its poor extrapolation.
                  semantic similarity to be overshadowed by position       4 Extrapolation Methods in LLMs Era
                  information. Hence, Zheng et al. (2024) proposed
                  Context-Adaptive Positional Encoding (CAPE) to           Based on PEs in §3, various methods have been
                  integrate both semantic and positional information:      developed to further enhance length extrapolation
                     q k T = (x W )(x W )T                                 of LLMs. This section is separated in response to
                       i j        i   q    j    k                          this wave, focusing on interpolation methods and
                                                    T
                            +f((xiWq)(xjWk) ,b(i,j)).              (21)    randomized PEs, as illustrated in Figure 3.
                  Here f : R × R → R is parameterized by a two-            4.1   Position Interpolation
                  layer LeakyReLU neural network and b(i,j) come           Despite the large quantity of PEs with better ex-
                  from other RPEs(e.g., ALiBi and FIRE).                   trapolation, RoPE has been most widely adopted
                    In addition to RPEs introduced previously, there       in recent LLMs due to its superior in-distribution
                  are some methods cannot be categorized into RoPE         performance. Hence, loads of methods have been
                  or T5-bias family.     He et al. (2024) introduce        proposedtoenhancetheextrapolationofRoPE,the
                  bilevel PE that employs two distinct PE for each         most prevalent of which is position interpolation.
                  position: an APE for intra-segment position to               Chen et al. (2023b) firstly 3 introduced posi-
                  help modelcapturethesemanticscontainedtherein,           tion interpolation for RoPE to extrapolate LLMs
                  while an RPE for inter-segment position to capture       to longer sequences by applying linear scaling
                  relationships betweensegmentsandexhibitsextrap-          to down-scale position indices so that the maxi-
                  olation. This decoupling offers greater flexibility      mumposition index matches the previous length
                  in addressing the length extrapolation problem.          limit during pre-training. Formally, this method
                    Based on the observation that existing PEs use         replaces RoPE f (Equation 14) by f′ defined as
                  token as the unit of measurement,        Golovneva       f′(x,i) = f(x, iL), where L is the length limit
                                                                                               ′
                  et al. (2024) claimed that this feature prevents PEs                       L         ′
                                                                           during pre-training and L is the longer sequence
                  from generalizing to higher levels of abstraction        length at inference. The scale ratio κ = L′/L
                  such as sentences and paragraphs. Therefore, they        transforms position n to n/κ. This method reduces
                  proposed Contextual Positional Encoding (CoPE),                                                ′
                                                                           absolute position indices from [0,L ) to [0,L) and
                  which allows the model to determine semantic unit        maximumrelative distance from L′ to L, aligning
                  (e.g., word and sentence) and assign tokens therein      the ranges of position indices and relative distances
                  a same position index. Since CoPE can distribute         to mitigate effects on attention score computation.
                  positions to a much larger number of tokens and             However, from the perspective of Neural Tan-
                  focus attention on semantic units at a higher level      gent Kernel (NTK) theory (Jacot et al., 2018),
                  of abstraction, it exhibits stronger extrapolation.      simply interpolating RoPE’s Fourier space linearly
                    Highlights: Earlier RPEs had been greatly in-          will cause the loss of high-frequency information
                  fluenced by sinusoidal APEs by modifying terms           and prevent models from distinguishing nearby po-
                  in Equation 7 and replacing absolute embeddings          sitions.Hence, NTK-Aware Scaled RoPE (NTK-
                  with relative embeddings. These methods usually          aware interpolation) (bloc97, 2023b) has been pro-
                  leverage clipping or binning strategy to avoid out-      posed by modifying the base of RoPE:
                  of-distribution position embeddings and enhance
                                                                                         ∗           d   −2(m−1)/d
                  extrapolation. Since RPEs decouple the one-to-one                    θm =(b·κd−2)                ,        (22)
                  correspondence between position and position rep-
                  resentation, incorporating bias term directly into       where b is the original base and κ is still the scale
                  compatibility score (Equation 10) becomes a fea-         ratio. The core idea here is to scale high frequen-
                  sible and even better way to encode positional in-       cies less and low frequencies more to reduce infor-
                  formation, which is much simpler and naturally           mation loss of high frequencies. As NTK-aware
                  disentangles value vectors and position informa-         interpolation does not scale the Fourier features
                  tion. However, despite the strong extrapolation of          3There is a concurrent work:  https://kaiokendev.
                  these bias methods, they cannot represent complex        github.io/til#extending-context-to-8k
                                                                       9964


===== Page 7 =====
                             Randomized Positional Encoding                                                      late dimensions of small wavelengths at all while
                                          0    1
                                                                           ...                        M          always interpolating those of big ones.
                             Training                           Ordered Subsampling                                  Similar observations with NTK-by-parts have
                                          0    0            ...           L
                                           0    0             ...          L
                                            0     3            ...          L+2                                  been made by Pal et al. (2023), based on which
                                                          L tokens                                               they proposed to use the truncated basis:
                            Evaluation    0    1             ...          L    L+1     ...      E                                           
                                                                                                                                            
                                                                                   Beyond L but                                                 θi     for θi ≥ b,
                                                                                   in-distribution                                 θ∗ =                                                   (24)
                                                                                                                                                ρ      for a < θ < b,
                             Position Interpolation                                                                                  i                               i
                                                                                                                                            
                             Training     0    0            ...           L                                                                     0      for θi < a.
                                           0    0             ...          L
                                            0     1            ...           L
                                          0    1                       ...                      L’               where ρ is a fixed value that is relatively small, and
                            Evaluation      L               Position Interpolation                               a and b are chosen cutoff values. This way, mod-
                                         0  L’              ...            L                                     els will experience all values of the basis in the
                           Figure 3: Essentials of position interpolation and ran-                               context length used during fine-tuning by choos-
                           domized PE. Randomized PE aims to ensure that posi-                                   ing appropriate cutoff values, and are supposed to
                           tions falling outside the context window at inference re-                             extrapolate better during inference.
                           main in distribution through advanced exposure in train-                                  Additionally, Peng et al. (2023b) observed that
                           ing. Position interpolation, on the other hand, works                                 by introducing a temperature t into compatibility
                           during the inference stage by scaling a longer position                               score before Softmax, perplexity decreases consis-
                           range into the original context window.                                               tently. Combining this finding with NTK-by-parts
                                                                                                                 interpolation, they subsequently proposed YaRN
                           directly, all positions are distinguishable from each                                 that surpasses previous interpolation methods in
                           other. Moreover, this method does not require any                                     both fine-tuned and non-fine-tuned scenarios.
                           fine-tuning to extend the context window.                                                 Theinterpolation methods reflect the critical im-
                               Further, Dynamic-NTK interpolation (emozilla,                                     pact of the rotary base of RoPE on length extrapo-
                           2023) combined NTK-aware interpolation with dy-                                       lation, prompting efforts to enhance extrapolation
                           namic scaling, using exact positions for tokens                                       of RoPE-based LLMbyfine-tuningitwithascaled
                          within pre-trained context window to prevent per-                                      base (Xiong et al., 2023; Rozière et al., 2023; Liu
                           formance degradation and dynamically increases                                        et al., 2023d). However, fixed scaling factors over-
                           scale ratio κ as current sequence length increases                                    look the gradual length-extension process and im-
                           to adjust positions beyond the window:                                                pair performance at shorter lengths, leading to the
                                                    (                                                            proposal of dynamic scaling methods (Chen et al.,
                                            κ= L′/L, ifL′/L>1,                                       (23)        2023a;Zhangetal.,2024b;Dingetal.,2024). Inno-
                                                       1,            otherwise,                                  vatively, Wang et al. (2024) scale each dimension’s
                                                                                                                 base by rounding its wavelength to the nearest inte-
                                        ′                                                                        ger, avoiding phase shifts after each full rotation.
                          where L is the sequence length of the current se-
                           quence, which will increase after each step.                                              Highlights: Recently, position interpolation
                               Either scaling position indices or modifying                                      methods have raised widespread interest in the re-
                           bases, all position representations become closer to                                  search community, as a natural result of their supe-
                           each other, impairing LLM’s ability to distinguish                                    rior extrapolation performance and extremely low
                           the positional order of close-by tokens. Besides,                                     overhead. Current interpolation methods either in-
                           bloc97 (2023a) observed that some RoPE dimen-                                         terpolate position indices or RoPE’s base, guided
                           sions have wavelengths longer than the pre-trained                                    by sound theoretical intuition. Besides, different
                           context window, where they presume absolute po-                                       from other extrapolation methods, position interpo-
                           sitional information remains intact4. Hence, they                                     lation methods have already seen their presence in
                           proposed NTK-by-parts, which does not interpo-                                        the open-source models (Bai et al., 2023a; Touvron
                               4                                                                                 et al., 2023b; AI et al., 2024).
                                 Fromtheperspective of frequency, the full range of high-
                           frequency components have been seen by the model during                               4.2      RandomizedPositional Encoding
                           training, while low-frequency components have not. Thus,
                           every position within the context window leads to a unique                            For PEs without clipping mechanism, length ex-
                          value in these low-frequency components, based on which
                           models can determine the absolute position of each token.                             trapolation means positions beyond those that have
                                                                                                          9965


===== Page 8 =====
                 been observed during training, leading to out-of-        empirical results of trending PEs on language mod-
                 distribution position representations and thus per-      eling in Appendix A.2. However, it has become
                 formance degradation. To address this, an intu-          clear that perplexity alone does not adequately re-
                 itive way is enabling models to observe all possible     flect downstream task performance and is insuffi-
                 position representations during training, which is       cient (Tayetal.,2021;Kazemnejadetal.,2023;Pal
                 exactly the core idea behind randomized PEs.             et al., 2023; Hu et al., 2024). Therefore, dedicated
                    Asarealization of this idea, Ruoss et al. (2023)      benchmarks and evaluation methods are needed to
                 proposed to simulate a much longer range of posi-        further advance the field of length extrapolation.
                 tions (M) and randomly selects an ordered subset           To stimulate subsequent research, we present
                 to fit the training context window for each iteration.   several preliminary thoughts on the construction of
                 Thus,throughadequatetraining,wecanensurethat             a standardized benchmark in Appendix A.3.
                 the model encounters enough unique positions and           Explainability and Principle. Despite the re-
                 all M positions have been fully trained, leading to      markable progress, our understanding of length
                 consistent extrapolation performance.                    extrapolation remains limited, lacking a general
                    Different from Ruoss et al. (2023), PoSE (Zhu         and solid theoretical foundation. The decaying-
                 et al., 2023) partitions a sequence into chunks         with-distance pattern was initially thought to be
                 and adjusts the position indices by adding dis-          crucial for extrapolatable PEs (Press et al., 2021;
                 tinct skipping bias terms between chunks. Hence,         Suetal., 2024), but it was later shown to merely ac-
                 PoSEkeepsthepositionscontinuousineachchunk,              commodatetherecency bias of language modeling
                 which bears a close resemblance to pre-training,        (Chi et al., 2023c). Although Qin et al. (2024) fur-
                 while simultaneously help the model adapt to all         ther provided a theoretical analysis and elaborated
                 positions within a longer context window.                that exponential convergence is a sufficient condi-
                    Highlights: Essentially, randomized PEs sim-          tion for RPEs to length extrapolate, their definition
                 ply decouple the trained context window with the         of length extrapolation is also based on language
                 longer inference one by introducing randomized           modeling and perplexity, which may limit the ap-
                 positions during training or fine-tuning, boosting       plicability of their theorem. Besides, extrapolation
                 exposure of all possible positions in advance. This      methods tend to avoid out-of-distribution positions
                 idea is quite different from that of position interpo-  via interpolation or advanced exposure. Thus, it
                 lation methods, where the latter tries to interpolate    remains unclear when or if Transformers length
                 positions during inference to make them fall into        extrapolate in real-world scenarios and whether or
                 the trained range. For the same reason, position         howexisting methods help with it.
                 interpolation methods are mostly plug-and-play             LongContextUtilization. Existing length ex-
                 while randomized PEs usually need further fine-          trapolation methods mostly focus on expanding
                 tuning, which makes position interpolation much          context window of Transformers, while much less
                 moreappealing due to its low overhead.                   attention has been paid to the investigation and
                                                                          optimization of the utilization of long context. In
                 5 FutureDirections                                       fact, as a result of recent advances, state-of-the-art
                                                                          LLMsareclaimedtobecapableofprocessingse-
                 Evaluation and Benchmark. Initially, researchers         quenceswithupto128ktokens (Abdinetal.,2024;
                 evaluated length extrapolation by training models       AI, 2024). Given such a long context, the extent to
                 onsequences with a length limit and testing them        which the models can effectively utilize it becomes
                 on slightly longer sequences (Liu et al., 2020;          a critical question. Previous study has revealed
                 Likhomanenko et al., 2021). During this phase,           that LLMs tend to "lost in the middle" (Liu et al.,
                 evaluation samples and metrics came from vari-           2023b), i.e., they cannot effectively leverage infor-
                 ous downstream tasks such as machine translation         mation in the middle of a long context. Despite
                 and question answering. Given the demonstrated           a few preliminary explorations trying to improve
                 versatility of pre-trained language models in vari-      long context utilization (Staniszewski et al., 2023;
                 ous downstream tasks (Raffel et al., 2020; Brown         Ravaut et al., 2024), recent long-context bench-
                 et al., 2020), language modeling and perplexity          marks (Li et al., 2023a; An et al., 2024; Bai et al.,
                 have emerged as the standard metrics for evaluat-        2024; Zhang et al., 2024a) suggest that trending
                 ing length extrapolation (Press et al., 2021; Haviv      long-contextLLMsstillstruggleonlongsequences,
                 et al., 2022). Thus, we statistically present some       and significant advancements are required.
                                                                     9966


===== Page 9 =====
                 6 Discussions                                          6.2  LengthExtrapolation and Generalization
                 6.1   Length-Extrapolated and Long-Context             In parallel to research efforts that deem length ex-
                       Transformers                                     trapolation as a promising approach to extend con-
                 Throughout this survey, we position length extrapo-    text window of LLMs, another line of research
                 lation as a promising avenue towards long-context      treats it as a generalization problem and analyzes
                 transformers. However, as stated in §1, it’s the       the length generalization behavior of Transform-
                 length limit and poor length extrapolation together    ers within small context window on synthetic tasks
                 that prevents transformers from processing long        suchasarithmeticanddeductivereasoninginacon-
                 sequences, thus the more direct way to extend the      trolled setup (Lake and Baroni, 2018; Dubois et al.,
                 context window is to simply relax the length limit.    2020; Abbe et al., 2023), where some intriguing
                   The most intuitive way to achieve large con-         observations and insights have been discovered.
                 text window is directly pre-training the model or        Onecommonobservation is that Transformers
                 fine-tuning (continual pre-training) a pre-trained     often struggle with length generalization, whether
                 modelonlongsequences. Xiong et al. (2023) em-          they are trained from scratch on synthetic tasks
                 pirically demonstrated that long context continual    (Lee et al., 2023; Kazemnejad et al., 2023), fine-
                 pre-training is more efficient and similarly effec-    tuned from pre-trained LLMs (Anil et al., 2022) or
                 tive compared to pre-training from scratch with        tested in in-context learning (Saparov et al., 2023).
                 long sequences. However, both pre-training and           Asexplanations, Dziri et al. (2023) hypothesize
                 fine-tuning (continual pre-training) are costly and    certain tasks may not possess the inherent com-
                 demandlarge-scale high-quality long data, which        positionality and allow for shortcut pattern match-
                 is scarce (Kazemnejad et al., 2023). To reduce         ing. On the other side, Transformers are proven
                 memoryandcomputationaloverhead during train-           to length generalize on specific tasks (Zhou et al.,
                 ing, recurrent Transformer variances integrate re-     2023; Xiao and Liu, 2024) or with the right combi-
                 currence with attention (Dai et al., 2019; Bulatov     nation of data format and PE (Zhou et al., 2024).
                 et al., 2022) while efficient Transformer variants     Meanwhile, some studies show other factors in
                 (Tay et al., 2022; Fournier et al., 2023) mainly aim   length generalization. Anil et al. (2022) find that
                 at improving the quadratic complexity of attention     fine-tuning regime, scaling data, model sizes, and
                 mechanism, but both usually compromise some of         compute does not improve length generalization,
                 the modeling capability and still need large-scale    while scratchpad (Nye et al., 2022) or chain-of-
                 long sequence data. Flash Attention (Dao et al.,       thought (Weietal.,2022)inthein-contextlearning
                 2022; Dao, 2023) greatly improves both training        regime do. In addition, Kazemnejad et al. (2023)
                 and inference efficiency of Transformers with little   showthat explicit PE is not essential for decoder-
                 to no overhead, leading to models with much larger     only Transformer to length generalize on small-
                 context window (Jiang et al., 2023; Gunasekar          scale synthetic tasks. These studies have deep-
                 et al., 2023; Li et al., 2023a).                       ened our understanding of length extrapolation in a
                   On the other side, there are more radical re-        mechanistic way and broadened our perspectives
                 search efforts that attempt to abandon attention and   to go beyond PE, demonstrating that the extrapola-
                 its quadratic complexity with regard to sequence       tion ability needs a systematic design where PE is
                 length completely, such as S4 (Gu et al., 2022),       crucial but by no means the sole component.
                 RWKV (Peng et al., 2023a), and Hyena (Poli
                 et al., 2023). Further, some recent studies have       7 Conclusion
                 attempted to scale these novel architectures to bil-
                 lions of parameters, leading to the emergence of      Through this survey, we systematically summa-
                 Mamba (GuandDao,2023)andRWKV-5/6 (Peng                 rized existing methods and recent advances in
                 et al., 2024). However, it has been demonstrated       length extrapolation from the perspective of PE.
                 that Transformer models perform dramatically bet-      Specifically, we meticulously categorize extrapo-
                 ter than state space models like S4 at copying and     latable PEs and further dive into methods based on
                 retrieving information from context (Jelassi et al.,   these PEs in LLMs era. In addition, we highlight
                 2024). Thus, whether these novel architectures are     existing challenges and identify new trends in this
                 better than Transformer and how they perform on        research field, hoping to facilitate researchers and
                 real-world scenarios remains to be evaluated.          provide stimuli for future research.
                                                                   9967


===== Page 10 =====
                 Limitation                                               GuanhuaWang,PhilippWitte,MichaelWyatt,Can
                                                                          Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang,
                 This survey presented a systematic review of exist-      Donghan Yu, Chengruidong Zhang, Cyril Zhang,
                 ing methods and recent trends in length extrapola-       Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yunan
                 tion of Transformers. However, due to the lack of        Zhang, and Xiren Zhou. 2024. Phi-3 Technical Re-
                 standardized benchmark and evaluation methods,           port: A Highly Capable Language Model Locally on
                 weprimarily focus on high-level comparisons and          Your Phone. Preprint, arXiv:2404.14219.
                 distinctions in principle of different approaches,     Josh Abramson, Jonas Adler, Jack Dunger, Richard
                 rather than fine-grained empirical analysis. Further-    Evans, Tim Green, Alexander Pritzel, Olaf Ron-
                 more, in this work, we focus on length extrapola-        neberger, Lindsay Willmore, Andrew J. Ballard,
                                                                          Joshua Bambrick, Sebastian W. Bodenstein, David A.
                 tion studies aimed at extending the context window       Evans, Chia-Chun Hung, Michael O’Neill, David
                 of LLMs in real-world scenarios. Although we             Reiman, Kathryn Tunyasuvunakool, Zachary Wu,
                                                                               ˙          ˙
                 acknowledge the importance of studies analyzing          Akvile Žemgulyte, Eirini Arvaniti, Charles Beattie,
                 length generalization in synthetic tasks within a        Ottavia Bertolli, Alex Bridgland, Alexey Cherepanov,
                 small context window as well, we provide only a          Miles Congreve, Alexander I. Cowen-Rivers, An-
                                                                          drew Cowie, Michael Figurnov, Fabian B. Fuchs,
                 brief discussion on them due to the page limitation.     HannahGladman,RishubJain,YousufA.Khan,Car-
                                                                          oline M. R. Low, Kuba Perlin, Anna Potapenko, Pas-
                 Acknowledgements                                         cal Savy, Sukhdeep Singh, Adrian Stecula, Ashok
                                                                          Thillaisundaram, Catherine Tong, Sergei Yakneen,
                 Xiaocheng Feng is the corresponding author of            Ellen D. Zhong, Michal Zielinski, Augustin Žídek,
                 this work, We thank the anonymous review-                Victor Bapst, Pushmeet Kohli, Max Jaderberg, Demis
                 ers for their insightful comments.      This work        Hassabis, and John M. Jumper. 2024. Accurate struc-
                 was supported by the National Natural Science            ture prediction of biomolecular interactions with Al-
                                                                          phaFold 3. Nature, pages 1–3. Publisher: Nature
                 Foundation of China (NSFC) (U22B2059, grant              Publishing Group.
                 62276078),theKeyR&DProgramofHeilongjiang               01AI,AlexYoung,BeiChen,ChaoLi,ChengenHuang,
                 via grant 2022ZX01A32, the International Cooper-         Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng
                 ation Project of PCL, PCL2022D01and the Funda-           Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng
                 mental Research Funds for the Central Universities       Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming
                 (Grant No.HIT.OCEF.2023018).                             Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui
                                                                          Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi
                                                                          Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu
                 References                                               Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi:
                                                                          OpenFoundationModelsby01.AI. arXiv preprint.
                 EmmanuelAbbe,SamyBengio,AryoLotfi,andKevin               ArXiv:2403.04652 [cs].
                   Rizk. 2023. Generalization on the Unseen, Logic      Mistral   AI.    2024.           Mistral    NeMo.
                   Reasoning and Degree Curriculum. arXiv preprint.       https://mistral.ai/news/mistral-nemo/.
                   ArXiv:2301.13105 [cs, stat].
                 MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,                Chenxin An, Shansan Gong, Ming Zhong, Xingjian
                   Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,           Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and
                   Nguyen Bach, Amit Bahree, Arash Bakhtiari,             Xipeng Qiu. 2024. L-Eval: Instituting Standardized
                   Harkirat Behl, Alon Benhaim, Misha Bilenko, Jo-        Evaluation for Long Context Language Models. In
                   han Bjorck, Sébastien Bubeck, Martin Cai, Caio         Proceedings of the 62nd Annual Meeting of the As-
                   César Teodoro Mendes, Weizhu Chen, Vishrav             sociation for Computational Linguistics (Volume 1:
                   Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo     LongPapers), pages 14388–14411, Bangkok, Thai-
                   de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Ab-     land. Association for Computational Linguistics.
                   hishek Goswami, Suriya Gunasekar, Emman Haider,      Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor
                   Junheng Hao, Russell J. Hewett, Jamie Huynh, Mo-       Lewkowycz, Vedant Misra, Vinay Ramasesh, Am-
                   jan Javaheripi, Xin Jin, Piero Kauffmann, Nikos        brose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam
                   Karampatziakis, Dongwoo Kim, Mahoud Khademi,           Neyshabur. 2022. Exploring Length Generalization
                   LevKurilenko, James R. Lee, Yin Tat Lee, Yuanzhi       in Large Language Models. Advances in Neural In-
                   Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin,      formation Processing Systems, 35:38546–38556.
                   Piyush Madan, Arindam Mitra, Hardik Modi, Anh
                   Nguyen, Brandon Norick, Barun Patra, Daniel Perez-   RohanAnil, Andrew M. Dai, Orhan Firat, Melvin John-
                   Becker, Thomas Portet, Reid Pryzant, Heyang Qin,       son, Dmitry Lepikhin, Alexandre Passos, Siamak
                   MarkoRadmilac,CorbyRosset, SambudhaRoy,Olli            Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
                   Saarikivi, Amin Saied, Adil Salim, Michael San-        Chen, Eric Chu, Jonathan H. Clark, Laurent El
                   tacroce, Shital Shah, Ning Shang, Hiteshi Sharma,      Shafey, Yanping Huang, Kathy Meier-Hellstern, Gau-
                   Xia Song, Olatunji Ruwase, Xin Wang, Rachel Ward,      rav Mishra, Erica Moreira, Mark Omernick, Kevin
                                                                   9968


===== Page 11 =====
                     Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao,           and Juanzi Li. 2024. LongBench: A Bilingual, Mul-
                    Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez             titask Benchmark for Long Context Understanding.
                    Abrego, Junwhan Ahn, Jacob Austin, Paul Barham,           In Proceedings of the 62nd Annual Meeting of the
                     Jan Botha, James Bradbury, Siddhartha Brahma,            Association for Computational Linguistics (Volume 1:
                     Kevin Brooks, Michele Catasta, Yong Cheng, Colin         LongPapers), pages 3119–3137, Bangkok, Thailand.
                     Cherry, Christopher A. Choquette-Choo, Aakanksha         Association for Computational Linguistics.
                     Chowdhery, Clément Crepy, Shachi Dave, Mostafa         bloc97. 2023a.   Add NTK-Aware interpolation "by
                     Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,           parts" correction.
                     Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu
                     Feng, Vlad Fienber, Markus Freitag, Xavier Gar-        bloc97. 2023b.    NTK-Aware Scaled RoPE allows
                     cia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-        LLaMAmodelstohaveextended(8k+)contextsize
                    Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua            withoutanyfine-tuningandminimalperplexitydegra-
                     Howland, Andrea Hu, Jeffrey Hui, Jeremy Hur-             dation.
                    witz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-
                     ski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,       Tom Brown, Benjamin Mann, Nick Ryder, Melanie
                     Sneha Kudugunta, Chang Lan, Katherine Lee, Ben-          Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
                     jamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,        Neelakantan, Pranav Shyam, Girish Sastry, Amanda
                     Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,       Askell, Sandhini Agarwal, Ariel Herbert-Voss,
                     FrederickLiu,MarcelloMaggioni,AromaMahendru,             Gretchen Krueger, Tom Henighan, Rewon Child,
                     Joshua Maynez, Vedant Misra, Maysam Moussalem,           Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
                     Zachary Nado, John Nham, Eric Ni, Andrew Nys-            Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
                     trom, Alicia Parrish, Marie Pellat, Martin Polacek,      teusz Litwin, Scott Gray, Benjamin Chess, Jack
                    Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,       Clark, Christopher Berner, Sam McCandlish, Alec
                     Bryan Richter, Parker Riley, Alex Castro Ros, Au-        Radford, Ilya Sutskever, and Dario Amodei. 2020.
                     rko Roy, Brennan Saeta, Rajkumar Samuel, Renee           Language Models are Few-Shot Learners. In Ad-
                     Shelby, Ambrose Slone, Daniel Smilkov, David R.          vances in Neural Information Processing Systems,
                     So, Daniel Sohn, Simon Tokumine, Dasha Valter,           volume 33, pages 1877–1901. Curran Associates,
                    Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,           Inc.
                     Pidong Wang, Zirui Wang, Tao Wang, John Wiet-
                     ing, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting          Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev.
                    Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven         2022. Recurrent Memory Transformer. Advances in
                     Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav          Neural Information Processing Systems, 35:11079–
                     Petrov, and Yonghui Wu. 2023. PaLM 2 Technical           11091.
                     Report. arXiv preprint. ArXiv:2305.10403 [cs].
                                                                            Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong
                  Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.             Liang, and Lidong Bing. 2023a. CLEX: Continuous
                     Hinton. 2016.     Layer Normalization.      CoRR,        Length Extrapolation for Large Language Models.
                     abs/1607.06450. ArXiv: 1607.06450.                       arXiv preprint. ArXiv:2310.16450 [cs].
                  Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,     ShouyuanChen,ShermanWong,LiangjianChen,and
                    Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei           YuandongTian. 2023b. Extending Context Window
                     Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,         of Large Language Models via Positional Interpola-
                     Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,         tion. arXiv preprint. ArXiv:2306.15595 [cs].
                     KemingLu,JianxinMa,RuiMen,XingzhangRen,                Ta-Chung Chi. 2024. Toward Length-Extrapolatable
                    XuanchengRen,ChuanqiTan,SinanTan,Jianhong                 Transformers. Thesis, Carnegie Mellon University.
                    Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
                     guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,       Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander
                     Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,            Rudnicky, and Peter Ramadge. 2023a. Latent Posi-
                     Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-           tional Information is in the Self-Attention Variance
                     uan Zhang, Yichang Zhang, Zhenru Zhang, Chang            of Transformer Language Models Without Positional
                     Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang          Embeddings. In Proceedings of the 61st Annual
                     Zhu. 2023a. Qwen Technical Report. arXiv preprint.       Meeting of the Association for Computational Lin-
                    ArXiv:2309.16609 [cs].                                    guistics (Volume 2: Short Papers), pages 1183–1193,
                  Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,             Toronto, Canada. Association for Computational Lin-
                     Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao          guistics.
                     Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,       Ta-Chung Chi, Ting-Han Fan, Peter J. Ramadge, and
                     andJuanzi Li. 2023b. LongBench: A Bilingual, Mul-        Alexander Rudnicky. 2022. KERPLE: Kernelized
                     titask Benchmark for Long Context Understanding.         Relative Positional Embedding for Length Extrapo-
                     arXiv preprint. ArXiv:2308.14508 [cs].                   lation. Advances in Neural Information Processing
                  Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,             Systems, 35:8386–8399.
                     Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao        Ta-Chung Chi, Ting-Han Fan, Alexander Rudnicky,
                     Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,         and Peter Ramadge. 2023b. Dissecting Transformer
                                                                       9969


===== Page 12 =====
                     Length Extrapolation via the Lens of Receptive Field     gkamradt.         2024.                         Gkam-
                     Analysis. In Proceedings of the 61st Annual Meeting        radt/LLMTest_NeedleInAHaystack.
                     of the Association for Computational Linguistics (Vol-   TeamGLM,AohanZeng,BinXu,BowenWang,Chen-
                     ume1: LongPapers), pages 13522–13537, Toronto,             huiZhang,DaYin,DanZhang,DiegoRojas,Guanyu
                     Canada. Association for Computational Linguistics.         Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning
                  Ta-Chung Chi, Ting-Han Fan, and Alexander I. Rud-             Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi
                     nicky. 2023c. Attention Alignment and Flexible Po-         Gui, Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li,
                     sitional Embeddings Improve Transformer Length             Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu,
                     Extrapolation. arXiv preprint. ArXiv:2311.00684            Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu,
                     [cs].                                                      Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun
                  NoamChomsky.1957. Syntactic structures.                       Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao
                                                                                Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan
                  Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-              Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai
                     bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.           Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao
                     Transformer-XL:AttentiveLanguageModelsbeyond               Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi,
                     a Fixed-Length Context. In Proceedings of the 57th         Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu
                     AnnualMeetingoftheAssociationforComputational              Hou,andZihanWang.2024. ChatGLM:AFamilyof
                     Linguistics, pages 2978–2988, Florence, Italy. Asso-       Large Language Models from GLM-130B to GLM-4
                     ciation for Computational Linguistics.                     All Tools. Preprint, arXiv:2406.12793.
                  Tri Dao. 2023.     FlashAttention-2: Faster Attention       Olga Golovneva, Tianlu Wang, Jason Weston, and Sain-
                     with Better Parallelism and Work Partitioning. arXiv       bayar Sukhbaatar. 2024. Contextual Position Encod-
                     preprint. ArXiv:2307.08691 [cs].                           ing: Learning to Count What’s Important. arXiv
                                                                                preprint. ArXiv:2405.18719 [cs].
                  Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and             Albert Gu and Tri Dao. 2023. Mamba: Linear-Time Se-
                     Christopher Ré. 2022.        FlashAttention:    Fast       quence Modeling with Selective State Spaces. arXiv
                     and Memory-Efficient Exact Attention with IO-              preprint. ArXiv:2312.00752 [cs].
                     Awareness. Advances in Neural Information Pro-
                     cessing Systems, 35:16344–16359.                         Albert Gu, Karan Goel, and Christopher Ré. 2022. Ef-
                  Yiran Ding, Li Lyna Zhang, Chengruidong Zhang,                ficiently Modeling Long Sequences with Structured
                     Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,             State Spaces. arXiv preprint. ArXiv:2111.00396
                     and MaoYang.2024. LongRoPE:ExtendingLLM                    [cs].
                     Context Window Beyond 2 Million Tokens. arXiv            Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
                     preprint. ArXiv:2402.13753 [cs].                           César Teodoro Mendes, Allie Del Giorno, Sivakanth
                  Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and               Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
                     Elia Bruni. 2020. Location Attention for Extrapola-        de Rosa, Olli Saarikivi, Adil Salim, Shital Shah,
                     tion to Longer Sequences. In Proceedings of the 58th       Harkirat Singh Behl, Xin Wang, Sébastien Bubeck,
                     AnnualMeetingoftheAssociationforComputational              RonenEldan, AdamTaumanKalai, Yin Tat Lee, and
                     Linguistics, pages 403–413, Online. Association for        Yuanzhi Li. 2023. Textbooks Are All You Need.
                     Computational Linguistics.                                 Preprint, arXiv:2306.11644.
                  Philipp Dufter, Martin Schmitt, and Hinrich Schütze.        AdiHaviv, Ori Ram, Ofir Press, Peter Izsak, and Omer
                     2022. Position Information in Transformers: An             Levy. 2022. Transformer Language Models without
                     Overview. Computational Linguistics, 48(3):733–            Positional Encodings Still Learn Positional Informa-
                     763. Place: Cambridge, MA Publisher: MIT Press.            tion. In Findings of the Association for Computa-
                                                                                tional Linguistics: EMNLP 2022, pages 1382–1390,
                  Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang (Lor-            AbuDhabi, United Arab Emirates. Association for
                     raine) Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck,     Computational Linguistics.
                     Peter West, Chandra Bhagavatula, Ronan Le Bras,          KaimingHe,XiangyuZhang,ShaoqingRen,andJian
                     Jena Hwang,SoumyaSanyal,XiangRen,AllysonEt-                Sun.2016. DeepResidualLearningforImageRecog-
                     tinger, Zaid Harchaoui, and Yejin Choi. 2023. Faith        nition. pages 770–778.
                     and Fate: Limits of Transformers on Composition-         Zhenyu He, Guhao Feng, Shengjie Luo, Kai Yang,
                     ality. Advances in Neural Information Processing           DiHe,Jingjing Xu, Zhi Zhang, Hongxia Yang, and
                     Systems, 36:70293–70332.                                   LiweiWang.2024. TwoStonesHitOneBird: Bilevel
                  emozilla. 2023. Dynamically Scaled RoPE further in-           Positional Encoding for Better Length Extrapolation.
                     creases performance of long context LLaMA with             arXiv preprint. ArXiv:2401.16421 [cs, stat].
                     zero fine-tuning.                                        DanHendrycks,CollinBurns,StevenBasart,AndyZou,
                  Quentin Fournier, Gaétan Marceau Caron, and Daniel            Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
                     Aloise. 2023.    A Practical Survey on Faster and          2020. Measuring Massive Multitask Language Un-
                     Lighter Transformers.    ACM Computing Surveys,            derstanding. In International Conference on Learn-
                     55(14s):304:1–304:40.                                      ing Representations.
                                                                         9970


===== Page 13 =====
                 Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang,         DachengLi,Rulin Shao, Anze Xie, Ying Sheng, Lian-
                    and Yansong Feng. 2024. Can Perplexity Reflect         min Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe
                    Large Language Model’s Ability in Long Text Un-        Ma,andHaoZhang.2023a. HowLongCanContext
                    derstanding?  arXiv preprint. ArXiv:2405.06105         Length of Open-Source LLMs truly Promise? In
                    [cs].                                                  NeurIPS 2023 Workshop on Instruction Tuning and
                 ZhihengHuang,DavisLiang,PengXu,andBingXiang.              Instruction Following.
                    2020. Improve Transformer Models with Better Rel-    Shanda Li, Chong You, Guru Guruganesh, Joshua
                    ative Position Embeddings. In Findings of the Associ-  Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit
                    ation for Computational Linguistics: EMNLP 2020,       Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh
                    pages 3327–3335, Online. Association for Computa-      Bhojanapalli. 2023b. Functional Interpolation for
                    tional Linguistics.                                    Relative Positions Improves Long Context Trans-
                 Arthur Jacot, Franck Gabriel, and Clement Hongler.        formers. arXiv preprint. ArXiv:2310.04418 [cs].
                    2018. Neural Tangent Kernel: Convergence and         Tatiana Likhomanenko, Qiantong Xu, Gabriel Syn-
                    Generalization in Neural Networks. In Advances in      naeve, Ronan Collobert, and Alex Rogozhnikov.
                    Neural Information Processing Systems, volume 31.      2021. CAPE: Encoding Relative Positions with Con-
                    Curran Associates, Inc.                                tinuous Augmented Positional Embeddings. In Ad-
                 SamyJelassi, David Brandfonbrener, Sham M. Kakade,        vances in Neural Information Processing Systems,
                    and Eran Malach. 2024. Repeat After Me: Trans-         volume34,pages16079–16092.CurranAssociates,
                    formers are Better than State Space Models at Copy-    Inc.
                    ing. arXiv preprint. ArXiv:2402.01032 [cs].          Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning,
                 Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-      PengJin, and Li Yuan. 2023. Video-LLaVA: Learn-
                    sch, Chris Bamford, Devendra Singh Chaplot, Diego      ing United Visual Representation by Alignment Be-
                    delasCasas,FlorianBressand,GiannaLengyel,Guil-         fore Projection. arXiv preprint. ArXiv:2311.10122
                    laumeLample,LucileSaulnier,LélioRenardLavaud,          [cs].
                    Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,     Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
                    Thibaut Lavril, Thomas Wang, Timothée Lacroix,         Lee. 2023a. Visual instruction tuning. In NeurIPS.
                    and William El Sayed. 2023. Mistral 7B. arXiv
                    preprint. ArXiv:2310.06825 [cs].                     Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
                 Amirhossein      Kazemnejad,        Inkit     Padhi,      jape, Michele Bevilacqua, Fabio Petroni, and Percy
                    Karthikeyan Natesan Ramamurthy, Payel Das,             Liang. 2023b.    Lost in the Middle: How Lan-
                    and Siva Reddy. 2023. The Impact of Positional         guage Models Use Long Contexts. arXiv preprint.
                    Encoding on Length Generalization in Transformers.     ArXiv:2307.03172 [cs] rate: 0.
                    arXiv preprint. ArXiv:2305.19466 [cs].
                 Guolin Ke, Di He, and Tie-Yan Liu. 2020. Rethinking     Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
                    Positional Encoding in Language Pre-training.          Hiroaki Hayashi, and Graham Neubig. 2023c. Pre-
                                                                           train, Prompt, and Predict: A Systematic Survey of
                 ShunKiyono,SosukeKobayashi, Jun Suzuki, and Ken-          Prompting Methods in Natural Language Processing.
                    taro Inui. 2021. SHAPE: Shifted Absolute Position      ACMComputingSurveys,55(9):195:1–195:35.
                    EmbeddingforTransformers. In Proceedings of the      Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An,
                    2021 Conference on Empirical Methods in Natural        Xipeng Qiu, and Dahua Lin. 2023d.         Scaling
                    Language Processing, pages 3309–3321, Online and       LawsofRoPE-basedExtrapolation. arXiv preprint.
                    Punta Cana, Dominican Republic. Association for        ArXiv:2310.05209 [cs].
                    Computational Linguistics.
                 Brenden Lake and Marco Baroni. 2018. Generalization     XuanqingLiu,Hsiang-FuYu,Inderjit Dhillon, and Cho-
                    without Systematicity: On the Compositional Skills     Jui Hsieh. 2020. Learning to Encode Position for
                    of Sequence-to-Sequence Recurrent Networks. In         Transformer with Continuous Dynamical Model. In
                    Proceedings of the 35th International Conference on    Proceedings of the 37th International Conference on
                    Machine Learning, pages 2873–2882. PMLR. ISSN:         Machine Learning, pages 6327–6335. PMLR. ISSN:
                    2640-3498.                                             2640-3498.
                 NayoungLee,Kartik Sreenivasan, Jason D. Lee, Kang-      Jeff Mitchell, Pontus Stenetorp, Pasquale Minervini,
                    wookLee,andDimitrisPapailiopoulos. 2023. Teach-        and Sebastian Riedel. 2018. Extrapolation in NLP.
                    ing Arithmetic to Small Transformers.                  In Proceedings of the Workshop on Generalization
                                                                           in the Age of Deep Learning, pages 28–33, New
                 Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei,            Orleans, Louisiana. Association for Computational
                    Nanning Zheng, Han Hu, Zheng Zhang, and                Linguistics.
                    Houwen Peng. 2024.       Common 7B Language
                    Models Already Possess Strong Math Capabilities.     RICHARDMONTAGUE.1970. Universal grammar.
                    https://arxiv.org/abs/2403.04706v1.                    Theoria, 36(3):373–398.
                                                                    9971


===== Page 14 =====
                    Masato Neishi and Naoki Yoshinaga. 2019. On the Re-             BowenPeng,JeffreyQuesnelle,HongluFan,andEnrico
                       lation between Position Information and Sentence                Shippole. 2023b. YaRN: Efficient Context Window
                       Length in Neural Machine Translation. In Proceed-               ExtensionofLargeLanguageModels. arXivpreprint.
                       ings of the 23rd Conference on Computational Nat-               ArXiv:2309.00071 [cs].
                       ural Language Learning (CoNLL), pages 328–338,
                       HongKong,China.Association for Computational                 MichaelPoli, Stefano Massaroli, Eric Nguyen, Daniel Y.
                       Linguistics.                                                    Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Ste-
                                                                                       fano Ermon, and Christopher Re. 2023. Hyena Hi-
                    MaxwellNye,AndersJohanAndreassen, Guy Gur-Ari,                     erarchy: Towards Larger Convolutional Language
                       Henryk Michalewski, Jacob Austin, David Bieber,                 Models. In Proceedings of the 40th International
                       David Dohan, Aitor Lewkowycz, Maarten Bosma,                    Conference on Machine Learning, pages 28043–
                       David Luan, Charles Sutton, and Augustus Odena.                 28078. PMLR. ISSN: 2640-3498.
                       2022. Show Your Work: Scratchpads for Intermedi-
                       ate Computation with Language Models.                        Ofir Press, Noah Smith, and Mike Lewis. 2021. Train
                    Santiago Ontanon, Joshua Ainslie, Zachary Fisher, and              Short, Test Long: Attention with Linear Biases En-
                       Vaclav Cvicek. 2022. Making Transformers Solve                  ables Input Length Extrapolation.
                       Compositional Tasks. In Proceedings of the 60th              ZhenQin,YiranZhong,andHuiDeng.2024. Exploring
                       AnnualMeetingoftheAssociationforComputational                   Transformer Extrapolation. Proceedings of the AAAI
                       Linguistics (Volume 1: Long Papers), pages 3591–                Conference on Artificial Intelligence, 38(17):18897–
                       3607,Dublin,Ireland.AssociationforComputational                 18905.
                       Linguistics.
                    OpenAI.2023. GPT-4TechnicalReport. arXivpreprint.               Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
                       ArXiv:2303.08774 [cs].                                          Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
                                                                                       WeiLi,andPeterJ.Liu. 2020. Exploring the limits
                    Arka Pal, Deep Karkhanis, Manley Roberts, Samuel                   of transfer learning with a unified text-to-text trans-
                       Dooley,    Arvind Sundararajan,         and Siddartha           former. The Journal of Machine Learning Research,
                       Naidu. 2023.       Giraffe:   Adventures in Expand-             21(1):140:5485–140:5551.
                       ing Context Lengths in LLMs.            arXiv preprint.      Mathieu Ravaut, Aixin Sun, Nancy Chen, and Shafiq
                       ArXiv:2308.10882 [cs].                                          Joty. 2024. On Context Utilization in Summariza-
                    Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai,                  tion with Large Language Models. In Proceedings
                       Meredith Ringel Morris, Percy Liang, and Michael S.             of the 62nd Annual Meeting of the Association for
                       Bernstein. 2023.     Generative Agents: Interactive             Computational Linguistics (Volume 1: Long Papers),
                       Simulacra of Human Behavior.            arXiv preprint.         pages 2764–2781, Bangkok, Thailand. Association
                       ArXiv:2304.03442 [cs].                                          for Computational Linguistics.
                    Bo Peng, Eric Alcaide, Quentin Anthony, Alon Al-                Yasaman Razeghi, Robert L Logan IV, Matt Gardner,
                       balak, Samuel Arcadinho, Stella Biderman, Huanqi                and Sameer Singh. 2022. Impact of Pretraining Term
                       Cao, Xin Cheng, Michael Chung, Leon Derczynski,                 Frequencies on Few-Shot Numerical Reasoning. In
                       Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng                 Findings of the Association for Computational Lin-
                       He, Haowen Hou, Przemyslaw Kazienko, Jan Ko-                    guistics: EMNLP 2022, pages 840–854, Abu Dhabi,
                       con, Jiaming Kong, Bartłomiej Koptyra, Hayden                   United Arab Emirates. Association for Computa-
                       Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand             tional Linguistics.
                       Mom,AtsushiSaito, Guangyu Song, Xiangru Tang,
                       Johan Wind, Stanisław Wozniak,´      Zhenyuan Zhang,         Baptiste Rozière, Jonas Gehring, Fabian Gloeckle,
                       Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023a.                 Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi
                       RWKV:ReinventingRNNsfortheTransformerEra.                       Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
                       In Findings of the Association for Computational                Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
                       Linguistics: EMNLP 2023, pages 14048–14077, Sin-                Bhatt, Cristian CantonFerrer, AaronGrattafiori, Wen-
                       gapore. Association for Computational Linguistics.              han Xiong, Alexandre Défossez, Jade Copet, Faisal
                                                                                       Azhar,HugoTouvron,LouisMartin,NicolasUsunier,
                    Bo Peng, Daniel Goldstein, Quentin Anthony, Alon                   ThomasScialom, and Gabriel Synnaeve. 2023. Code
                       Albalak, Eric Alcaide, Stella Biderman, Eugene                  Llama: Open Foundation Models for Code. arXiv
                       Cheah, Xingjian Du, Teddy Ferdinan, Haowen                      preprint. ArXiv:2308.12950 [cs].
                       Hou,PrzemysławKazienko,Kranthi Kiran Gv, Jan
                             ´
                       Kocon, Bartłomiej Koptyra, Satyapriya Krishna,               Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi
                       Ronald McClelland Jr., Niklas Muennighoff, Fares                Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane
                       Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu,                  Legg, and Joel Veness. 2023. Randomized Positional
                       Stanisław Wozniak,´     Ruichong Zhang, Bingchen                Encodings Boost Length Generalization of Trans-
                       Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, and                     formers. In Proceedings of the 61st Annual Meet-
                       Rui-Jie Zhu. 2024.        Eagle and Finch: RWKV                 ing of the Association for Computational Linguistics
                       with Matrix-Valued States and Dynamic Recurrence.               (Volume2: ShortPapers), pages1889–1903,Toronto,
                       https://arxiv.org/abs/2404.05892v3.                             Canada. Association for Computational Linguistics.
                                                                               9972


===== Page 15 =====
                 Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Pad-    Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
                    makumar, Nitish Joshi, Mehran Kazemi, Najoung           bert, Amjad Almahairi, Yasmine Babaei, Nikolay
                    Kim,andHeHe.2023. TestingtheGeneralDeduc-               Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
                    tive Reasoning Capacity of Large Language Models        Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
                    Using OODExamples. Advances in Neural Informa-          Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
                    tion Processing Systems, 36:3083–3105.                  Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
                                                                            Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
                 Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant,        thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
                    and Omer Levy. 2023. ZeroSCROLLS: A Zero-Shot           Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
                    Benchmark for Long Text Understanding. In Find-         Isabel Kloumann,ArtemKorenev,PunitSinghKoura,
                    ings of the Association for Computational Linguis-      Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
                    tics: EMNLP 2023, pages 7977–7989, Singapore.           anaLiskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
                    Association for Computational Linguistics.              tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
                                                                            bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
                 Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.     stein, Rashi Rungta, Kalyan Saladi, Alan Schel-
                    Self-Attention with Relative Position Representa-       ten, Ruan Silva, Eric Michael Smith, Ranjan Sub-
                    tions. In Proceedings of the 2018 Conference of         ramanian, Xiaoqing Ellen Tan, Binh Tang, Ross
                    the North American Chapter of the Association for       Taylor, Adina Williams, Jian Xiang Kuan, Puxin
                    Computational Linguistics: Human Language Tech-         Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-
                    nologies, Volume 2 (Short Papers), pages 464–468,       gela Fan, Melanie Kambadur, Sharan Narang, Aure-
                    NewOrleans, Louisiana. Association for Computa-         lien Rodriguez, Robert Stojnic, Sergey Edunov, and
                    tional Linguistics.                                     Thomas Scialom. 2023b. Llama 2: Open Founda-
                                                                            tion and Fine-Tuned Chat Models. arXiv preprint.
                 Koustuv Sinha, Amirhossein Kazemnejad, Siva Reddy,         ArXiv:2307.09288 [cs].
                    Joelle Pineau, Dieuwke Hupkes, and Adina Williams.
                    2022. The Curious Case of Absolute Position Em-      Yao-HungHubertTsai, Shaojie Bai, Makoto Yamada,
                    beddings. arXiv preprint. ArXiv:2210.12574 [cs].        Louis-Philippe Morency, and Ruslan Salakhutdinov.
                                                                            2019. Transformer Dissection: An Unified Under-
                 Konrad Staniszewski, Szymon Tworkowski, Sebastian          standing for Transformer’s Attention via the Lens of
                                                             ´              Kernel. In Proceedings of the 2019 Conference on
                    Jaszczur, Henryk Michalewski, ŁukaszKucinski, and
                              ´                                             Empirical Methods in Natural Language Processing
                    Piotr Miłos. 2023. Structured Packing in LLM Train-
                    ing Improves Long Context Utilization.                  andthe9thInternational Joint Conference on Natu-
                                                                            ral Language Processing (EMNLP-IJCNLP), pages
                 Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,          4344–4353,HongKong,China.AssociationforCom-
                    Wen Bo, and Yunfeng Liu. 2024. RoFormer: En-            putational Linguistics. Rate: 3.
                    hanced transformer with Rotary Position Embedding.   Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
                    Neurocomputing, 568:127063.                             Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
                 Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shao-         Kaiser, and Illia Polosukhin. 2017. Attention is All
                    han Huang, Alon Benhaim, Vishrav Chaudhary, Xia         you Need. In Advances in Neural Information Pro-
                    Song, and Furu Wei. 2023. A Length-Extrapolatable       cessing Systems, volume 30. Curran Associates, Inc.
                    Transformer. InProceedingsofthe61stAnnualMeet-       Benyou Wang, Lifeng Shang, Christina Lioma, Xin
                    ing of the Association for Computational Linguis-       Jiang, Hao Yang, Qun Liu, and Jakob Grue Simonsen.
                    tics (Volume 1: Long Papers), pages 14590–14604,        2020. On Position Embeddings in BERT.
                    Toronto, Canada. Association for Computational Lin-
                    guistics.                                            BenyouWang,DonghaoZhao,ChristinaLioma,Qiuchi
                 Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald           Li, Peng Zhang, and Jakob Grue Simonsen. 2019.
                    Metzler. 2022. Efficient Transformers: A Survey.        Encoding word order in complex embeddings.
                    ACMComputingSurveys,55(6):109:1–109:28.              Suyuchen Wang, Ivan Kobyzev, Peng Lu, Mehdi
                 Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus,      Rezagholizadeh, and Bang Liu. 2024.         Reso-
                    Samira Abnar, Hyung Won Chung, Sharan Narang,           nance RoPE: Improving Context Length General-
                    Dani Yogatama, Ashish Vaswani, and Donald Met-          ization of Large Language Models. arXiv preprint.
                    zler. 2021. Scale Efficiently: Insights from Pretrain-  ArXiv:2403.00071 [cs].
                    ing and Finetuning Transformers.                     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
                                                                            Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V. Le,
                 HugoTouvron,Thibaut Lavril, Gautier Izacard, Xavier        and Denny Zhou. 2022. Chain-of-Thought Prompt-
                    Martinet, Marie-Anne Lachaux, Timothée Lacroix,         ing Elicits Reasoning in Large Language Models.
                    Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal     Advances in Neural Information Processing Systems,
                    Azhar, Aurelien Rodriguez, Armand Joulin, Edouard       35:24824–24837.
                    Grave, and Guillaume Lample. 2023a.      LLaMA:
                    Open and Efficient Foundation Language Models.       Ulme Wennberg and Gustav Eje Henter. 2021. The
                    arXiv preprint. ArXiv:2302.13971 [cs].                  Case for Translation-Invariant Self-Attention in
                                                                     9973


===== Page 16 =====
                  Transformer-Based Language Models. In Proceed-    Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin,
                   ingsofthe59thAnnualMeetingoftheAssociationfor      OmidSaremi,JoshSusskind,SamyBengio,andPree-
                  ComputationalLinguisticsandthe11thInternational     tum Nakkiran. 2023. What Algorithms can Trans-
                  Joint Conference on Natural Language Processing     formers Learn? A Study in Length Generalization.
                  (Volume 2: Short Papers), pages 130–140, Online.    arXiv preprint. ArXiv:2310.16028 [cs, stat].
                  Association for Computational Linguistics.        YongchaoZhou,UriAlon,XinyunChen,XuezhiWang,
                Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek,    Rishabh Agarwal, and Denny Zhou. 2024. Trans-
                   BoyuanChen,BailinWang,NajoungKim,JacobAn-          formers Can Achieve Length Generalization But Not
                   dreas, and Yoon Kim. 2024. Reasoning or Reciting?  Robustly. arXiv preprint. ArXiv:2402.09371 [cs].
                   Exploring the Capabilities and Limitations of Lan-
                   guage Models Through Counterfactual Tasks. arXiv DaweiZhu,NanYang,LiangWang,YifanSong,Wen-
                   preprint. ArXiv:2307.02477 [cs].                   hao Wu, Furu Wei, and Sujian Li. 2023. PoSE:
                Changnan Xiao and Bing Liu. 2024. A Theory for        Efficient Context Window Extension of LLMs via
                   Length Generalization in Learning to Reason. arXiv Positional Skip-wise Training.  arXiv preprint.
                   preprint. ArXiv:2404.00560 [cs].                   ArXiv:2309.10400 [cs].
                WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,
                   Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
                   Rungta, Karthik Abinav Sankararaman, Barlas Oguz,
                   MadianKhabsa,HanFang,YasharMehdad,Sharan
                   Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale,
                   Sergey Edunov, Mike Lewis, Sinong Wang, and Hao
                   Ma.2023. Effective Long-Context Scaling of Foun-
                   dation Models. arXiv preprint. ArXiv:2309.16039
                   [cs].
                ChulheeYun,SrinadhBhojanapalli, Ankit Singh Rawat,
                   Sashank Reddi, and Sanjiv Kumar. 2019.    Are
                  Transformers universal approximators of sequence-
                   to-sequence functions?
                JingqingZhang,YaoZhao,MohammadSaleh,andPeter
                   Liu. 2020. PEGASUS: Pre-training with Extracted
                   Gap-sentences for Abstractive Summarization. In
                  Proceedings of the 37th International Conference
                   onMachineLearning, pages 11328–11339. PMLR.
                   ISSN: 2640-3498.
                Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang
                  Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai,
                   ShuoWang,ZhiyuanLiu,andMaosongSun.2024a.
                   ınftyBench: Extending Long Context Evaluation Be-
                  yond100KTokens. InProceedingsof the 62nd An-
                   nual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), pages 15262–
                  15277, Bangkok, Thailand. Association for Compu-
                   tational Linguistics.
                Yikai Zhang, Junlong Li, and Pengfei Liu. 2024b. Ex-
                   tending LLMs’ Context Window with 100 Samples.
                   arXiv preprint. ArXiv:2401.07004 [cs].
                Chuanyang Zheng, Yihang Gao, Han Shi, Minbin
                   Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren,
                   Michael Ng, Xin Jiang, Zhenguo Li, and Yu Li. 2024.
                   CAPE: Context-Adaptive Positional Encoding for
                   Length Extrapolation.
                Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
                   Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
                   Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
                  Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
                   LLM-as-a-Judge with MT-Bench and Chatbot Arena.
                  Advances in Neural Information Processing Systems,
                   36:46595–46623.
                                                                9974


===== Page 17 =====
                 A Appendix                                                   position matters rather than absolute posi-
                 A.1    LengthExtrapolation on Generation                     tions).
                       Tasks                                                • RPEsdemonstratebetter extrapolation ca-
                 To help readers gain a deeper understanding of               pability. In the length extrapolation setting
                 the challenges of length extrapolation, we leverage          that this survey concerns most, RPEs also out-
                 LongBench-E (Bai et al., 2023b) as our testbed               perform APEsduetointrinsic shift-invariance
                 andchoosethreetrendingLLMswithdifferentcon-                  and binning strategy (for T5 bias) or expo-
                 text window sizes to evaluate their performance on           nentially decay with distance (for ALiBi and
                 various generation tasks and different evaluation            RoPE).
                 length ranges. The results are shown in Table 2.           • RPEs seek a balance between expressive-
                    From the results, some intriguing conclusions             ness (embedding-based RPE) and extrapo-
                 can be drawn:                                                lation(bias-basedRPE)andperplexityisin-
                   1. Whenevaluatingmodelsonsequencesbeyond                   sufficient. As in comparisons between RPEs,
                      the original context window, a consistent per-          we can see that bias methods (T5 bias and
                      formance degradation can be observed across             ALiBi) lead to lower perplexity on sequences
                      models and tasks, which strongly supports the           with length both within and beyond the con-
                      necessity of studying length extrapolation.             text window, which indicates bias methods
                                                                              are better at language modeling by explicitly
                   2. Thanks to the shift-invariance and decay-with-          pandering recency bias. Note that it does
                      distance property of RPE these LLMs use,                not mean our claim that embedding-based
                      they can maintain a reasonable performance              methods like RoPE are more expressive is
                      whendealing with sequences beyond the con-              wrong, considering that models with ALiBi
                      text window, i.e., the performance will grad-           have worse performance than RoPE-based
                      ually decline rather than immediately crush             models on current trending benchmarks (Pal
                      after length exceeding the context window.              et al., 2023) like MMLU (Hendrycks et al.,
                   3. Even evaluating on sequences within the con-            2020) and LMSys arena (Zheng et al., 2023).
                      text window, the increase in sequence length            This further shows that perplexity is insuffi-
                      still leads to degraded performance. This may           cient to reflect performance in these down-
                      be as a result of the increasing difficulty with        stream tasks.
                      increasinglengthorduetothesparsityoflong-          A.3   ThoughtsonStandardizedBenchmark
                      range dependencies in concatenated training
                      long sequences, meaning length extrapolation       Realizing the difficulty and complexity of con-
                      as a problem even exists within training con-      structing a standardized benchmark for length ex-
                      text window and long-context transformers          trapolation, we present some preliminary thoughts
                      trained on long sequences do not necessarily       onit as follows:
                      possess strong length extrapolation capability.       • The benchmark should have no position bias.
                 A.2    Results on Language Modeling                          This means the model cannot consistently rely
                                                                              on tokens at specific locations to reach the
                 Tooffer an empirical comparison between popular              correct answer. Thus, language modeling is
                 PEs, we statistically collect results from published         not an ideal task due to its recency bias, which
                 literatures and form Table 3.                                makesitpossibleforthemodeltogeneratethe
                    Wehighlightseveralimportantconclusionsfrom                correct token based solely on nearby tokens.
                 these results:                                             • The benchmark should require modeling the
                    • RPEs demonstrate better in-distribution                 full range. This indicates the model cannot
                      performance.      On sequences with length              depend on a small portion of the input but
                      within context window, RPEs already demon-              needs to attend and model the full range of
                      strate better performance, compared to APEs.            context to give correct responses. Thus, the
                      Weexplain the results as RPE is consistent              popular Needle In A Haystack test (gkam-
                      with the nature of natural language (relative           radt, 2024) is not an ideal benchmark, as it
                                                                    9975


===== Page 18 =====
                Task             Evaluation Window Llama2-7B-Chat (4K) ChatGLM3-6B (8K) Vicuna-v1.5-7b-16k
                QA
                                       0-4K               34.56               21.86             31.19
                2WikiMQA               4-8K               23.95               21.85             17.71
                                       8K+                23.12               13,72             12.33
                                       0-4K               37.59               25.92             37.35
                HotpotQA               4-8K               27.84               19.63             24.09
                                       8K+                23.17               15.96             21.91
                                       0-4K               41.42               44.04              47.1
                MultiFieldQA-en        4-8K               34.29               29.31             33.83
                                       8K+                21.21               28.45             28.29
                Summarization
                                       0-4K               26.67               25.71             27.96
                MultiNews              4-8K               22.33               21.37             23.62
                                       8K+                22.46               20.4              21.22
                                       0-4K               30.66               30.7              33.95
                GovReport              4-8K               27.39               23.39             29.91
                                       8K+                 25.6               22.2              24.89
                CodeCompletion
                                       0-4K               63.73               52.18             56.14
                LCC                    4-8K               61.59               43.63             57.69
                                       8K+                56.83               40.37             43.25
               Table 2: Performance of Llama2-7B-Chat (Touvron et al., 2023b), ChatGLM3-6B (GLM et al., 2024) and
               Vicuna-v1.5-7b (Zheng et al., 2023) on LongBench-E, where the context window of each model is indicated in
               parentheses.
                 Dataset                         WikiText-103             OpenWebText2           ArXiv
                 Context Window                512            1024                       512
                 Evaluation Window         512    1012 1024 2024 512                1024      512 1024
                 APE
                 Sinusoidal              20.05 43.54 19.34 51.09           26      14168       5.8   1070
                 RPE
                 T5Bias                  19.65 18.79 18.8 18.34 22.6                22.2      5.16 4.91
                 ALiBi                   19.73 18.73 18.66 18.05 22.8               23.3      5.25 5.41
                 RoPE                    20.07 21.37 19.33 31.17           23        61       5.25 16.02
               Table 3: Empirical comparisons of different PEs on language modeling. The results on WikiText-103 are obtained
               from Sunet al. (2023) and the results on OpenWebText2 and ArXiv are obtained from Chi (2024). Note that the
               results may not be fairly comparable across dataset due to differences in model and training.
                                                            9976


===== Page 19 =====
         only requires the model to search and retrieve
         only a small portion of the input that is signif-
         icantly different from other content, which is
         quite different from understanding and use of
         context (Liu et al., 2023b).
        • This benchmark should offer flexibility in se-
         quence length with relatively stable diffi-
         culty. This means the benchmark should con-
         sist of enough sequences at increasing lengths
         but not increasing difficulty. Thus, the bench-
         mark can directly help with the fine-grained
         evaluation of the length extrapolation capabil-
         ity of Transformers without the need to crop a
         complete sequence, where the consistency of
         difficulty ensures the evaluation is only rele-
         vant to the increasing length.
        Asforaconcrete example, calculating long se-
       quences containing only addition and subtraction
       withinten(andkeepingtheintermediateresultsina
       small range) might be a promising evaluation task,
       considering that the task itself is simple enough for
       commonLLMs (Lietal.,2024)andwecanthus
       focus on the impact of increasing length.
                           9977
