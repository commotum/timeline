                             Published as a conference paper at ICLR 2017
                                                                    2      1                              even positive numbers
                                )                                64      3         0                      even negative number
                                n                                       5
                                (2                                   7                                    odd positive numbers
                                φ
                                                                                   -2     -1
                                n                                                  -4                     odd negative numbers
                                o                                                 -6
                                i                                                        -3
                                s                                                       -5                zero
                                n                                                      -7
                                e
                                m                                                                         Null (padding value)
                                i
                                d
                                 
                                g
                                n
                                i
                                d
                                d          254
                                e
                                b                 255
                                m
                                e
                                 
                                d
                                n                            -256
                                o                   Null          -255
                                c
                                e
                                S
                                                 First embedding dimension φ1(n)
                             Figure 8: A learned embedding of integers {−256,−255,...,−1,0,1,...,255} in R2. The color
                             intensity corresponds to the magnitude of the embedded integer.
                             Whenthesearch procedure extends a partial program by a new function, it has to try the functions
                             in the DSL in some order. At this point DFS can opt to consider the functions as ordered by their
                             predicted probabilities from the neural network. The probability of a function consisting of a higher-
                             order function and a lambda is taken to be the minimum of the probabilities of the two constituent
                             functions.
                             E TRAININGLOSSFUNCTION
                             In Sect. 4.5 we outlined a justiﬁcation for using marginal probabilities of individual functions as
                             a sensible intermediate representation to provide a solver employing a Sort and add scheme (we
                             considered Enumerative search and the Sketch solver with this scheme). Here we provide a more
                             detailed discussion.
                             Predicting program components from input-output examples can be cast as a multilabel classiﬁcation
                             problem, where each instance (set of input-output examples) is associated with a set of relevant
                             labels (functions appearing in the code that generated the examples). We denote the number of labels
                             (functions) by C, and note that throughout this work C = 34.
                             Whenthetaskistopredict a subset of labels y ∈ {0,1}C, different loss functions can be employed
                             to measure the prediction error of a classiﬁer h(x) or ranking function f(x). Dembczynski et al.
                             (2010) discuss the following three loss functions:
                                    • Hamminglosscountsthenumberoflabelsthat are predicted incorrectly by a classiﬁer h:
                                                                                        C
                                                                     LH(y,h(x)) = X1
                                                                                             {y 6=h (x)}
                                                                                               c   c
                                                                                       c=1
                                    • Rank loss counts the number of label pairs violating the condition that relevant labels are
                                       ranked higher than irrelevant ones by a scoring function f:
                                                                                         C
                                                                  Lr(y,f(x)) =          X 1
                                                                                                   {f <f }
                                                                                                     i   j
                                                                                   (i,j):y =1,y =0
                                                                                        i     j
                                    • Subset Zero-One loss indicates whether all labels have been correctly predicted by h:
                                                                         Ls(y,h(x)) = 1{y6=h(x)}
                                                                                 16
