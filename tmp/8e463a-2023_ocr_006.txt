For experiments in the zero-shot setting, our inputs
are structured representations of new knowledge
and questions to be answered by the model. For
experiments in the few-shot setting, we include sev-
eral examples of the same form together with the
answers, which we hope will help the model un-
derstand. For the zero-shot CoT, we append "Let’s
think step by step." at the end of questions, and
for the few-shot CoT, the reasoning process for the
answers of examples is also attached. Please refer
to Appendix D for the detailed method description.
An example of prompt used in our experiment is
shown in Table 12.

4.3 Evaluation Metric

Since our questions are in the form of multiple
choice, fill-in-the-blank or Boolean questions, the
golden answers are usually just one or a few words.
Therefore, we determine the correctness of the
model output by matching it with the answer (like
Accuracy). Since there may be more than one possi-
ble answer to some questions, such as those asking
about the geographical distribution of entities, etc.,
we consider the answer to be correct as long as
it matches one of the correct answers. This is a
less stringent measurement, but as seen in Section
5, most models still perform poorly. Using the
proposed dataset, we evaluate each model’s ability
with each method for knowledge understanding,
knowledge differentiation, and knowledge associa-
tion, respectively. We report the average score on
the entire benchmark in each setting.

4.4 Data Filtering

Since there are differences of internal/existing
knowledge of different models due to the differ-
ent model size and training data, we further fil-
ter the samples in our dataset for each model be-
fore testing, based on previous work (Petroni et al.,
2019), in order not to be influenced by the differ-
ence (which is not the theme of our paper) and to
compare the models’ performance in face of new
knowledge in a more focused and fair way. For
our method of filtering questions, please refer to
Appendix E. We experiment and analyze the four
models mentioned in Section 4.1 based on the fil-
tered new knowledge, using the evaluation settings
introduced in Section 4.2.

5 Result and Analysis
5.1 Overall Results

The performance of the LLMs on our benchmark
under different settings is shown in Table 1. We can
see that ChatGPT has the best performance in all
settings, which is consistent with our usual beliefs.
Vicuna has the second best performance among all
models. In terms of methods, the few-shot setting
performs better than the zero-shot setting overall,
and CoT performs better than the vanilla form in
most cases.

In face of new knowledge, as seen in Table 1,
LLMs do perform poorly except for ChatGPT on
KU and KD experiments. Among all abilities,
knowledge association is obviously the most diffi-
cult for LLMs, and all of them have difficulty in
relating to their internal knowledge through new
knowledge provided, and thus in making multi-hop
reasoning correctly. The performance of knowl-
edge understanding and knowledge differentiation
is better than that of knowledge association, but yet
not satisfactory for most LLMs.

In summary, current LLMs perform relatively
poorly in face of new knowledge, slightly better
in knowledge understanding and knowledge dif-
ferentiation, and have more difficulty in reasoning
across new and existing knowledge. In order to
have a clearer view of models output, please refer
to Appendix F for the analysis of models output.

Considering that it is expensive, slow and un-
stable to call ChatGPT’s API, without loss of gen-
erality, all the following comparison experiments
for analysis are conducted on three other models.
In addition, for convenience, the following analy-
sis experiments are performed in the setting of the
vanilla few-shot method, and structured input arti-
ficial entity knowledge, if not specifically stated.

5.2. Impact of Entity Similarity

In this section we explore the effect of the similar-
ity between the artificial entity and the parent entity
on the model performance over the KD questions,
which are designed to assess the model’s ability
to distinguish between new knowledge and exist-
ing knowledge. Specifically, we explore attribute
similarity and name similarity.

The More Similar, the More Confusing (unless
powerful enough) We define the proportion of
overlap of properties between entities as the prop-
erty similarity of entities. As shown in Figure 3,
