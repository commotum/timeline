                     Published as a conference paper at ICLR 2025
                     AsshowninTable9,theabsenceoftheintrospection mechanism had the most significant negative effect,
                     increasing the perplexity to 17.3 and raising FLOPs to 89.4G. Removing FPI mechanisms from the attention
                     andFFNlayersalsoresultedinadegradationofperformance,highlightingtheimportanceofthesecomponents
                     in maintaining computational efficiency and reducing perplexity. The introspection network plays a critical
                     role in dynamically allocating resources based on input complexity, leading to the superior performance of
                     the full MIND-Transformer.
                     F.2   EARLY EXIT EXPERIMENTS
                     In early exit experiments, we compared MIND’s introspection-based early exit mechanism with BranchyNet
                     and ResNet-50. Experiments were conducted on CIFAR-100 and ImageNet, where input complexity was
                     analyzed using softmax entropy. The MIND model’s introspection network dynamically adjusted the
                     number of layers and FPI iterations based on input complexity, allowing early exits for simpler inputs.
                     Specifically, inputs with softmax entropy below 0.4 typically used only 2-3 layers, reducing computation
                     by30-50%. Inference time was measured across different input complexities, showing that MIND achieved
                     a 28% reduction in average inference time compared to static models with early exits, while maintaining
                     a comparable accuracy of 88.2% Top-1 on ImageNet. Early exits were activated dynamically based on
                     introspection, leading to a reduction in FLOPs by 20-30% for lower complexity inputs. BranchyNet, in
                     contrast, used fixed threshold-based early exits, which underperformed MIND in both efficiency and accuracy.
                     TheMINDmodelrepresentsadistinctapproachfromCALM(Schusteretal.,2022)andLayerSkip(Elhoushi
                     et al., 2024), focusing on lightweight architectures for vision and simpler language tasks. The MIND-
                     Transformer employs a learned introspection mechanism that dynamically adjusts computation based on input
                     complexity, requiring minimal memory overhead. In contrast, CALM’s (Schuster et al., 2022) confidence-
                     based strategy requires additional classifiers and 15% memory overhead, while LayerSkip’s layer dropout
                     approachshowslargerperformancedegradationwith10%memoryoverhead. AllevaluationsusedBERT-base
                     as the foundation model, tested on WikiText-103 for language modeling, CNN/DailyMail (Nallapati et al.,
                     2016) for summarization, and SQuAD v2.0 (Rajpurkar et al., 2018) for question answering. The MIND
                     model’s adaptive computation framework shows particular promise for future integration with larger language
                     models, potentially combining benefits from both confidence-based and layer-dropout approaches while
                     maintaining computational efficiency as shown in Table
                                       Table 10: Performance Metrics on models like LayerSkip and CALM
                              Model                ROUGE-1(%) ROUGE-2(%) Avg.InferenceTime(ms)
                              MIND-Transformer          42.3             19.8                   180
                              CALM                      41.9             19.5                   165
                              LayerSkip                 41.5             19.2                   210
                     The results demonstrate that the general introspection+FPI approach presented in our paper can be used
                     with the Transformer architecture (MIND-Transformer) to offer an excellent balance between efficiency,
                     implementation complexity, and performance maintenance. Although a more specialized approach like
                     CALMmayachievea9%higheraveragespeedupwhilehavingaperformancedropof0.3%,thegeneralityof
                     our approach may offer further opportunities for improvement. This makes MIND-Transformer, in particular,
                     specifically suitable for practical applications where memory constraints and implementation simplicity are
                     important considerations alongside computational efficiency.
                     F.3   ABLATION ON INPUT COMPLEXITY
                     To evaluate robustness and efficiency, we conducted extensive ablation studies to analyze the impact of
                     individual components. The results are summarized in Table 11, which provides an overview of model
                     performance, computational cost, and inference time under various configurations. The full model, which
                                                                 29
