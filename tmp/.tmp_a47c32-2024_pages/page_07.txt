                       Baseline (DiT)
                                        1 Iter.        2 Iters.
                                                                     3 Iters.       5 Iters.       6 Iters.       8 Iters.
                                                                                                                                              18 Iters.
                                                                                                                                12 Iters.
                                                                                                                                                              26 Iters.     33 Iters.      38 Iters.      54 Iters.      68 Iters.
                      Figure 5. Qualitative Results for Smoothing Computation Across Timesteps. We show visual results of FPDM using different numbers of fixed point
                      solving iterations, while keeping the total amount of sampling compute fixed (560 transformer blocks). Our method demonstrates similar performance
                      compared to the baseline with 20 to 30 iterations per timestep and superior generation quality with 4 to 8 iterations, as reflected quantitatively in Fig. 4.
                                Train Iters. (M, N)                     3          6         12            24                     5. Analysis and Ablation Studies
                                FID                                43.0        43.2       61.5        567.6                       5.1. Smoothing Computation Across Timesteps
                     Table 3. Performance For Varying Numbers of Fixed Point Iterations                                           InFig.4and5,weexaminetheeffectofsmoothingtimesteps
                      in Training. This table compares various choices of M and N values in                                       as described in Sec. 3.3. We sample across a range of fixed
                     Algorithm 1. They represent a tradeoff between training speed and fixed                                      point iterations and timesteps, keeping the total sampling
                      point convergence accuracy. Results indicate that the optimal values for M                                  cost (i.e. the total number of transformer block forward
                      and N range from 3 to 6.                                                                                    passes) constant. This explores the trade-off between the
                             Train iters without grad (N)                               Method             FID                    convergence of the fixed point iteration at each timestep and
                                                                        JFB(1-Step Grad)                 567.6                    the discretization error of the larger diffusion process.
                             6                                               Multi-Step JFB                48.2                        Balancingthenumberofiterationsandtimestepsiskeyto
                                                                             Stochastic JFB                43.2                   obtaining optimal performance. Intuitively, when using very
                                                                        JFB(1-Step Grad)                 567.6                    few iterations per timestep, the process fails to converge ade-
                             12                                              Multi-Step JFB                79.9                   quately at each step, and the resulting error compounds. Con-
                                                                             Stochastic JFB                61.5                   versely, allocating too many iterations to too few timesteps
                                                                                                                                  results in unnecessary computation on already converged
                     Table 4. Performance of Stochastic Jacobian-Free Backpropagation (S-                                         solving iterations, resulting in discretization errors arising
                     JFB)comparedtoJFB(1-stepgradient). Wefindthat1-stepgradient,the                                              from larger gaps between timesteps. An ideal strategy in-
                      most commonmethodfortraining DEQ[5] models, struggles to optimize                                           volves using just enough fixed-point iterations to achieve
                      models on the large-scale ImageNet dataset, whereas a multi-step version                                    a satisfactory solution, thereby maximizing the number of
                      of it performs well and our stochastic multi-step version performs (S-JFB)
                      even better. The 1-step gradient always unrolls with gradient through a                                     possible timesteps. For instance, with 280 transformer block
                      single iteration (M = 1) of fixed point solving, whereas the stochastic                                     forward passes, we see that the optimal range lies between 4
                     version unrolls though m âˆ¼ U(1,M) iterations for M = 12.                                                     and 8 iterations per timestep.
                           Iters. per Step         3          5           6           8          12          26                   5.2. Reallocating Computation Across Timesteps
                           Constant                48.0       45.8        46.6        47.3       48.5        62.5                 In Tab. 5, the increasing heuristic outperforms the contant
                           Decreasing              48.0       46.3        47.3        48.3       49.1        63.2                 and decreasing ones; allocating resources more toward the
                           Increasing              46.7       44.8        45.9        45.6       48.0        61.7                 later stages of the denoising process improves generation
                     Table 5. Performance of Iteration Allocation Heuristics. Constant                                            quality and detail. Note that such flexibility in resource allo-
                      uses a fixed iteration count per diffusion timestep, while Increasing and                                   cation is a novel feature of FPDM, not possible in previous
                     Decreasing vary their iteration counts linearly with respect to the timestep.                                explicit diffusion models.
                      smoothing. Our model produces sharper images with more                                                      5.3. Reusing Solutions
                      detail, likely due to its ability to spread the computation                                                 Asdescribed in Sec. 3.3, we explore reusing the fixed point
                      amongtimesteps, as discussed in Sec. 5.1.                                                                   solution from each timestep to initialize the subsequent step.
                                                                                                                            7
