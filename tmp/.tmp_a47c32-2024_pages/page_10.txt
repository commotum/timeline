                    IEEETransactions on Computational Imaging, 7:1123–1133,                International Conference on Learning Representations, 2021.
                    2021. 3                                                                2
              [20] Andrzej Granas and James Dugundji. Fixed point theory.             [34] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan
                    Springer, 2003. 3                                                      Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffu-
              [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-               sion probabilistic model sampling in around 10 steps. arXiv
                    hard Nessler, and Sepp Hochreiter. Gans trained by a two               preprint arXiv:2206.00927, 2022. 6
                    time-scale update rule converge to a local nash equilibrium.      [35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
                    In Advances in Neural Information Processing Systems 30:               Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
                    Annual Conference on Neural Information Processing Sys-                MarkChen. Glide: Towards photorealistic image generation
                    tems2017,December4-9,2017,LongBeach,CA,USA,pages                       and editing with text-guided diffusion models. arXiv preprint
                    6626–6637, 2017. 6                                                     arXiv:2112.10741, 2021. 2
              [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-        [36] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,
                    sion probabilistic models. In Advances in Neural Information           Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever,
                    Processing Systems, pages 6840–6851, 2020. 1, 2                        and Mark Chen. GLIDE: Towards photorealistic image gen-
              [23] V.I. Istratescu. Fixed Point Theory: An Introduction. Springer          eration and editing with text-guided diffusion models. In
                    Dordrecht, Dordrecht, 1 edition, 1981. eBook Packages                  International ConferenceonMachineLearning,pages16784–
                    Springer Book Archive. 3                                               16804, 2022. 1
              [24] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.         [37] William Peebles and Saining Xie. Scalable diffusion models
                    Progressive growing of gans for improved quality, stability,           with transformers. CoRR, abs/2212.09748, 2022. 1, 2, 5, 6
                    and variation. ArXiv, abs/1710.10196, 2017. 1, 2
              [25] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.           [38] Ashwini Pokle, Zhengyang Geng, and J Zico Kolter. Deep
                    Elucidating the design space of diffusion-based generative             equilibrium approaches to diffusion models. Advances in
                    models. arXiv preprint arXiv:2206.00364, 2022. 6                       Neural Information Processing Systems, 35:37975–37990,
              [26] Patrick Kidger, James Morrill, James Foster, and Terry J.               2022. 3
                    Lyons. Neural controlled differential equations for irregular     [39] Machel Reid, Edison Marrese-Taylor, and Yutaka Matsuo.
                    time series. In Advances in Neural Information Processing              Subformer: Exploringweightsharingforparameterefficiency
                    Systems 33: Annual Conference on Neural Information Pro-               in generative transformers. In Findings of the Association for
                    cessing Systems 2020, NeurIPS 2020, December 6-12, 2020,               Computational Linguistics: EMNLP 2021, pages 4081–4090,
                    virtual, 2020. 3                                                       2021. 9
              [27] Patrick Kidger, James Foster, Xuechen Li, and Terry J. Lyons.      [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
                    Neural sdes as infinite-dimensional gans. In Proceedings                                      ¨
                                                                                           Patrick Esser, and Bjorn Ommer. High-resolution image
                    of the 38th International Conference on Machine Learning,              synthesis with latent diffusion models. In IEEE Conference
                    ICML2021,18-24July2021,VirtualEvent,pages5453–5463.                    onComputerVisionandPattern Recognition, pages 10684–
                    PMLR,2021. 3                                                           10695, 2022. 1, 2, 3, 5
              [28] Diederik P. Kingma and Max Welling. Auto-encoding varia-           [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
                    tional bayes. In 2nd International Conference on Learning              Convolutional networks for biomedical image segmentation.
                    Representations, ICLR 2014, Banff, AB, Canada, April 14-16,            In Medical Image Computing and Computer-Assisted Inter-
                    2014, Conference Track Proceedings, 2014. 2, 3, 5                      vention - MICCAI 2015 - 18th International Conference Mu-
              [29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Im-            nich, Germany, October 5 - 9, 2015, Proceedings, Part III,
                    agenet classification with deep convolutional neural networks.         2015. 1, 2
                    In Advances in Neural Information Processing Systems 25:          [42] W. Rudin. Principles of Mathematical Analysis. McGraw-
                    26th Annual Conference on Neural Information Processing                Hill, 3 edition, 1976. 5
                    Systems 2012. Proceedings of a meeting held December 3-6,         [43] Tim Salimans and Jonathan Ho. Progressive distillation for
                    2012, Lake Tahoe, Nevada, United States, pages 1106–1114,              fast sampling of diffusion models. In International Confer-
                    2012. 3                                                                ence on Learning Representations, 2021. 5
              [30] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
                    Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert      [44] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
                    for self-supervised learning of language representations. In           Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
                    International Conference on Learning Representations, 2019.            Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-
                    9                                                                      400M: open dataset of clip-filtered 400 million image-text
                                     ´                                                     pairs. CoRR, abs/2111.02114, 2021. 1
              [31] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick
                    Haffner. Gradient-based learning applied to document recog-       [45] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
                    nition. Proc. IEEE, 86(11):2278–2324, 1998. 3                          CadeGordon,RossWightman,MehdiCherti,TheoCoombes,
              [32] ShanchuanLin,BingchenLiu,JiashiLi,andXiaoYang. Com-                     Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick
                    mondiffusion noise schedules and sample steps are flawed.              Schramowski, Srivatsa Kundurthy, Katherine Crowson, Lud-
                    CoRR,abs/2305.08891, 2023. 5, 6                                        wig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-
              [33] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo                   5B: an open large-scale dataset for training next generation
                    numerical methods for diffusion models on manifolds. In                image-text models. In NeurIPS, 2022. 1, 9
                                                                                 10
