                 LETTER                                                 CommunicatedbyYannLeCun
                 AFastLearningAlgorithmforDeepBeliefNets
                 GeoffreyE.Hinton
                 hinton@cs.toronto.edu
                 SimonOsindero
                 osindero@cs.toronto.edu
                 Department of Computer Science, University of Toronto, Toronto, Canada M5S 3G4
                 Yee-WhyeTeh
                 tehyw@comp.nus.edu.sg
                 Department of Computer Science, National University of Singapore,
                 Singapore 117543
                 Weshowhowtouse“complementarypriors”toeliminatetheexplaining-
                 awayeffectsthatmakeinferencedifﬁcultindenselyconnectedbeliefnets
                 that have many hidden layers. Using complementary priors, we derive a
                 fast, greedy algorithm that can learn deep, directed belief networks one
                 layer at a time, provided the top two layers form an undirected associa-
                 tive memory. The fast, greedy algorithm is used to initialize a slower
                 learning procedure that ﬁne-tunes the weights using a contrastive ver-
                 sionofthewake-sleepalgorithm.Afterﬁne-tuning,anetworkwiththree
                 hidden layers forms a very good generative model of the joint distribu-
                 tion of handwritten digit images and their labels. This generative model
                 gives better digit classiﬁcation than the best discriminative learning al-
                 gorithms. The low-dimensional manifolds on which the digits lie are
                 modeled by long ravines in the free-energy landscape of the top-level
                 associative memory, and it is easy to explore these ravines by using the
                 directedconnectionstodisplaywhattheassociativememoryhasinmind.
                 1 Introduction
                 Learning is difﬁcult in densely connected, directed belief nets that have
                 manyhiddenlayersbecause it is difﬁcult to infer the conditional distribu-
                 tion of the hidden activities when given a data vector. Variational methods
                 use simple approximations to the true conditional distribution, but the ap-
                 proximations may be poor, especially at the deepest hidden layer, where
                 thepriorassumesindependence.Also,variationallearningstillrequiresall
                 of the parameters to be learned together and this makes the learning time
                 scale poorly as the number of parameters increases.
                     Wedescribe a model in which the top two hidden layers form an undi-
                 rected associative memory (see Figure 1) and the remaining hidden layers
                 Neural Computation 18, 1527–1554 (2006)   C 2006 Massachusetts Institute of Technology
