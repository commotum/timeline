                           Published as a conference paper at ICLR 2022
                           A.1   ABLATION STUDIES
                           In the following section, we performed ablation studies to investigate the effects of various hy-
                           perparameters. Unless otherwise speciﬁed, we carried out these experiments with a memorizing
                           transformer with context size 512, XL cache 512 with a memory size of 8192.
                           Multiple kNNlayers.     WeexperimentedwithusingtwokNNlayers,ratherthanjustone. However,
                           wedidnotseefurther beneﬁts brought by more than multiple retrieval layers.
                           kNNlayerindex Weexperimentedwithaddingtheexternalmemorytolayer3,6,9and12ina
                           12-layer transformer, with results shown in Table 14. We found that adding memory to the middle of
                           the layer stack will obtain the best result, whereas adding memory to layers either too close to the
                           input or to the output obtained less gains.
                                                            Table 14: Different layer index.
                                                              Layer index     Perplexity
                                                              3               2.40
                                                              6               2.36
                                                              9               2.37
                                                              12              2.43
                           Numberofneighbors Westudiedtheeffectsofthenumberofneighborsweretrievefrommemory,
                           with results shown in Table 15. We found that even with 32 number of neighbors, we can already
                           obtain a comparable results with 128 or 256 neighbors.
                                                            Table 15: Number of neighbors.
                                                          Numberofneighbors        Perplexity
                                                          32                       2.38
                                                          128                      2.37
                                                          256                      2.37
                           Randomseeds Wemeasuredthestatisticalsigniﬁcant of the results reported. We did 3 runs with
                           3randomseedsforTransformerXLofsize512,andalsoamemorizingtransformerwithmemory
                           size 8192. We measured the standard deviation of perplexities after 500K steps of training, shown in
                           Table 16. We saw the standard deviation between different runs of the same experiment appears to be
                           muchsmaller than the gap between different models.
                                                               Table 16: Random seeds.
                                                         Models                   Perplexity
                                                         Transformer XL           2.67 ±0.01
                                                         Memorizing Transformer   2.37 ±0.005
                                                                          15
