                             Published as a conference paper at ICLR 2015
                             A.2    DETAILED DESCRIPTION OF THE MODEL
                             A.2.1    ENCODER
                             In this section, we describe in detail the architecture of the proposed model (RNNsearch) used in the
                             experiments (see Sec. 4–5). From here on, we omit all bias terms in order to increase readability.
                             Themodeltakesasourcesentence of 1-of-K coded word vectors as input
                                                                                               K
                                                                 x=(x ,...,x ), x ∈R x
                                                                         1        Tx     i
                             and outputs a translated sentence of 1-of-K coded word vectors
                                                                 y=(y ,...,y ), y ∈ RKy,
                                                                         1       Ty     i
                             where K and K are the vocabulary sizes of source and target languages, respectively. T and T
                                      x        y                                                                             x       y
                             respectively denote the lengths of source and target sentences.
                             First, the forward states of the bidirectional recurrent neural network (BiRNN) are computed:
                                                       →−     (       →−     →−       →−    →−
                                                        hi = (1− zi)◦ hi−1+ zi◦ hi ,ifi>0
                                                                0                                 , if i = 0
                             where                                                    h             i
                                                            →−            −→        →−  →−    →−
                                                            h =tanh WEx +U r ◦ h
                                                              i                 i          i     i−1
                                                            →−      −→          →− →−     
                                                            z =σ W Ex +U h
                                                              i      z      i      z   i−1
                                                            →−       −→          →− →−
                                                            r =σ W Ex +U h                   .
                                                              i         r    i      r   i−1
                                     m×Kx                                       −→ −→ −→            n×m →− →−      →−        n×n
                             E ∈ R           is the word embedding matrix. W,Wz,Wr ∈ R                   , U, Uz, Ur ∈ R           are
                             weight matrices. m and n are the word embedding dimensionality and the number of hidden units,
                             respectively. σ(·) is as usual a logistic sigmoid function.
                                                    ←−        ←−
                             Thebackwardstates(h1,··· , hT )arecomputedsimilarly. Wesharethewordembeddingmatrix
                                                                  x
                             EbetweentheforwardandbackwardRNNs,unliketheweightmatrices.
                             We concatenate the forward and backward states to to obtain the annotations (h ,h ,··· ,h              ),
                                                                                                                     1   2        Tx
                             where                                              "       #
                                                                                   →−
                                                                          h =      hi                                              (7)
                                                                           i       ←−
                                                                                   hi
                             A.2.2    DECODER
                             Thehiddenstate s of the decoder given the annotations from the encoder is computed by
                                                i
                                                                  s =(1−z )◦s          +z ◦s˜,
                                                                   i         i     i−1     i    i
                             where
                                                          s˜ =tanh(WEy           +U[r ◦s        ] + Cc )
                                                           i                i−1         i   i−1        i
                                                          z =σ(W Ey          +U s       +C c )
                                                           i         z   i−1      z i−1      z i
                                                          r =σ(W Ey          +U s       +C c )
                                                           i         r   i−1      r i−1      r i
                             Eisthewordembeddingmatrix for the target language. W,W ,W ∈ Rn×m, U,U ,U ∈ Rn×n,
                                                                                                z    r                z   r
                             and C,C ,C ∈ Rn×2n are weights. Again, m and n are the word embedding dimensionality
                                       z   r
                             and the number of hidden units, respectively. The initial hidden state s         is computed by s      =
                                       ←−                                                                 0                    0
                             tanh Wsh1 ,whereWs ∈Rn×n.
                             Thecontext vector ci are recomputed at each step by the alignment model:
                                                                               Tx
                                                                         ci =Xαijhj,
                                                                               j=1
                                                                                13
