                                                                                                 TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                   significant distribution shifts in the semi-private evaluation                                                                                                     70                                                        Data Ablations                   Optimization Ablations
                                   dataset. If the semi-private evaluation results become pub-                                                                                                        60                                         57.8                                           59.8
                                   licly available in future, we will provide a detailed analysis                                                                                                                                                                55.7                                           54.4           55.9
                                                                                                                                                                                                      50                          50.5                                          51.5
                                   of these performance differences.                                                                                                                                              40.9
                                                                                                                                                                                                      40
                                   5. BIG-Bench Hard                                                                                                                                                  30
                                                                                                                                                                                                     Accuracy (%)20
                                   5.1. Background                                                                                                                                                    10
                                   BIG-Bench Hard (BBH; Srivastava et al., 2023; Suzgun                                                                                                                 0                                          T                                              T
                                                                                                                                                                                                                                  ICL            TT                                              T
                                   et al., 2023) is a benchmark comprising 27 challenging tasks                                                                                                             Zero-Shot                                 No Example          Direct I/O    Shared T
                                                                                                                                                                                                                                                        ermutations                                 No Demo Loss      and Outputs
                                   across 23 task types, designed to evaluate large language                                                                                                                                                           P                                                          Loss on Inputs
                                   models on reasoning, compositionality, and generalization.                                                                                                  Figure 8. Overall BIG-Bench Hard Results. TTT outperforms
                                   UnlikeARC,BBHfeaturesabroadernaturallanguagestruc-                                                                                                          standard in-context learning by 7.3 absolute percentage points,
                                   ture and lacks a shared input format, making it unsuitable                                                                                                  from 50.5% to 57.8%. Our performance improvement over direct
                                   for invertible transformations. However, this broader scope                                                                                                 input-output data shows that using in-context leave-one-out tasks
                                   offers a valuable testbed for evaluating TTT’s effectiveness                                                                                                is crucial. Not taking demonstration loss or taking loss on inputs
                                   in a more generalized setting. Despite the absence of invert-                                                                                               results in a performance decrease. Unlike with ARC, using a
                                   ible transformations—previouslyusedinARCtoexpandthe                                                                                                         shared adapter across all tasks improves performance.
                                   TTTdataset and enhance inference—TTT still significantly
                                   improves performance on BBH.
                                                                                                                                                                                               benefit the most from TTT (Section 5.4). Unlike with ARC,
                                   5.2. Experimental Details                                                                                                                                   wedonothaveacollection of invertible transformations to
                                   Model architecture & optimization                                                                        We use Llama                                       run augmented inference. Instead, we use greedy decoding.
                                   3.1 (8B; Llama Team, 2024). For each task d, we train                                                                                                       Further hyperparameter details and evaluation details are
                                   a separate set of LoRA parameters at test-time, with a LoRA                                                                                                 given in Appendix F.2.
                                   rank of 64 over 40 random shuffles of the demonstration                                                                                                     5.3. Impact of TTT Design
                                   pairs to produce leave-one-out in-context tasks. More hy-
                                   perparameter details are given in Appendix F.1.                                                                                                             In this section, we evaluate our method and its ablations,
                                   OnBIG-Bench Hard, our base language model is able to                                                                                                        primarily comparing the zero-shot baseline, ICL, and TTT.
                                   achieve non-trivial scores out-of-the-box. Consequently, we                                                                                                 NoExamplePermutationupdatesthemodelonasingle
                                   do not perform any initial fine-tuning on synthetic tasks                                                                                                   in-context prompt instead of multiple shuffled versions. Di-
                                   outside of BBH like we do for ARC. Furthermore, since                                                                                                       rect I/O treats each input-output pair as separate training
                                   models achieve nonzero performance in a zero-shot setting,                                                                                                  instances. Shared TTT uses a single adapter across tasks
                                   weprovide the zero-shot results and analyze how TTT and                                                                                                     instead of task-specific adapters. No Demonstration Loss
                                   ICLimproveuponthem.                                                                                                                                         removes the loss applied to demonstration outputs. Loss
                                                                                                                                                                                               on Inputs and Outputs extends the loss calculation to
                                   Evaluation                         For the 27 tasks in BBH, we consider the 10-                                                                             both inputs and outputs. These ablations are as detailed in
                                   shot setting, where we select 10 random pairs from each                                                                                                     Section 3. As these results are averages over 5 runs, the
                                   task’s dataset to be demonstration pairs and evaluate on                                                                                                    standard errors of the mean for each method are given in
                                   the remaining data. Each of the 27 tasks is analogous to                                                                                                    Appendix F.1, averaging 0.4%.
                                   a single ARC task, consisting of 10 labeled examples as                                                                                                     TheresultsinFigure8showthatTTTachievesanoverallac-
                                   demonstration pairs given at test-time. We report average                                                                                                   curacy of 57.8%, outperforming standard ICL (50.5%) and
                                   results over five random seeds, where each seed specifies                                                                                                   Direct I/O learning (51.5%). This demonstrates that TTT’s
                                   which10examplesformthedemonstrationsubset. Formore                                                                                                          capabilities extend beyond ARC to more diverse and com-
                                   control over the evaluation process with test-time training,                                                                                                plex reasoning tasks, proving its effectiveness in a broader
                                   we write our own evaluation function, which is available                                                                                                    range of natural language problem-solving scenarios.
                                   in our codebase (for more details, see Appendix F.1). The
                                   number of evaluation examples for each task is then 240                                                                                                     We observe that TTT without example permuta-
                                   for all tasks except three: Causal Judgment, Penguins in a                                                                                                  tions—performing multiple gradient steps on a single
                                  Table, and Snarks, which have 177, 136, and 168 evaluation                                                                                                   in-context prompt before inference—reduces accuracy to a
                                   examples respectively. Note that the large number of evalua-                                                                                                still-impressive 55.7%. Computing the loss only on the test
                                   tion samples for each task compared to ARC means we can                                                                                                     output lowers accuracy to 54.4%, while applying it to both
                                   doatask-specific analysis to analyze which types of tasks                                                                                                   inputs and outputs achieves 55.9%.
                                                                                                                                                                                        7
