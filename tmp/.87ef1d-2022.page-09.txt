                                  Published as a conference paper at ICLR 2022
                                  Figure 7: Difference in loss for each token in a randomly chosen paper, using the same model once
                                  with a memory size of 8K and once with 32K. Higher numbers mean the longer memory helped in
                                  comparison to the shorter memory. This paper is 22K tokens long.
                                    Query index  Input   Target  Surrounding context                 Retrieved index  Retrieved surrounding context
                                    29721        mark    ov      rule prob_space. markov_inequality  8088             M.t\<le>Xa}\<le>expectationX/t"
                                    40919        _       th      =( subgraph_threshold H n / p n)    27219            threshold H n = n powr (-(1 / max_density’
                                    49699        S       w       assumes " orthonormal_system S w"   28050            deﬁnition orthonormal_system :: "
                                  Table 8: Examples of memory retrieval in the Isabelle dataset. The model is able to ﬁnd the deﬁnition of
                                  a lemma from a reference to it. The retrieved surrounding context (highlighted) is the deﬁnition body of the
                                  mathematical object highlighted in the querying context.
                                  Retrieving mathematical deﬁnitions. Our case study on the Isabelle corpus provides one of the
                                  clearest illustrations of how a model can make good use of external memory. When predicting the
                                  nameofamathematical object or a lemma, the model looked up the deﬁnition from earlier in the
                                  proof. Examples of this behavior are shown in Table 8. In example 1, the model retrieves a deﬁnition
                                  within the body of a lemma, markov_inequality. In example 2, it retrieves the deﬁnition of a
                                  previously deﬁned concept subgraph_threshold. In example 3, it retrieves the deﬁnition of
                                  orthonormal_system. Wemanuallychecked10exampleswherethemodelmadeaprediction
                                  of lemma names, and 8 out of 10 times model found the body of the lemma it needs to predict. In
                                  the other two cases, the model also looked up materials in the immediate vicinity. To the best of
                                  our knowledge, this is the ﬁrst demonstration that attention is capable of looking up deﬁnitions and
                                  function bodies from a large corpus. The Isabelle case study used a model with two memory layers
                                  of size 32K.
                                  5     CONCLUSION
                                  Wepresent a simple extension to the Transformer architecture, called kNN-augmented attention,
                                  which dramatically increases the length of the context that a language model can attend to by using
                                  k-nearest-neighbor lookup into a large external memory. We demonstrate the effectiveness of external
                                  memory in a series of language modeling experiments over a variety of long-document datasets,
                                  including LaTeX documents, source code, formal proofs, and books.
                                  TheMemorizingTransformer shows large improvements in perplexity over the baseline for all of
                                  the data sets and architectures that we studied; it is comparable to a vanilla transformer that has
                                  5 times the number of parameters. Perplexity continues to improve with increasing memory size,
                                  although there is a point of diminishing returns. Moreover, external memory continues to provide
                                  beneﬁts even as the transformer is scaled up from 200M to 8B parameters. Perhaps most intriguingly,
                                  a Memorizing Transformer does not need to be pre-trained from scratch; it is possible obtain large
                                  gains from adding memory to an existing pre-trained model, and then ﬁne-tuning it.
                                  Unlike other forms of attention, kNN retrieval can be easily scaled up to huge memory sizes, and is
                                  thus potentially able to leverage vast knowledge bases or code repositories. How to make the best use
                                  of this capability is a topic for future work.
                                  ACKNOWLEDGMENTS
                                  WewanttothankCharlesStaatsforthemanyfruitful discussions and detailed comments, Henryk
                                  Michalewskiforearlyversionofofthememoryimplementation,PetrosManiatisforhishelpwithour
                                  code datasets, Aitor Lewkowycz for his help with larger scale memorizing transformer experiments,
                                  Behnam Neyshabur for his comments on ﬁnetuning non-memory models, Imanol Schlag for his
                                  proofreadanddetailedcomments,andDennisLeeandManzilZaheerfordiscussionsaboutlarge-scale
                                  attention and retrieval.
                                                                                                9
