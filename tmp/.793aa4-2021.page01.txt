                              ASimpleandEffectivePositional Encoding for Transformers
                                                                ∗                 ∗                              ∗
                                             Pu-ChinChen,HenryTsai,SrinadhBhojanapalli,
                                          HyungWonChung,Yin-WenChang,Chun-SungFerng
                                                                     Google Research
                                          Abstract                                additional segment embeddings can be added just
                        Transformer models are permutation equivari-              like the position embeddings (Devlin et al., 2018).
                        ant.  To supply the order and type informa-                  Therehavebeenmultipleworksexploringdiffer-
                        tion of the input tokens, position and seg-               ent ways to include position information in Trans-
                        ment embeddings are usually added to the in-              formers (Shaw et al., 2018; Yang et al., 2019; Raf-
                        put. Recent works proposed variations of po-              fel et al., 2020). Many of those note the advan-
                        sitional encodings with relative position en-             tages of using a relative position encoding scheme
                        codings achieving better performance.        Our          over absolute position encodings (see also Fig 1).
                        analysis shows that the gain actually comes               However what causes this difference is not clear.
                        from moving positional information to atten-              Yun et al. (2020) have shown that Transformers
                        tion layer from the input. Motivated by this,
                        we introduce Decoupled posItional attEntion               with absolute position encodings are universal ap-
                        for Transformers (DIET), a simple yet effec-              proximators of all sequence to sequence functions,
                        tive mechanism to encode position and seg-                proving that absolute position encodings can cap-
                        ment information into the Transformer mod-                ture the position information. Hence what causes
                        els. The proposed method has faster training              the superiority of relative position encodings? A
                        and inference time, while achieving compet-               systematic study and understanding of the beneﬁts
                        itive performance on GLUE, XTREME and                     anddrawbacksofdifferent position encoding meth-
                        WMTbenchmarks. We further generalize our                  ods is missing. Ke et al. (2020) hypothesised that
                        method to long-range transformers and show
                        performance gain.                                         the cross correlation between word and position
                   1    Introduction                                              embeddings while computing attention could be
                                                                                  the cause of poor performance of absolute position
                   Transformers are sequence-to-sequence models                   encodings. However such cross terms are present
                   that achieve state of the art performance in many              in some of the relative position encoding methods
                   Natural Language Processing (NLP) tasks, such as               (Shaw et al., 2018; Yang et al., 2019), and these
                   machine translation, language modeling and ques-               methods perform on par or better than the other
                   tion answering (Vaswani et al., 2017; Devlin et al.,           position encoding schemes (see §4).
                   2018; Yang et al., 2019; Liu et al., 2020). Trans-                In this paper we undertake a systematic study
                   formers have two major components: self-attention              to understand different position encoding methods.
                   and a position-wise feed forward layer. Both are               Wearguethatabsolutepositionembeddingsmainly
                   permutation equivariant and are not sensitive to               suffer from being addedattheinput. Weshow,with
                   the order of input tokens. To make these mod-                  our experiments on classiﬁcation, question answer-
                   els position-aware, the position information of the            ing and machine translation tasks, that absolute po-
                   input words is typically added as an additional em-            sition encodings added to attention matrices with
                   bedding to the input token embeddings (Vaswani                 different parameters for each head improves sig-
                   et al., 2017). For example, input embedding (W)                niﬁcantly over absolute position encodings added
                   of a sentence is added to the position embeddings              to the input. This highlights that where the posi-
                   (P), resulting in input W + P to the Transformer.              tion information is included in the Transformer is
                   These position embeddings only depend on the lo-               important, providing an explanation for the gap in
                   cation the word appears. For multi-segment tasks,              performance between absolute and relative posi-
                        ∗ The authors contribute equally to this paper. Corre-    tion encodings. We also compare different position
                   sponding author email: puchin@google.com                       encodings and the effect of sharing position encod-
                                                                             2974
                           Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2974–2988
                                                                   c
                                            November7–11,2021. 2021Association for Computational Linguistics
