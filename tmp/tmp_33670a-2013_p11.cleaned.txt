                       And:            Z                   Z
                                         q (z)logq (z)dz =   N(z;µ,σ2)logN(z;µ,σ2)dz
                                          θ       θ
                                                                          J
                                                             J          1 X          2
                                                         =−2log(2π)− 2      (1+logσj)
                                                                         j=1
                       Therefore:                        Z
                                    −D ((q (z)||p (z)) =    q (z)(logp (z)−logq (z)) dz
                                       KL   φ     θ         θ        θ         θ
                                                            J
                                                         1 X             2       2      2
                                                       =       1+log((σ ) )−(µ ) −(σ )
                                                         2              j       j      j
                                                           j=1
                       When using a recognition model qφ(z|x) then µ and s.d. σ are simply functions of x and the
                       variational parameters φ, as exempliﬁed in the text.
                       C MLP’sasprobabilisticencodersanddecoders
                       In variational auto-encoders, neural networks are used as probabilistic encoders and decoders. There
                       are many possible choices of encoders and decoders, depending on the type of data and model. In
                       our example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs).
                       For the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with
                       either Gaussian or Bernoulli outputs, depending on the type of data.
                       C.1  Bernoulli MLP as decoder
                       In this case let p (x|z) be a multivariate Bernoulli whose probabilities are computed from z with a
                                    θ
                       fully-connected neural network with a single hidden layer:
                                                      D
                                          logp(x|z) = Xx logy +(1−x )·log(1−y )
                                                          i    i       i          i
                                                      i=1
                                           where y = f (W tanh(W z+b )+b )                          (11)
                                                      σ    2       1     1    2
                       where f (.) is the elementwise sigmoid activation function, and where θ = {W ,W ,b ,b } are
                             σ                                                          1   2  1   2
                       the weights and biases of the MLP.
                       C.2  Gaussian MLPasencoderordecoder
                       In this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:
                                                  logp(x|z) = logN(x;µ,σ2I)
                                                   where µ = W h+b
                                                                4     4
                                                     logσ2 = W h+b
                                                                5     5
                                                         h=tanh(W z+b )                             (12)
                                                                    3     3
                       where {W ,W ,W ,b ,b ,b }aretheweightsandbiasesoftheMLPandpartofθ whenused
                               3    4   5  3  4  5
                       as decoder. Note that when this network is used as an encoder qφ(z|x), then z and x are swapped,
                       and the weights and biases are variational parameters φ.
                       D Marginallikelihoodestimator
                       Wederivedthefollowingmarginallikelihoodestimatorthatproducesgoodestimatesofthemarginal
                       likelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and
                       sufﬁcient samples are taken. Let p (x,z) = p (z)p (x|z) be the generative model we are sampling
                                                  θ         θ   θ
                       from, and for a given datapoint x(i) we would like to estimate the marginal likelihood p (x(i)).
                                                                                              θ
                       Theestimation process consists of three stages:
                                                              11
