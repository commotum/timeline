                                                    Agent57: Outperforming the Atari Human Benchmark
                                                                                  Figure 8. Best arm chosen by the evaluator of Agent57 over train-
               Figure 7. Performancecomparisonforadaptiveexplorationonthe         ing for different games.
               10-game challenging set.
               separate networks is more modest than for R2D2. This in-
               dicates that there is a slight overlap in the contributions of     ent skills that are required to be performant on such diverse
               the separate network parameterization and the use of the           set of games: exploration and exploitation and long-term
               meta-controller. The bandit algorithm can adaptively de-           credit assignment. To do that, we propose simple improve-
               crease the value of β when the difference in scale between         mentstoanexistingagent, Never Give Up, which has good
               intrinsic and extrinsic rewards is large. Using the meta-          performance on hard-exploration games, but in itself does
               controller allows to include very high discount values in          not have strong overall performance across all 57 games.
               the set {γj}N . Speciﬁcally, running R2D2 with a high              These improvements are i) using a different parameteri-
                            j=0
               discount factor, γ = 0.9999 surpasses the human baseline           zation of the state-action value function, ii) using a meta-
               in the game of Skiing. However, using that hyperparameter          controller to dynamically adapt the novelty preference and
               across the full set of games, renders the algorithm very un-       discount, and iii) the use of longer backprop-through time
               stable and damages its end performance. All the scores in          windowtolearn from using the Retrace algorithm.
               the challenging set for a ﬁxed high discount (γ = 0.9999)          Thismethodleveragesagreatamountofcomputationtoits
               variant of R2D2 are reported in App. H.1. When using a             advantage: similarly to NGU, it is able to scale well with
               meta-controller, the algorithm does not need to make this          increasing amounts of computation. This has also been the
               compromise: it can adapt it in a per-task manner.                  case with the many recent achievements in deep RL (Sil-
               Finally, the results and discussion above show why it is           ver et al., 2016; Andrychowicz et al., 2018; Vinyals et al.,
               beneﬁcial to use different values of β and γ on a per-task         2019). While this enables our method to achieve strong
               basis. At the same time, in Sec. 3 we hypothesize it would         performance, an interesting research direction is to pursue
               also be useful to vary those coefﬁcients throughout train-         ways in which to improve the data efﬁciency of this agent.
               ing. In Fig. 8 we can see the choice of (β , γ ) produc-           Additionally, this agent shows an average capped human
                                                              j   j
               ing highest returns on the meta-controller of the evaluator        normalized score of 100%. However, in our view this by
               acrosstrainingforseveralgames. Somegamesclearlyhave                nomeansmarkstheendofAtariresearch,notonlyinterms
               a preferred mode: on Skiing the high discount combination          of efﬁciency as above, but also in terms of general perfor-
               is quickly picked up when the agent starts to learn, and on        mance. We offer two views on this: ﬁrstly, analyzing the
               Heroahighβ andlowγ isgenerally preferred at all times.             performance among percentiles gives us new insights on
               On the other hand, some games have different preferred             howgeneral algorithms are. While Agent57 achieves great
               modes throughout training: on Gravitar, Crazy Climber,             results on the ﬁrst percentiles of the 57 games and holds
               Beam Rider, and Jamesbond, Agent57 initially chooses to            better mean and median performance than NGU or R2D2,
               focus on exploratory policies with low discount, and, as           as MuZero shows, it could still obtain much better average
               training progresses, the agent shifts into producing experi-       performance. Secondly,aspointedoutbyToromanoffetal.
               ence from higher discount and more exploitative policies.          (2019), all current algorithms are far from achieving opti-
                                                                                  malperformanceinsomegames. Tothatend,keyimprove-
               5. Conclusions                                                     ments to use might be enhancements in the representations
                                                                                  that Agent57 and NGU use for exploration, planning (as
               Wepresenttheﬁrstdeepreinforcementlearningagentwith                 suggested by the results achieved by MuZero) as well as
               performance above the human benchmark on all 57 Atari              better mechanisms for credit assignment (as highlighted by
               games. The agent is able to balance the learning of differ-        the results seen in Skiing).
