                              family of smooth distributions eliminates the need to tune both S and p. Lastly, we can study
                              whether a smooth decay in frequency might be even more beneficial than a non-continuous two-set
                              partition.
                              In this section, we consider a discrete exponential distribution:
                                                                                      −βi/N
                                                                          P(i) ∼ βe          ,
                              with β > 0, suitably normalized1. If β tends to 0, P tends to the uniform distribution, and imple-
                              ments the single-set strategy of Section 3. As β becomes large, a small fraction of the full training
                              set is sampled (99% of the probability mass lies on the 4.6N/β first elements, 99.99% on the first
                              9.2N/β). For intermediate values of β, the model oversamples the first elements in the training
                              set, and undersamples the last: we have a continuous version of two-sample training. To allow for
                              comparison with two-sample training, we define S          such that the first S   examples in the training
                                                                                     eff                     eff
                              set jointly are sampled with probability 25%. In this setting, 10% of the probability mass is on the
                              0.37Seff first training examples, and 99% on the first 16Seff.
                                                                                                                                        2
                              ForGCD,weexperimentwithvaluesofβ rangingfrom5.8to1152(Seff from25,000to5million) .
                              Table 7 shows that for our training budget of 600 million examples, the best model (Seff = 3M)
                              predicts 65 correct GCD, slightly less than what was achieved with two-set training (Section 4).
                              Table 7: GCD for different exponential distributions. Correctly predicted GCD, best of 5 models, trained
                              on600million examples.
                                Seff     25k     50k    100k    250k    500k    1M 1.5M 2M 2.5M 3M 3.5M 4M 5M
                                β        1152    576    288      115     58      29     19      14     11.5    9.6    8.2     7.2    5.8
                                GCD       19     21      29      38      46      55     56      57     61      65      63     62     56
                              For modular multiplication, we need lower β (i.e larger Seff) for our training budget of 600M. We
                              report the number of models (out of 25 for each setting) that learn to accuracy above 50% and 95%
                              respectively (Table 8). Again we see that these results are comparable to two-set training (Section
                              4).
                              Table8: Modularmultiplicationwithdifferentexponentialdistributions. 25modelstrainedon600million
                              examples.
                                            S                               2.5M    5M 6M 8M 10M 12M 14M
                                             eff
                                            β                               11.5    5.8    4.8   3.6     2.9     2.4     2.1
                                            # Models with 95% accuracy        2      9     11     13      7       4      3
                                            # Models with 50% accuracy        4     16     25     22     17      13      6
                              Weconcludethat the benefits observed in two-set training do not pertain to the specific two-set par-
                              tition of the training set; rather, it seems that the core of the effect lies in the non-uniform sampling
                              frequency distribution over the (randomly ordered) training set, with a range of frequencies.
                              D.5    Varying the optimizer
                              Someeffects observed in deep learning depend on the optimizer, with grokking being a prominent
                              example(Poweretal.,2022). Hereweprovideexperimentalevidencetoshowthatourfindingshold
                              for a variety of optimizers and are thus robust and universal. We rerun models used for the GCD
                              problem with different optimizers. Specifically, we trained models to predict GCD, with a training
                              budget of 600 million examples, single and two-set training (with |S| = 50,000 and p = 0.25), and
                              data budgets of 25 million, 50 million and unlimited. We considered four optimizer settings:
                                     • Adamwithoutdropout or weight decay,
                                 1                                   −β −1
                                  The normalization factor is (1 − e    )   . In our calculations we will approximate it by 1 to simplify
                              computing S . For the range of β we consider, the resulting approximation error is negligible. In general,
                                           eff
                              for fixed p, to compute the size of the set S(p) of first elements that carry probability mass p, we can use
                              β ≈−ln(1−p)N/|S(p)|.
                                 2Note that for these values of β the distinction between DB 100M and unlimited DB becomes essentially
                              meaningless, as the tails of the training set are sampled exceedingly rarely.
                                                                                   12
