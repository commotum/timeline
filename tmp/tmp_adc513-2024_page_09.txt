               References                                                                         Shan. Planting a seed of vision in large language model.
                 [1] Gpt-4v(ision) system card. 2023. 2                                           arXiv preprint arXiv:2307.08041, 2023. 2, 3
                                                                                            [14] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li,
                 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan                      Xintao Wang, and Ying Shan. Making llama see and draw
                     Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren                         withseedtokenizer. arXiv preprint arXiv:2310.01218, 2023.
                     Zhou. Qwen-vl: Afrontierlargevision-languagemodelwith                        2, 3
                     versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 2,         [15] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
                     3, 7                                                                         Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping
                 [3] Shuai Bai, Shusheng Yang, Jinze Bai, Peng Wang, Xingxuan                     Luo, and Kai Chen. Multimodal-gpt: A vision and language
                     Zhang, Junyang Lin, Xinggang Wang, Chang Zhou, and Jin-                      modelfor dialogue with humans, 2023. 2, 3, 7
                     gren Zhou. Touchstone: Evaluating vision-language mod-                                                              ´
                                                                                            [16] Hugo Laurenc¸on, Lucile Saulnier, Leo Tronchon, Stas Bek-
                     els by language models. arXiv preprint arXiv:2308.16890,                     man, Amanpreet Singh, Anton Lozhkov, Thomas Wang,
                     2023. 2, 3                                                                   Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela,
                 [4] James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng                      Matthieu Cord, and Victor Sanh. Obelics: An open web-
                     Wang, Linjie Li, LongOuyang, JuntangZhuang, JoyceLee,                        scale filtered dataset of interleaved image-text documents,
                     YufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu,                         2023. 7
                     YunxinJiao, and Aditya Ramesh. Improving image gener-                  [17] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
                     ation with better captions. 2                                                Jingkang Yang, and Ziwei Liu.           Otter:  A multi-modal
                 [5] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao,                      model with in-context instruction tuning.        arXiv preprint
                     Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori,                       arXiv:2305.03726, 2023. 7
                     and Ludwig Schimdt. Visit-bench: A benchmark for vision-               [18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
                     language instruction following inspired by real-world use.                   Blip-2:   Bootstrapping language-image pre-training with
                     arXiv preprint arXiv:2308.06595, 2023. 2                                     frozen image encoders and large language models. ICML,
                 [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-                           2023. 3, 7
                     biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-              [19] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai
                     tan, PranavShyam,GirishSastry,AmandaAskell,etal. Lan-                        Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
                     guage models are few-shot learners. Advances in neural in-                   Videochat: Chat-centric video understanding. arXiv preprint
                     formation processing systems, 33:1877–1901, 2020. 6                          arXiv:2305.06355, 2023. 2, 3, 7
                 [7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret                        [20] ShengzhiLiandNimaTajbakhsh. Scigraphqa: Alarge-scale
                     Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,                           synthetic multi-turn question-answering dataset for scientific
                     Mostafa Dehghani, Siddhartha Brahma, et al.              Scaling             graphs. arXiv preprint arXiv:2308.03349, 2023. 2
                     instruction-finetuned language models.          arXiv preprint         [21] Yu Lili, Shi Bowen, Pasunuru Ram, Miller Benjamin,
                     arXiv:2210.11416, 2022. 2, 3                                                 Golovneva Olga, Wang Tianlu, Babu Arun, Tang Binh, Kar-
                 [8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat                        rer Brian, Sheynin Shelly, Ross Candace, Polyak Adam,
                     Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale                         Howes Russ, Sharma Vasu, Xu Jacob, Singer Uriel,
                     Fung, and Steven Hoi.         Instructblip:  Towards general-                Li (AI) Daniel, Ghosh Gargi, Taigman Yaniv, Fazel-Zarandi
                     purpose vision-language models with instruction tuning.                      Maryam, Celikyilmaz Asli, Zettlemoyer Luke, and Agha-
                     arXiv preprint arXiv:2305.06500, 2023. 2, 3, 6, 7                            janyan Armen. Scaling autoregressive multi-modal models:
                 [9] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng                        Pretraining and instruction tuning. 2023. 2, 3
                     Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,               [22] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa:
                     Haoran Wei, et al. Dreamllm: Synergistic multimodal com-                     Measuring how models mimic human falsehoods.               arXiv
                     prehension and creation. arXiv preprint arXiv:2309.11499,                    preprint arXiv:2109.07958, 2021. 6
                     2023. 2, 3                                                             [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
               [10] FastChat. Vicuna. https://github.com/lm-sys/FastChat, 2023.                   Improved baselines with visual instruction tuning.         arXiv
                     2, 3                                                                         preprint arXiv:2310.03744, 2023. 2, 3, 7
                                                                                            [24] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
               [11] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,                            Visual instruction tuning. arXiv preprint arXiv:2304.08485,
                     MengdanZhang,XuLin,ZhenyuQiu,WeiLin,JinruiYang,                              2023. 2, 3, 7
                     Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: A                  [25] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang
                     comprehensive evaluation benchmark for multimodal large                      Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,
                     language models. arXiv preprint arXiv:2306.13394, 2023.                      Ziwei Liu, et al. Mmbench: Is your multi-modal model an
                     2, 3                                                                         all-around player? arXiv preprint arXiv:2307.06281, 2023.
               [12] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie                         2, 3, 6
                     Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xi-                   [26] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin
                     angyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2:                      Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan
                     Parameter-efficient visual instruction model. arXiv preprint                 Li, Lianwen Jin, et al. On the hidden mystery of ocr in large
                     arXiv:2304.15010, 2023. 7                                                    multimodalmodels. arXivpreprintarXiv:2305.07895,2023.
               [13] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying                       2
                                                                                        13307
