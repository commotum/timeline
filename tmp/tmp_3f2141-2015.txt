                          Learning to Transduce with Unbounded Memory
                           EdwardGrefenstette    KarlMoritzHermann            MustafaSuleyman
                            Google DeepMind         Google DeepMind            Google DeepMind
                           etg@google.com          kmh@google.com         mustafasul@google.com
                                                         Phil Blunsom
                                              Google DeepMind and Oxford University
                                                   pblunsom@google.com
                                                          Abstract
                              Recently, strong results have been demonstrated by Deep Recurrent Neural Net-
                              works on natural language transduction problems. In this paper we explore the
                              representational power of these models using synthetic grammars designed to ex-
                              hibit phenomena similar to those found in real transduction problems such as ma-
                              chinetranslation. These experiments lead us to propose new memory-based recur-
                              rent networks that implement continuously differentiable analogues of traditional
                              data structures such as Stacks, Queues, and DeQues. We show that these architec-
                              turesexhibitsuperiorgeneralisationperformancetoDeepRNNsandareoftenable
                              to learn the underlying generating algorithms in our transduction experiments.
                      1   Introduction
                      Recurrent neural networks (RNNs) offer a compelling tool for processing natural language input in
                      a straightforward sequential manner. Many natural language processing (NLP) tasks can be viewed
                      as transduction problems, that is learning to convert one string into another. Machine translation is
                      a prototypical example of transduction and recent results indicate that Deep RNNs have the ability
                      to encode long source strings and produce coherent translations [1, 2]. While elegant, the appli-
                      cation of RNNs to transduction tasks requires hidden layers large enough to store representations
                      of the longest strings likely to be encountered, implying wastage on shorter strings and a strong
       arXiv:1506.02516v3  [cs.NE]  3 Nov 2015dependency between the number of parameters in the model and its memory.
                      In this paper we use a number of linguistically-inspired synthetic transduction tasks to explore the
                      ability of RNNs to learn long-range reorderings and substitutions. Further, inspired by prior work on
                      neural network implementations of stack data structures [3], we propose and evaluate transduction
                      models based on Neural Stacks, Queues, and DeQues (double ended queues). Stack algorithms are
                      well-suitedtoprocessingthehierarchicalstructuresobservedinnaturallanguageandwehypothesise
                      that their neural analogues will provide an effective and learnable transduction tool. Our models
                      provide a middle ground between simple RNNs and the recently proposed Neural Turing Machine
                      (NTM) [4] which implements a powerful random access memory with read and write operations.
                      Neural Stacks, Queues, and DeQues also provide a logically unbounded memory while permitting
                      efﬁcient constant time push and pop operations.
                      Our results indicate that the models proposed in this work, and in particular the Neural DeQue, are
                      able to consistently learn a range of challenging transductions. While Deep RNNs based on long
                          This version of the paper is identical to the version found in the proceedings of Advances in Neural Infor-
                      mationProcessingSystems,2015,withtheadditionofsomemissingreferences. Figureshavebeenmadelarger
                      for increased legibility.
                                                              1
                           short-term memory (LSTM) cells [1, 5] can learn some transductions when tested on inputs of the
                           same length as seen in training, they fail to consistently generalise to longer strings. In contrast,
                           our sequential memory-based algorithms are able to learn to reproduce the generating transduction
                           algorithms, often generalising perfectly to inputs well beyond those encountered in training.
                           2   Related Work
                           String transduction is central to many applications in NLP, from name transliteration and spelling
                           correction, to inﬂectional morphology and machine translation. The most common approach lever-
                           ages symbolic ﬁnite state transducers [6, 7], with approaches based on context free representations
                           also being popular [8]. RNNs offer an attractive alternative to symbolic transducers due to their sim-
                           ple algorithms and expressive representations [9]. However, as we show in this work, such models
                           are limited in their ability to generalise beyond their training data and have a memory capacity that
                           scales with the number of their trainable parameters.
                           Previous work has touched on the topic of rendering discrete data structures such as stacks con-
                           tinuous, especially within the context of modelling pushdown automata with neural networks
                           [10, 11, 3, 12]. We were inspired by the continuous pop and push operations of these architec-
                           tures and the idea of an RNN controlling the data structure when developing our own models.
                           The key difference is that our work adapts these operations to work within a recurrent continu-
                           ousStack/Queue/DeQue-likestructure, the dynamics of which are fully decoupled from those of the
                           RNNcontrolling it. In our models, the backwards dynamics are easily analysable in order to obtain
                           the exact partial derivatives for use in error propagation, rather than having to approximate them as
                           done in previous work.
                           In a parallel effort to ours, researchers are exploring the addition of memory to recurrent networks.
                           The NTMandMemoryNetworks[4,13,14]provide powerful random access memory operations,
                           whereas we focus on a more efﬁcient and restricted class of models which we believe are sufﬁcient
                           for natural language transduction tasks. More closely related to our work, [15] have sought to
                           develop a continuous stack controlled by an RNN. Note that this model—unlike the work proposed
                           here—rendersdiscretepushandpopoperationscontinuousby“mixing”informationacrosslevelsof
                           the stack at each time step according to scalar push/pop action values. This means the model ends up
                           compressing information in the stack, thereby limiting its use, as it effectively loses the unbounded
                           memorynatureoftraditional symbolic models.
                           3   Models
                           In this section, we present an extensible memory enhancement to recurrent layers which can be set
                           up to act as a continuous version of a classical Stack, Queue, or DeQue (double-ended queue). We
                           begin by describing the operations and dynamics of a neural Stack, before showing how to modify
                           it to act as a Queue, and extend it to act as a DeQue.
                           3.1  Neural Stack
                           Let a Neural Stack be a differentiable structure onto and from which continuous vectors are pushed
                           and popped. Inspired by the neural pushdown automaton of [3], we render these traditionally dis-
                           crete operations continuous by letting push and pop operations be real values in the interval (0,1).
                           Intuitively, we can interpret these values as the degree of certainty with which some controller wishes
                           to push a vector v onto the stack, or pop the top of the stack.
                                                                          2
                                          Vt[i] =  Vt−1[i]      if 1 ≤ i < t        (Note that Vt[i] = vi for all i ≤ t)          (1)
                                                      vt         if i = t
                                                                                        t−1
                                                   max(0,s        [i] − max(0,u − P s             [j]))   if 1 ≤ i < t
                                          st[i] =              t−1                t   j=i+1 t−1                                   (2)
                                                      dt                                                   if i = t
                                                 t                              t
                                          rt = X(min(st[i],max(0,1− X st[j])))·Vt[i]                                               (3)
                                                i=1                           j=i+1
                             Formally, a Neural Stack, fully parametrised by an embedding size m, is described at some timestep
                             t by a t × m value matrix Vt and a strength vector st ∈ Rt. These form the core of a recurrent layer
                             whichis acted upon by a controller by receiving, from the controller, a value vt ∈ Rm, a pop signal
                             u ∈(0,1), and a push signal d ∈ (0,1). It outputs a read vector r ∈ Rm. The recurrence of this
                               t                               t                                     t
                             layer comes from the fact that it will receive as previous state of the stack the pair (V    , s   ), and
                                                                                                                      t−1   t−1
                             produceasnextstatethepair(Vt,st)followingthedynamicsdescribedbelow. Here,Vt[i]represents
                             the ith row (an m-dimensional vector) of Vt and st[i] represents the ith value of st.
                             Equation 1 shows the update of the value component of the recurrent layer state represented as a
                             matrix, the number of rows of which grows with time, maintaining a record of the values pushed to
                             the stack at each timestep (whether or not they are still logically on the stack). Values are appended
                             to the bottom of the matrix (top of the stack) and never changed.
                             Equation 2 shows the effect of the push and pop signal in updating the strength vector st−1 to
                             produce st. First, the pop operation removes objects from the stack. We can think of the pop value
                             u as the initial deletion quantity for the operation. We traverse the strength vector s         from the
                               t                                                                                        t−1
                             highest index to the lowest. If the next strength scalar is less than the remaining deletion quantity, it
                             is subtracted from the remaining quantity and its value is set to 0. If the remaining deletion quantity
                             is less than the next strength scalar, the remaining deletion quantity is subtracted from that scalar and
                             deletion stops. Next, the push value is set as the strength for the value added in the current timestep.
                             Equation 3 shows the dynamics of the read operation, which are similar to the pop operation. A
                             ﬁxed initial read quantity of 1 is set at the top of a temporary copy of the strength vector s which
                                                                                                                              t
                             is traversed from the highest index to the lowest. If the next strength scalar is smaller than the
                             remaining read quantity, its value is preserved for this operation and subtracted from the remaining
                             read quantity. If not, it is temporarily set to the remaining read quantity, and the strength scalars of
                             all lower indices are temporarily set to 0. The output rt of the read operation is the weighted sum
                             of the rows of Vt, scaled by the temporary scalar values created during the traversal. An example
                             of the stack read calculations across three timesteps, after pushes and pops as described above, is
                             illustrated in Figure 1a. The third step shows how setting the strength s3[2] to 0 for V3[2] logically
                             removes v2 from the stack, and how it is ignored during the read.
                             This completes the description of the forward dynamics of a neural Stack, cast as a recurrent layer,
                                                                                                                     1
                             as illustrated in Figure 1b. All operations described in this section are differentiable . The equations
                             describing the backwards dynamics are provided in Appendix A of the supplementary materials.
                             3.2   Neural Queue
                             Aneural Queue operates the same way as a neural Stack, with the exception that the pop operation
                             reads the lowest index of the strength vector st, rather than the highest. This represents popping and
                             reading from the front of the Queue rather than the top of the stack. These operations are described
                             in Equations 4–5.
                                                                                        i−1
                                                    max(0,s         [i] − max(0,u − P s          [j]))  if 1 ≤ i < t
                                           st[i] =              t−1                t    j=1 t−1                                   (4)
                                                       dt                                                if i = t
                                1The max(x,y) and min(x,y) functions are technically not differentiable for x = y. Following the work
                             onrectiﬁed linear units [16], we arbitrarily take the partial differentiation of the left argument in these cases.
                                                                                 3
                                                                                   t = 1   u  = 0   d  = 0.8         t = 2   u  = 0.1   d  = 0.5        t = 3   u  = 0.9   d  = 0.9
                                                                                            1        1                        2          2                       3          3
                                                            ds      row 3                                                                                       v             0.9
                                                                                                                                                                  3
                                                                                                                                                                                                 v  removed
                                                                    row 2                                                    v             0.5                  v              0                   2
                                                            ows upwar                                                         2                                   2                              from stack
                                                                    row 1                 v             0.8                  v             0.7                  v             0.3
                                                            stack gr                       1                                  1                                   1
                                                                                         r  = 0.8 ∙ v                  r  = 0.5 ∙ v + 0.5 ∙ v         r  = 0.9 ∙ v + 0 ∙ v + 0.1 ∙ v
                                                                                          1          1                  2          2         1         3          3       2          1
                                                                                           (a) Example Operation of a Continuous Neural Stack
                                                                                               prev. values (V        )                                next values (V )
                                                                                                                   t-1                                                     t
                                                                                previous state                                                                           next state
                                                                                          prev. strengths (s         )       Neural  next strengths (st)
                                                                                                                  t-1
                                                                                                          push (d )
                                                                                                                    t          Stack
                                                                                        input              pop (u )                                             output (r )
                                                                                                                    t                                                       t
                                                                                                      value (v )
                                                                                                                 t                                                 Split
                                                                                                                                                                   Join
                                                                                                       (b) Neural Stack as a Recurrent Layer
                                                                                        (V   , s   )
                                                                                          t-1   t-1
                                                                                                                            V                                       V
                                                                  previous               h                                    t-1                                    t
                                                                     state                 t-1
                                                                      H                                     ht                                                                              next
                                                                        t-1                         R                                                                     (V , s )          state
                                                                                       rt-1                                                                                 t  t              H
                                                                                                    N                     st-1         Neural                       s                           t
                                                                                                                                 d                                   t
                                                                                                    N                  …          t      Stack
                                                                       input                               (ot, …)             u                                rt
                                                                                                                                t
                                                                          it            (i , r )
                                                                                         t  t-1                  ot                                                                        output
                                                                                                                             vt
                                                                                                                                                                                               ot
                                                                                                              (c) RNN Controlling a Stack
                                                             Figure 1: Illustrating a Neural Stack’s Operations, Recurrent Structure, and Control
                                                                                                                                   4
                                                             t                            i−1
                                                      r =X(min(s [i],max(0,1−Xs [j])))·V [i]                                       (5)
                                                       t                t                      t          t
                                                            i=1                           j=1
                             3.3   Neural DeQue
                             Aneural DeQue operates likes a neural Stack, except it takes a push, pop, and value as input for
                             both “ends” of the structure (which we call top and bot), and outputs a read for both ends. We write
                               top       bot                 top        bot
                             u    and u     instead of u , v     and v     instead of v , and so on. The state, V and s are now
                               t         t               t   t         t                t                           t       t
                             a 2t × m-dimensional matrix and a 2t-dimensional vector, respectively. At each timestep, a pop
                             from the top is followed by a pop from the bottom of the DeQue, followed by the pushes and reads.
                             The dynamics of a DeQue, which unlike a neural Stack or Queue “grows” in two directions, are
                             described in Equations 6–11, below. Equations 7–9 decompose the strength vector update into three
                             steps purely for notational clarity.
                                               vbot             if i = 1
                                               t
                                      V [i] =     vtop           if i = 2t                                                         (6)
                                       t       t
                                                  V    [i − 1]   if 1 < i < 2t
                                                   t−1
                                                                                   2(t−1)−1
                                       top                                   top      X
                                      s   [i] = max(0,st−1[i] − max(0,u          −           st−1[j]))   if 1 ≤ i < 2(t − 1)       (7)
                                       t                                     t
                                                                                     j=i+1
                                                                                    i−1
                                       both                top                bot   Xtop
                                      s    [i] = max(0,s      [i] − max(0,u      −      s   [j]))   if 1 ≤ i < 2(t − 1)            (8)
                                       t                   t                  t          t
                                                                                   j=1
                                               sboth[i−1]       if 1 < i < 2t
                                                   t
                                      s [i] =     dbot           if i = 1                                                          (9)
                                       t       t
                                                  dtop           if i = 2t
                                                   t
                                              2t                              2t
                                      rtop = X(min(st[i],max(0,1− X st[j])))·Vt[i]                                                (10)
                                       t
                                              i=1                          j=i+1
                                              2t                            i−1
                                      rbot = X(min(s [i],max(0,1−Xs [j])))·V [i]                                                  (11)
                                       t                  t                      t         t
                                             i=1                           j=1
                             To summarise, a neural DeQue acts like two neural Stacks operated on in tandem, except that the
                             pushes and pops from one end may eventually affect pops and reads on the other, and vice versa.
                             3.4   Interaction with a Controller
                             Whilethethreememorymodulesdescribedcanbeseenasrecurrentlayers,withtheoperationsbeing
                             usedtoproducethenextstateandoutputfromtheinputandpreviousstatebeingfullydifferentiable,
                             they contain no tunable parameters to optimise during training. As such, they need to be attached
                             to a controller in order to be used for any practical purposes. In exchange, they offer an extensible
                             memory,thelogical size of which is unbounded and decoupled from both the nature and parameters
                             of the controller, and from the size of the problem they are applied to. Here, we describe how any
                             RNNcontroller may be enhanced by a neural Stack, Queue or DeQue.
                             Webegin by giving the case where the memory is a neural Stack, as illustrated in Figure 1c. Here
                             we wish to replicate the overall ‘interface’ of a recurrent layer—as seen from outside the dotted
                             lines—which takes the previous recurrent state H          and an input vector i , and transforms them
                                                                                  t−1                        t
                             to return the next recurrent state H and an output vector o . In our setup, the previous state H
                                                                  t                         t                                     t−1
                             of the recurrent layer will be the tuple (h    , r   , (V    , s    )), where h     is the previous state
                                                                        t−1    t−1     t−1   t−1             t−1
                             of the RNN, rt−1 is the previous stack read, and (Vt−1, st−1) is the previous state of the stack
                             as described above. With the exception of h0, which is initialised randomly and optimised during
                             training, all other initial states, r0 and (V0, s0), are set to 0-valued vectors/matrices and not updated
                             during training.
                                                                                 5
                                            The overall input it is concatenated with previous read rt−1 and passed to the RNN controller as
                                            input along with the previous controller state h                                   .  The controller outputs its next state h and a
                                                                          0                                              t−1                                                                    t
                                            controller output o , from which we obtain the push and pop scalars d and u and the value vector
                                                                          t                                                                                  t          t
                                            vt, which are passed to the stack, as well as the network output ot:
                                                                   d =sigmoid(W o0 +b )                                                u =sigmoid(W o0 +b )
                                                                     t                          d t         d                            t                          u t         u
                                                                                              0                                                                        0
                                                                   v =tanh(W o +b )                                                          o =tanh(W o +b )
                                                                      t                   v t          v                                       t                   o t          o
                                            where W and W are vector-to-scalar projection matrices, and b and b are their scalar biases;
                                                           d             u                                                                             d           u
                                            W and W are vector-to-vector projections, and b and b are their vector biases, all randomly
                                                v             o                                                                 d            u
                                            intialised and then tuned during training. Along with the previous stack state (Vt−1, st−1), the stack
                                            operations d and u and the value v are passed to the neural stack to obtain the next read r and
                                                                t           t                           t                                                                                          t
                                            next stack state (V , s ), which are packed into a tuple with the controller state h to form the next
                                                                         t     t                                                                                               t
                                            state H of the overall recurrent layer. The output vector o serves as the overall output of the
                                                        t                                                                                      t
                                            recurrent layer. The structure described here can be adapted to control a neural Queue instead of a
                                            stack by substituting one memory module for the other.
                                            The only additional trainable parameters in either conﬁguration, relative to a non-enhanced RNN,
                                            are the projections for the input concatenated with the previous read into the RNN controller, and the
                                            projections from the controller output into the various Stack/Queue inputs, described above. In the
                                            case of a DeQue, both the top read rtop and bottom read rbot must be preserved in the overall state.
                                            TheyarebothconcatenatedwiththeinputtoformtheinputtotheRNNcontroller. Theoutputofthe
                                            controller must have additional projections to output push/pop operations and values for the bottom
                                            of the DeQue. This roughly doubles the number of additional tunable parameters “wrapping” the
                                            RNNcontroller, compared to the Stack/Queue case.
                                            4      Experiments
                                            In every experiment, integer-encoded source and target sequence pairs are presented to the candidate
                                            modelasabatchofsinglejoint sequences. The joint sequence starts with a start-of-sequence (SOS)
                                            symbol, and ends with an end-of-sequence (EOS) symbol, with a separator symbol separating the
                                            source and target sequences. Integer-encoded symbols are converted to 64-dimensional embeddings
                                            via an embedding matrix, which is randomly initialised and tuned during training. Separate word-
                                            to-index mappings are used for source and target vocabularies. Separate embedding matrices are
                                            used to encode input and output (predicted) embeddings.
                                            4.1      Synthetic Transduction Tasks
                                            Theaimofeachofthefollowingtasksistoreadaninputsequence,andgenerateastargetsequencea
                                            transformed version of the source sequence, followed by an EOS symbol. Source sequences are ran-
                                            domlygeneratedfromavocabularyof128meaninglesssymbols. Thelengthofeachtrainingsource
                                            sequence is uniformly sampled from unif{8,64}, and each symbol in the sequence is drawn with
                                            replacement from a uniform distribution over the source vocabulary (ignoring SOS, and separator).
                                            Adeterministic task-speciﬁc transformation, described for each task below, is applied to the source
                                            sequence to yield the target sequence. As the training sequences are entirely determined by the
                                            source sequence, there are close to 10135 training sequences for each task, and training examples
                                            are sampled from this space due to the random generation of source sequences. The following steps
                                            are followed before each training and test sequence are presented to the models, the SOS symbol
                                            (hsi) is prepended to the source sequence, which is concatenated with a separator symbol (|||) and
                                            the target sequences, to which the EOS symbol (h/si) is appended.
                                            Sequence Copying                    The source sequence is copied to form the target sequence. Sequences have
                                            the form:
                                                                                                      hsia ...a |||a ...a h/si
                                                                                                             1         k      1         k
                                            Sequence Reversal                   The source sequence is deterministically reversed to produce the target se-
                                            quence. Sequences have the form:
                                                                                                  hsia a ...a |||a ...a a h/si
                                                                                                         1 2           k      k         2 1
                                                                                                                          6
                       Bigram ﬂipping  The source side is restricted to even-length sequences. The target is produced
                       by swapping, for all odd source sequence indices i ∈ [1,|seq|] ∧ odd(i), the ith symbol with the
                       (i + 1)th symbol. Sequences have the form:
                                          hsia a a a ...a   a |||a a a a ...a a  h/si
                                              1 2 3 4    k−1 k   2 1 4 3    k k−1
                       4.2 ITGTransductionTasks
                       Thefollowingtasks examine how well models can approach sequence transduction problems where
                       the source and target sequence are jointly generated by Inversion Transduction Grammars (ITG) [8],
                       a subclass of Synchronous Context-Free Grammars [17] often used in machine translation [18]. We
                       present two simple ITG-based datasets with interesting linguistic properties and their underlying
                       grammars. WeshowthesegrammarsinTable1,inAppendixCofthesupplementarymaterials. For
                       each synchronised non-terminal, an expansion is chosen according to the probability distribution
                       speciﬁed by the rule probability p at the beginning of each rule. For each grammar, ‘A’ is always the
                       root of the ITG tree.
                       Wetunedthegenerative probabilities for recursive rules by hand so that the grammars generate left
                       and right sequences of lengths 8 to 128 with relatively uniform distribution. We generate training
                       data by rejecting samples that are outside of the range [8,64], and testing data by rejecting samples
                       outside of the range [65,128]. For terminal symbol-generating rules, we balance the classes so
                       that for k terminal-generating symbols in the grammar, each terminal-generating non-terminal ‘X’
                       generates a vocabulary of approximately 128/k, and each each vocabulary word under that class is
                       equiprobable. These design choices were made to maximise the similarity between the experimental
                       settings of the ITG tasks described here and the synthetic tasks described above.
                       Subj–Verb–Obj to Subj–Obj–Verb  Apersistent challenge in machine translation is to learn to
                       faithfully reproduce high-level syntactic divergences between languages. For instance, when trans-
                       lating an English sentence with a non-ﬁnite verb into German, a transducer must locate and move
                       the verb over the object to the ﬁnal position. We simulate this phenomena with a synchronous
                       grammar which generates strings exhibiting verb movements. To add an extra challenge, we also
                       simulate simple relative clause embeddings to test the models’ ability to transduce in the presence
                       of unbounded recursive structures.
                       Asample output of the grammar is presented here, with spaces between words being included for
                       stylistic purposes, and where s, o, and v indicate subject, object, and verb terminals respectively, i
                       and o mark input and output, and rp indicates a relative pronoun:
                        si1 vi28 oi5 oi7 si15 rpi si19 vi16 oi10 oi24 ||| so1 oo5 oo7 so15 rpo so19 vo16 oo10 oo24 vo28
                       Genderless to gendered grammar  Wedesign a small grammar to simulate translations from a
                       language with gender-free articles to one with gender-speciﬁc deﬁnite and indeﬁnite articles. A
                       real world example of such a translation would be from English (the, a) to German (der/die/das,
                       ein/eine/ein).
                       The grammar simulates sentences in (NP/(V/NP)) or (NP/V) form, where every noun phrase
                       canbecomeaninﬁnitesequenceofnounsjoinedbyaconjunction. Eachnouninthesourcelanguage
                       has a neutral deﬁnite or indeﬁnite article. The matching word in the target language then needs to be
                       preceeded by its appropriate article. A sample output of the grammar is presented here, with spaces
                       between words being included for stylistic purposes:
                                      we11theen19andtheem17|||wg11dasgn19unddergm17
                       4.3 Evaluation
                       For each task, test data is generated through the same procedure as training data, with the key dif-
                       ference that the length of the source sequence is sampled from unif {65,128}. As a result of this
                       change, we not only are assured that the models cannot observe any test sequences during training,
                       but are also measuring how well the sequence transduction capabilities of the evaluated models gen-
                       eralise beyond the sequence lengths observed during training. To control for generalisation ability,
                       wealso report accuracy scores on sequences separately sampled from the training set, which given
                       the size of the sample space are unlikely to have ever been observed during actual model training.
                                                               7
                           For each round of testing, we sample 1000 sequences from the appropriate test set. For each se-
                           quence, the model reads in the source sequence and separator symbol, and begins generating the
                           next symbol by taking the maximally likely symbol from the softmax distribution over target sym-
                           bols produced by the model at each step. Based on this process, we give each model a coarse
                           accuracy score, corresponding to the proportion of test sequences correctly predicted from begin-
                           ning until end (EOS symbol) without error, as well as a ﬁne accuracy score, corresponding to the
                           average proportion of each sequence correctly generated before the ﬁrst error. Formally, we have:
                                                                                        #seqs
                                               coarse = #correct        fine =      1    X#correcti
                                                           #seqs                 #seqs i=1     |targeti|
                           where #correct and #seqs are the number of correctly predicted sequences (end-to-end) and the
                           total number of sequences in the test batch (1000 in this experiment), respectively; #correcti is the
                           numberofcorrectlypredicted symbols before the ﬁrst error in the ith sequence of the test batch, and
                           |target | is the length of the target segment that sequence (including EOS symbol).
                                  i
                           4.4   ModelsComparedandExperimentalSetup
                           For each task, we use as benchmarks the Deep LSTMs described in [1], with 1, 2, 4, and 8 layers.
                           Againstthesebenchmarks,weevaluateneuralStack-,Queue-,andDeQue-enhancedLSTMs. When
                           running experiments, we trained and tested a version of each model where all LSTMs in each model
                           have a hidden layer size of 256, and one for a hidden layer size of 512. The Stack/Queue/DeQue
                           embeddingsizewasarbitrarilysetto256,halfthemaximumhiddensize. Thenumberofparameters
                           for each model are reported for each architecture in Table 2 of the appendix. Concretely, the neural
                           Stack-, Queue-, and DeQue-enhanced LSTMs have the same number of trainable parameters as a
                           two-layer Deep LSTM.Theseallcomefromtheextraconnectionstoandfromthememorymodule,
                           which itself has no trainable parameters, regardless of its logical size.
                           ModelsaretrainedwithminibatchRMSProp[19],withabatchsizeof10. Wegrid-searchedlearning
                                                     −3        −3         −4        −4        −5
                           rates across the set {5×10   , 1×10    , 5×10    , 1×10     , 5×10    }. Weusedgradientclipping
                           [20], clipping all gradients above 1. Average training perplexity was calculated every 100 batches.
                           Training and test set accuracies were recorded every 1000 batches.
                           5    Results and Discussion
                           Becauseoftheimpossibilityofoverﬁttingthedatasets,weletthemodelstrainanunboundednumber
                           of steps, and report results at convergence. We present in Figure 2a the coarse- and ﬁne-grained
                           accuracies, for each task, of the best model of each architecture described in this paper alongside
                           the best performing Deep LSTM benchmark. The best models were automatically selected based on
                           average training perplexity. The LSTM benchmarks performed similarly across the range of random
                           initialisations, so the effect of this procedure is primarily to try and select the better performing
                           Stack/Queue/DeQue-enhanced LSTM. In most cases, this procedure does not yield the actual best-
                           performing model, and in practice a more sophisticated procedure such as ensembling [21] should
                           produce better results.
                           For all experiments, the Neural Stack or Queue outperforms the Deep LSTM benchmarks, often by
                           a signiﬁcant margin. For most experiments, if a Neural Stack- or Queue-enhanced LSTM learns
                           to partially or consistently solve the problem, then so does the Neural DeQue. For experiments
                           where the enhanced LSTMs solve the problem completely (consistent accuracy of 1) in training,
                           the accuracy persists in longer sequences in the test set, whereas benchmark accuracies drop for
                           all experiments except the SVO to SOV and Gender Conjugation ITG transduction tasks. Across
                           all tasks which the enhanced LSTMs solve, the convergence on the top accuracy happens orders of
                           magnitude earlier for enhanced LSTMs than for benchmark LSTMs, as exempliﬁed in Figure 2b.
                           The results for the sequence inversion and copying tasks serve as unit tests for our models, as the
                           controller mainly needs to learn to push the appropriate number of times and then pop continuously.
                           Nonetheless, the failure of Deep LSTMs to learn such a regular pattern and generalise is itself
                           indicative of the limitations of the benchmarks presented here, and of the relative expressive power
                           of our models. Their ability to generalise perfectly to sequences up to twice as long as those attested
                           during training is also notable, and also attested in the other experiments. Finally, this pair of
                                                                           8
                                                                              Training         Testing
                                             Experiment    Model           Coarse   Fine    Coarse   Fine
                                                           4-layer LSTM    0.98     0.98    0.01     0.50
                                             Sequence      Stack-LSTM      0.89     0.94    0.00     0.22
                                             Copying       Queue-LSTM      1.00     1.00    1.00     1.00
                                                           DeQue-LSTM      1.00     1.00    1.00     1.00
                                                           8-layer LSTM    0.95     0.98    0.04     0.13
                                             Sequence      Stack-LSTM      1.00     1.00    1.00     1.00
                                             Reversal      Queue-LSTM      0.44     0.61    0.00     0.01
                                                           DeQue-LSTM      1.00     1.00    1.00     1.00
                                                           2-layer LSTM    0.54     0.93    0.02     0.52
                                             Bigram        Stack-LSTM      0.44     0.90    0.00     0.48
                                             Flipping      Queue-LSTM      0.55     0.94    0.55     0.98
                                                           DeQue-LSTM      0.55     0.94    0.53     0.98
                                                           8-layer LSTM    0.98     0.99    0.98     0.99
                                             SVOto         Stack-LSTM      1.00     1.00    1.00     1.00
                                             SOV           Queue-LSTM      1.00     1.00    1.00     1.00
                                                           DeQue-LSTM      1.00     1.00    1.00     1.00
                                             Gender        8-layer LSTM    0.98     0.99    0.99     0.99
                                             Conju-        Stack-LSTM      0.93     0.97    0.93     0.97
                                             gation        Queue-LSTM      1.00     1.00    1.00     1.00
                                                           DeQue-LSTM      1.00     1.00    1.00     1.00
                                                   (a) Comparing Enhanced LSTMs to Best Benchmarks
                                                  (b) Comparison of Model Convergence during Training
                                        Figure 2: Results on the transduction tasks and convergence properties
                                                                          9
                   experiments illustrates how while the neural Queue solves copying and the Stack solves reversal, a
                   simple LSTMcontroller can learn to operate a DeQue as either structure, and solve both tasks.
                   Theresults of the Bigram Flipping task for all models are consistent with the failure to consistently
                   correctly generate the last two symbols of the sequence. We hypothesise that both Deep LSTMs and
                   our models economically learn to pairwise ﬂip the sequence tokens, and attempt to do so half the
                   time when reaching the EOS token. For the two ITG tasks, the success of Deep LSTM benchmarks
                   relative to their performance in other tasks can be explained by their ability to exploit short local
                   dependencies dominating the longer dependencies in these particular grammars.
                   Overall, the rapid convergence, where possible, on a general solution to a transduction problem
                   in a manner which propagates to longer sequences without loss of accuracy is indicative that an
                   unboundedmemory-enhancedcontroller can learn to solve these problems procedurally, rather than
                   memorising the underlying distribution of the data.
                   6  Conclusions
                   The experiments performed in this paper demonstrate that single-layer LSTMs enhanced by an un-
                   bounded differentiable memory capable of acting, in the limit, like a classical Stack, Queue, or
                   DeQue, are capable of solving sequence-to-sequence transduction tasks for which Deep LSTMs
                   falter. Even in tasks for which benchmarks obtain high accuracies, the memory-enhanced LSTMs
                   converge earlier, and to higher accuracies, while requiring considerably fewer parameters than all
                   but the simplest of Deep LSTMs. We therefore believe these constitute a crucial addition to our neu-
                   ral network toolbox, and that more complex linguistic transduction tasks such as machine translation
                   or parsing will be rendered more tractable by their inclusion.
                                                                 ´ˇ  ˇ  ´        ¨
                   Acknowledgements WethankAlexGraves,DemisHassabis, Tomas Kocisky , Tim Rocktaschel,
                   SamRitter, Geoff Hinton, Ilya Sutskever, Chris Dyer, and many others for their helpful comments.
                                                   10
             References
             [1] Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. Sequence to sequence learning with neural networks.
               In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in
               Neural Information Processing Systems 27, pages 3104–3112. Curran Associates, Inc., 2014.
             [2] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua
               Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation.
               arXiv preprint arXiv:1406.1078, 2014.
             [3] GZ Sun, C Lee Giles, HH Chen, and YC Lee. The neural network pushdown automaton: Model, stack
               and learning simulations. 1998.
             [4] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014.
             [5] Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks, volume 385 of Studies in
               Computational Intelligence. Springer, 2012.
             [6] Markus Dreyer, Jason R. Smith, and Jason Eisner. Latent-variable modeling of string transductions with
               ﬁnite-state methods. In Proceedings of the Conference on Empirical Methods in Natural Language Pro-
               cessing, EMNLP ’08, pages 1080–1089, Stroudsburg, PA, USA, 2008. Association for Computational
               Linguistics.
             [7] Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. OpenFST: A gen-
               eral and efﬁcient weighted ﬁnite-state transducer library. In Implementation and Application of Automata,
               volume 4783 of Lecture Notes in Computer Science, pages 11–23. Springer Berlin Heidelberg, 2007.
             [8] Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Com-
               putational linguistics, 23(3):377–403, 1997.
             [9] Alex Graves. Sequence transduction with recurrent neural networks. In Representation Learning Work-
               sop, ICML. 2012.
             [10] Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. Learning context-free grammars: Capabilities and
               limitations of a recurrent neural network with an external stack memory. In Proceedings of The Fourteenth
               Annual Conference of Cognitive Science Society. Indiana University, 1992.
             [11] Sreerupa Das, C Lee Giles, and Guo-Zheng Sun. Using prior knowledge in a {NNPDA} to learn context-
               free languages. Advances in neural information processing systems, 1993.
             [12] Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A Smith. Transition-based de-
               pendency parsing with stack long short-term memory. In Proceedings of the 53rd Annual Meeting of the
               Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan-
               guage Processing (Volume 1: Long Papers), ACL ’15, pages 334–343, Beijing, China, 2015. Association
               for Computational Linguistics.
             [13] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. Weakly supervised memory net-
               works. CoRR, abs/1503.08895, 2015.
             [14] Wojciech Zaremba and Ilya Sutskever. Reinforcement learning neural turing machines. arXiv preprint
               arXiv:1505.00521, 2015.
             [15] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets.
               arXiv preprint arXiv:1503.01007, 2015.
             [16] VinodNairandGeoffreyEHinton. Rectiﬁedlinearunitsimproverestrictedboltzmannmachines. InPro-
               ceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814, 2010.
             [17] Alfred V Aho and Jeffrey D Ullman. The theory of parsing, translation, and compiling. Prentice-Hall,
               Inc., 1972.
             [18] Dekai Wu and Hongsing Wong. Machine translation with a stochastic grammatical channel. In Pro-
               ceedings of the 17th international conference on Computational linguistics-Volume 2, pages 1408–1415.
               Association for Computational Linguistics, 1998.
             [19] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of
               its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012.
             [20] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem.
               Computing Research Repository (CoRR) abs/1211.5063, 2012.
             [21] Zhi-Hua Zhou, Jianxin Wu, and Wei Tang. Ensembling neural networks: many could be better than all.
               Artiﬁcial intelligence, 137(1):239–263, 2002.
                                  11
                            A AnalysisoftheBackwardsDynamicsofaNeuralStack
                            We describe here the backwards dynamics of the neural stack by examining the relevant partial
                            derivatives of of the outputs with regard to the inputs, as deﬁned in Equations 1–3. We use δ        to
                                                                                                                              ij
                            indicate the Kronecker delta (1 if i = j, 0 otherwise). The equations below hold for any valid row
                            numbers i and n.
                                    ∂V [i]                               ∂V [i]                             ∂s [i]
                                       t     =δ         (12)                t    =δ        (13)                t   =δ         (14)
                                  ∂V     [n]     in                       ∂v        it                       ∂d        it
                                     t−1                                     t                                  t
                                                                            t
                                        ∂r                                 X                    ∂r        ∂r
                                          t   =min(s [n],max(0,1−               s [j]))   and      t =      t  =d             (15)
                                      ∂V [n]           t                         t              ∂v      ∂V [t]      t
                                         t                               j=n+1                     t       t
                                                                                                  t−1
                                                                                                   P
                                                    1       if i < n < t and s [i] > 0 and u −         s    [j] > 0
                                                                               t             t           t−1
                                                   
                                        ∂st[i]                                               t−1j=i+1
                                      ∂st−1[n] =  δ         if i < t and s [i] > 0 and u −    P s [j]≤0                      (16)
                                                    in                    t             t           t−1
                                                   
                                                                                            j=i+1
                                                 0         otherwise                    t−1
                                      ∂st[i]     -1     if i < t and s [i] > 0 and u −   P s [j]>0
                                             =                       t              t   j=i+1 t−1                            (17)
                                       ∂u
                                          t        0     otherwise
                                                  t
                                        ∂rt   =Xh(i,n)·Vt[i]
                                      ∂st[n]     i=1
                                                                                          P
                                                             δ      if s [i] ≤ max(0,1 −     t      s [j])
                                                          in          t                     j=i+1 t                          (18)
                                                                                                      P
                                                                    if i < n and st[i] > max(0,1 −       t     st[j])
                                      where    h(i,n) =      −1           P                               j=i+1
                                                                    and    t      s [j] ≤ 1
                                                                           j=i+1 t
                                                          0        otherwise
                            All partial derivatives other than those obtained by the chain rule for derivatives can be assumed
                            to be 0. The backwards dynamics for neural Queues and DeQues can be similarly derived from
                            Equations 4–11.
                            B ANoteonControllerInitialisation
                            Duringinitialexperimentswiththecontinuousstackpresentedinthispaper,wenotedthatthestack’s
                            ability to learn the solution to the transduction tasks detailed here varied greatly based on the random
                            initialisation of the controller. This initially required us to restart training with different random
                            seeds to obtain behaviour consistent with the learning of an algorithmic solution (i.e. rapid drop in
                            validation perplexity after a short number of iterations).
                            Analysis of the backwards dynamics presented in Section A demonstrates that error on push and
                            pop decisions is a function of read error “carried” back through time by the vectors on the stack
                            (cf. Equation 14 and Equations 17–18), which is accumulated as the vectors placed onto the stack
                            by a push, or retained after a pop, are read at further timesteps. Crucially, this means that if the
                            controller operating the stack is initially biased in favour of popping over pushing (i.e. u     >d
                                                                                                                         t+1      t
                            for most or all timesteps t), vectors are likely to be removed from the stack the timestep after they
                            were pushed, resulting in the continuous stack being used as an extra recurrent hidden layer, rather
                            than as something behaving like a classical stack.
                            Theconsequenceofthis is that gradient for the decision to push at time t only comes via the hidden
                            state of the controller at time t + 1, so for problems where the vector would ideally have been
                                                                              12
                        preserved on the stack until some later time, signal encouraging the controller to push with higher
                        certainty is unlikely to be propagated back if the RNN controller suffers from vanishing gradient
                        issues. Likewise, the gradient for the decision to pop is 0 (as each pop empties the stack). We
                        conclude that under-using the memory in such a way makes its proper manipulation hard to learn by
                        the controller.
                        Conversely, over-using the stack (even incorrectly) means that gradient obtained with regard to the
                        (mis)use is properly communicated, as the pop gradient will not be zero (Equation 17) for all t.
                        Additionally, the (non-vanishing) gradient propagated through the stack state (Equation 12) will
                        allow the decision to push at some timestep to be rewarded or penalised based on reads at some
                        muchlater time. These remarks also apply to the continuous queue and double-ended queue.
                        Sinceinoursettingthedecisiontopushandpopisproducedbytakingabiasedlineartransformofan
                        RNNhiddenstatefollowedbyacomponent-wisesigmoidoperation,wehypothesised,basedonthe
                        above analysis, that initialising the bias for popping to a negative number would solve the variance
                        issue described above. We tested this on short sequences of the copy task, and found that a small
                        bias of −1 produced the desired algorithmic behaviour of the stack-enhanced controller across all
                        seeds tested. Setting this initialisation policy for the controller across all experiments allowed us to
                        reproduce the results produced in the paper without need for repeated initialisation. We recommend
                        that other controller implementations provide similar trainable biases for the decision to pop, and
                        initialise them following this policy (and likewise for controllers controlling other continuous data
                        structures presented in this paper).
                        C InversionTransductionGrammarsusedinExperiments
                        We present here, in Table 1, the inverse transduction grammars described in Section 4.2. Sets of
                        terminal-generating rules are indicated by the form ‘X → ...’, where i ∈ [1,k] and p(X ) ≈
                                                                        i                               i
                        (100/k)−1 for k terminal generating non-terminal symbols (classes of terminals), so that the gener-
                        ated vocabulary is balanced across classes and of a size similar to other experiments.
                                                                         p      ITGRules
                                                                         1      A→B1|B1
                           p      ITGRules                               1/4    B→B1orB2|B1oderB2
                                                                         1/4    B→S1andS2|S1undS2
                           1      A→S1VT2O3|S1O3VT2                      1/2    B→B1V1|B1V1
                           1/5    S→S1S2|S1S2                            3/4    V→W1B2|W1B2
                           1/5    S→S1rpiS2VT3|S1rpoS2VT3                1/4    V→W1|W1
                           3/5    S→ST1|ST1                              1/6    S→theM1|derM1
                           1/5    O→O1O2|O1O2                            1/6    S→theF1|dieF1
                           1/5    O→S1rpiS2VT3|S1rpoS2VT3                1/6    S→theN1|dasN1
                           3/5    O→OT1|OT1                              1/6    S→aM1|einM1
                           1/33   ST →si |so                             1/6    S→aF1|eineF1
                                    i    i   i
                           1/33   OTi →oii | ooi                         1/6    S→aN1|einN1
                           1/33   VT →vi |vo                             1/25   W →we |wg
                                    i    i    i                                  i     i    i
                                                                         1/25   M →me |mg
                                    (a) SVO-SOVGrammar                           i     i    i
                                                                         1/25   F →fe |fg
                                                                                 i    i   i
                                                                         1/25   N →ne |ng
                                                                                 i    i    i
                                                                       (b) English-German Conjugation Grammar
                                        Table 1: Inversion Transduction Grammars used in ITG Tasks
                        D ModelSizes
                        Weshow,inTable2,thenumberofparameterspermodel,forallmodelsusedintheexperimentsof
                        the paper.
                                                                 13
                                                                 Hiddenlayersize
                                             Model              256           512
                                                                     5            6
                                             1-layer LSTM    3.3 ×10       1.2 ×10
                                                                     5            6
                                             2-layer LSTM    9.1 ×10       3.4 ×10
                                                                     6            6
                                             4-layer LSTM    2.1 ×10       7.8 ×10
                                                                     6            7
                                             8-layer LSTM    4.5 ×10       1.7 ×10
                                                                     5            6
                                             Stack-LSTM      6.7 ×10       1.9 ×10
                                                                     5            6
                                             Queue-LSTM      6.7 ×10       1.9 ×10
                                                                     6            6
                                             DeQue-LSTM      1.0 ×10       2.5 ×10
                                           Table 2: Number of trainable parameters per model
                       E FullResults
                       WeshowinTable3thefull results for each task of the best performing models. The procedure for
                       selecting the best performing model is described in Section 5.
                                                                       Training       Testing
                                   Experiment         Model         Coarse  Fine   Coarse  Fine
                                                      1-layer LSTM  0.62    0.87   0.00    0.38
                                                      2-layer LSTM  0.80    0.95   0.00    0.47
                                                      4-layer LSTM  0.98    0.98   0.01    0.50
                                   SequenceCopying    8-layer LSTM  0.57    0.83   0.00    0.31
                                                      Stack-LSTM    0.89    0.94   0.00    0.22
                                                      Queue-LSTM    1.00    1.00   1.00    1.00
                                                      DeQue-LSTM    1.00    1.00   1.00    1.00
                                                      1-layer LSTM  0.78    0.87   0.01    0.09
                                                      2-layer LSTM  0.91    0.94   0.02    0.06
                                                      4-layer LSTM  0.93    0.96   0.03    0.15
                                   SequenceReversal   8-layer LSTM  0.95    0.98   0.04    0.13
                                                      Stack-LSTM    1.00    1.00   1.00    1.00
                                                      Queue-LSTM    0.44    0.61   0.00    0.07
                                                      DeQue-LSTM    1.00    1.00   1.00    1.00
                                                      1-layer LSTM  0.53    0.93   0.01    0.53
                                                      2-layer LSTM  0.54    0.93   0.02    0.52
                                                      4-layer LSTM  0.52    0.93   0.01    0.56
                                   BigramFlipping     8-layer LSTM  0.52    0.93   0.01    0.53
                                                      Stack-LSTM    0.44    0.90   0.00    0.48
                                                      Queue-LSTM    0.55    0.94   0.55    0.98
                                                      DeQue-LSTM    0.55    0.94   0.53    0.98
                                                      1-layer LSTM  0.96    0.98   0.96    0.99
                                                      2-layer LSTM  0.97    0.99   0.96    0.99
                                                      4-layer LSTM  0.97    0.99   0.97    0.99
                                   SVOtoSOV           8-layer LSTM  0.98    0.99   0.98    0.99
                                                      Stack-LSTM    1.00    1.00   1.00    1.00
                                                      Queue-LSTM    1.00    1.00   1.00    1.00
                                                      DeQue-LSTM    1.00    1.00   1.00    1.00
                                                      1-layer LSTM  0.97    0.99   0.97    0.99
                                                      2-layer LSTM  0.98    0.99   0.98    0.99
                                                      4-layer LSTM  0.98    0.99   0.98    0.99
                                   GenderConjugation  8-layer LSTM  0.98    0.99   0.99    0.99
                                                      Stack-LSTM    0.93    0.97   0.93    0.97
                                                      Queue-LSTM    1.00    1.00   1.00    1.00
                                                      DeQue-LSTM    1.00    1.00   1.00    1.00
                                           Table 3: Summary of Results for Transduction Tasks
                                                                14
