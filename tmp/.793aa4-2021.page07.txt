                 Appendix C.                                           (en-de), German-to-English (de-en), English-to-
                   For long range input, we consider Linformer          Czech (en-cs) and Czech-to-English (cs-en) (Bo-
                 (Wangetal., 2020) with a projection dimension of       jar et al., 2018). We test the corresponding mod-
                 32. Due to down-projection, we see non-trivial per-    els on Newstest 2018 datasets respectively and re-
                 formance drop, when compared to a Transformer.         port the BLEU score output by SacreBLEU (Post,
                 Evenfor this setting we see that our absolute posi-    2018) with default setting.    Our setup follows
                 tional attention DIET-ABS can be used to improve      Vaswani et al. (2017) closely and use their Ten-
                 the model’s performance.                               sor2Tensor framework (Vaswani et al., 2018). Fol-
                 4.2   Cross-lingual Model Results                      lowingVaswanietal.(2017)weusea6layerTrans-
                                                                        former with encoder-decoder architecture.      For
                 Datasets and Model       For our multilingual ex-      moredetails of our experimental setup please see
                 periments, we pre-train the models on Wikipedia       Appendix A
                 corpus in 100 languages similar to (Lample and         Results   Wereport the BLEU scores of the mod-
                 Conneau, 2019) for 125K steps with a sequence          els in Table 4. We observe that moving positional
                 length of 512, and then ﬁne-tune on downstream         information from input to per-head attention layer
                 XTREMEtasks(Huetal.,2020). Weuselanguage-              improves BLEU scores. Different variations of
                 independent tokenizer, Sentence Piece (Kudo and        per-head positional attention do not make much
                 Richardson, 2018) model, with 120,000 token vo-        difference with DIET-REL being competitive with
                 cabulary to encode input text.                         Shawetal. (2018).
                 Classiﬁcation    Weconduct5trials of ﬁne-tuning        4.4  Ablation Study
                 for each model on the MultiNLI (Williams et al.,
                 2018) training data, then perform zero-shot predic-    In this section, we share our ﬁndings of key factors
                 tions on XNLI (Conneau et al., 2018), choosing         that affect performance of decoupled positional
                 median accuracy to report.                             attention.
                 QuestionAnswering Weconduct5trialsofﬁne-               Sharing the Positional Encoding          Previous
                 tuning for each model on SQuAD V1.1 dataset,          works (Raffel et al., 2020; Ke et al., 2020; Shaw
                 following by zero-shot predictions on XQuAD (11        et al., 2018) used different sharing methods for the
                 languages), MLQA (7 languages) and TyDiQA-             positional encodings to reduce the model parame-
                 GoldP (9 languages), choosing median F1 / EM           ters. We present a detailed study on different forms
                 scores to report.                                      of sharing positional encodings and its effect on
                 Results   We present our results on the classiﬁ-       performance. In particular, we compare the fol-
                 cation and question answering ﬁnetuning tasks in       lowing variations in sharing the position encoding
                 XTREMEfordifferent position and segment en-            parameters across different heads and the layers in
                 coding methods in Table 3. Again all per-head          the Transformer.
                 position encoding methods outperform input addi-         • head-wise - Same parameters are used for all
                 tive position encodings. Interestingly, our simple          heads in a layer, with different layers using
                 DIET-ABS turns out to be the best model, better             different parameters (Shaw et al., 2018; Ke
                 than other models using relative position features.         et al., 2020).
                 Layer-wise sharing and per-head segment attention        • layer-wise - Sharing of position encoding pa-
                 allows DIET-ABS to outperform DIET-REL. We                  rameters across layers with different parame-
                 present a detailed ablation study in Table 5 to un-         ters for each head (Raffel et al., 2020).
                 derstand effect of decoupled positional attention
                 variants. Finally, we notice similar advantages in       • none - Every layer and head uses different
                 using DIET-ABS with the Linformer (Wang et al.,             position encoding parameters.
                 2020) model in the long range setting.
                 4.3   Translation Results                             We present results comparing different sharing
                                                                        methods in Table 5 for XTREME tasks. We make
                 DatasetsandModel Forthemachinetranslation              the following observations 1) head-wise sharing is
                 task we consider two language pairs (both direc-       consistently worse than layer-wise, 2) sharing hurts
                 tions) for training - WMT 2018 English-to-German       the performance of DIET-REL whereas it improves
                                                                   2980
