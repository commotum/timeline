                       +
                  [LCG 19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
                             cut. ALBERT: A lite BERT for self-supervised learning of language representations. arXiv preprint
                             arXiv:1909.11942, 2019.
                       +
                  [LCH 20] XiaodongLiu,HaoCheng,PengchengHe,WeizhuChen,YuWang,HoifungPoon,andJianfengGao.
                             Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994, 2020.
                    [LDL19] ZhongyangLi,XiaoDing,andTingLiu. Storyendingprediction by transferable bert. arXiv preprint
                             arXiv:1905.07504, 2019.
                   [LDM12] HectorLevesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In Thirteenth
                             International Conference on the Principles of Knowledge Representation and Reasoning, 2012.
                       +
                  [LGG 20] YinhanLiu,Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and
                             LukeZettlemoyer. Multilingual denoising pre-training for neural machine translation. arXiv preprint
                             arXiv:2001.08210, 2020.
                       +
                  [LGH 15] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-Yi Wang. Representation
                             learning using multi-task deep neural networks for semantic classiﬁcation and information retrieval. In
                             Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational
                             Linguistics: Human Language Technologies, 2015.
                     [LH17] Ilya Loshchilov and Frank Hutter.   Decoupled weight decay regularization.   arXiv preprint
                             arXiv:1711.05101, 2017.
                 [LHCG19a] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Improving multi-task deep neural
                             networksviaknowledgedistillationfornaturallanguageunderstanding. arXivpreprintarXiv:1904.09482,
                             2019.
                 [LHCG19b] XiaodongLiu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for
                             natural language understanding. arXiv preprint arXiv:1901.11504, 2019.
                     [Lin20] TalLinzen. Howcanweaccelerateprogresstowardshuman-likelinguisticgeneralization? arXivpreprint
                             arXiv:2005.00955, 2020.
                       +
                  [LLG 19] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,AbdelrahmanMohamed,OmerLevy,
                             Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural
                             language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.
                     [LM17] KeLiandJitendraMalik. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017.
                       +
                  [LOG 19] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,
                             LukeZettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach.
                             arXiv preprint arXiv:1907.11692, 2019.
                   [LPP+20] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
                                       ¨                                       ¨
                             Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, Sebastian Riedel, and Kiela Douwe.
                             Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401,
                             2020.
                   [LSP+18] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
                             Shazeer. Generating Wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.
                  [LWS+20] ZhuohanLi,EricWallace,ShengShen,KevinLin,KurtKeutzer,DanKlein,andJosephE.Gonzalez.
                             Train large, then compress: Rethinking model size for efﬁcient training and inference of transformers,
                             2020.
                       +
                  [LXL 17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading
                             comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.
                       +
                  [LYN 20] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju Wang, and Jimmy
                             Lin. Tttttackling winogrande schemas. arXiv preprint arXiv:2003.08380, 2020.
                    [Mac92] David. MacKay. Information-based objective functions for active data selection. Neural Computation,
                             1992.
                                                                  71
