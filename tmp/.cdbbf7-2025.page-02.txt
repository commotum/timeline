                this architecture to receive 1D serialized inputs. The conver-        computation cost,
                sion of dense voxel features into sparse representations can        • An accurate, explicit multi-modal fusion approach, real-
                beachievedthroughstraightforwardfilteringofzero-valued                ized by the benefit of 3D voxel features carrying 3D po-
                features. Obtaining voxel features with a higher resolution           sitional coordinates,
                is done by deriving intermediate features from a conven-            • Multi-modal feature refinement and additional feature
                tional LiDARbackbone,whichimplyingthatweratheruses                    sparsification for more sparse yet well-fused multi-modal
                a less computational cost than that of BEV features.                  features, which also enable flexible control over the num-
                   Utilizing 3D voxel features has another benefit in multi-          ber of sparse features.
                modal fusion with a image modality. 3D positional coor-
                dinates of all voxels are embedded to the voxel features            2. Related Work
                so that each voxel feature can be explicitly and accurately         Typically, 3D object detection approaches for autonomous
                projected to a corresponding image feature. By simply con-          driving utilize datasets such as KITTI [9], Waymo Open
                catenating each voxel and the corresponding image feature,          Dataset [31], and nuScenes [3]. Given the differences in
                we can perform explicit and accurate multi-modal feature            view coverage and multi-modality among these datasets,
                fusion. On the other hand, previous approaches depend on            mostapproachesfocusononespecificdataset.Inthispaper,
                depth estimation for explicit fusion [17, 24, 42] or implicit       wetarget multi-modal 3D object detection and thus specifi-
                fusion via transformer attention [39], which can eventually         cally focus on the nuScenes dataset, which is unique in that
                lead to erroneous multi-modal alignment and weak multi-
                modal information fusion.                                           it is the only one to provide 360◦ view coverage and full
                   While our fundamental design has already successfully            multi-modality with LiDAR and camera sensors.
                improved the performance of previous BEV-based models
                whilesignificantly reducing the numberofmulti-modalfea-             3D object detection with a single modality            3D ob-
                tures used, further enhancements can be achieved through            ject detection with a camera modality is a common set-
                the utilization of sparse feature-specific operations. Firstly,     ting in computer vision and has been extensively studied.
                the majority of parameters in a previous LiDAR backbone             In a standard 3D object detection task from a single im-
                are used for processing dense features and are thus not uti-        age [2, 25, 29, 35, 37], the input camera space could be
                lized in our architecture, implying that our basic detector         sufficient for detection outputs. However, in autonomous
                could use incompletely encoded features. To address this,           driving, the output space needs to be a 3D world space sur-
                we employ a recently proposed sparse feature refinement             rounding the ego-vehicle, covered by multiple images with
                module [33] to fully encode the geometric information in            normal field-of-views. This introduces a representation gap
                our sparse voxel features. We also discover that this refine-       between the input and output spaces. For this reason, sev-
                ment can be more effectively applied to our multi-modal             eral works [11, 18] use a BEV space for their representation
                fusedfeaturesratherthansolelytotheLiDARfeatures.Sec-                space. However, transforming image features to a 3D world
                ondly, a problem arises due to the varying number of trans-         space or BEV space requires additional depth information.
                former tokens from LiDAR samples, which is caused by                   In the field of 3D object detection for autonomous driv-
                differing levels of sparsity. To address this, we implement         ing, using a LiDAR modality [19, 28, 40] is another main-
                an additional feature elimination technique. This not only          stream area of research. Given that LiDAR data takes the
                reduces the computational cost but also maintains a consis-         form of point clouds, it provides high localization accuracy
                tent number of transformer tokens, irrespective of the spar-        for objects. Unlike the camera modality, the LiDAR space
                sity variations in the LiDAR data.                                  can be considered as a unified space for both input and out-
                   Extensive experimental results validate that sparse but          put modalities. However, 3D voxel features of LiDAR data
                accurately fused multi-modal features can effectively de-           require significant computational resources due to the num-
                tect long-range objects by exploiting fine-level geomet-            berofcells.Hence,toreducethecomputationalcomplexity,
                ric features inherent in high-resolution 3D voxels. Finally,        recent works [14, 27, 36] also adopt a BEV space instead
                our approach achieves state-of-the-art performance on the           of a 3D space. As a result, recent approaches with either
                nuScenes dataset [3] with a faster inference speed.                 a LiDAR or camera [20, 41] modality tend to use a BEV
                   Tosummarize, our contributions include:                          space for feature representation. However, this may result
                • A novel multi-modal 3D object detection framework,                in overlooking 3D geometric information in high-resolution
                  breaking the current paradigm to use a BEV space and              features, especially information along the z-axis.
                  achieving a state-of-the-art performance in 3D object de-            More recently, Chen et al. [6] and Zhang et al. [45]
                  tection on the nuScenes dataset,                                  present frameworks for fully sparse 3D object detection
                • Direct utilization of sparse 3D voxel features to exploit a       based on a LiDAR modality, which uses sparse voxel fea-
                  higher-resolution geometric information while reducing a          tures in 3D object detection heads. However, there are three
