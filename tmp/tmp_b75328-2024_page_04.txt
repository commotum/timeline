                        Figure 2: Visualization of data formats and positional embeddings. Abacus Embeddings give the
                        samepositional embeddings to all digits of the same significance.
                        Experimental Setup.  Wetrain decoder-only causal language models to solve addition problems.
                        Following prior work [Zhou et al., 2023, 2024, Shen et al., 2023, Kazemnejad et al., 2023, Lee et al.,
                        2023], inputs are formatted least significant digit first, e.g. 98282 + 3859172 = 2787472. Unlike
                        prior work, we do not add any padding between digits [Shen et al., 2023] and do not pad any numbers
                        with zeros, neither in the case of carry digits [Zhou et al., 2024], nor to make all operands the same
                        length [Shen et al., 2023]. We train on all combinations of operand lengths less than or equal to i and
                        j where i and j are the maximum lengths of the first and second operands, respectively. For this study
                        all training sets have 20 million samples and i = j, hence we can use one number to define the dataset
                        i, where i is the maximum length of either operand. We sample data with replacement and we stratify
                        the data, so that all length pairs (i,j) are equally sampled during training. To facilitate training
                        of many models from scratch, we use a language model cramming setup [Geiping and Goldstein,
                        2023] and limit each training run to 8 exaFLOP of compute (a single Nvidia RTXA4000 GPU for
                        24 hours); for multiplication results we allow 64 exaFLOP (eight Nvidia RTXA4000 GPUs for 24
                        hours). During training, we mask the input question and only compute loss on the answer digits. For
                        further details on data construction and training we refer to Appendix A.2.
                        Wereport model accuracy for each (i,j) length pair and unlike most existing work, we also include
                        accuracy for pairs where i ̸= j to highlight all instances of extrapolation. This extensive tabulation is
                        costly and makes inference the main computational burden of this study. Since our training pipeline
                        produces fairly consistent results, we report the mean over three runs (rather than using a best-of-ten
                        reporting scheme [Zhou et al., 2024]). We measure accuracy in the strict sense where only exact
                        matches of all output digits are counted as correct, i.e. if a single digit is incorrect then the example is
                        markedaswrongandwerefertothisasexactmatchaccuracy. Wehavethefollowingthreeevaluation
                        categories: (i) in distribution (ID) where the models are tested on problems up to the maximum
                        size seen during training; (ii) out of distribution (OOD) where the models are tested on problems
                        greater than the maximum size seen during training but both operands are at most 100 digits; (iii) and
                        extreme out of distribution (100+ digit OOD) where the models are tested on problems where both
                        operands are of the same length and are both more than 100 digits and less than 160 digits. In the
                        100+OODsetting,weonlyanalyzeproblemswheretheoperandsarethesamelength(i = j) due to
                        inference costs at this scale.
                        Weconsider two standard transformer architectures. First, we use a standard autoregressive trans-
                        former model where multiple decoder layers are stacked in a feedforward manner. Second, we
                        enhance this standard transformer model by incorporating input injection, where the embedded inputs
                        are added to the input of each decoder layer [Ma et al., 2022, Bansal et al., 2022, Anil et al., 2022a].
                        Wevisually describe the architectures in the Appendix Figure 22.
                        3.1  AbacusEmbeddingsHelpAlignDigits
                        From prior work and our own initial experiments, we observe that even when input numbers are
                        presented least-significant digit first and training data is stratified and abundant (several million
                        examples), standard transformers struggle to learn multi-digit addition. We also observe that humans
                        do long addition by first aligning the digits of the same significance into columns. Thus, our first
                        hypothesis is that the significance of each digit (i.e. each digit’s position relative to the beginning of
                        the number) is not easy for transformers to represent, and that this sub-problem presents more of a
                        hurdle than the actual addition itself.
                        Prior work addresses this by proposing explicit index hints in the inputs and outputs of the addition,
                        for example a6b7c5 + a1b6c3 = a7b3c9, finding that transformers perform much better on addition
                        with the information provided by such hints [Zhou et al., 2023, 2024]. However, index hints of this
                        form increase the input context length required and double the output length and inference cost of
                        solving a given addition problem. Furthermore, Zhou et al. [2024] find that the ability of models
                        trained with index hints to generalize is sensitive to the particular random initialization. Zhou et al.
                                                                  4
