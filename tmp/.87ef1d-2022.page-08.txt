                           Published as a conference paper at ICLR 2022
                                                       2.8
                                                                           Memory Fine-tuning
                                                                           Transformer
                                                       2.6                 Memorizing Transformer
                                                       2.4
                                                      Perplexity (arXiv Math)2.2
                                                          0             300K       500K 600K
                                                                       Training steps
                               Figure 6: Finetuning a 1B vanilla Transformer model to use external memory of size 65K.
                           Finetuning a non-memory model to use memory          Pretraining can be very costly both in time
                           and computational resources. Thus, a natural question to ask is: can one ﬁne-tune a pretrained
                          Transformer to use external memory? The answer is yes!
                           Wetookapre-trained 1B vanilla Transformer model, and ﬁne-tuned it to use external memory (the
                          1BmodelsusedinSection4.4). The ﬁne-tuning result is shown in Figure 6. Notice that the model
                           quickly learns to use external memory. Within 20K steps (4% of the pre-training time) the ﬁne-tuned
                           model has already closed 85% of the gap between it and the 1B Memorizing Transformer, and after
                          100ksteps it has closed the gap entirely.
                           4.6  INFORMATION RETRIEVAL PATTERNS
                           Weconductedaqualitative study of what the model was actually retrieving from external memory,
                           by ﬁnding which tokens showed the biggest improvements in cross-entropy loss when the size of
                           the memory was increased, and then examining the top-k retrieved memories for those tokens. We
                           found that the model gained the most when looking up rare words, such as proper names, references,
                           citations, and function names, where the ﬁrst use of a name is too far away from subsequent uses to
                           ﬁt in the local context. This result is in keeping with the prior analysis of long-context Transformers
                           on PG19 (Sun et al., 2021), which found similar lookup patterns. For this experiment, we used a
                           slightly older version of the architecture without the gating mechanism.
                           Which tokens show a beneﬁt from memory?         Figure 7 shows a visualization of which tokens
                           showanimprovementwhenthesizeoftheexternalmemoryisincreased. Weselectedamathpaperat
                           random, and plotted the difference in cross entropy loss for each token xi in the paper, comparing two
                           models with the same parameters, but with memories of different sizes. ∆ = cross-entropy    (x )
                                                                                                  i                8192  i
                           −cross-entropy     (x ). Positive values show an improvement in loss.
                                          32K   i
                           Thex-axis on the chart is the token number i, while the y-axis is ∆i. For the ﬁrst 8192 tokens, the
                           difference between the two models is zero, since the larger capacity of the 32K memory isn’t being
                           used yet. However, after token 8193, we can see that the larger memory helps, on average, over the
                           smaller memory. The beneﬁt is not universal, since the predictions for some tokens become worse,
                           possibly due to the fact that a relevant retrieved memory no longer makes it into the top-k when the
                           size of the external memory is increased. This ﬁgure also shows that the beneﬁt of external memory
                           is somewhat sparse. The improvement in perplexity seems to be mainly driven by a small percentage
                           of tokens that obtain a large improvement in cross-entropy loss when using the larger memory.
                           Whatinformation is being looked up?      Given that only a subset of tokens shows improvement
                           from external memory, we did a further investigation into what, exactly, those tokens are using the
                           memoryfor. Wetook those tokens which showed the largest improvement in cross-entropy loss, and
                           for each of them tokens, we examined the top-k retrieved memories. We studied arXiv math, Github
                           and Isabelle corpus. For arXiv math and Github, we found the model retrieved function and variable
                           names. See more details with examples in Appendix B.
                                                                          8
