              Accelerating Program Synthesis using Learned Probabilistic Models              PLDI’18, June 18–22, 2018, Philadelphia, PA, USA
                                                                  Î        !       Example3.6. ConsiderthesecondCEGISiterationofthe
                 ∀A ∈ N. h(A) =        max      q(A → β | c) ×       h(βi) .       weighted enumeration described in Table 2 where pts =
                                   A→β∈R,c∈C                     βi ∈N             {“-.”}. Supposewehavetwosententialformsn1 = (“-”+“.”)+S
              Thefunctionhcanbeobtainedbythefollowingsteps:i)start                 andn =x +S alongwiththeircostsinthepriority queue
              withh(A) = 0forallA ∈ N;ii)repeatedlyupdateh usingthe                      2
                                                                                   duringthesearch.Then,n ≈          n holdsandwecanremove
               aboveequationuntilsaturation.Notethati)theconditioning                                         1  pts  2
               setC should be finite to do the above fixpoint computation,         either n1 or n2 from the priority queue for the following
               andii) we can arbitrarily choose any non-terminal at each           reason. Let P1 = (“-” + “.”) and P2 = x. Then, P1 < n1 and
                                                                                                           J   K          J  K
               iteration. After a finite number of iterations, the estimate h      P2 < n2. In addition, P1 (“-.”) =       P2 (“-.”) = “-.”. Also,
                                                                                   n [P /ϵ] ≈     n [P /ϵ] because n [P /ϵ] = n [P /ϵ] = +S.
               always converges.                                                    1   1      pts  2  2               1  1        2  2
                                                                                   Therefore,n ≈       n .
               Example3.2. Consider the following PCFG in which each                            1  pts  2
               production rule is associated with a probability.                      Therelation is sound in the following sense.
                       S   → aSb (0.9)              S   → c (0.1)                  Theorem3.7. ∀pts.n ≈           n =⇒ n ∼ n
                                                                                                           i  pts  j        i  pts  j
              wherea,b,andc areterminalsymbols.Atthebeginning,h(S)                    Wedetail the lines 10-14 in Algorithm 2. We group multi-
               is set to be 0. At the 1st iteration, h(S) = max(0.9×0, 0.1) =      ple sentential forms together to abstract search space. For
               0.1. At the 2nd iteration, h(S) = max(0.9 × 0.1, 0.1) = 0.1. It     each equivalence class, only a representative that has the
               converges in two iterations.                                        highest probability is maintained in the queue (line 11). If
                 To conclude, our heuristic function д always underesti-           anytwosententialformsn andn′ areequivalent,weremove
               mates the exact future distances.                                   one of the smaller scores from the queue to avoid exploring
                                                                                   all paths reachable from that node. In the implementation,
                                             ∗          ∗                          in order to save computation, we maintain a map that keeps
              Theorem3.3. ∀n ∈ (N ∪Σ) .д(n) ≤ д (n).                               track of the representatives of equivalence. This map let us
               3.4  Optimizations                                                  avoiding redundant comparisons between sentential forms.
               In this section, we illustrate how to incorporate two pow-          Theorem 3.8. For a given synthesis problem, assuming P
               erful orthogonal optimization techniques employed by the            is finite, weighted_search generates a sequence of candi-
               existing search strategies into the basic algorithm.                                             e
                                                                                   date programs satisfying the prioritization, correctness, and
               3.4.1  PruningwithEquivalenceClasses                                completeness properties.
              WefurtherimprovethesearchefÏciencyviathenotionofthe                  3.4.2   Divide-and-ConquerEnumeration
               equivalence class of sentential forms, which is an extended         Wecanfurther improve the search efÏciency by adopting
               notion of the equivalence classes of programs used in the           the divide-and-conquer enumerative approach [4] when we
               existing enumerative search strategy.                               aimtosynthesizeprogramswithconditionals.Thisapproach
               Definition3.4(Equivalenceofsententialforms). Foragiven              allows synthesizing large conditional expressions. The idea
               derivationgraphofsententialformsG(G )andasetofinputs                is to find different expressions that work for different subsets
                                                        q                          of the inputs, and unify them into a solution that works for
                                                            ∗
               pts, two sentential forms ni,nj ∈ (N ∪ Σ) are equivalent            all inputs. The sub-expressions are found using enumera-
               modulo pts (denoted ni ∼pts nj) if all pairs of programs            tion techniques and are then unified into a program using
               (Pi,Pj) derivable from ni and nJ respectively have the same         techniques for decision tree learning.
               input-output behavior with respect to pts, formally:                   Thealgorithmenumeratestermsandpredicatesseparately
                                       r          r         J  K      q y          andunifies them into a single large conditional expression.
                ∀Pi,Pj ∈ P,x ∈ pts. ni ⇝ Pi ∧nj ⇝ Pj =⇒      Pi (x) = Pj (x).      For example, in the if-then-else expression ite(x ≤ y,y,x),
                 Computing the above equivalence relation is infeasible            the terms arex andy, and the predicate isx ≤ y. To this end,
               in general because there may be infinitely many programs            the algorithm initially automatically decomposes a given
               reachable from given sentential forms. We instead use the           context-free grammarG into a pair of grammars ⟨G ,G ⟩
               following relation.                                                                                                         T   P
                                                                                   where (a) the term grammar GT is a grammar generating
               Definition 3.5 (Weak equivalence of sentential forms). For          terms of type of target program; and (b) the predicate gram-
               a given graph of sentential forms G(Gq) and a set of inputs         marGP is a grammar generating boolean terms. We refer
                                                            ∗                      the reader to [4] for more details.
               pts, two sentential forms ni,nj ∈ (N ∪ Σ) are equivalent               Ourweightedenumerationwiththedivide-and-conquer
               modulopts(denotedn ≈          n ) iff n = n or
                                       i  pts  j     i    j                        strategy is described in Algorithm 3. It takes two statistical
                                                           J  K       J  K
                ∃Pi,Pj ∈ P.   Pi < ni,Pj < nj,∀x ∈ pts. Pi (x) = Pj (x)            programmodels:thetermmodelGT andthepredicatemodel
                              n [P /ϵ] ≈     n [P /ϵ]                                                                  q
                                i  i     pts  j  j                                 GP,andthetwoheuristicfunctionsbasedonthosegrammars,
              where < denotes the subsequence relation.                              q
                                                                                   respectively. That means we need to train two statistical
