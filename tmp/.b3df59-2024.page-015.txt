                                      TheSurprising Effectiveness of Test-Time Training for Few-Shot Learning
                                                                FT                 FT                         36
                              35                                FT+TTT             FT+TTT
                              30                                 29                29           29
                              25              24                                                             +22
                              20                       20                                       +18
                                                                +24               +24
                             asks Solved15   +19                             asks Solved
                             T                         +15                   T
                              10    9
                                                                                                11            14
                               5    +9
                                              5         5        5                 5
                               0
                                   No FT      All               No-LM              1B            3B           8B
                                                      No-Geom
              Figure 13. Left: Accuracy when fine-tuning with different data sources. While all fine-tuned models perform similarly, their
              performance after TTT shows considerable variance. As expected, removing geometric transformations from the fine-tuning data reduces
              performance compared to the model trained on the full dataset. Surprisingly, excluding LM-generated data from fine-tuning actually
              outperforms the model trained on all data. Right: Performance results across different model sizes. As expected, performance of the
              base fine-tuned model improves with increasing model size, aligning with current scaling law trends. However, the scaling behavior after
              TTTisless clear. For instance, the final performance of the 1B and 3B models is identical after TTT. Full discussion in Section B.3.
              B.2. ARCInitial Fine-tuning Hyperparameters
              We perform full fine-tuning on LLama-3 family models by using the torchtune library. We train each model up to
              16000 steps. We use 2xNVIDIA A100 GPU for 1B models, 4xNVIDIA A100 GPU for 3B and 8B models. We present
              hyperparameters in Table 4.
                                                  Table 4. ARC Initial Fine-tuning Hyperparameters
                                            Hyperparameter      Search Space
                                            learning rate       2.5e-5
                                            epochs              2
                                            batch size          32
                                            optimizer           AdamW(Loshchilov&Hutter,2018)
                                            scheduler           Cosine LR Schedule with 2k warmup
              B.3. Results
              We perform full fine-tuning 1B, 3B Llama 3.2 instruction-tuned, and 8B Llama 3 instruction-tuned using augmented
              data. The format and training objective is same as the ones described for TTT in 3. Hyperparameter details are given in
              Appendix C.2. We do the following ablations for augmented data:
                1. No FT: The original Llama 3 instruction-tuned model without any fine-tuning.
                2. All: We use all methods described in Section B.1, including REARC, rule-based augmentation, and LM generation.
                3. No-Geom: Weremovegeometrictransformations from all tasks.
                4. No-LM:WeonlyuseREARCandrule-basedaugmentation,excludingtasksgeneratedbytheLM.
              Weshowresults using different model sizes in Figure 13. Increasing the model size consistently improves FT performance,
              with the 8B model achieving the highest accuracy of 36%. We also observe that TTT effectively closes the performance gap
              for smaller models, with the 1B and 3B models achieving similar accuracy after TTT.
                                                                       15
