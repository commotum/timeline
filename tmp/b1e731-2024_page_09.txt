                            5    ARC-AGIExperiments
                            In this work, we consider the ARC-AGI 2024 challenge [Chollet et al., 2024] as our testing domain for
                            the proposed method. It is a challenging program synthesis dataset that encompasses a wide variety
                            of unique tasks. Because very little training data is available (400 training tasks), the benchmark is
                            resistant to memorization and rather tests for adaptation and out-of-distribution generalization. As
                            justified in Chollet [2019], it is a benchmark that tests developer-aware generalization, which means
                            one possesses limited information regarding the test tasks. Indeed, developers attempting to solve
                            ARC-AGIcannotseetheprivatetest set but are given a set of Core Knowledge priors upon which
                            all tasks are built. Such priors include objectness, goal-directedness (agency), simple arithmetic
                            (numbers and counting), and basic geometry and topology.
                            5.1   LPNArchitecture
                              Rows Columns                Padding tokens        Rows Columns               Padding tokens
                                3   4                                       ...  3   4                                       ...
                                    Input             Output
                                                                         Encoder
                                                                                                   Input         Output
                                       3x4                3x4
                                                                                    Latent
                                                                                        ........
                                                                                                                    ?
                                                          Decoder
                                                                                                     4x3             4x3
                                          4   3                                ...  4   3                                ...
                                      ...
                                Latent   Rows Columns          Padding tokens      Rows Columns         Padding tokens
                            Figure 4: LPN architecture for ARC-AGI. Both the encoder and the decoder are small transformers
                            that take in flattened padded grids as inputs. The actual number of rows and columns are prefixed to
                            each sequence.
                            In all our experiments, programs are defined in the input-output space of ARC-AGI, i.e. 2D grids
                            whose cells can take 10 different values and shapes are (n,m) with n,m ∈ [1,30]. To train a
                            high-performing latent space on such input-output pairs, it is critical to design an encoder that can
                            process both input and output grids to infer a distribution over possible programs and a decoder that
                            can execute a large number of programs on input grids.
                            Weimplementboththeencoderanddecoderassmall transformers [Vaswani et al., 2017] specifically
                            designed for this benchmark, in contrast to the more general large language models (LLMs) typically
                            used [Wang et al., 2023]. By foregoing the transfer learning benefits of large pre-trained models,
                            weaimtoinvestigate the effectiveness of learning programs in this narrow domain and evaluate the
                            potential of test-time latent search. The code used in this research is open source and available at
                            https://github.com/clement-bonnet/lpn. Our codebase is implemented in JAX [Bradbury
                            et al., 2018] and uses neural network building blocks from the Flax library [Heek et al., 2024].
                            Wemodeltheinputandoutputimagesas2Dgridsthatwepadandflatteninaraster-scan fashion
                            to form sequences of pixel values each of size 30 ∗ 30 = 900 (see figure 4). We prefix each grid
                            sequence with shape information, namely 2 extra values for the number of rows and columns, leading
                            to sequences of 902 values.
                                                                               9
