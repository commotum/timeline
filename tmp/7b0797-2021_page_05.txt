                                             Training data-efﬁcient image transformers & distillation through attention
                Table 1. Variants of our DeiT architecture. The larger model, DeiT-     Table 3. Distillation experiments on ImageNet-1k with DeiT, 300
                B,hasthesamearchitectureastheViT-B(Dosovitskiyetal.,2020).              epochs of pre-training. We report the results for the architecture
                Theonlyparameters that vary across models are the embedding di-         augmented with an additional token/embedding in the last three
                mension and the number of heads, and we keep the dimension per          rows. We separately report the performance when classifying
                head constant (equal to 64). Smaller models have a lower parame-        with only one of the class or distillation embedding, and then
                ter count, and a faster throughput. The throughput is measured for      with a classiﬁer taking both of them as input. In the last row
                images at resolution 224×224.                                           (class+distillation), the result correspond to the late fusion of the
                 Model     embedding #heads #layers #params    training throughput      class and distillation classiﬁers.
                           dimension                          resolution (im/sec)                               supervision       ImageNet top-1 (%)
                 DeiT-Ti       192       3      12       5M      224       2536          DeiT: method ↓       label teacher   Ti224 S224 B224 B↑384
                 DeiT-S        384       6      12     22M       224        940          no distillation        ✓      ✗       72.2   79.8   81.8    83.1
                 DeiT-B        768      12      12     86M       224        292          usual distillation     ✗     soft     72.2   79.8   81.8    83.2
                                                                                         hard distillation      ✗    hard      74.3   80.9   83.0    84.0
                Table 2. ImageNet-1ktop-1accuracyofthestudentasafunctionof               class embedding        ✓    hard      73.9   80.9   83.0    84.2
                the teacher model used for distillation. The convolutional Regnet        distil. embedding      ✓    hard      74.6   81.1   83.1    84.4
                                                                                         DeiT : class+distil.   ✓    hard      74.5   81.2   83.4    84.5
                by Radosavovic et al. (2020) have been trained with a similar                 ⚗
                training as our transformers, except that we used SGD. We provide
                more details about their performance and efﬁciency in Table 5.
                Interestingly, image transformers learn more from a convnet than        strategy from Section 4 further improves the performance,
                from another transformer with comparable performance.                   showing that the two tokens provide complementary infor-
                                    Teacher           Student: DeiT-B                   mation useful for classiﬁcation: the classiﬁer on the two
                            Models             acc.   pretrain    ↑384                  tokens is signiﬁcantly better than the independent class and
                                                                                        distillation classiﬁers, which by themselves already outper-
                            DeiT-B             81.8     81.9      83.1                  form the distillation baseline.
                            RegNetY-4GF        80.0     82.7      83.6
                            RegNetY-8GF        81.7     82.7      83.8                  Theembeddingassociated with the distillation token gives
                            RegNetY-12GF       82.4     83.0      83.9                  slightly better results than the class token. It is also more
                            RegNetY-16GF       82.9     83.0      84.0                  correlated to the convnets prediction. In all cases, including
                                                                                        it improves the performance of the different classiﬁers. We
                                                                                        give more details and an analysis in the next paragraph.
                performs the best Vit-B model pre-trained on JFT-300M
                and ﬁne-tuned on ImageNet-1k at resolution 384 (84.15%).                Agreement with the teacher & inductive bias?               Asdis-
                Note, the current state of the art of 88.55% achieved with              cussed above, the architecture of the teacher has an im-
                extra training data is the ViT-H model (632M parameters)                portant impact. Does it inherit existing inductive bias that
                trained on JFT-300M and ﬁne-tuned at resolution 512. Here-              would facilitate the training? While we believe it difﬁcult
                after we provide several analysis and observations.                     to formally answer this question, we analyze in Table 4 the
                                                                                        decision agreement between the convnet teacher, our image
                Convnets teachers.        Wehaveobserved that using a con-              transformer DeiT learned from labels only, and our trans-
                vnet teacher gives better performance than using a trans-               former DeiT . Our distilled model is more correlated to the
                                                                                                      ⚗
                former. Table 2 compares distillation results with different            convnet than with a transformer learned from scratch. As
                teacher architectures. The fact that the convnet is a better            to be expected, the classiﬁer associated with the distillation
                teacher is probably due to the inductive bias inherited by              embeddingis closer to the convnet that the one associated
                the transformers through distillation, as explained in Abnar            with the class embedding, and conversely the one associated
                et al. (2020). In all of our subsequent distillation experi-            with the class embedding is more similar to DeiT learned
                ments the default teacher is a RegNetY-16GF (Radosavovic                without distillation. Unsurprisingly, the joint class+distil
                et al., 2020) with 84M parameters, that we trained with                 classiﬁer offers a middle ground.
                the same data and same data-augmentation as DeiT. This
                teacher reaches 82.9% top-1 accuracy on ImageNet.                       Analysis of the tokens.       Weobservethat the learned class
                                                                                        and distillation tokens converge towards different vectors:
                Comparison of distillation methods.            We compare the           the average cosine similarity (cos) between these tokens
                performance of different distillation strategies in Table 3.            equal to 0.06. The class and distillation embeddings com-
                Harddistillation signiﬁcantly outperforms soft distillation             puted at each layer gradually become more similar through
                for transformers, even when using only a class token: hard              the network, all the way through the last layer at which their
                distillation reaches 83.0% at resolution 224×224, compared              similarity is high (cos=0.93), but still lower than 1. This
                to the soft distillation accuracy of 81.8%. Our distillation            is expected since as they aim at producing targets that are
