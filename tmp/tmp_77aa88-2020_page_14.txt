                             Published as a conference paper at ICLR 2021
                             A APPENDIX
                             A.1   PACBAYESIANGENERALIZATION BOUND
                             Below, we state a generalization bound based on sharpness.
                             Theorem 2. For any œÅ > 0 and any distribution D, with probability 1 ‚àí Œ¥ over the choice of the
                             training set S ‚àº D,                 v
                                                                 u                         q        2!
                                                                 u              kwk2           log(n)              n
                                                                 u                  2                                    Àú
                                                                 uklog 1+ 2             1+                 +4log +O(1)
                                                                 t                œÅ               k                Œ¥
                               LD(w)‚â§ max LS(w+)+                                            n‚àí1                               (4)
                                           kk2‚â§œÅ
                             where n = |S|, k is the number of parameters and we assumed L (w) ‚â§ E                   [L (w+)].
                                                                                                D          i‚àºN(0,œÅ)    D
                             The condition LD(w) ‚â§ E               [LD(w+)]meansthatadding Gaussian perturbation should
                                                         i‚àºN(0,œÅ)
                             not decrease the test error. This is expected to hold in practice for the Ô¨Ånal solution but does not
                             necessarily hold for any w.
                             Proof. First, note that the right hand side of the bound in the theorem statement is lower bounded
                                p                 2   2                                             2      2
                             by    klog(1+kwk /œÅ )/(4n) which is greater than 1 when kwk > œÅ (exp(4n/k) ‚àí 1). In
                                                  2                                                 2
                             that case, the right hand side becomes greater than 1 in which case the inequality holds trivially.
                                                                                                       2     2
                             Therefore, in the rest of the proof, we only consider the case when kwk ‚â§ œÅ (exp(4n/k) ‚àí 1).
                                                                                                       2
                             The proof technique we use here is inspired from Chatterji et al. (2020). Using PAC-Bayesian
                             generalization bound McAllester (1999) and following Dziugaite & Roy (2017), the following gen-
                             eralization bound holds for any prior P over parameters with probability 1 ‚àí Œ¥ over the choice of
                             the training set S, for any posterior Q over parameters: s
                                                                                          KL(Q||P)+logn
                                                Ew‚àºQ[LD(w)]‚â§Ew‚àºQ[LS(w)]+                                      Œ¥                 (5)
                                                                                                2(n‚àí1)
                             Moreover, if P = N(¬µ ,œÉ2I) and Q = N(¬µ ,œÉ2I), then the KL divergence can be written as
                                                      P   P                    Q Q
                             follows:                                                                   !
                                                                      2                 2                  2
                                                                1 kœÉ +k¬µ ‚àí¬µ k                            œÉ
                                               KL(P||Q)=              Q       P      Q 2 ‚àík+klog           P                    (6)
                                                                2            œÉ2                          œÉ2
                                                                              P                            Q
                             GivenaposteriorstandarddeviationœÉ ,onecouldchooseapriorstandarddeviationœÉ tominimize
                                                                    Q                                                P
                                                                                                                    10
                             the above KL divergence and hence the generalization bound by taking the derivative       of the above
                                                                                                    ‚àó 2      2                 2
                             KLwith respect to œÉ     and setting it to zero. We would then have œÉ       =œÉ +k¬µ ‚àí¬µ k /k.
                                                   P                                                P        Q       P      Q 2
                             However, since œÉ    should be chosen before observing the training data S and ¬µ ,œÉ       could depend
                                              P                                                                Q Q
                             on S, we are not allowed to optimize œÉ       in this way. Instead, one can have a set of predeÔ¨Åned
                                                                       P
                             values for œÉ   and pick the best one in that set. See Langford & Caruana (2002) for the discussion
                                         P
                             around this technique. Given Ô¨Åxed a,b > 0, let T = {cexp((1 ‚àí j)/k)|j ‚àà N} be that predeÔ¨Åned
                             set of values for œÉ2 . If for any j ‚àà N, the above PAC-Bayesian bound holds for œÉ2 = cexp((1 ‚àí
                                                P                                                                  P
                             j)/k) with probability 1 ‚àí Œ¥ with Œ¥      = 6Œ¥ , then by the union bound, all above bounds hold
                                                           j        j     œÄ2j2
                                                                           P
                             simultaneously with probability at least 1 ‚àí    ‚àû 6Œ¥ =1‚àíŒ¥.
                                                                             j=1 œÄ2j2
                             Let œÉ   =œÅ,¬µ =wand¬µ =0.Therefore,wehave:
                                  Q         Q             P
                                                 2                 2       2         2       2
                                                œÉ +k¬µ ‚àí¬µ k /k‚â§œÅ +kwk /k‚â§œÅ (1+exp(4n/k))                                         (7)
                                                 Q       P      Q 2                  2
                                                                                                 2        2
                             Wenowconsidertheboundthatcorrespondstoj = b1‚àíklog((œÅ +kwk /k)/c)c. Wecanensure
                                                                                             2            2
                             that j ‚àà N using inequality equation 7 and by setting c = œÅ (1 + exp(4n/k)). Furthermore, for
                             œÉ2 =cexp((1‚àíj)/k),wehave:
                              P
                                                        2        2        2                2         2   
                                                      œÅ +kwk /k ‚â§œÉ ‚â§exp(1/k) œÅ +kwk /k                                          (8)
                                                                 2       P                           2
                               10Despite the nonconvexity of the function here in œÉ2 , it has a unique stationary point which happens to be
                             its minimizer.                                     P
                                                                               14
