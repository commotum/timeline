                                           ProceedingsoftheTwenty-NinthInternationalJointConferenceonArtiﬁcialIntelligence(IJCAI-20)
                                 LogiQA:AChallengeDatasetforMachineReadingComprehension
                                                                               with Logical Reasoning
                                 1                         2,3                            2,3                              2,3                       2,3                             2,3∗
                   Jian Liu , Leyang Cui                       ,  HanmengLiu , DandanHuang , YileWang                                                     and YueZhang
                                                                1School of Computer Science, Fudan University
                                                                   2School of Engineering, Westlake University
                                           3Institute of Advanced Technology, Westlake Institute for Advanced Study
                   jianliu17@fudan.edu.cn, {cuileyang, liuhanmeng, huangdandan, wangyile, zhangyue}@westlake.edu.cn
                                                    Abstract                                               P1:	David,	Jack	and	Mark	are	colleagues	in	a	company.	David	supervises	Jack,
                                                                                                           and	Jack	supervises	Mark.	David	gets	more	salary	than	Jack.
                         Machine reading is a fundamental task for test-
                                                                                                           	Q:	What	can	be	inferred	from	the	above	statements?
                         ing the capability of natural language understand-                                							A.	Jack	gets	more	salary	than	Mark.
                         ing, which is closely related to human cognition                                  							B.	David	gets	the	same	salary	as	Mark.
                                                                                                           							C.	One	employee	supervises	another	who	gets	more	salary	than	himself.
                         in many aspects. With the rising of deep learning                                 							D.	One	employee	supervises	another	who	gets	less	salary	than	himself.
                         techniques, algorithmic models rival human perfor-
                         mances on simple QA, and thus increasingly chal-                                  P2:	 Our	 factory	 has	 multiple	 dormitory	 areas	 and	 workshops.	 None	 of	 the
                                                                                                           employees	who	live	in	dormitory	area	A	are	textile	workers.	We	conclude	that
                         lenging machine reading datasets have been pro-                                   some	employees	working	in	workshop	B	do	not	live	in	dormitory	area	A.	
                         posed.       Though various challenges such as ev-                                	Q:	What	may	be	the	missing	premise	of	the	above	argument?
                         idence integration and commonsense knowledge                                          A.	Some	textile	workers	do	not	work	in	workshop	B.
                         have been integrated, one of the fundamental ca-                                      B.	Some	employees	working	in	workshop	B	are	not	textile	workers.
                         pabilities in human reading, namely logical reason-                                   C.	Some	textile	workers	work	in	workshop	B.
                                                                                                               D.	Some	employees	living	in	dormitory	area	A	work	in	the	workshop	B.
                         ing, is not fully investigated. We build a compre-                                 Figure 1: Examples of LogiQA. (3 indicates the correct answer.)
                         hensive dataset, named LogiQA, which is sourced
                         from expert-written questions for testing human                                     One important aspect of human reading comprehension
                         Logical reasoning.           It consists of 8,678 QA in-                         and question answering is logical reasoning, which was also
                         stances, covering multiple types of deductive rea-                                                                                         [
                         soning. Results show that state-of-the-art neural                                one of the main research topics of early AI McCarthy, 1989;
                                                                                                                                                    ]
                         models perform by far worse than human ceiling.                                  Colmerauer and Roussel, 1996 . To this end, there has been
                         Our dataset can also serve as a benchmark for re-                                relatively very few relevant datasets in modern NLP. Figure 1
                         investigating logical AI under the deep learning                                 gives two examples of such problems. In particular, P1 con-
                         NLP setting.         The dataset is freely available at                          sists of a paragraph of facts, and a question that asks the testee
                         https://github.com/lgw863/LogiQA-dataset.                                        to select a valid conclusion by taking the facts as premises. In
                                                                                                          order to select the correct candidate, a machine is expected
                                                                                                          to understand the premises and the candidate answers. The
                   1     Introduction                                                                     correct answer can be found by categorical reasoning. P2
                                                                                                          of Figure 1 is more challenging in providing a premise and
                   Machine reading [Hermann et al., 2015; Chen et al., 2016]                              a conclusion in the paragraph, while asking for a missing
                   is a popular task in NLP, which is useful for downstream                               premise. In particular, three sets of workers are involved, in-
                   tasks such as open domain question answering and informa-                              cluding those living in dormitory area A, those who work in
                   tion retrieval. In a typical task setting, a system is given a                         workshop B and those who are textile workers. A testee can
                   passage and a question, and asked to select a most appro-                              ﬁnd the answer by drawing logical correlations between the
                   priate answer from a list of candidate answers. With recent                            three sets of workers.
                   advances of deep learning in NLP, reading comprehension                                   The type of machine reading comprehension questions
                   research has seen rapid advances, with a development from                              aboverequiresacombinationofnaturallanguageunderstand-
                                                                   [                               ]      ing and logical reasoning.              Compared with factual ques-
                   simple factual question answering Rajpurkar et al., 2016
                   to questions that involve the integration of different pieces                          tion answering, lexical overlap between the paragraph and
                                                                        [                                 the candidate answers plays a relatively less important role.
                   of evidences via multi-hop reasoning Welbl et al., 2018;
                                          ]                                                               Compared with commonsense reading comprehension, such
                   Yang et al., 2018 and questions that involve commonsense
                                                                    [                                     questions do not rely heavily on external knowledge. Instead
                   knowledgeoutsidethegivenpassage Ostermannetal.,2018;
                                           ]                                                              they focus on logical inference. In this aspect, the questions
                   Huangetal.,2019 ,wheremorevarietiesofchallengesinhu-
                                                                                                                                                                            [
                   manreading comprehension are investigated.                                             can be viewed as a re-investigation of logical AI McCarthy,
                                                                                                                                     ]
                                                                                                          1989; Nilsson, 1991 in the age of neural NLP. One solution
                       ∗Corresponding Author                                                              can be to perform semantic parsing into formal logic repre-
                                                                                                    3622
                                    ProceedingsoftheTwenty-NinthInternationalJointConferenceonArtiﬁcialIntelligence(IJCAI-20)
               sentation, and then perform symbolic reasoning. However,                      Dataset        Logic            Domain              Expert
               with the abundance of deep learning methods in the NLP                        SQuAD            7             Wikipedia               7
               toolkit, it can also be interesting to learn the potentials of neu-          TriviaQA          7               Trivia                7
               ral AI for solving such tasks.                                                 RACE            7      Mid/High School Exams          3
                  To facilitate such research, we create a new read-                         DuoRC            7         Wikipedia & IMDb            7
               ing comprehension dataset, LogiQA, which contains 8,678                    Narrative QA        7      MovieScripts, Literature       7
               paragraph-question pairs, each with four candidate answers.                    DROP            7             Wikipedia               7
                                                                                            COSMOS            7              Webblog                7
               Our dataset is sourced from publically available logical ex-                  MuTual           7           Daily Dialogue            3
               amination papers for reading comprehension, which are de-                  LogiQA(Ours)        3        Civil Servants Exams         3
               signed by domain experts for evaluating the logical reason-             Table 1: Comparison with existing reading comprehension datasets.
               ing ability and test participants. Thus the quality and topic           “Logic” indicates that dataset mainly requires logical reasoning.
               coverage of the questions are reliable. We manually select              “Expert” indicates that dataset is designed by domain experts.
               problemsfromtheoriginaldataset,ﬁlteringoutproblemsthat
               involve ﬁgures, charts, or those that are heavy of mathemat-            on logical reasoning and most of the necessary facts are di-
               ics, and ensuring a wide coverage of logical reasoning types,           rectly included in the given passage. In addition, most of the
               including categorical reasoning, conditional reasoning, dis-            existing datasets are labeled by crowd sourcing. In contrast,
                                                                  [              ]
               junctive reasoning and conjunctive reasoning Hurley, 2014 .             our dataset is based on examination problems written by hu-
                  ToestablishbaselineperformancesonLogiQA,weexplore                    manexpertsforstudents, and therefore has a better guarantee
               several state-of-the-art neural models developed for reading            of the quality. This is particularly important for datasets that
               comprehension. Experimental results demonstrate a signif-               involve abstract reasoning skills.
               icant gap between machine (35.31% accuracy) and human                      Thecorrelation and difference between our dataset and ex-
               ceiling performance (95.00%). We provide detailed analysis              isting QA datasets are shown in Table 1.
               to give insights into potentially promising research directions.
                                                                                       Logical reasoning.       There have been existing datasets re-
               2    Related Work                                                                                                                   [      ]
                                                                                       lated to logical reasoning. In particular, Habernal et al. 2018
               Existing Datasets For Reading Comprehension                             design a dataset for argument reasoning, where a claim is
               Aseminal dataset for large-scale reading comprehension is               givenandthemodelisaskedtochooseacorrectpremisefrom
                         [                         ]                                   two candidates to support the claim. Similar to our dataset,
               SQuAD Rajpurkar et al., 2016 , which requires selecting a               the dataset concerns deductive reasoning. The biggest dif-
               factual answer from all possible spans in a given passage.              ference between our dataset and this dataset is that ours is
               Many neural methods have been developed for this dataset,               a machine reading comprehension test while theirs focuses
               achieving results that rival human testees. As a consequence,           on argumentation. The form of their task is closer to NLI
               more reading comprehension datasets with increasing chal-               as compared with reading comprehension. In addition, our
               lenges are proposed. These datasets can be classiﬁed accord-            dataset has more instances (8,678 vs 1,970), more choices
                                                                         [
               ing to the main challenges. In particular, TriviaQA Joshi et            per question (4 vs 2) and is written by relevant experts rather
                         ]
               al., 2017 requires evidence integration across multiple sup-                                                       [     ]
                                                                            [          than being crowd-sourced. CLUTRR 2019 is a dataset for
               porting documents to answer the questions. DuoRC Saha                   inductivereasoningoverfamilyrelations. Theinputisagiven
                            ]                      [    ˇ   ´              ]
               et al., 2018 and Narrative QA Kocisky et al., 2018 raise                passage and a query pair, the output is a relationship between
               challenges by introducing two passages about the same facts.            the pair. The dataset concerns reasoning on a ﬁxed domain
                              [     ]                   [                    ]
               Welbl et al. 2018 and HotpotQA Yang et al., 2018 test                   (i.e., family relationship), which is in line with prior work on
               models for text understanding with sequential multi-step rea-                                       [                       ]
                               [                  ]                                    social relation inference Bramsen et al., 2011 . In contrast,
               soning. Drop Dua et al., 2019 tests discrete numerical rea-             our dataset investigates general logical reasoning.
                                                     [                 ]
               soning over the context. MuTual Cui et al., 2020 tests dia-
               logue reasoning ability via the next utterance prediction task.         Datasets from examinations.          Ourdataset is also related to
               Thesedatasetsarefactualinthesensethattheanswer(orcan-                   datasets extracted from examination papers, aiming at eval-
               didate in multi-choice-questions) is mostly a text span in the          uating systems under the same conditions as how humans
               given passage. Several types of reasoning are necessary, such           are evaluated. For example, the AI2 Elementary School Sci-
               as geolocational reasoning and numerical computation. Dif-                                        [                       ]
                                                                                       enceQuestionsdataset Khashabietal.,2016 contains1,080
               ferent from these datasets, our dataset contains answers not            questions for students in elementary schools; RACE [Lai et
               directly included in the input passage, and requires compre-                       ]
                                                                                       al., 2017 collects passages and questions from the English
               hensive reasoning methods beyond text matching based tech-              examsformiddleschoolandhighschoolChinesestudentsin
               niques.                                                                 ages between 12 to 18. These datasets are based on English
                  Similar to our dataset, recent datasets for commonsense              tests, examining testees on general language understanding.
                                                  [                          ]
               reasoning, including MCScript Ostermann et al., 2018 and                They target students in language learning. In contrast, our
                            [                     ]
               COSMOS Huang et al., 2019 , also contain candidate an-                  datasets are based on examinations of logical skills in addi-
               swers not directly included in the input passage. They test             tion to languageskills. Tothebestofourknowledge, LogiQA
               a model’s capability of making use of external background               is the ﬁrst large-scale dataset containing different types of
               knowledge about spatial relations, cause and effect, scientiﬁc          logical problems, where problems are created based on ex-
               facts and social conventions. In contrast, our dataset focuses          amsdesigned by human experts.
                                                                                  3623
                                                ProceedingsoftheTwenty-NinthInternationalJointConferenceonArtiﬁcialIntelligence(IJCAI-20)
                         Reasoning Type                                                Paragraph                                                               Question-Answers
                                                     P1: David knows Mr. Zhang's friend Jack, and Jack knows David's friend              Q: Who is from Shanghai and has a master's degree?
                         Categorical reasoning                                                                                               A. David.
                                (30.8%)              Ms. Lin. Everyone of them who knows Jack has a master's degree, and                     B. Jack. 
                                                                                                                                                Mr. Zhang.
                                                     everyone of them who knows Ms. Lin is from Shanghai.                                    C. 
                                                                                                                                             D. Ms. Lin.
                                                     P2: Jimmy asked Hank to go to the mall the next day. Hank said, "If it              Q: Which of the following comments is appropriate?
                         Sufﬁcient conditional       doesn't rain tomorrow, then I'll go climbing." The next day, there was a                A. This argument between Jimmy and Hank is meaningless.
                              reasoning              drizzle. Jimmy thought that Hank would not go climbing, so he went to                   B. Jimmy's reasoning is illogical.
                                (27.6%)              pick  up  Henry  to  the  mall.  Nevertheless,  Hank  went  climbing  the               C. The two people have different understandings of a drizzle.
                                                     mountain. When the two met again, Jimmy blamed Hank for not keeping                        Hank broke his promise and caused the debate.
                                                     his word.                                                                               D. 
                                                                                                                                         Q: Which can be inferred from the statements above?
                                                                                                                                             A. The whole society should be focused on education.
                        Necessary conditional        P3: Only if the government reinforces basic education can we improve our                B. In order to stand out among nations, we should
                               reasoning             nation's  education  to  a  new  stage.  In  order  to  stand  out  among  other        reinforce basic education.
                                (24.7%)              nations, we need to have a strong educational enterprise.                               C. In order to improve our education to a new stage, it is
                                                                                                                                             necessary to increase the salary of college teachers.
                                                                                                                                             D. In order to reinforce basic education, all primary school
                                                                                                                                             teachers must have a bachelor degree or above.
                                                     P4:  Last night, Mark either went to play in the gym or visited his teacher         Q: Which is true based on the above statements?
                         Disjunctive reasoning       Tony. If Mark drove last night, he didn't go to play in the gym. Mark would go          A. Mark went to the gym with his teacher Tony last night.
                                (18.5%)              visit his teacher Tony only if he and his teacher had an appointment. In fact,          B. Mark visited his teacher Tony last night.
                                                     Mark had no appointment with his teacher Tony in advance.                               C. Mark didn't drive last night.
                                                                                                                                             D. Mark didn't go to the gym last night.
                                                     P5:The coach of a national football team found that the best cooperative            Q: If U and Z are both on the ﬁeld, for best performance, which
                                                     arrangement of the players U, V, W, X, Y, and Z during training are: (1) V          of the following arrangement is appropriate?
                        Conjunctive reasoning        and X can not be on the ﬁeld at the same time, and neither can be off the               A. X is on the ﬁeld and Y is not on the ﬁeld. 
                                (21.3%)              ﬁeld the same time. (2) V is not on the ﬁeld only if U is not on the ﬁeld. (3) If       B. V is on the ﬁeld and Y is not on the ﬁeld.
                                                     W is on the ﬁeld, then X is on the ﬁeld. (4) If Y and  Z are on the ﬁeld, then          C. V and W are both on the ﬁeld.
                                                                                                                                                                         ﬁeld.
                                                                                                                                                V and Y are not on the 
                                                     W must be on the ﬁeld. This arrangement can yield the best performance.                 D. 
                                                 Figure 2: Examples of each type of logical reasoning in LogiQA. (3 indicates the correct answer.)
                     3     Dataset                                                                                             Parameter                                                     Value
                     3.1      DataCollection and Statistics                                                                    #Paragraphs-Question Pair                                      8,678
                     We construct LogiQA by collecting the logical comprehen-                                                  Ave./Max. # Tokens / Paragraph                             76.87 / 323
                     sion problems from publically available questions of the Na-                                              Ave./Max. # Tokens / Question                              12.03 / 54
                     tional Civil Servants Examination of China, which are de-                                                 Ave./Max. # Tokens / Candidate Answer                      15.83 / 111
                     signed to test the civil servant candidates’ critical thinking                                                             Table 2: Statistics of LogiQA.
                     and problem solving. We collected raw data released at the
                     ofﬁcial website, obtaining 13,918 paragraph-question-choice                                      datasets, the averageparagraphlengthisrelativelysmallsince
                     triples with the correct answers.                                                                logical reasoning problems do not heavily rely on complex
                        The following steps are conducted to clean the raw data.                                      context.
                     First, we remove all the instances that do not have the for-                                         Wealsorelease the Chinese version of LogiQA (named as
                     mat of our problem setting, i.e., a question is removed if                                       Chinese LogiQA) for Chinese reasoning-based reading com-
                     the number of candidate choices is not four. Second, we                                          prehension research.
                     ﬁlter all the paragraphs and questions that are not self-
                     contained based on the text information, i.e. we remove the                                      3.2       Reasoning Types of the Dataset
                     paragraphs and questions containing images or tables. We
                     also remove all questions containing the keywords “under-                                        The test set of our benchmark consists of 867 paragraph-
                     lined” and “sort sentences”, since it can be difﬁcult to re-                                     question pairs. We manually categorize the instances accord-
                     produce the effect of underlines and sentence number order                                       ing to the ﬁve types of logical reasoning deﬁned by Hurley
                     for a typical machine reader. Finally, we remove all dupli-                                      [        ]
                                                                                                                        2014 ,includingcategoricalreasoning,sufﬁcientconditional
                     cated paragraph-question pairs. The resulting dataset con-                                       reasoning, necessary conditional reasoning, disjunctive rea-
                     tains 8,678 paragraph-questions pairs.                                                           soning and conjunctive reasoning. These types of reasoning
                        Since the original dataset was written in Chinese, ﬁve                                        belong to deductive reasoning, for which a deﬁnite conclu-
                     professional English speakers are employed to translate the                                      sion can be derived given a set of premises. As a result, such
                     dataset manually. To ensure translation quality, we further                                      reasoning can be most suitable for evaluating performances
                     employthree proofreaders. A translated instance is sent back                                     quantitatively. Figure 2 shows the statistics and representa-
                     to the translators for revision if proofreaders reject the in-                                   tive examples of the reasoning types in our dataset. Note that
                     stance.      The detailed statistics for LogiQA is summarized                                    the sum of percentage values is above 100%, which is be-
                     in Table 2. Compared with existing reading comprehension                                         cause one problem can involve multiple types of reasoning.
                                                                                                               3624
                                 ProceedingsoftheTwenty-NinthInternationalJointConferenceonArtiﬁcialIntelligence(IJCAI-20)
              Formally, the ﬁve types of reasoning can be described as fol-     as one concatenated sentence, using a pre-trained contextual-
              lows:                                                             ized embedding model to encode the sentence for calculating
                 • Categorical reasoning: The goal is to reason whether         its score. Given four candidate answers, four concatenated
                   a speciﬁc concept belongs to a particular category. This     sentences are constructed by pairing each candidate answer
                   type of reasoning is commonly associated with quanti-        with the paragraph and question, and the one with the highest
                                                                                                                                        [
                   ﬁers such as “all/everyone/any”, “no” and “some”, etc.       modelscoreischosenastheanswer. Inparticular,BERT De-
                                                                                vlin et al., 2019] treats the paragraph as sentence A and the
                 • Sufﬁcient conditional reasoning: The type of hypo-           concatenation of the question and each candidate as sentence
                   thetical reasoning is based on conditional statements of     B, before further concatenating them into [CLS] A [SEP] B
                   the form “If P, then Q”, in which P is the antecedent and                                   [                ]
                                                                                [SEP] for encoding; RoBERTa Liu et al., 2019 replaces the
                   Qistheconsequent.                                            BERT model using the RoBERTa model. The hidden state
                 • Necessary conditional reasoning: This type of hypo-          of the [CLS] token is used for MLP + softmax scoring. The
                   thetical reasoning is based on conditional statements of     embedding models are ﬁne-tuned during training.
                   the form “P only if Q”, “Q whenever P”, etc., where Q        Humanperformance. Weemploythreepost-graduatestu-
                   is a necessary condition for P.                              dents for human performance evaluation, reporting the aver-
                 • Disjunctive reasoning: In this type of reasoning, the        age scores on 500 randomly selected instances from the test
                   premises are disjunctive, in the form “either . . . or . .   set. For calculating the ceiling performances, we consider a
                   .” , where the conclusion holds as long as one premise       question as being correctly answered if one of the students
                   holds.                                                       gives the correct answer.
                 • Conjunctive reasoning: In this type of reasoning, the        Implementation details.     We re-implement the rule-based
                                                                                                                                 [
                   premises are conjunctives, in the form “both ...and...”,     methods strictly following the original papers Yih et al.,
                                                                                                              ]
                   wheretheconclusionholdsonlyifallthepremiseshold.             2013; Richardson et al., 2013 . For the deep learning meth-
                                                                                ods, we directly use the implementations released in the
              4    Methods                                                      original papers. 100-dimensional Glove word embeddings
                                                                                are used as embedding initialization. For pre-trained meth-
                                                                                                                                    [
              Weevaluate the performances of typical reading comprehen-         ods, we follow the HuggingFace implementation Wolf et
                                                                                         ]
              sion models, including rule-based methods, deep learning          al., 2019 .   We take the off-the-shelf model BERT-base
              methods as well as methods based on pre-trained contextual-       and RoBERTa-base for LogiQA, and Chinese BERT-base
                                                                                                              [                ]
              ized embedding. In addition, human performances are evalu-        and Chinese RoBERTa-base       Cui et al., 2019 for Chinese
              ated and ceiling performances are reported.                       LogiQA.Allhyper-parametersaredecidedbythemodelper-
              Rule-based methods.      We adopt two rule-based methods,         formance on the development sets.
              which rely on simple lexical matching. In particular, word        5   Results
                        [                ]
              matching Yih et al., 2013 is a baseline that selects the can-
              didate answer that has the highest degree of unigram over-        Werandomly split the dataset, using 80% for training, 10%
              lap with the given paragraph-question pair; sliding window        for development and the remaining 10% for testing. Table 3
              [                        ]                                        showstheresults of the models discussed in the previous sec-
               Richardson et al., 2013 calculates the matching score for
              each candidate answer by extracting TF-IDF type features          tion. In particular, the human performance is 86.00% and the
              from n-grams in the given paragraph-question pair.                ceiling performance is 95.00%, which shows that the difﬁ-
                                                                  [             culty level of the dataset is not high for human testees. In
              Deep learning methods.      Most existing methods Chen et         contrast, all of the algorithmic models perform signiﬁcantly
                                                                  ]
              al., 2016; Dhingra et al., 2017; Wang et al., 2018 ﬁnd the        worse than human, demonstrating that the methods are rela-
              answer by text matching techniques, calculating the similar-      tively weak in logical reasoning reading comprehension. In
              ity between the given paragraph, the question and each can-       addition, results on the Chinese dataset are on the same level
                                                                     [
              didate answer. In particular, Stanford attentive reader Chen      compared with those on the English dataset.
                          ]
              et al., 2016 computes the similarity between a paragraph-           In particular, the rule-based methods give accuracies of
              question pair and a candidate answer using LSTM encod-            28.37%and22.51%,respectively, the latter being even lower
              ing and a bi-linear attention function; gated attention reader    than a random guess baseline. This shows that the ques-
              [                     ]
               Dhingra et al., 2017 adopts a multi-hop architecture with        tions are extremely difﬁcult to solve by using lexical match-
              a more ﬁne-grained mechanism for matching candidate an-           ing alone. Figure 1 serves as one intuitive example. The
              swers with paragraph-question pairs; co-matching network          deep learning methods such as the Stanford attentive reader,
              [                 ]
               Wangetal.,2018 furtherenhancesmatchingtheparagraph-              the gated attention reader and the co-matching network give
              question pair and paragraph-candidate answer pair by encod-       accuracies around 30%, which is better compared with the
              ingeachpieceoftextandcalculatingmatchingscorebetween              random guess baseline but far behind human performance.
              each pair, respectively.                                          Onelikely reason is that the methods are trained end-to-end,
              Pre-trained methods.     Pre-trained models give the current      where it turns out difﬁcult for attention-based text matching
              state-of-the-art results on machine reading. Different from       to learn underlying logical reasoning rules.
              the above deep learning methods, pre-trained methods con-           It has beenshownthatpre-trainedmodelshaveacertainde-
                                                                                                                               [
              sider the paragraph, the question and each candidate answer       gree of commonsense and logical capabilities Huang et al.,
                                                                           3625
                                            ProceedingsoftheTwenty-NinthInternationalJointConferenceonArtiﬁcialIntelligence(IJCAI-20)
                                                         Category                          Model                            LogiQA          Chinese LogiQA
                                                                                                                         Dev       Test     Dev        Test
                                                                                     Random(theoretical)                 25.00    25.00     25.00     25.00
                                                        Rule-based            WordMatching[Yihetal., 2013]               27.49    28.37     26.55     25.74
                                                                          Sliding Window [Richardson et al., 2013]       23.58    22.51     23.85     24.27
                                                                         Stanford Attentive Reader [Chen et al., 2016]   29.65    28.76     28.71     26.95
                                                                                                [                   ]
                                                       Deeplearning     Gated-Attention Reader Dhingra et al., 2017      28.30    28.98     26.82     26.43
                                                                                                [                 ]
                                                                          Co-Matching Network Wang et al., 2018          33.90    31.10     30.59     31.27
                                                                                        [                  ]
                                                        Pre-trained               BERT Devlinetal., 2019                 33.83    32.08     30.46     34.77
                                                                                 RoBERTa[Liuetal., 2019]                 35.85    35.31     39.22     37.33
                                                          Human                      HumanPerformance                      -      86.00       -       88.00
                                                                                     Ceiling Performance                   -      95.00       -       96.00
                                                                            Table 3: Main results on LogiQA (accuracy%).
                       36                                                                                                                 Model                Dev      Test
                       34                                                                                                          Random(theoretical)        25.00     25.00
                       32                                                                                                            RoBERTaLogiQA            35.85     35.31
                                                                                                                                      RoBERTa                 29.19     26.86
                                                                                                                                               RACE
                       30                                                                                                           RoBERTaCOSMOS             25.14     28.73
                                                                                                                                 RoBERTa                      34.60     35.07
                       28                                                                                                       RoBERTa RACE−→LogiQA          36.44     35.11
                                                                                                                                         COSMOS−→LogiQA
                       26                                                                                                Table 4: Transfer learning results (accuracy%).
                       24                                                                                               Length       (0,100]    (100,150]     (150,200]    (200,+∞)
                              Random        RoBERTa    w/o Paragraph w/o Question w/o Paragraph                        #Instances      253         364           198           61
                                                                                     & Question                        RoBERTa        31.31       36.25         37.13         40.38
                         Figure 3: Ablation of paragraph or question (accuracy%).                                    Table 5: Performance of different length (accuracy%).
                                                                                                             6.2     Transfer Learning
                          ]
                   2019 . On LogiQA, such models give better performances                                    We conduct a set of transfer learning experiments to under-
                   compared with the methods without contextualized embed-                                   stand the degree of overlap in terms of necessary knowledge
                   dings. However, the best result by RoBERTa is 35.31%, still                               for solving problems in our dataset and existing datasets.
                   muchbelowhumanperformance. Thisshowsthatknowledge                                         In particular, we ﬁrst ﬁne-tune the RoBERTa model on a
                   in pre-trained models is rather weak for logical reasoning.                               source dataset, before ﬁne-tuning the model on LogiQA. If
                   It remains an open question on how deep learning machine                                  the required knowledge is similar, the model performance
                   readers can be equipped with strong reasoning capability.                                 is expected to increase. RACE and COSMOS are adopted
                                                                                                             as the source datasets.            The former tests English reading
                   6     Discussion                                                                          skills while the latter tests commonsense knowledge.                           As
                                                                                                             shown in Table 4, the RoBERTa model trained only on ei-
                   We give detailed analysis based on the empirical results of                               ther source dataset gives signiﬁcantly lower accuracies on
                   RoBERTaandothermodelson LogiQAtestset.                                                    LogiQA test set compared with the RoBERTa model trained
                                                                                                             on LogiQA.TheperformanceofRoBERTatrainedonRACE
                   6.1     Ablation Tests                                                                    is even close to the random guess baseline. In addition, fur-
                                                                                                             ther ﬁne-tuning on LogiQA leads to improvements over the
                   Following recent studies, we conduct a set of ablation ex-                                source-trained baselines, but the resulting models do not out-
                   periments using RoBERTa to measure bias in the dataset by                                 perform a model trained only on LogiQA. The observation
                   checking the performance based on the partial information                                                                                       [
                                                                                                             is different from most other datasets Huang et al., 2019;
                   [                      ]                                                                                       ]
                    Cai et al., 2017 . Figure 3 shows the results on the test set.                           Sap et al., 2019 , which demonstrates that LogiQA contains
                   There is a signiﬁcant drop of accuracies without the para-                                highly different challenges compared with existing datasets.
                   graph, the question or both, which indicates that the bias on                             6.3     PerformanceAcrossDifferent Lengths
                   thedatasetisweak. Inparticular, withouttheinputparagraph,
                   the accuracy drops from 35.31% to 28.92%, which is com-                                   WemeasuretheaccuracyofRoBERTaagainsttheinputsize.
                   parable to a drop from 67.1% to 55.9% by the same model                                   In particular, the number of words in the paragraph, the ques-
                                                      [                          ]
                   on the COSMOS dataset Huang et al., 2019 , and 66.0% to                                   tion and the candidate answers are added together as the
                                                                [                     ]
                   52.7%ontheSocialIQadataset Sap et al., 2019 .                                             length of a test instance. The statistics and performances are
                      Ablating question causes a relatively smaller performance                              all shown in Table 5. Interestingly, the model performances
                   drop as compared with the paragraph, which is consistent                                  are not negatively associated with the input size, which is dif-
                                                                   [       ]
                   with observations by Huang et al. 2019 . This is likely be-                               ferent from most NLP benchmarks. This shows that the level
                   cause the diversity of questions is lower. The above results                              of challenge in logical reasoning can be independent of the
                   showthat our dataset does not have a strong bias.                                         input verbosity.
                                                                                                      3626
                                            ProceedingsoftheTwenty-NinthInternationalJointConferenceonArtiﬁcialIntelligence(IJCAI-20)
                                                                                                                       Model                        Overlap Ratio      Accuracy(%)
                    P1:	Children's	products	are	any	products	intended	for	play	or	use	by	children	12
                    years	of	age	or	younger.                                                                           WordMatching                     100.00             28.37
                                                                                                                       Stanford Attentive Reader         35.47             35.82
                     	Q:	Based	on	the	above	definition,	which	of	the	following	are	children's	products?
                                                                                                                       Gated-Attention Reader            37.33             34.96
                         A.	Milk	powders	for	infants	aged	from	0	to	1.
                                                                                                                       Co-Matching Network               40.74             36.85
                         B.	Comic	books	suitable	for	kids	around	10	years	old.                                         BERT                              34.23             37.73
                         C.	Brightly	packed	lollipops.                                                                 RoBERTa                           32.08             40.38
                         D.	Bumper	cars	in	the	amusement	park	children	love	to	play.
                                                                                                                       Gold-standard                     28.37            100.00
                    P2:	Flower	Bay	is	an	ideal	river	for	salmons	swimming.	If	there	is	a	hydropower
                    dam	downstream,	then	salmons	will	not	be	able	to	swim	here.	Salmons	swim	here                      Table 6: Overlap ratio (%) against the model type.
                    only	if	the	trees	on	the	shore	of	Flower	Bay	have	lost	their	leaves.	If	many	sea
                    eagles	and	brown	bears	gather	in	this	river	bay,	then	you	can	tell	that	the	salmons                        Reasoning Type                     RoBERTa
                    are	migrating.	Now	there	are	a	lot	of	salmons	swimming	in	Flower	Bay.	                                     Categorical reasoning                55.00
                                                                                                                               Sufﬁcient conditional reasoning      17.11
                     	Q:	Based	on	the	above	statements,	which	of	the	following	can	be	derived?
                                                                                                                               Necessary conditional reasoning      19.29
                         A.	The	leaves	on	the	shore	of	Flower	Bay	are	gone.
                                                                                                                               Disjunctive reasoning                22.67
                         B.	There	are	many	sea	eagles	and	brown	bears	in	Flower	Bay.
                                                                                                                               Conjunctive reasoning                21.98
                         C.	There	is	a	hydropower	dam	downstream	of	Flower	Bay.
                         D.	Sea	Eagle	and	Brown	Bear	Feed	on	Salmon.                                             Table 7: RoBERTa accuracy (%) against the reasoning type.
                    P3:	A	company	decided	to	select	4	people	from	3	women	(A,	B,	C)	and	5	men               Conditional reasoning.               P2 of Figure 4 is a representative
                    (D,	E,	F,	X,	Y)	to	set	up	a	group	for	an	important	negotiation.	Here	are	the
                    prerequisites:	(1)	The	group	members	must	have	both	women	and	men.	(2)	D	and            example of the most challenging conditional reasoning ques-
                    A	cannot	be	selected	at	the	same	time.	(3)	B	and	C	cannot	be	selected	at	the	same       tions. In particular, a variety of sufﬁcient and necessary con-
                    time.	(4)	If	Y	is	selected,	then	F	won't	be	selected.                                   ditional relations are given in the paragraph, which include:
                     	Q:	If	D	must	be	selected,	which	of	the	following	can	be	derived?                                     x=“Salmonsswim”
                         A.	If	the	company	selects	F,	then	need	also	select	Y.
                         B.	If	the	company	selects	E,	then	need	also	select	X.                                             y = “Sea eagles and brown bears gather”
                         C.	Either	selecting	Y	or	X.                                                                       z = “Hydropower dam exists downstream”
                         D.	Either	selecting	B	or	C.
                   Figure 4: Example mistakes of RoBEETa. (3 indicates the correct                                         w=“Treesloseleaves”
                   answers and 7 indicates the RoBERTa prediction.)                                                        x=>w(Necessaryconditionalrelation)
                   6.4     Lexical Overlap                                                                                 y =>x (Sufﬁcient conditional relation)
                                                                                                                           x=>z (Sufﬁcientconditional relation)
                   Weaimto understand a bias of models in selecting the can-                                   The correct answer depends on fully understanding both
                   didate answers that have the best surface matching with the                              the necessary and sufﬁcient conditional reasoning facts.
                   paragraph. To this end, we calculate the unigram overlap                                 RoBERTa makes a mistake by ignoring the “not” operator
                   between each candidate answer and the given paragraph for                                in the x => z¯condition, which coincides with prior observa-
                   each problem, and mark the best-matching candidate. We re-                                                                      [                              ]
                   port the “Overlap Ratio” by calculating the accuracy between                             tions on BERT and negation Niven and Kao, 2019 .
                   model prediction and the best-matching candidate. The re-                                Conjunctive and disjunctive reasoning.                        P3 of Figure 4
                   sults are shown in Table 6. As can be seen, the gold-standard                            represents one of the most challenging questions in the
                   output has an accuracy of 28.37%, whilst all of the mod-                                 dataset, where the premises and candidate give a set of con-
                   els give accuracies above this number, which shows a ten-                                straints in both conjunctive and disjunctive forms, and the
                   dency of superﬁcial matching. In particular, the word match-                             question asks which candidate conforms to the premises. The
                   ing method gives an accuracy of 100% due to its mechanism.                               testee is expected to enumerate different possible situations
                   RoBERTa gives the lowest matching accuracy, showing that                                 and then match the cases to the candidates by thoroughly un-
                   it relies the least on lexical patterns. We additionally measure                         derstanding the candidates also. Intuitively, RoBERTa is not
                   the accuracy of the models on the “correct” instances accord-                            directly equipped with such reasoning capacity.
                   ing to best matching. RoBERTa still outperforms the other                                7     Conclusion
                   models, demonstrating relative strength in logical reasoning.
                   6.5     Reasoning Types                                                                  We have presented LogiQA, a large-scale logical reasoning
                                                                                                            reading comprehension dataset. In addition to testing reason-
                   Table 7 gives the performances of RoBERTa over the 5 rea-                                ing capacities of machine reading, our dataset can also serve
                   soning types discussed in Section 3.2.                   The method gives                as a benchmark for re-examining the long pursued research
                   the best accuracy on categorical reasoning. However, for the                             of logical AI in the deep learning NLP era. Results show that
                   other four reasoning types, the results are signiﬁcantly lower.                          thestate-of-the-art machinereadersstillfallfarbehindhuman
                   TounderstandwhythesetasksarechallengingforRoBERTa,                                       performance, making our dataset one of the most challenging
                   wegivequalitative discussion via case study.                                             test for reading comprehension.
                   Categorical reasoning.              P1 of Figure 4 shows a typical ex-                   Acknowledgments
                   ample, where the deﬁnition of children’s products is given in
                   the paragraph and the testee is asked to select a correct in-                            ThisworkissupportedbytheNationalScienceFoundationof
                   stance. A key here is the age range (i.e., under 12). RoBERTa                            China (Grant No. 61976180). We also acknowledge funding
                   incorrectly chooses the candidate that is superﬁcially similar                           support from the Westlake University and Bright Dream Joint
                   to the paragraph, while ignoring the reasoning process.                                  Institute for Intelligent Robotics.
                                                                                                     3627
                                  ProceedingsoftheTwenty-NinthInternationalJointConferenceonArtiﬁcialIntelligence(IJCAI-20)
                                                                                    [                ]
               References                                                            Lai et al., 2017  G. Lai, Q. Xie, H. Liu, Y. Yang, and
               [                      ]                                                E. Hovy.    RACE: Large-scale Reading Comprehension
                Bramsenetal., 2011 P. Bramsen, M. Escobar-Molano,                      dataset from Examinations. In EMNLP, pages 785–794,
                  A. Patel, and R. Alonso. Extracting social power relation-           2017.
                  ships from natural language. In NAACL, pages 773–782,
                                                                                    [                ]
                  2011.                                                              Liu et al., 2019  Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi,
               [                ]                                                      D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoy-
                Cai et al., 2017  Z.Cai,L.Tu,andK.Gimpel. Payattention                 anov. Roberta: A robustly optimized bert pretraining ap-
                  to the ending: Strong neural baselines for the roc story             proach. arXiv, 2019.
                  cloze task. In ACL, pages 616–622, 2017.
                                                                                    [                 ]
               [                  ]                                                  McCarthy, 1989 John McCarthy.          Artiﬁcial intelligence,
                Chenetal., 2016 D. Chen, J. Bolton, and C. D. Manning.                 logic and formalizing common sense. In Philosophical
                  A thorough examination of the CNN/daily mail reading                 logic and artiﬁcial intelligence. 1989.
                  comprehension task. In ACL, pages 2358–2367, 2016.
                                                                                    [              ]
               [                                ]                                    Nilsson, 1991 Nils J. Nilsson. Logic and artiﬁcial intelli-
                Colmerauer and Roussel, 1996 A.           Colmerauer       and         gence. Artiﬁcial intelligence, 1991.
                  P. Roussel. The birth of prolog. In History of program-
                                                                                    [                      ]
                  ming languages—II. ACM, 1996.                                      Niven and Kao, 2019 T. Niven and Hung-Yu Kao. Probing
               [                ]                                                      neural network comprehension of natural language argu-
                Cui et al., 2019  Y. Cui, W. Che, T. Liu, B. Qin, Z. Yang,             ments. In ACL, pages 4658–4664, 2019.
                  S. Wang, and G. Hu. Pre-training with whole word mask-
                                                                                    [                        ]
                  ing for chinese bert. arXiv, 2019.                                 Ostermann et al., 2018    S. Ostermann, M. Roth, A. Modi,
               [                ]                                                      S. Thater, and M. Pinkal. Task 11: Machine comprehen-
                Cui et al., 2020  L. Cui, Y. Wu, S. Liu, Y. Zhang, and                 sion using commonsense knowledge. In SemEval, pages
                  M. Zhou. Mutual: A dataset for multi-turn dialogue rea-              747–757, 2018.
                  soning. In ACL, 2020.
                                                                                    [                       ]
               [                   ]                                                 Rajpurkar et al., 2016   P. Rajpurkar, J. Zhang, K. Lopyrev,
                Devlin et al., 2019   J. Devlin, M-W Chang, K. Lee, and                and P. Liang. SQuAD: 100,000+ questions for machine
                  K. Toutanova. BERT: Pre-training of bidirectional trans-             comprehension of text. In EMNLP, 2016.
                  formers for language understanding. In NAACL, 2019.
                                                                                    [                        ]
               [                     ]                                               Richardson et al., 2013   M.Richardson, C.J.C. Burges, and
                Dhingra et al., 2017   B. Dhingra, H. Liu, Z. Yang, W. Co-             E. Renshaw. MCTest: A challenge dataset for the open-
                  hen,andR.Salakhutdinov. Gated-attentionreadersfortext                domainmachinecomprehensionoftext. InEMNLP,2013.
                  comprehension. ACL, pages 1832–1846, 2017.
                                                                                    [                 ]
               [                 ]                                                   Saha et al., 2018   A. Saha, R. Aralikatte, Mitesh M.
                Duaetal., 2019 D.Dua,Y.Wang,P.Dasigi,G.Stanovsky,                      Khapra, and K. Sankaranarayanan.          DuoRC: Towards
                  S. Singh, and M. Gardner. DROP: A reading compre-                    complex language understanding with paraphrased read-
                  hensionbenchmarkrequiringdiscretereasoningoverpara-                  ing comprehension. In ACL, pages 1683–1693, 2018.
                  graphs. In NAACL, pages 2368–2378, 2019.
                                                                                    [                ]
               [                      ]                                              Sapet al., 2019 M. Sap, H. Rashkin, D. Chen, R. Le Bras,
                Habernal et al., 2018   I.   Habernal,      H.    Wachsmuth,           and Y. Choi. Social iqa: Commonsense reasoning about
                  I. Gurevych, and B. Stein.        The argument reasoning             social interactions. EMNLP, pages 4453–4463, 2019.
                  comprehension task: Identiﬁcation and reconstruction of
                                                                                    [                  ]
                  implicit warrants. In NAACL, pages 1930–1940, 2018.                Sinha et al., 2019  K.Sinha, S. Sodhani, J. Dong, J. Pineau,
               [                      ]                                                and W. L. Hamilton. Clutrr: A diagnostic benchmark for
                Hermannetal., 2015 K. Moritz Hermann, T. Kocisky,                      inductive reasoning from text. In EMNLP, pages 4496–
                  E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and               4505, 2019.
                  P. Blunsom. Teaching machines to read and comprehend.
                                                                                    [                  ]
                  In NIPS, pages 1693–1701, 2015.                                    Wangetal., 2018 S. Wang, M. Yu, J. Jiang, and S. Chang.
               [                   ]                                                   A co-matching model for multi-choice reading compre-
                Huangetal., 2019 L. Huang, R. Le Bras, C. Bhagavatula,                 hension. In ACL, pages 746–751, 2018.
                  and Y. Choi. Cosmos QA: Machine comprehension with
                                                                                    [                   ]
                  contextual commonsense reasoning. In EMNLP, 2019.                  Welbl et al., 2018   J. Welbl, P. Stenetorp, and S. Riedel.
               [             ]                                                         Constructing datasets for multi-hop reading comprehen-
                Hurley, 2014 Patrick J. Hurley. A concise introduction to              sion across documents. TACL, 2018.
                  logic. Nelson Education, 2014.
                                                                                    [                 ]
               [                  ]                                                  Wolf et al., 2019   T. Wolf, L. Debut, V. Sanh, J. Chaumond,
                Joshi et al., 2017  M.Joshi,E.Choi,D.Weld,andL.Zettle-                                                                   ´
                  moyer. TriviaQA: A large scale distantly supervised chal-            C. Delangue, A. Moi, P. Cistac, T. Rault, Remi Louf,
                  lenge dataset for reading comprehension. In ACL, 2017.               M. Funtowicz, and J. Brew. Huggingface’s transformers:
               [                      ]                                                State-of-the-art natural language processing. ArXiv, 2019.
                Khashabi et al., 2016   D. Khashabi, T. Khot, A. Sabharwal,         [                  ]
                  P. Clark, O. Etzioni, and D. Roth. Question answering              Yangetal., 2018 Z. Yang, P. Qi, S. Zhang, Y. Bengio,
                  via integer programming over semi-structured knowledge.              W. Cohen, R. Salakhutdinov, and C. D. Manning. Hot-
                  arXiv, 2016.                                                         potQA: A dataset for diverse, explainable multi-hop ques-
               [   ˇ   ´             ]      ´ˇ    ˇ    ´                               tion answering. In EMNLP, pages 2369–2380, 2018.
                Kocisky et al., 2018   Tomas Kocisky, J. Schwarz, P. Blun-          [                ]
                                                            ´                        Yih et al., 2013  Wen-tau Yih, Ming-Wei Chang, C. Meek,
                  som, C. Dyer, K. M. Hermann, Gabor Melis, and                        andA.Pastusiak. Questionansweringusingenhancedlex-
                  E. Grefenstette. The NarrativeQA reading comprehension               ical semantic models. In ACL, pages 1744–1753, 2013.
                  challenge. TACL, pages 317–328, 2018.
                                                                               3628
