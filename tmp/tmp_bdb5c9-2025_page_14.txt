                          Preprint.
                                                        run0    run1     run2   k=1            k=2             k=3
                           iteration  setting
                           0          baseline         46.00   45.00    47.00   46.00 (1.00)   54.67 (3.21)  59.00
                                      cheatsheet       48.00   44.00    50.00   47.33 (3.06)   57.67 (2.52)  64.00
                                      ArcMemo-OE       49.00   47.00    48.00   48.00 (1.00)   56.67 (1.53)  61.00
                                      ArcMemo-PS       49.00   49.00    49.00   49.00 (0.00)   59.00 (0.00)  63.00
                           1          baseline         57.00   55.00    61.00   57.67 (3.06)   66.33 (4.04)  71.00
                                      cheatsheet       56.00   56.00    57.00   56.33 (0.58)   65.33 (1.15)  70.00
                                      ArcMemo-OE       59.00   55.00    56.00   56.67 (2.08)   65.67 (1.53)  70.00
                                      ArcMemo-PS       58.00   60.00    55.00   57.67 (2.52)   67.00 (1.73)  72.00
                           2          baseline         59.00   59.00    65.00   61.00 (3.46)   69.00 (2.65)  73.00
                                      cheatsheet       65.00   61.00    61.00   62.33 (2.31)   71.33 (1.53)  76.00
                                      ArcMemo-OE       62.00   59.00    61.00   60.67 (1.53)   67.67 (2.52)  71.00
                                      ArcMemo-PS       60.00   66.00    58.00   61.33 (4.16)   70.33 (3.06)  75.00
                         Table 5: Strict Scoring. While the official evaluation scheme allows different test cases solved
                          bydifferent attempts to be ensembled, in contrast, the strict scoring regime only marks a puzzle as
                          solved if a single attempt (generated program) solves all test cases.
                          ConceptSpecificity Investigation  Aspart of the development of our concept representations, we
                          conducted small-scale experiments using tiny (n=10) subsets of the validation split to investigate
                          the level of detail needed for concepts to be useful. Manually writing maximum detail situation-
                          suggestion style concepts and using iteratively LLM-summarized versions (for a total of 5 levels of
                          specificity for each concept), we found the reasonably expected result that higher levels of specificity
                          correspond to better solve rates. Maximum specificity solved 4/10 with a generally decreasing pattern
                         with lower specificity. While this result was entirely expected, the main goal of the experiment was
                          to determine how much we could compromise on concept detail to improve the retrieval aspect.
                         There is a tension between retrieval and puzzle solving in the sense that puzzle solving benefits from
                          moredetailed suggestions, but retrieval struggles to apply them to new situations when the concept’s
                          relevant situation is so precisely defined.
                          Embedding-basedRetrieval Experiments      Early in testing, we investigated standard embedding-
                          based retrieval approaches. Following the ArcMemo-OE setting, we used a VLM-generated caption
                          to query a vector database of ArcMemo-OE-style concept embeddings. While computationally
                          cheap compared to autoregressive generation (especially so when leveraging long-form reasoning),
                         wesawgenerally poor retrieval results. Using OpenAI’s embeddings API and an o3-mini puzzle-
                          solving backbone proved ineffective, lowering the score from 0.26 to 0.22, marking a 15% reduction.
                          Qualitative analysis found that retrieved concepts were largely irrelevant to the puzzle and seemed to
                          over-index on lexical similarity. Moreover, a small set of broadly defined (non-specific) concepts was
                          found to be grossly overrepresented.
                          D LIMITATIONS AND FUTURE WORK
                          D.1   LIMITATIONS
                          Following Akyürek et al. (2025), our evaluation focuses on a 100-task subset of ARC-AGI-1, selected
                          to make large-scale experimentation feasible under compute and cost constraints. This choice was
                          further motivated by the substantial sampling variance we observed—identical prompts can yield
                          noticeably different scores, requiring multiple runs to obtain stable estimates. As a result, our study
                          concentrates on a relatively small frontier of puzzles where memory augmentation can yield new
                          solves, limiting the absolute magnitude of observable gains.
                          In addition, ARC-AGI-2 was released mid-project as a harder successor benchmark, with state-of-
                          the-art performance still below 30%. Re-running all baselines and experiments on ARC-AGI-2 was
                          infeasible within scope due to cost and time constraints, but extending memory-based approaches to
                         ARC-AGI-2remainsanimportantdirection for future work.
                                                                      14
