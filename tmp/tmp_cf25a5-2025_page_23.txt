                                          Test-Time Learning for Large Language Models
                            Table 10. Comparison of experimental results on the Geography dataset of DomainBench.
                 Method              BERTScore↑    BLEURT↑     BLEU↑     Rouge-1 ↑   Rouge-2 ↑   Rouge-L↑
                 Llama3.2-3B-Instruct  0.6905       -0.6794     0.0638     0.2661     0.1020      0.1917
                   •Tent               0.6661       -1.1320     0.0444     0.2149     0.1149      0.1780
                   •EATA               0.5274       -1.4276     0.0032     0.0065     0.0007      0.0064
                   •TLM(Ours)          0.7160       -0.7278     0.0952     0.3203     0.1526      0.2534
                 Llama3-8B-Instruct    0.6953       -0.6161     0.0709     0.2711     0.1040      0.1977
                   •Tent               0.6007       -1.5575     0.0087     0.0932     0.0100      0.0756
                   •EATA               0.6751       -1.0290     0.0575     0.2432     0.1290      0.1949
                   •TLM(Ours)          0.7284       -0.5860     0.1064     0.3540     0.1650      0.2835
                 Llama2-13B-chat       0.6707       -0.7337     0.0495     0.2430     0.0975      0.1706
                   •Tent               0.4541       -1.2410     0.0011     0.0321     0.0000      0.0320
                   •EATA               0.6999       -0.7661     0.0703     0.3132     0.1441      0.2421
                   •TLM(Ours)          0.6902       -0.6738     0.0722     0.2996     0.1319      0.2225
                 Qwen2.5-7B-Instruct   0.7003       -0.5802     0.0823     0.2911     0.1125      0.2128
                   •Tent               0.6925       -0.9422     0.0810     0.2703     0.1293      0.2242
                   •EATA               0.6856       -0.9806     0.0723     0.2444     0.1182      0.2063
                   •TLM(Ours)          0.7214       -0.5093     0.1039     0.3412     0.1481      0.2632
            a rank of r = 8. The LoRA is applied only to the Wq and Wv.
            Tent (Wang et al., 2021) is a Test-Time Adaptation (TTA) method originally designed for image classification tasks, which
            adapts a model’s parameters during inference based on the entropy of predictions. In the context of dynamic parameter
            updates for Large Language Models (LLMs), we adapt the Tent method to work with LLMs by leveraging the prediction
            entropy of the 80 tokens generated by the model. Specifically, we calculate the entropy of the model’s predictions for these
            80tokens during each test-time update and use this information to adjust the LLM’s parameters.
            EATA(Niuetal., 2022a) is a state-of-the-art TTA method for image classification models, which adjusts model parameters
            based on low-entropy samples during inference. In the context of dynamic parameter updates for LLMs, we adapt the EATA
            methodbyleveraging the prediction entropy of 80 tokens generated by the model to select samples. Specifically, during
            each test-time update, we compute the entropy of the model’s predictions for these 80 tokens and, following the setup of
            EATA,adjust the parameters of the LLM (with the hyperparameter E0 of EATA set to 0.4). However, performing EATA
            updates on Llama2-13B-chat may result in out-of-memory errors. To address this issue, we reduce the number of tokens
            from 80 to 30 when applying EATA updates on Llama2-13B-chat.
            Offline and Online Settings. In our experiments, we consider two distinct settings: Offline and Online. In the Offline
            setting, all test data is processed at once, and the model’s parameters are updated using all available test samples before
            any testing is performed. In the Online setting, test data sequentially, where the model is updated after each individual
            test sample or batch. This setting better reflects real-world scenarios where data arrives in a continuous stream, requiring
            real-time updates to the model.
            D. MoreResults of Experiment
            Tocomprehensively evaluate the effectiveness of the proposed TLM, we report additional evaluation metrics and results.
            As shown in Table 10, the proposed method outperforms both the entropy-minimization-based method (Tent) and the
            original LLMs on the Geography dataset. Specifically, the proposed method achieves a 4.76% improvement in BERTScore
            compared to Llama3-8B-Instruct. As shown in Table 11, our proposed TLM outperforms Tent on the Agriculture dataset.
            For instance, our proposed TLM achieves a 146.61% improvement in the BLEU metric compared to Qwen2.5-7B-Instruct.
            FromTable 12, the proposed method outperforms the original LLM on the Medicine dataset. Specifically, compared to
            Llama3-8B-Instruct, the proposed method achieves a relative improvement of 7.05% in BERTScore. As shown in Table
            13, our proposed TLM outperforms Tent on the Finance dataset. For instance, our proposed TLM achieves a relative
            improvement 122.50% improvement in the BLEU metric compared to Llama3.2-3B-Instruct. From Table 14, the proposed
            method outperforms the original LLM on the Alpaca-GPT4 dataset. Specifically, compared to Llama3-8B-Instruct, the
            proposed method achieves a relative improvement of 4.30% in BERTScore. As shown in Table 15, the proposed method
                                                            23
