                                                                      · · ·                     Traditional Backprop
                                   1      TΘ(·,d)       2   TΘ(·,d)           TΘ(·,d)    K−1     TΘ(·,d)       K
                                  u                    u              · · ·            u                     u
                                               Forward Prop                     Proposed Backprop
                                   d                      Jacobian-based Backprop
              Figure 4: Diagram of backpropagation schemes for recurrent implicit depth models. Forward propagation is tracked via solid
              arrows point to the right (n.b. each forward step uses d). Backpropagation is shown via dashed arrows pointing to the left.
              Traditional backpropagation requires memory capacity proportional to depth (which is implausible for large K). Jacobian-
              based backpropagation solves an associated equation dependent upon the data d and operator TΘ. JFB uses a single backward
              step, which avoids both large memory capacity requirements and solving a Jacobian-type equation.
                                        MNIST                                 DEQs(Bai, Koltun, and Kolter 2020), and MONs (Winston
               Method                               Network size    Acc.      and Kolter 2020). We also compare with corresponding ex-
               Explicit                                     54K    99.4%      plicit versions of our ResNet-based networks given in (1) as
               Neural ODE†                                  84K    96.4%      well as with state-of-the-art ResNet results (He et al. 2016)
               Aug. Neural ODE†                             84K    98.2%      on the augmented CIFAR10 dataset. The explicit networks
               MON‡                                         84K    99.2%      aretrainedwiththesamesetupastheirimplicitcounterparts.
               JFB-trained Implicit ResNet (ours)           54K    99.4%      Table 1 shows JFBs are an effective way to train implicit
                                                                              networks, substantially outperform all the ODE-based net-
                                         SVHN                                 works as well as MONs using similar or fewer parameters.
               Method                               Network size    Acc.      Moreover, JFB is competitive with Multiscale DEQs (Bai,
                                                                              Koltun, and Kolter 2020) despite having less than a tenth as
               Explicit                                   164K     93.7%      manyparameters. See appendix for additional results.
               Neural ODE†                                172K     81.0%
               Aug. Neural ODE†                           172K     83.5%      ComparisontoJacobian-basedBackpropagation
                                 ‡
               MON(Multi-tier lg)                         170K     92.3%
               JFB-trained Implicit ResNet (ours)         164K    94.1%       Table 2 compares performance between using the standard
                                       CIFAR-10                               Jacobian-based backpropagation and JFB. The experiments
               Method                               Network size    Acc.      are performed on all the datasets described in Section . To
                                                                              apply the Jacobian-based backpropagation in (13), we use
               Explicit (ResNet-56)∗                      0.85M    93.0%      the conjugate gradient (CG) method on an associated set of
                                 ‡∗                                           normal equations similarly to (Liao et al. 2018). To main-
               MON(Multi-tier lg)                         1.01M    89.7%
               JFB-trained Implicit ResNet (ours)∗        0.84M   93.7%       tain similar costs, we set the maximum number of CG iter-
               Multiscale DEQ∗                             10M     93.8%      ations to be the same as the maximum depth of the forward
                                                                              propagation. The remaining experimental settings are kept
              Table 1: Test accuracy of JFB-trained Implicit ResNet com-      the same in our proposed approach. Note the network ar-
              paredtoNeuralODEs,AugmentedNODEs,andMONs;†as                    chitectures trained with JFB contain batch normalization in
              reported in (Dupont, Doucet, and Teh 2019); ‡as reported in     the latent space whereas those trained with Jacobian-based
              (Winston and Kolter 2020); *with data augmentation              backpropagationdonot.Removalofbatchnormalizationfor
                                                                              the Jacobian-based method was necessary due to a lack of
                                                                              convergence when solving (13), thereby increasing training
              Classiﬁcation                                                   loss (see appendix for further details). This phenomena is
                                                                              also observed in previous works (Bai, Koltun, and Kolter
              Wetrainimplicitnetworksonthreebenchmarkimageclassi-             2020; Bai, Kolter, and Koltun 2019). Thus, we ﬁnd JFB to
              ﬁcation datasets licensed under CC-BY-SA: SVHN (Netzer          be (empirically) effective on a wider class of network archi-
              et al. 2011), MNIST (LeCun, Cortes, and Burges 2010), and       tectures (e.g. including batch normalization). The purpose
              CIFAR-10(KrizhevskyandHinton2009).Table1compares                of the Jacobian-based results in Figure 5 and Table 2 is to
              ourresults with state-of-the-art results for implicit networks, show speedups in training time while maintaining a com-
              including Neural ODEs (Chen et al. 2018), Augmented             petitive accuracy with previous state-of-the-art implicit net-
              Neural ODEs (Dupont, Doucet, and Teh 2019), Multiscale          works. More plots are given in the appendix.
                                                                        6653
