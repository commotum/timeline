60
s

B55

3

% 50

2

35

2

$40

§ 35 PRM + Active Learning

2 — PRM (PRMarge Supervised)
&30 — ORM (PRMiarge Supervised)
¥o5 — RM (final-answer supervised)

60
2
S55
8 50
i=
345
g
340
E35
g
FS
B30 — PRM (PRMige Supervised)
225 — ORM (PRMirge Supervised)
* — ORM (final-answer supervised)

10° 10? 10?
Number of solutions labelled per problem

(a) Four series of reward models
trained using different data collection
strategies, compared across training
sets of varying sizes.

10° 10" 10? 103
N = number of solutions per problem

(b) Three reward models trained on
200 samples/problem using different
forms of supervision, compared across
many test-time compute budgets.

Figure 4: A comparison of different forms of outcome and process supervision.
Mean and standard deviation is shown across three seeds.

We experimented with using RM-weighted voting (Li et al., 2022; Uesato et al.,
2022) to combine the benefits of the PRM and majority voting, but this did not
noticeably improve performance. We use a specific subset of the MATH test set
for evaluation, which we describe in Appendix C. We further break down these
results by problem difficulty in Appendix G.

4 Small-scale Synthetic Supervision

We find that the PRM outperforms the ORM at large-scale, but this result alone
paints an incomplete picture. To better compare outcome and process supervi-
sion, there are two confounding factors that must be isolated. First, the training
sets for the ORM and the PRM are not directly comparable: the PRM training
set was constructed using active learning, is biased towards answer-incorrect
solutions, and is an order of magnitude smaller. Second, the final-answer grad-
ing will provide positive labels to spurious solutions that reach the correct final
answer despite incorrect reasoning. This could damage ORM performance, an
effect we may or may not want to attribute to outcome supervision more gen-
erally.

Due to the high cost of collecting human feedback, we cannot easily ablate
these factors using human labelers. We instead perform the relevant ablations
by using the large-scale PRM to supervise smaller models. This setup enables
us to simulate a large amount of data collection at a modest cost. For the

remainder of this section, we refer to the large-scale PRM from Section 3 as
PRMaiarge-

