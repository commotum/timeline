                            A AnalysisoftheBackwardsDynamicsofaNeuralStack
                            We describe here the backwards dynamics of the neural stack by examining the relevant partial
                            derivatives of of the outputs with regard to the inputs, as deﬁned in Equations 1–3. We use δ        to
                                                                                                                              ij
                            indicate the Kronecker delta (1 if i = j, 0 otherwise). The equations below hold for any valid row
                            numbers i and n.
                                    ∂V [i]                               ∂V [i]                             ∂s [i]
                                       t     =δ         (12)                t    =δ        (13)                t   =δ         (14)
                                  ∂V     [n]     in                       ∂v        it                       ∂d        it
                                     t−1                                     t                                  t
                                                                            t
                                        ∂r                                 X                    ∂r        ∂r
                                          t   =min(s [n],max(0,1−               s [j]))   and      t =      t  =d             (15)
                                      ∂V [n]           t                         t              ∂v      ∂V [t]      t
                                         t                               j=n+1                     t       t
                                                                                                  t−1
                                                                                                   P
                                                    1       if i < n < t and s [i] > 0 and u −         s    [j] > 0
                                                                               t             t           t−1
                                                   
                                        ∂st[i]                                               t−1j=i+1
                                      ∂st−1[n] =  δ         if i < t and s [i] > 0 and u −    P s [j]≤0                      (16)
                                                    in                    t             t           t−1
                                                   
                                                                                            j=i+1
                                                 0         otherwise                    t−1
                                      ∂st[i]     -1     if i < t and s [i] > 0 and u −   P s [j]>0
                                             =                       t              t   j=i+1 t−1                            (17)
                                       ∂u
                                          t        0     otherwise
                                                  t
                                        ∂rt   =Xh(i,n)·Vt[i]
                                      ∂st[n]     i=1
                                                                                          P
                                                             δ      if s [i] ≤ max(0,1 −     t      s [j])
                                                          in          t                     j=i+1 t                          (18)
                                                                                                      P
                                                                    if i < n and st[i] > max(0,1 −       t     st[j])
                                      where    h(i,n) =      −1           P                               j=i+1
                                                                    and    t      s [j] ≤ 1
                                                                           j=i+1 t
                                                          0        otherwise
                            All partial derivatives other than those obtained by the chain rule for derivatives can be assumed
                            to be 0. The backwards dynamics for neural Queues and DeQues can be similarly derived from
                            Equations 4–11.
                            B ANoteonControllerInitialisation
                            Duringinitialexperimentswiththecontinuousstackpresentedinthispaper,wenotedthatthestack’s
                            ability to learn the solution to the transduction tasks detailed here varied greatly based on the random
                            initialisation of the controller. This initially required us to restart training with different random
                            seeds to obtain behaviour consistent with the learning of an algorithmic solution (i.e. rapid drop in
                            validation perplexity after a short number of iterations).
                            Analysis of the backwards dynamics presented in Section A demonstrates that error on push and
                            pop decisions is a function of read error “carried” back through time by the vectors on the stack
                            (cf. Equation 14 and Equations 17–18), which is accumulated as the vectors placed onto the stack
                            by a push, or retained after a pop, are read at further timesteps. Crucially, this means that if the
                            controller operating the stack is initially biased in favour of popping over pushing (i.e. u     >d
                                                                                                                         t+1      t
                            for most or all timesteps t), vectors are likely to be removed from the stack the timestep after they
                            were pushed, resulting in the continuous stack being used as an extra recurrent hidden layer, rather
                            than as something behaving like a classical stack.
                            Theconsequenceofthis is that gradient for the decision to push at time t only comes via the hidden
                            state of the controller at time t + 1, so for problems where the vector would ideally have been
                                                                              12
