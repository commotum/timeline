                       1   Introduction
                       Much of the recent work on Large Language Models (LLMs) focuses on their ability to solve
                       problems in natural language and code generation. Despite progress in these domains, transformers
                       still struggle to perform complex multi-step and algorithmic reasoning tasks in a zero shot setting
                       without resorting to tool use. To study algorithmic reasoning in a sterile laboratory setting, the
                       academic community focuses on simple arithmetic test problems like addition. Addition is simple
                       enough that modest-sized LLMs can (in principle) be trained from scratch to do it without running
                       into capacity and training budget limitations, yet complex enough that even large industrial models
                       fail on large numbers without a code interpreter [Loeber, 2024].
                       Training transformers for arithmetic enables us to study several important questions. First, we ask
                       what architectural design choices, dataset characteristics, and training pipeline variants are required
                       to learn a many-step reasoning process like multi-digit addition? Going deeper, we then investigate
                       whether these models are capable of logical extrapolation—can they solve problems of greater size
                       and difficulty than those that appear in their training set?
                       Prior studies indicate that addition is hard for transformers [Lee et al., 2023, Shen et al., 2023, Zhou
                       et al., 2023, 2024]. Our experiments indicate that this difficulty stems from their inability to clearly
                       represent the exact position of a digit within a long sequence of digits. To address this problem, we
                       propose a simple modification to the data representation that directly addresses this shortcoming.
                       OurAbacusEmbeddingsaresimplelearned positional embeddings that are used to encode positions
                       within each span of numerical tokens. Combining Abacus Embeddings and standard positional
                       embeddings, we observe dramatic improvements in accuracy such that models trained with at most 20
                       digit operands can generalize to problems with 120 digit operands. This represents a state-of-the-art
                       generalization factor of 6×, with the previous state of the art being only 2.5×. To the best of our
                       knowledge, these are the longest sequences on which learned addition has ever been demonstrated.
                       Wealso study several other methods of improving arithmetic and generalization in transformers.
                       Wefindthat incorporating input injection—skip connections inserted between the input layer and
                       each decoder layer—can reduce generalization errors by 50% over the Abacus Embedding baseline.
                       Wealso find that together with our embeddings looped transformer architectures, which contain
                       recurrent layers in which the same parameters are re-used multiple times, can achieve near-perfect
                       generalization on addition problems we consider.
                       Sinceourproposedmethodssolvelargeadditionproblemssuccessfully,weevaluatewhetherthesame
                       approaches can be used to improve other kinds of algorithmic learning. We explore multiplication
                       problems of up to 15 digit numbers and sorting over arrays of up to 10 numbers, making this the first
                       study of extreme length generalization techniques for addition that transfer to other algorithmic tasks.
                       Ourcontributions can be summarized as follows.
                             • Wepropose a new positional embedding called Abacus Embeddings to better capture the
                               significance of each digit, which leads to near-perfect in-distribution generalization.
                             • We show that when we combine Abacus Embeddings with input injection and looped
                               transformers performance further improves, increasing from 92.9% to 99.1% in out of
                               distribution accuracy, an 87% reduction in error compared to using the embeddings with
                               standard architectures alone.
                             • Wepushlength generalization beyond existing work and show that our models can solve
                               problems with six times as many digits as the largest samples in the training set, whereas
                               the previous state of the art is only two and a half times.
                             • We extend our findings to more complex problems including multiplication and sorting
                               where we show length generalization in these domains.
                       2   Related Work
                       ArithmeticandAlgorithmicReasoning.  Solvingarithmeticwithnexttokenpredictionisadifficult
                       problem that attracts a lot of attention [e.g. Saxton et al., 2019]. However, in zero-shot settings,
                       even incredibly strong commercial API models struggle with very large addition problems (e.g.
                       up to 100 digits) without access to tools. Among attempts to improve arithmetic performance of
                       transformer-based models, reversing the digits so the arguments are written with the least significant
                                                                2
