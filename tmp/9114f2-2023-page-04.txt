minimize overfitting, we include data from 4.5K MATH test problems in the
PRM800K training set, and we therefore evaluate our models only on the re-
maining 500 MATH test problems. More details about this test set can be found
in Appendix C.

During data collection, we must decide which solutions to surface to data-
labelers. The most straightforward strategy is to uniformly surface solutions
produced by the generator. However, if we surface solutions that make obvious
errors, the human feedback we get is less valuable. We would prefer to surface
solutions that are more likely to fool our best reward model. To that end, we at-
tempt to strategically select which solutions to show data-labelers. Specifically,
we choose to surface convincing wrong-answer solutions. We use the term con-
vincing to refer to solutions that are rated highly by our current best PRM, and
we use wrong-answer to refer to solutions that reach an incorrect final answer.
We use this slightly verbose phrasing to emphasize the fact that correctness is
determined solely by checking the final answer, a process which occasionally
leads to misgraded solutions. We expect to gain more information from labeling
convincing wrong-answer solutions, since we know the PRM is mistaken about
at least one step in each such solution.

In addition to using this selection strategy, we also iteratively re-train our
PRM using the latest data at several points in the data collection process. At
each iteration, we generate N solutions per problem and surface only the top K
most convincing wrong-answer solutions to data-labelers. We experiment with
either applying this top-K filtering at a problem level (K solutions per problem)
or globally across the dataset (K solutions in total, unequally distributed among
problems). Since the data collection process is expensive, it was not feasible
to conduct at-scale ablations of these decisions. However, we perform several
surrogate ablations in Section 4, using our largest PRM as a labelling oracle for
asmaller PRM. More details about data collection can be found in Appendix B.

2.5 Outcome-supervised Reward Models (ORMs)

We train ORMs following a similar methodology to Cobbe et al. (2021). We
uniformly sample a fixed number of solutions per problem from the generator,
and we train the ORM to predict whether each solution is correct or incorrect.
In practice, we usually determine correctness by automatically checking the
final answer, but in principle these labels could be provided by humans. At test
time, we use the ORMâ€™s prediction at the final token as the overall score for the
solution. We note the automatic grading used to determine ORM targets is not
perfectly reliable: false positives solutions that reach the correct answer with
incorrect reasoning will be misgraded. We discuss additional ORM training
details in Appendix E.

2.6 Process-supervised Reward Models (PRMs)

We train PRMs to predict the correctness of each step after the last token in
each step. This prediction takes the form of a single token, and we maximize the

ot
