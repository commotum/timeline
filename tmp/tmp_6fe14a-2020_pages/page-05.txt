                     Training  Retriever                      Top-20                                  Top-100
                                              NQ TriviaQA WQ TREC SQuAD NQ TriviaQA WQ TREC SQuAD
                     None      BM25           59.1    66.9     55.0   70.9     68.8    73.7    76.7     71.1   84.1    80.0
                     Single    DPR            78.4    79.4     73.2   79.8     63.2    85.4    85.0     81.4   89.1    77.2
                               BM25+DPR 76.6          79.8     71.0   85.2     71.5    83.8    84.5     80.5   92.7    81.3
                     Multi     DPR            79.4    78.8     75.0   89.1     51.6    86.0    84.7     82.9   93.9    67.6
                               BM25+DPR 78.0          79.9     74.7   88.5     66.2    83.9    84.4     82.3   94.1    78.6
                  Table 2: Top-20 & Top-100 retrieval accuracy on test sets, measured as the percentage of top 20/100 retrieved
                  passages that contain the answer. Single and Multi denote that our Dense Passage Retriever (DPR) was trained
                  using individial or combined training datasets (all the datasets excluding SQuAD). See text for more details.
                  traditional retrieval methods, the effects of different      90
                  training schemes and the run-time efﬁciency.
                     The DPRmodelusedinourmainexperiments                      80
                  is trained using the in-batch negative setting (Sec-
                  tion 3.2) with a batch size of 128 and one additional        70
                  BM25negative passage per question. We trained                                                BM25
                  the question and passage encoders for up to 40               60                              # Train: 1k
                  epochs for large datasets (NQ, TriviaQA, SQuAD)              Top-k accuracy (%)              # Train: 10k
                  and 100 epochs for small datasets (TREC, WQ),                50                              # Train: 20k
                                                                                                               # Train: 40k
                  with a learning rate of 10−5 using Adam, linear                                              # Train: all (59k)
                  scheduling with warm-up and dropout rate 0.1.                40        20      40       60      80      100
                     While it is good to have the ﬂexibility to adapt                      k: # of retrieved passages
                  the retriever to each dataset, it would also be de-      Figure 1: Retriever top-k accuracy with different num-
                  sirable to obtain a single retriever that works well     bers of training examples used in our dense passage re-
                  across the board. To this end, we train a multi-         triever vs BM25. The results are measured on the de-
                  dataset encoder by combining training data from          velopment set of Natural Questions. Our DPR trained
                  all datasets excluding SQuAD.8 In addition to DPR,       using 1,000 examples already outperforms BM25.
                  wealsopresent the results of BM25, the traditional
                                   9
                  retrieval method and BM25+DPR, using a linear            tiple datasets, TREC, the smallest dataset of the
                  combination of their scores as the new ranking           ﬁve, beneﬁts greatly from more training examples.
                  function. Speciﬁcally, we obtain two initial sets        In contrast, Natural Questions and WebQuestions
                  of top-2000 passages based on BM25 and DPR,              improve modestly and TriviaQA degrades slightly.
                  respectively, and rerank the union of them using         Results can be improved further in some cases by
                  BM25(q,p) + λ·sim(q,p) as the ranking function.          combining DPR with BM25 in both single- and
                  Weusedλ=1.1basedontheretrievalaccuracyin                 multi-dataset settings.
                  the development set.                                        We conjecture that the lower performance on
                  5.1   MainResults                                        SQuADis due to two reasons. First, the annota-
                  Table 2 compares different passage retrieval sys-        tors wrote questions after seeing the passage. As
                  tems on ﬁve QA datasets, using the top-k accuracy        a result, there is a high lexical overlap between
                  (k ∈ {20,100}). With the exception of SQuAD,             passages and questions, which gives BM25 a clear
                  DPRperforms consistently better than BM25 on             advantage. Second, the data was collected from
                  all datasets. The gap is especially large when k is      only 500+ Wikipedia articles and thus the distribu-
                  small (e.g., 78.4% vs. 59.1% for top-20 accuracy         tion of training examples is extremely biased, as
                  on Natural Questions). When training with mul-           argued previously by Lee et al. (2019).
                     8SQuADislimitedtoasmallset of Wikipedia documents     5.2   Ablation Study on Model Training
                  and thus introduces unwanted bias. We will discuss this issue
                  moreinSection 5.1.                                       Tounderstand further how different model training
                     9Lucene implementation. BM25 parameters b = 0.4 (doc-
                  ument length normalization) and k = 0.9 (term frequency  options affect the results, we conduct several addi-
                                                 1
                  scaling) are tuned using development sets.               tional experiments and discuss our ﬁndings below.
