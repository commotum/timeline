                        Training Model                                      NQ TriviaQA WQ TREC SQuAD
                        Single       BM25+BERT(Leeetal.,2019)               26.5      47.1       17.7    21.3       33.2
                        Single       ORQA(Leeetal.,2019)                    33.3      45.0       36.4    30.1       20.2
                        Single       HardEM(Minetal.,2019a)                 28.1      50.9        -        -         -
                        Single       GraphRetriever (Min et al., 2019b)     34.5      56.0       36.4      -         -
                        Single       PathRetriever (Asai et al., 2020)      32.6        -         -        -        56.5
                        Single       REALMWiki (Guuetal., 2020)             39.2        -        40.2    46.8        -
                        Single       REALMNews (Guuetal., 2020)             40.4        -        40.7    42.9        -
                                     BM25                                   32.6      52.4       29.9    24.9       38.1
                        Single       DPR                                    41.5      56.8       34.6    25.9       29.8
                                     BM25+DPR                               39.0      57.0       35.2    28.0       36.7
                        Multi        DPR                                    41.5      56.8       42.4    49.4       24.1
                                     BM25+DPR                               38.8      57.9       41.1    50.6       35.8
                  Table 4: End-to-end QA (Exact Match) Accuracy. The ﬁrst block of results are copied from their cited papers.
                  REALM        and REALM        are the same model but pretrained on Wikipedia and CC-News, respectively. Single
                           Wiki            News
                  andMultidenotethat our Dense Passage Retriever (DPR) is trained using individual or combined training datasets
                  (all except SQuAD). For WQ and TREC in the Multi setting, we ﬁne-tune the reader trained on NQ.
                  see that higher retriever accuracy typically leads to    trained, following Lee et al. (2019). This approach
                  better ﬁnal QA results: in all cases except SQuAD,       obtains a score of 39.8 EM, which suggests that our
                  answers extracted from the passages retrieved by         strategy of training a strong retriever and reader in
                  DPR are more likely to be correct, compared to           isolation can leverage effectively available supervi-
                  those from BM25. For large datasets like NQ and          sion, while outperforming a comparable joint train-
                  TriviaQA, models trained using multiple datasets         ing approach with a simpler design (Appendix D).
                  (Multi) perform comparably to those trained using           Onething worth noticing is that our reader does
                  the individual training set (Single). Conversely,        consider more passages compared to ORQA, al-
                  onsmaller datasets like WQ and TREC, the multi-          though it is not completely clear how much more
                  dataset setting has a clear advantage. Overall, our      time it takes for inference. While DPR processes
                  DPR-basedmodelsoutperform the previous state-            up to 100 passages for each question, the reader
                  of-the-art results on four out of the ﬁve datasets,      is able to ﬁt all of them into one batch on a sin-
                  with1%to12%absolutedifferencesinexactmatch               gle 32GB GPU, thus the latency remains almost
                  accuracy. It is interesting to contrast our results to   identical to the single passage case (around 20ms).
                  those of ORQA (Lee et al., 2019) and also the            Theexact impact on throughput is harder to mea-
                  concurrently developed approach, REALM (Guu              sure: ORQAuses2-3xlongerpassages compared
                  et al., 2020). While both methods include addi-          to DPR (288 word pieces compared to our 100
                  tional pretraining tasks and employ an expensive         tokens) and the computational complexity is super-
                  end-to-end training regime, DPR manages to out-          linear in passage length. We also note that we
                  perform them on both NQ and TriviaQA, simply             found k = 50 to be optimal for NQ, and k = 10
                  byfocusing on learning a strong passage retrieval        leads to only marginal loss in exact match accu-
                  model using pairs of questions and answers. The          racy (40.8 vs. 41.5 EM on NQ), which should be
                  additional pretraining tasks are likely more useful      roughly comparable to ORQA’s 5-passage setup.
                  only when the target training sets are small. Al-
                  though the results of DPR on WQ and TREC in the          7    Related Work
                  single-dataset setting are less competitive, adding      Passage retrieval has been an important compo-
                  morequestion–answer pairs helps boost the perfor-        nent for open-domain QA (Voorhees, 1999). It
                  mance, achieving the new state of the art.               not only effectively reduces the search space for
                     Tocompareourpipeline training approach with           answer extraction, but also identiﬁes the support
                  joint learning, we run an ablation on Natural Ques-      contextforuserstoverifytheanswer. Strongsparse
                  tions where the retriever and reader are jointly         vector space models like TF-IDF or BM25 have
