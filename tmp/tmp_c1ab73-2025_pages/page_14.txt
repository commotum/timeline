                  Preprint: Don’t throw the baby out with the bathwater: How and why deep learning for ARC
                  [12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-
                       lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,
                       TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,Jeffrey Wu, Clemens Winter, Chris Hesse, Mark
                       Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
                       Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,
                       M.Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems,
                       volume 33, pages 1877–1901. Curran Associates, Inc., 2020.
                  [13] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is
                       in-context learning? investigations with linear models. In The Eleventh International Conference on Learning
                       Representations, 2023.
                  [14] Luca H. Thoms, Karel A. Veldkamp, Hannes Rosenbusch, and Claire E. Stevenson. Solving arc visual analogies
                       with neural embeddings and vector arithmetic: A generalized method. ArXiv, abs/2311.08083, 2023.
                  [15] Jake Snell, Kevin Swersky, and Richard S. Zemel.     Prototypical networks for few-shot learning.  CoRR,
                       abs/1703.05175, 2017.
                  [16] Natasha Butt, Blazej Manczak, Auke Wiggers, Corrado Rainone, David Zhang, Michaël Defferrard, and Taco
                       Cohen. Codeit: Self-improving language models with prioritized hindsight replay, 2024.
                  [17] Charles Jin and Martin Rinard. Evidence of meaning in language models trained on programs, 2023.
                  [18] Belinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural language models.
                       In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of
                       the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language
                       Processing (Volume 1: Long Papers), pages 1813–1827, Online, August 2021. Association for Computational
                       Linguistics.
                  [19] MandyGuo,JoshuaAinslie, David C. Uthus, Santiago Ontañón, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.
                       Longt5: Efficient text-to-text transformer for long sequences. CoRR, abs/2112.07916, 2021.
                  [20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei
                       Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR,
                       abs/1910.10683, 2019.
                  [21] Jean Piaget. The origins of intelligence in children. The origins of intelligence in children. W W Norton & Co,
                       NewYork,NY,US,1952.
                  [22] Mikel Bober-Irizar and Soumya Banerjee. Neural networks for abstraction and reasoning: Towards broad
                       generalization in machines. arXiv preprint arXiv:2402.03507, 2024.
                  [23] Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz. General-purpose in-context learning by
                       meta-learning transformers. In Sixth Workshop on Meta-Learning at the Conference on Neural Information
                       Processing Systems, 2022.
                  [24] Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. At which training
                       stage does code data help LLMs reasoning? In The Twelfth International Conference on Learning Representations,
                       2024.
                  [25] A. Koepf. Arc research: Riddle synthesis, 2022. Software.
                  [26] Icecuber Top-Quarks. Top-quarks/arc-solution: Code for 1st place solution to kaggle’s abstraction and reasoning
                       challenge, 2020.
                  [27] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei A. Efros, and Moritz Hardt. Test-time training for
                       out-of-distribution generalization, 2020.
                  [28] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
                       LostintheMiddle: HowLanguageModelsUseLongContexts. TransactionsoftheAssociationforComputational
                       Linguistics, 12:157–173, 02 2024.
                  [29] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
                       Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie
                       Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models
                       to follow instructions with human feedback, 2022.
                  [30] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,
                       and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning
                       Representations, 2022.
                                                                        14
