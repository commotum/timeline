                 Figure2.2: Totalcomputeusedduringtraining. BasedontheanalysisinScalingLawsForNeuralLanguageModels
                 [KMH+20]wetrainmuchlargermodelsonmanyfewertokensthanistypical. Asaconsequence, although GPT-3 3B
                 is almost 10x larger than RoBERTa-Large (355M params), both models took roughly 50 petaﬂop/s-days of compute
                 during pre-training. Methodology for these calculations can be found in Appendix D.
                                                            Quantity     Weight in      Epochs elapsed when
                                 Dataset                     (tokens)   training mix  training for 300B tokens
                                 CommonCrawl(ﬁltered)      410billion       60%                0.44
                                 WebText2                   19billion       22%                 2.9
                                 Books1                     12billion       8%                  1.9
                                 Books2                     55billion       8%                 0.43
                                 Wikipedia                   3 billion      3%                  3.4
                 Table 2.2: Datasets used to train GPT-3. “Weight in training mix” refers to the fraction of examples during training
                 that are drawn from a given dataset, which we intentionally do not make proportional to the size of the dataset. As a
                 result, when we train for 300 billion tokens, some datasets are seen up to 3.4 times during training while other datasets
                 are seen less than once.
                 Amajormethodological concern with language models pretrained on a broad swath of internet data, particularly large
                 models with the capacity to memorize vast amounts of content, is potential contamination of downstream tasks by
                 having their test or development sets inadvertently seen during pre-training. To reduce such contamination, we searched
                 for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.
                 Unfortunately, a bug in the ﬁltering caused us to ignore some overlaps, and due to the cost of training it was not feasible
                 to retrain the model. In Section 4 we characterize the impact of the remaining overlaps, and in future work we will
                 moreaggressively remove data contamination.
                 2.3  Training Process
                                   +
                 Asfoundin[KMH 20,MKAT18],largermodelscantypicallyusealargerbatchsize,butrequireasmaller learning
                 rate. We measure the gradient noise scale during training and use it to guide our choice of batch size [MKAT18]. Table
                 2.1 shows the parameter settings we used. To train the larger models without running out of memory, we use a mixture
                 of model parallelism within each matrix multiply and model parallelism across the layers of the network. All models
                 were trained on V100 GPU’s on part of a high-bandwidth cluster provided by Microsoft. Details of the training process
                 and hyperparameter settings are described in Appendix B.
                                                                      9
