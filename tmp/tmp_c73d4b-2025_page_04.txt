                           Figure 2: Nested Learning Paradigm that represent a machine learning model and its training
                           procedure as a set of nested optimization problems. (Left) An example of Hybrid architecture. While
                           deep learning perspective, as the flattened image of NL, does not provide insight about the depth of
                           computation in the blocks, NL transparently represent all the inner gradient flows. (Right) A Neural
                           Learning Module: A computational model that learns how to compress its own context flow. For
                           example, the first level corresponds to the model’s the most outer-loop training, often refer to as
                          “pre-training” step.
                           2.1  Associative Memory
                           Associative memory—the ability to form and retrieve connections between events—is a fundamental
                           mental process and is an inseparable component of human learning [56]. Often in the literature, the
                           concept of memorization and learning are used interchangeably; in neuropsychology literature, how-
                           ever, these two are clearly distinguished. More specifically, following neuropsychology literature [57],
                           webuild our terminology based on the following definition of memory and learning:
                               Learning vs. Memorization:
                              Memoryis a neural update caused by an input, and learning is the process for acquiring
                               effective and useful memory.
                           In this work, our goal is to first show that all the elements of a computational sequence model,
                           including optimizers and neural networks, are associative memory systems that compress their own
                           context flow. Broadly speaking, associative memory is an operator that maps a set of keys to a set of
                           values. We follow the general definition of associative memory by Behrouz et al. [58]:
                                                                                        d                   d
                           Definition 1 (Associative Memory). Given a set of keys K ⊆ R k and values V ⊆ R v, associative
                           memory is an operator M : K → V that maps two sets of keys K and values V. To learn such
                                                                ˜
                           mapping from the data, an objective L(·;·) measures the quality of the mapping and M can be
                           defined as:
                                                              ∗              ˜
                                                           M =argmin L(M(K);V).                                         (1)
                                                                       M
                           While the operator itself is a memory and the mapping acts as a memorization process (i.e., memoriz-
                           ing the connections of events in the context), acquiring such effective operator based on the data, is a
                           learning process. It is notable that, here, keys and values can be any arbitrary events that memory aims
                           to map them and are not limited to tokens. Later in this section, we will discuss that given a context
                           flow, keys and values might be tokens, gradients, sub-sequences, etc. Furthermore, while the term
                           of associative memory is more common in neuroscience and neuropsychology literature, the above
                           formulation is also closely related to data compression and low-dimensional representation. That is,
                           one can interpret the optimization process in Equation 1 as the training process of a network M(.)
                           that aims to compress the mappings into its parameters and so represent them in a lower dimensional
                           space.
                           In sequence modeling, where keys and values are input tokens (e.g., tokenized text), the choice
                           of objective and the optimization process for solving Equation 1 can result in distinct sequence
                                                                          4
