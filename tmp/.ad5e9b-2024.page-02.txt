                                             2       Experimental settings
                                             Wefocusontwoproblemsofarithmetic: computingGCDandmultiplicationmodulo67. TheGCD
                                             wasstudied in prior work (Charton, 2024; Dohmatob et al., 2024).
                                             In the greatest common divisor problem, the model is tasked to predict the GCD of two integers
                                             uniformly distributed between 1 and 1 million, encoded in base 1000. Following Charton (2024),
                                             whoobserves that throughout training almost all pairs of integers with the same GCD are predicted
                                             the same, we evaluate model performance by the number of GCD below 100 predicted correctly,
                                             measured on a random test sample of 100,000 pairs: 1000 pairs for each GCD from 1 to 100. On
                                             this metric, Charton (2024) reports a best performance of 22 correct GCD for a model trained on
                                             uniformlydistributedinputs. Thistestmetricispreferabletoamorestandardmeasureofaccuracyon
                                             randominputpairs,becauseGCDaredistributedaccordingtoaninversesquarelaw(61%ofrandom
                                             pairs have GCD 1, 15% have GCD 2), making accuracy a very optimistic measure of performance.
                                             In modular multiplication, we train models to predict the product, modulo 67, of two integers
                                             between 1 and a million. Arithmetic modulo p was studied in several previous works, in the context
                                             of grokking (Power et al., 2022; Liu et al., 2022a) and mechanistic interpretability (Zhong et al.,
                                             2023), but with model inputs sampled from 0 to p − 1, which results in a very small problem space
                                             for small p. We evaluate model accuracy as the percentage of correct predictions of a×b mod 67,
                                             onatest set of 10,000 examples (generated afresh at every evaluation).
                                             Models and tokenizers. We use sequence-to-sequence transformers (Vaswani et al., 2017) with
                                             4 layers in the encoder and decoder, an embedding dimension of 512, and 8 attention heads (35
                                             million trainable parameters). Models are trained to minimize a cross-entropy loss, using the Adam
                                             optimizer (Kingma & Ba, 2014), with a learning rate of 10−5, and batches of 64. The integer inputs
                                             and outputs of both problems are tokenized as sequences of digits in base 1000, preceded by a sign
                                             which serves as a separator. All experiments are run on one V100 GPU with 32 GB of memory.
                                             3       Repetition Helps
                                             In a first series of experiments, we compare the performances of models trained on different data
                                             budgets (number of distinct examples) for increasing training budgets (total examples). We consider
                                             DBof1,5,10,25,50 and 100 million examples, together with the unlimited case, where examples
                                             are generated on the fly (yielding DB≈TB). Figure 1 (Left) presents the average number of GCD
                                             predicted by 5 models trained on different DB, for increasing TB. For a small TB of 30 million
                                             examples, models trained on 1 and 5M DB achieve the best performance: 20 GCD vs 13 for all
                                             others. As TB increases, 1M-models start overfitting, as shown by increasing test losses in Figure 1
                                             (Right), and their performance saturates at 21 correct GCD. The performance of the 5M models
                                             keeps improving to 36 GCD, for a TB of 150 million examples, then begins to overfit, and saturates
                                             around 38. For TB of 150 and 300 million examples, the best performing models are the 10M.
                                                                         1M                                                                   1M
                                                                 60      5M                                                                   5M
                                                                         10M                                                                  10M
                                                                         25M                                                                  25M
                                                                         50M                                                         20       50M
                                                                 50      100M                                                                 100M
                                                                         Infinite                                                             Infinite
                                                                 40
                                                                                                                                     15
                                                                GCD predicted30
                                                                                                                                     10
                                                                 20
                                                                 10                                                                   5
                                                                  0     30    150    300   450    600    750   900   1050                 0     150    300    450    600    750   900    1050
                                                                                       training examples (M)                                                training examples (M)
                                             Figure 1: GCDproblem: (Left) GCD accuracy for different data and training budgets (average of 5 models).
                                             (Right) Test loss of models as a function of training budget, for fixed data budgets.
                                                                                                                               2
