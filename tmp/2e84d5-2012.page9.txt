                          Neuron
                          Perspective
                                                                                                                                                                                                          “A bird in song”
                          Figure 3. Hierarchical Generative Models
                          Thisschematicshowsanexampleofagenerativemodel.Generativemodelsdescribehow(sensory)dataarecaused.Inthisﬁgure,sensorystates(bluecircleson
                          theperiphery)aregeneratedbyhiddenvariables(inthecenter).Left:themodelasaprobabilisticgraphicalmodel,inwhichunknownvariables(hiddencausesand
                          states) are associatedwiththenodesofadependencygraphandconditionaldependenciesareindicatedbyarrows.Hiddenstatesconfermemoryonthemodel
                          byvirtueofhavingdynamics,whilehiddencausesconnectnodes.Agraphicalmodeldescribestheconditionaldependenciesamonghiddenvariablesgenerating
                                                                                                                                                                                                                  ~ðiÞ
                          data.Thesedependenciesaretypicallymodeledas(differential)equationswithnonlinearmappingsandrandomﬂuctuationsu withprecision(inversevariance)
                             ðiÞ
                          P (seetheequationsintheinsertontheleft).Thisallowsonetospecifythepreciseformoftheprobabilisticgenerativemodelandleadstoasimpleandefﬁcient
                                                                                                               paðiÞ                                                                                                           ðiÞ                 ðiÞ
                                                                                                              ~                                                                                                               ~                   ~
                          inversionscheme(predictivecoding;seeFigure4).Herev                                         denotesthesetofhiddencausesthatconstitutetheparentsofsensorys orhiddenx states.The
                                                                                                                ~          0   00
                          ‘‘’’ indicatesstatesingeneralizedcoordinatesofmotion:x=ðx;x ;x ;.Þ.Right:anintuitiveversionofthemodel:here,weimaginethatasingingbirdisthecause
                          ofsensations,which—throughacascadeofdynamicalhiddenstates—producesmodality-speciﬁcconsequences(e.g.,theauditoryobjectofabirdsongandthe
                          visual object of a song bird). These intermediate causes are themselves (hierarchically) unpacked to generate sensory signals. The generative model therefore
                          mapsfrom causes (e.g., concepts) to consequences (e.g., sensations), while its inversion corresponds to mapping from sensations to concepts or represen-
                          tations. This inversion corresponds to perceptual synthesis, in which the generative model is used to generate predictions. Note that this inversion implicitly
                          resolves the binding problem by explaining multisensory cues with a single cause.
                          in the reverse direction, to update conditional expectations. This                                                           The ﬁrst pair of equalities just says that conditional expecta-
                                                                                                                                                   tions about hidden causes and states ðm~ðiÞ                                           ðiÞ
                          ensures an accurate prediction of sensory input and all its inter-                                                                                                                                         ; m~   Þ are updated
                                                                                                                                                                                                                                  v      x
                          mediate representations. This hierarchal message passing can                                                             baseduponthewaywewouldpredictthemtochange—theﬁrst
                          be expressed mathematically as a gradient descent on the                                                                 term—andsubsequenttermsthatminimizepredictionerror.The
                                                                                         ðiÞ        ðiÞ  ðiÞ
                                                                                                       ~
                          (sumofsquared)predictionerrorsx =P ε ,wherethepredic-                                                                    second pair of equations simply expresses prediction error
                          tion errors are weighted by their precision (inverse variance):                                                          ðxðiÞ; xðiÞÞ as the difference between conditional expectations
                                                                                                                                                       v     x
                                                                                                                                                   abouthiddencausesand(thechangesin)hiddenstatesandtheir
                                                             ðiÞ         ð Þ           ð Þ                                                                                                                                                  ðiÞ      ðiÞ
                                                           _     =Dm~ i v ~i ,xðiÞ  xði+1Þ
                                                          m~                         ε                                                             predicted values, weighed by their precisions ðP ;P Þ. These
                                                                         v          ~                  v                                                                                                                                    v        x
                                                             v                     v
                                                                                                                                                   predictions are nonlinear functions of conditional expectations
                                                                   ðiÞ                                                                                 ðiÞ    ðiÞ
                                                                  _             ðiÞ          ðiÞ    ðiÞ                                            ðg ;f Þ at each level of the hierarchy and the level above.
                                                                 m~    =Dm~ v ~ ,x
                                                                   x            x         ~ε
                                                                                          x                                                            It is difﬁcult to overstate the generality and importance of
                                                   ðiÞ        ðiÞ ðiÞ       ðiÞ    ði1Þ        ðiÞ   ðiÞ     ðiÞ                              Equation (1)—it grandfathers nearly every known statistical esti-
                                                                ~
                                                  x =P ε =P m~                             g m~ ;m~
                                                   v          v   v         v       v                   x       v                                  mation scheme, under parametric assumptions about additive
                                                                  ðiÞ                 ðiÞ          ðiÞ      ðiÞ                                noise. These range from ordinary least squares to advanced
                                                  xðiÞ        ðiÞ~          ðiÞ                 ðiÞ
                                                       =P ε =P Dm~ f m~ ;m~                                         :                   (1)
                                                   x          x   x         x          x               x      v                                    Bayesian ﬁltering schemes (see Friston, 2008). In this general
                                                                                                                                                                     Neuron 76, November 21, 2012 ª2012 Elsevier Inc. 703
