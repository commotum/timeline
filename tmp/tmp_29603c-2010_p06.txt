                         SARSA to an extent; however, it can be shown to outper-                                                                  Sutton, R.; McAllester, D.; Singh, S.; and Mansour, Y.
                         form it as the critic operates on a different, more sound cost                                                           2000. Policy gradient methods for reinforcement learning
                         function than traditional temporal difference learning, and                                                              with function approximation. In Advances in Neural Infor-
                         as its weighted “soft-max” policy update will promote suc-                                                               mation Processing Systems 12.
                         cessful actions faster than the standard soft-max. We have
                         shown that the method performs efﬁciently when used in a                                                                                                 Derivation of REPS
                         policy iteration setup. REPS is sound with function approx-                                                              We denote p                   = µπ(s)π(a|s) and µπ(s) = P p                                             for
                         imation and can be kernelized straightforwardly which of-                                                                                        sa                                                                   a sa
                         fers interesting possibilities for new algorithms. The relation                                                          brevity of the derivations, and give the Lagrangian for the
                         to PoWER (Kober and Peters 2009) and Reward-Weighted                                                                     program in Eqs.(5-8) by
                         Regression is not yet fully understood as these methods                                                                                                 !                                                !
                                                                                                                                                              X                                       X                    p
                         minimizeD(pπ(τ)||r(τ)q(τ))whichissuperﬁciallysimilar                                                                       L=                p Ra +η ε−                             p      log sa
                                                                                                                                                                        sa      s                              sa          q
                                                                                                    π                                                                                                                        sa
                         to maximizing Ep{r(τ)} subject to D(p (τ)||q(τ)). Both                                                                                s,a                                     s,a                !                              !
                         methods end up with very similar update equations for the                                                                        X X                                         X                                     X
                         episodic case. Application of REPS for reinforcement learn-                                                                               T                     a        0              0  0     0
                                                                                                                                                       + θ                     p P 0φ −                      p       φ       +λ 1−                  p        ,
                         ing of motor primitive selection for robot table tennis has                                                                                              sa ss s                      s a      s                             sa
                                                                                                                                                              0          s,a                              0                                  s,a
                         been successful in simulation.                                                                                                     s                                           a                                                !
                                                                                                                                                            X                                   p                               X
                                                                                                                                                                                a                  sa               T                     a     T
                                                                                                                                                        =          p        R −ηlog                    −λ−θ φ +                       P 0θ 0φ 0
                                                                   References                                                                                        sa         s               q                   s     s              ss     s     s
                                                                                                                                                            s,a                                   sa                               0
                         Atkeson, C. G. 1993. Using local trajectory optimizers to                                                                                                                                               s
                         speed up global optimization in dynamic programming. In                                                                       +ηε+λ,                                                                                          (11)
                         NIPS, 663–670.                                                                                                           whereη,θ andλdenotetheLagrangianmultipliers.Wesub-
                         Bagnell, J., and Schneider, J. 2003. Covariant policy search.                                                            stitute V = θTφ . We differentiate
                         In International Joint Conference on Artiﬁcial Intelligence.                                                                            s               s
                                                                                                                                                                                          p                           P
                                                                                                                                                                        a                   sa                                  a
                                                                                                                                                   ∂       L=R −ηlog                              +η−λ+ P 0V0−V =0,
                         Boyd,S.,andVandenberghe,L. 2004. ConvexOptimization.                                                                        p                  s                                                   0   ss     s           s
                                                                                                                                                       sa                                 q                               s
                         Cambridge University Press.                                                                                                                                        sa
                                                                                                                                                                                                 1      a              a                        λ
                                                                                                                                                                                                   (R +           0 P      V 0−V ) 1−
                         de Farias, D. P., and Roy, B. V. 2003. The linear program-                                                               and obtain p                   = q eη s                       s     ss0 s          s e        η .Given
                         ming approach to approximate dynamic programming. Op-                                                                                            saP             sa
                         erations Research 51(6):850–856.                                                                                         that we require                  s,a psa = 1, it is necessary that
                                                                                                                                                                                                                                       
                                                                                                                                                          1−λ              X                   1 Ra+               Pa V 0−V               −1
                         Deisenroth, M. 2009. Efﬁcient Reinforcement Learning us-                                                                                                                 ( s            0      0   s       s)
                                                                                                                                                        e       η =                    qsaeη                   s     ss                         ,      (12)
                         ing Gaussian Processes. Ph.D. thesis, Karlsruhe Institute of                                                                                             s,a
                         Technology, Karlsruhe, Germany.                                                                                          (hence, λ depends on θ), and we can compute
                         Furmston, T., and Barber, D. 2010. Variational methods for                                                                                                                     P                                  
                         reinforcement learning. In AISTATS.                                                                                                                             1        a                   a        0
                                                                                                                                                                       qsa exp              (R +                0 P 0Vs −Vs)
                                                                                                                                                     p      =                            η s                 s      ss                           (13)
                         Hachiya, H.; Akiyama, T.; Sugiyama, M.; Peters, J.; 2008.                                                                     sa         P                           1                P
                                                                                                                                                                                                       a                   a        0
                                                                                                                                                                            q      exp           (R +                0 P 0V −V )
                         Adaptive importance sampling with automatic model selec-                                                                                      s,a sa                 η        s           s       ss     s          s
                         tion in value function approximation. In AAAI, 1351–1356.                                                                We can extract a policy using π(a|s) = p /P p , and
                         Hernandez, J.                       2010.              http://www.dia.ﬁ.upm.es/ ja-                                                                                                                     sa         a sa
                         martin/download.htm.                                                                                                     hence optain Eq. (9). Reinserting these results into Eq.(11),
                         Kakade, S. A. 2002. Natural policy gradient. In Advances                                                                 weobtain the dual function
                                                                                                                                                                                                                               1−λ −ε
                         in Neural Information Processing Systems 14.                                                                                      g(θ,η,λ) = −η +ηε+λ=−ηlog e ηe                                                            ,
                         Kober, J.; Peters, J. 2009. Policy Search for Motor Primi-                                                               which can be rewritten as Eq.(10) by inserting Eq.(12).
                         tives in Robotics. In Advances in Neural Information Pro-
                         cessing Systems 22.
                         Mannor, S.; Simester, D.; Sun, P.; and Tsitsiklis, J. N. 2007.
                         Biases and variance in value function estimates. Manage-
                         ment Science 53(2):308–322.
                         Peshkin, L., and Shelton, C. R. 2002. Learning from scarce
                         experience. In ICML, 498–505.
                         Peters, J., and Schaal, S. 2008. Natural actor critic. Neuro-
                         computing 71(7-9):1180–1190.
                         Puterman, M. L. 2005. Markov Decision Processes: Dis-
                         crete Stochastic Dynamic Programming. New York, NY:
                         John Wiley and Sons.
                         Sutton, R., and Barto, A. 1998. Reinforcement Learning.
                         MITPress.
                                                                                                                                       1612
