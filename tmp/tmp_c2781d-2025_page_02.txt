           Preprint, Under Review.
           Figure 1: Dynamic planning strategies across environments and training stages. (a-b) Zero-
           shot results showing optimal “Goldilocks” planning frequency in Crafter and POGS (100 seeds,
           bars=standard-error). (c-d) SFT results demonstrating planning agents’ improved performance with
           lower KL divergence from base model. (e-f) RL results where SFT-primed planning agents are more
           sample efficient than non-planning baselines and more consistently reach complex achievements.
           To formalise this problem, we develop a framework for modelling the cost-benefit trade-offs of
           planning in partially-observable environments. According to our framework, agents should allocate
           test-time compute for planning only when anticipated improvements in policy performance outweigh
           the associated computational costs and any instability or noise induced by excessive replanning.
           Weexperimentally investigate these concepts in two distinct environments: Partially-Observable
           GraphSearch(POGS),asynthetic environment that we design to systematically evaluate planning
           abilities, and Crafter, a Minecraft-inspired grid-world environment (Hafner, 2022). Inspired by recent
           workshowingthat the presence of key inductive biases in training data are necessary for effective
           self-improvement (Gandhi et al., 2025), we develop a two-stage approach: first priming models with
           diverse planning behaviours through supervised fine-tuning (SFT), then applying RL. Using this
           approach, we successfully train agents that learn to plan strategically, execute their plans, and replan
           only when necessary, outperforming non-planning baselines trained via an equivalent two-stage
           pipeline. Furthermore, following the RL stage, agents that are trained to produce and follow their
           ownplanscanbeeffectively steered by plans produced by humans to achieve performance that the
           agents cannot reach alone. In summary, our experiments yield four key insights:
              1. Eachtaskhasa“Goldilocks”frequencyforplanningthatclearlyoutperformsnaivestrategies
               of always planning or never planning.
                               2
