                                          Evaluation Input
                                                                                                                 Evaluation Input
                                                                     Label: No
                                                            Now 
                               Problem     Previous Steps
                                                            Step
                               Collect response from LLM
                                to construct seed data
                                                                            SFT Dataset
                                                                                         R-PRM SFT
                               Previous Steps Analysis: This step starts by ...
                                                                                                                         Sampling
                                              ......
                               Verification: Is the step correct (Yes/No)? No
                                                                                                                       ...
                                                                                                                      ...
                                                                                                             Verification: Is the step correct
                                                                                                                    ......
                                                                                                           Verification: Is the step correct
                                                                                                                   (Yes/No)? No
                                              ......
                                                                                                          Verification: Is the step correct
                                                                                                                  (Yes/No)? No
                              Now Step Analysis: Now Step checks if 23 is ...
                                                                                                                (Yes/No)? No
                                                                              Preference 
                               Verification: Is the step correct (Yes/No)? No
                                                                               Dataset
                                                                                         R-PRM DPO
                                                                                                                  0.4 0.9 0.80.8
                                                                                                                                Average
                                              ......
                               Calculation Analysis: The calculation in the...
                               Verification: Is the step correct (Yes/No)? Yes
                                                                                                                  Reward: 0.7
                   Figure 1: Illustration of R-PRM framework. For brevity, only partial analytical reasoning trajectories are shown.
                   White robots indicate initial models, while colored ones represent models after our training procedure.
                   is generated conditioned on both the problem Q                 from PRM800K to generate (Q, s , A , J ) tu-
                                                                                                                           1:i    i   i
                   and all preceding steps {s1,...,si−1}. To evaluate             ples 1. We retain only those evaluation analyses
                   the quality of each reasoning step, current process-           that produce a judgment consistent with human la-
                   level reward models employ a direct prediction                 bel. Subsequently, we concatenate the analysis and
                   mechanismthat assigns a score to each step. This               judgmentasthetargetsequence,whichisthenused
                   evaluation process can be formally expressed as:               to fine-tune our PRM. Let Yi denote the evaluation
                                                                                  trajectory for s :
                                                                                                   i
                                   R =M(Q,s ,...,s )
                                      i             1      i
                                                                                            Y =A ⊕J ={y ,y ,...,y }
                   where M(·) represents the reward model that out-                          i      i     i       1   2        t
                                                                                                       t
                   puts a scalar reward R for the step s . However,                                  X
                                              i                 i                        L     =− logp(y |Q,s ,y                    )
                   evaluating reasoning steps on hard math questions                       SFT                    j      1:i  1:j−1
                                                                                                     j=1
                   is quite challenging, and direct prediction is rela-
                   tively difficult for the reward model. Additionally,           where y denotes the j-th token in the output se-
                                                                                           j
                   scores generated directly often suffer from a lack             quence Yi, and t is the total length of the sequence.
                   of explainability.                                             This is equivalent to standard instruction tuning,
                      To solve these issues, we propose a reasoning-              where the model learns to generate both the analy-
                   driven process reward model G that performs two                sis and the judgment in a single forward pass.
                   phases within a single generation process as illus-            3.2   Process Reward Modeling
                   trated in Figure 1. First, G generates a comprehen-                  Meta-Optimization
                   sive analysis A of each reasoning step s , consist-
                                     i                            i               Although cold start activates the model’s reason-
                   ing of multiple analytical dimensions: examining               ing ability, it may still yield incorrect judgments.
                   historical reasoning steps, assessing the objective            Facing the challenge of data scarcity, we further
                   and data sources of the current step, verifying its            explore how our process reward model can self-
                   coherence with preceding steps, and validating the             evolve without incorporating additional data. We
                   calculations involved. Then, G generates a natural             propose Meta-Optimization, which employs prefer-
                   language judgment J indicating the correctness of
                                           i                                      ence optimization method to refine the reasoning
                   the step, expressed as “Yes” or “No”.                          behavior of our R-PRM, thereby guiding it towards
                                    A =G(Q,s ,...,s )                             making accurate judgments.
                                      i            1       i
                                                                                     For simplicity, we implement our approach us-
                                  J =G(Q,s ,...,s ,A )                            ing Direct Preference Optimization (DPO, Rafailov
                                    i            1      i    i
                      TohelpLLMsfullyleveragetheirreasoningabil-                  et al., 2024), one of the popular preference opti-
                   ities, we designed a quick cold-start phase. In this           mization algorithms. DPO involves an input pair
                   phase, we prompt a stronger LLM with samples                      1The prompt we used is listed in Appendix G
                                                                            13452
