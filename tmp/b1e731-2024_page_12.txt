                                                                                  Inference
                                    Training       Mean         GA1          GA5           GA20         GA100        RS250
                                    Mean         3.2 (2.7)    3.6 (3.0)    18.8 (14.4)   52.5 (25.0)  67.5 (20.0)   3.2 (2.7)
                                    GA1          8.6 (4.4)   44.6 (10.9)   85.4 (7.6)    98.4 (1.4)    99.5 (0.5)   10.2 (5.3)
                                    GA1(g)       0.6 (0.1)    13.7 (3.0)   60.2 (7.5)    88.9 (6.0)    94.1 (3.8)   0.7 (0.2)
                                    GA5          0.0 (0.0)    0.4 (0.3)    31.9 (11.2)   88.5 (11.9)   98.1 (2.1)   0.5 (0.4)
                                    GA5(g)       0.0 (0.0)    0.0 (0.0)     9.9 (2.9)    87.1 (6.0)    95.1 (3.3)   0.0 (0.0)
                                    RS5          6.1 (4.4)    8.2 (6.5)    27.7 (21.6)   56.3 (27.5)  72.2 (21.2)   6.1 (4.4)
                                    RS25         10.8 (8.0)  13.3 (10.1)   39.9 (21.4)   72.3 (18.5)   87.9 (9.2)   10.8 (8.0)
                             Table 1: Ablation of different LPN training and inference methods on the Pattern task. For each
                             training method, training was done for 20k steps with 3 different seeds, and performance is aggregated
                             over the 3 runs with the standard deviation in parentheses. Methods with a (g) indicate that the
                             gradient over parameters flows through the latent gradient ascent (analog to meta-learning). Other
                             methods stop the parameter gradient during latent optimization. Two latent optimization methods
                             are compared. GA [N] stands for gradient ascent with N steps, RS [X] means random search with a
                             budget of X samples.
                                                                Inference Ablation
                                               Mean
                                               GA 1
                                            GA 1 (g)                                                 Inference Method
                                               GA 5                                                  Encoder mean
                                                                                                     GA 100
                                           raining MethodGA 5 (g)                                    Encoder mean + GA 100
                                           T   RS 5
                                              RS 25
                                                     0      20     40     60      80     100
                                                                  Accuracy (%)
                             Figure 8: Ablation on the initialization of latent optimization and the role of the encoder. Encoder
                             meansignifies that latent optimization is initialized using the encoder mean latents. GA 100 stands
                             for 100 steps of gradient ascent during latent optimization. The results demonstrate the importance of
                             using the encoder to find a good starting point before doing latent optimization.
                             Thefirst result demonstrated on this relatively simple benchmark is that inference using mean latents
                             (no latent optimization) performs poorly across all training methods. This shows that inferring the
                             correct latent in one shot is challenging and that some form of latent program search is needed for
                             this task and model size. Then, we see that all training methods show increasing performance when
                             given more budget for latent optimization. Indeed, from using the encoder mean prediction to using 1
                             to 100 steps of gradient ascent, accuracy on the Pattern task keeps increasing, showing the ability
                             of LPNtoutilize compute to adapt to a new task at test time. Also, we observe that training with 1
                             gradient ascent step of latent optimization shows higher returns than training with mean latents when
                             scaling the inference budget. With 100 steps of gradient ascent at inference time, mean training gets
                             an accuracy of 67.5% when training with one gradient step reaches 99.5%. This demonstrates the
                             benefits of training the latent space with the awareness that gradient ascent will be performed at test
                             time, an important inductive bias for the LPN architecture. Lastly, we observe that gradient ascent
                             vastly outperforms random search, validating that search without using the gradient signal is highly
                             inefficient.
                             In addition, we ablate the impact of initializing latent optimization with the encoder versus using the
                             prior. Specifically, we compare z ∼ p(z) and z ∼ q (z|x,y). Figure 8 shows that initializing search
                                                                                   ϕ
                             with the encoder is critical for performance across all training methods validating the intuition that
                                                                                12
