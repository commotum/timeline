               C AttentionVisualization
               In this section, we examine the model internals to understand how the proposed model works. We ﬁrst
               visualize the model internals of different modeling alternatives to argue our proposed model is sensible.
               WhyWeRemovetheInputEmbedding Tounderstandifitissensibletoremovetheinputadditive
               embedding after adding position scalars per-head, we add additive position embedding to our DIET-ABS
               model. Then, we examine the position embedding of the BERT model and our DIET-ABS variant with
               additive position embedding. Figure 5 shows that, when the model has both absolute scalar and additive
               absolute position embedding, the position embedding encodes almost no information — all position
               embeddings at input are similar.
               Figure 5: The cosine similarity distribution between all absolute position pairs of the input additive positional
               embeddingforthebaselineBERTmodelandtheproposedDIET-ABS. Weobservedthat,afterthepositionfeatures
               are added to each head as in DIET-ABS, the input position embedding contains almost no information — all input
               position pairs are similar.
               TheEffect of Segment Attention  Wealso examine the effect of adding segment attention on top of
               the position attention. Figure 6 shows some representative patterns. We observe that segment attention
               enables the model to attend more to parts of the sequence that belongs to certain segments.
                         (a) Attend to the Second Segment          (b) Down-weight Relative Position Attention
               Figure 6: We consider input of length 32 with two segments. The second segment starts at index 16. We observe
               the attention patterns in the DIET-REL model without token-to-token attention.
                                                          2986
