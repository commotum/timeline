                                        This ICCV paper is the Open Access version, provided by the Computer Vision Foundation.
                                                     Except for this watermark, it is identical to the accepted version;
                                                the final published version of the proceedings is available on IEEE Xplore.
                               Scaling Transformer-Based Novel View Synthesis with Models
                                              TokenDisentanglementandSyntheticData
                                                                 1‚àó                     2‚àó                 2                      1
                              Nithin Gopalakrishnan Nair               Srinivas Kaza          XuanLuo          Vishal M. Patel
                                                         Stephen Lombardi2           Jungyeon Park2
                                                              1 Johns Hopkins University   2 Google
                                    {ngopala2,vpatel36}@jhu.edu     {srinivaskaza,xuluo,salombardi,jungyeonp}@google.com
                                                           https://scaling3dnvs.github.io
                  LVSM                                                        LVSM                              LVSM
                  OURS                                                        OURS                              OURS
                Figure 1. Overview. Our method performs feed-forward novel-view synthesis from a series of input images, such as the pairs shown
                above. We demonstrate strong results in terms of quality and generalization capacity, performing well across a variety of common novel-
                view synthesis datasets, including scenes that are out-of-distribution.
                                         Abstract                                   rate synthetic training data generated from diffusion mod-
                Large transformer-based models have made significant                els, which improves generalization across unseen domains.
                progress in generalizable novel view synthesis (NVS) from           While synthetic data offers scalability, we identify artifacts
                sparse input views, generating novel viewpoints without the         introduced during data generation as a key bottleneck af-
                need for test-time optimization.     However, these models          fecting reconstruction quality. To address this, we propose
                are constrained by the limited diversity of publicly avail-         a token disentanglement process within the transformer ar-
                able scene datasets, making most real-world (in-the-wild)           chitecture, enhancing feature separation and ensuring more
                scenes out-of-distribution. To overcome this, we incorpo-           effective learning. This refinement not only improves re-
                                                                              28567
                   construction quality over standard transformers but also                        [10, 25, 35, 43].       However, previous feed-forward mod-
                   enables scalable training with synthetic data. As a result,                     els trained using synthetic data perform worse than those
                   our method outperforms existing models on both in-dataset                       trained with real data. We hypothesize that the inability of
                   andcross-datasetevaluations,achievingstate-of-the-artre-                        synthetic data to improve reconstruction quality stems from
                   sults across multiple benchmarks while significantly reduc-                     two types of degradation artifacts in scenes generated by
                   ing computational costs.                                                        diffusion models [14, 28, 37] (1) artifacts influenced by the
                   1. Introduction                                                                 initial noise of the diffusion process and (2) artifacts intro-
                                                                                                   duced during decoding, as most diffusion-based scene syn-
                   Novel view synthesis (NVS) [19, 26] is a well-studied and                       thesis models operate in latent space and rely on a diffusion
                   important problem in computer vision, where the task is to                      VAE[32]. Weaddressbothissues,leadingtoimprovedper-
                   generate unseen perspectives of a scene from a given set of                     formance when using synthetic data. We provide a detailed
                   images. Many approaches utilize volumetric [2, 5, 26, 27]                       explanation of our data pipeline in Section 4.2.
                   or differentiable rendering [19] to optimize for each scene                         In this work, we tackle a key challenge in developing
                   individually, achieving high-quality NVS from arbitrary                         a feed-forward NVS model that performs well on out-of-
                   viewpoints.      More recently, advancements have enabled                       distribution data ‚Äì the need for a scalable and efficient
                   training a single model that generalizes to novel scenes                        transformer-based NVS architecture. We introduce the To-
                   without requiring per-scene optimization.               Most existing           ken Distentangled (Tok-D) transformer block, which ap-
                   methods address NVS by incorporating hand-crafted 3D                            plies layer-wise modulation of source and target tokens, ex-
                   priors and architectural biases [4, 15, 38]. While these de-                    plicitly distinguishing the two at each layer. These model
                   sign choices provide structure, they limit scalability with                     modifications improve out-of-distribution training, which
                   data and hinder generalization.                                                 introduces the possibility of training on synthetic data. We
                      Recently, Large View Synthesis Model (LVSM) [18]                             use the CAT3D model to generate a large dataset of syn-
                   proposed a promising foundation for an NVS model scal-                          thetic multi-view samples. We then employ a novel data
                   able with large datasets.         LVSM introduces an architec-                  generation strategy that significantly improves the quality
                   ture that doesn‚Äôt require 3D inductive biases for scene re-                     of these synthetic samples. We show that the Tok-D trans-
                   construction.     It employs a decoder-only transformer ar-                     former block can be trained with synthetic data augmenta-
                   chitecture that achieves state-of-the-art results by a sig-                     tion, unlike the baseline LVSM method which suffers from
                   nificant margin, with the performance improving with in-                        the inclusion of synthetic data.
                   creased compute. However, we observed during our exper-                         ‚Ä¢ We enhance the scalability of transformer architectures
                   iments that the decoder-only design causes an inherent fea-                        for NVS, enabling more efficient modeling.
                   ture alignment problem which causes the target and source                       ‚Ä¢ Weintroduce a new training scheme that is less suscepti-
                   features to look similar at all layers. Thus, part of the trans-                   ble to artifacts from synthetic data.
                   former‚Äôs computational capacity is spent modifying source                       ‚Ä¢ We improve the training efficiency of transformer for
                   token information that is ultimately discarded at the end                          NVSbyintroducinganewtransformerblock.
                   of the transformer block, reducing efficiency. This design                      ‚Ä¢ Ourapproachachievesstate-of-the-artresults across mul-
                   choice also makes LVSM susceptible to unwanted noise                               tiple benchmarks for scene level NVS.
                   or compression artifacts that may be present in the source
                   views. In addition, we noticed that LVSM presents limited                       2. Related Works
                   cross-domain performance when tested on datasets outside                        2.1. Offline Novel View Synthesis
                   the training dataset domains.
                      Moreover, these issues are not unique to LVSM; many                          The advent of neural rendering in recent years has substan-
                   NVS models face similar challenges due to data scarcity                         tially improved the quality of NVS. Early neural scene rep-
                   in 3D vision. All existing multi-view 3D scene datasets                         resentations focused on the 4D plenoptic function [11, 22]
                   [23, 24, 48] combined contain fewer than 100,000 scenes,                        that represents the lightfield of a scene [1, 36, 38]. Other
                   severely limiting the performance of NVS models on in-                          methods modeled the geometry of the scene (e.g.                     as a
                   the-wild cases beyond the training distribution. One pos-                       signed distance function) separately from material proper-
                   sible solution for alleviating this 3D data scarcity is using                   ties [39, 44]. Either way, a differentiable rendering process
                   synthetic data from generative models. Recent research has                      wasusedtorenderthese neural representations into 2D im-
                   explored adapting pre-trained image [32, 33] and video dif-                     ages [26]. Most of these methods focused on fitting neu-
                   fusion models [13, 14] for multi-view dataset generation                        ral fields to sparse observations of a scene at test time‚Äî
                      *Equal contribution. Nair designed the methodology, conducted pre-           we refer to this as test-time or offline optimization. There
                   rebuttal experiments, and drafted the initial manuscript. Kaza helped ad-       is a substantial amount of heterogeneity in these methods,
                   vise the project, led the rebuttal, and conducted camera-ready experiments.     both in terms of the rendering method and the scene repre-
                                                                                            28568
                                                                                                             Source tokens
                                                                                                                                 UnPatchify
                                                                                                             Target tokens
                      Input View
                                                             Generated Views
                                                                                                           Tok-D Transformer Blocks
                                     Multi-View DiÔ¨Äusion Model
                                                                                                            Source Patchify 
                                                                                                                                   Patchify
                                                                                                                 NVS Model Training
                                         Synthetic Data Generation
                Figure 2. An illustration of the architecture. We use CAT3D, a multi-view diffusion model, to generate synthetic views conditioned on
                random spline camera trajectories and a random image. From the two random views form the generated views as the source views and
                the input conditioning view to be the target of our large reconstruction network. Our large reconstruction model uses a special transformer
                block which we name Tok-D Transformer. When real data is available, we just use the reconstruction transformer.
                sentation used. Multi-layer perceptrons (MLPs) [26], vox-           resolution. Other efforts in this space include GPNR [38]
                els [9, 25], hashing-based representations [3, 27], triplanes       and SRT [34], which are parameterized in a similar fash-
                [5], and, most recently, Gaussian splats [16, 19, 20, 30]           ion to IBRNet [40] and attempt to scale up the image and
                have been used as scene representations.        These meth-         ray transformers. LRF [21] attempts to perform 3D recon-
                ods have trade-offs between reconstruction quality, training        struction in the latent space of a VAE, bypassing learning
                time, inference time, memory/space requirements, capacity           3D representation altogether [47]. Finally, the LVSM [18]
                to model view-dependent effects, etc. Some of these offline         removes all 3D priors by simply using one transformer to
                methods can even fit dynamic scenes. These test-time opti-          perform NVS. LVSMperformsfavorablycomparedtoboth
                mization methods demonstrate compelling results given the           geometry-free and geometry-based feed-forward models.
                sparsity of the observations provided. However, they often          2.3. Synthetic Data
                struggle to incorporate priors learned from larger datasets.
                                                                                    Recent efforts have leveraged synthetic data to train exist-
                2.2. Online Novel View Synthesis                                    ing feed-forward NVS methods and investigate its efficacy
                Sometimes referred to as ‚Äúfeed-forward‚Äù or ‚Äúgeneraliz-              as a training dataset. However, it is important to note that
                able‚Äù NVS models, these methods attempt to directly pro-            the synthetic data in many of these efforts are generated
                duce 3D representations from input images. Early efforts            procedurally from systems like Blender, whereas ours are
                include the image-based rendering-inspired IBRNet [40],             generated from a multi-view diffusion model. Two recent
                which directly produces 2D images based on epipolar cor-            works LRM-Zero [42] and MegaSynth [17] are examples
                respondences on the viewing ray. The Large Reconstruc-              of models trained either entirely or mostly on procedurally
                tion Model (LRM) [15] family of methods attempt to pro-             generated synthetic data. In LRM-Zero, they demonstrate
                duceatriplane that represents an object, in some cases with         that the LRM model can be trained entirely on synthetic
                near-real time performance. PixelSplat [4], MVSplat [4],            data. However, the synthetic-data-only model shows a sub-
                andGS-LRM[46]attempttopredict3DGS[19]representa-                    stantial decrease in reconstruction quality compared to the
                tions, whichexploitthesparseGaussiansplatrepresentation             real-world-data equivalent. Improving training data diver-
                and fast rasterization to achieve quasi-interactive inference.      sity using synthetic data for 4D generation has also been
                These methods are trained on large datasets of real-world           explored in CAT4D [41].
                scenes, which helps them outperform even test-time opti-            3. Background
                mization methods. Quark [8] couples an easily-rasterizable
                layered depth map representation with a render-and-refine           LVSM is a feed-forward NVS method that has no 3D in-
                strategy to achieve state-of-the-art quality at a much higher       ductive bias. Since our model builds upon its architecture,
                                                                              28569
                            k
                                    2d
                                                                                     k        6d
                                                                                        Linear Layer
                               Linear Layer
                          Ì†µÌ∞ë
                                  Ì†µÌ∞ë
                                                                                   Ì†µÌ∞ë       Ì†µÌ∞ë
                       Embedding 
                                                                                 Embedding 
                     Layer
                                                                               Layer
                                                                                               Style Tokens
                                      Style Tokens
                                                                           0
                  0
                                                                           1
                  1
                                                                                                             P                   P
                                                                                                    P
                                                                                                                    P
                                                                                   Style 
                         Style 
                                                                                                             ost-Modulat         ost-Modulat
                                                                                                                              F
                                                              F
                                                                                                                        Layernorm
                                                                                                    r
                                                                                                                    r
                                                        Layernorm
                                                                                                         Multi-head 
                                                 Multi-head 
                                                                                                                              eedfor
                                                                                                    e-Modulat
                                                              eedfor
                                                                                                                    e-Modulat
                                         Modulat
                                                                                                       A
                                               A
                                                                                                                           Network
                                                            Network
                                                                                   embed
                         embed
                                                                                                       tt
                                               tt
                                                                                                       ention
                                               ention
                                                                                                                              war
                                                              war
                                       d
                                                                                                 d
                                      Ì†µÌ∞ë
                                                                                               Ì†µÌ∞ë
                                         e
                                                                                                                              d 
                                                              d 
                                                                                                    e
                                                                                                                    e
                                                                                                             e                   e
               Figure 3. An illustration of the Tok-D transformer block. Our transformer blocks that differentiates between source and target tokens.
               Tok-Dtransformer modulates the input to all transformer blocks. Tok-D plus transformer modulates the attention and MLP layers.
               weoutline the details here for clarity. Where i denotes the      that maybepresentinthesourceimagestothetarget. More-
               image index and j denotes the token index, source images         over this alignment also causes some part of the computa-
                                      s      p√óp√ó3           ¬®
               patchesarewrittenasI     ‚ààR         , source Pluckercoordi-      tional power of the model being used to model source token
                                      ij
                               s     p√óp√ó6               ¬®                      information although those tokens are discarded at the last
               natespatchesPij ‚àà R         , and target Plucker coordinates
                 t     p√óp√ó6                            ¬®                       layer. Hence, we call for a need to distinguish between the
               P ‚ààR          . The source images and plucker embeddings
                 j                                                              source and target tokens of the transformer network.
               are tokenized together using a linear layer embedder.
                                S =Linear([Is,Ps])                     (1)      4. Method
                                  ij             ij   ij
                             ¬®                                                  Our proposed method consists of two major contributions.
               The target Plucker coordinates are also embedded using a
               linear layer.                                                    First, our Token-Disentangled (Tok-D) transformer block
                                   T =Linear(Pt)                       (2)      is specialized for NVS and distinguishes information from
                                    ij              ij                          the source and target views, leading to more efficient allo-
               Finally, the transformer network is trained to reconstruct the   cation of representation capacity. Second, to address the
                                     t            ¬®
               target output tokens Oj from the Plucker patch embeddings.       scarcity of multi-view data, we generate synthetic data us-
                                                                                ing CAT3D [10] and propose a model training scheme that
                                    Ot =M(T |S )                       (3)      is robust to artifacts in this synthetic data. In this section,
                                      j        j   ij
               The target output tokens are detokenized using a linear          wedescribe each component in detail.
               layer which is converted to target image embeddings T ‚àà          4.1. Token-Disentangled Transformer
                                                                       j
               Rp√óp√ó3                                                           In LVSM, the transformer blocks process source and target
                                   T =Linear([Ot])                     (4)
                                    j               j                           tokens in the same manner, even though the source consists
                                                                                                 ¬®
               The target patches are unpatchified to get the target image      of images and Plucker rays, while the target includes only
                       H√óW√ó3                                                      ¬®
               T ‚àà R            (see Figure 2). The training is supervised      Plucker rays. Additionally, source and target image quality
               using MSElossandperceptuallossdesignedtoreconstruct.             candifferwhentrainingwithsyntheticdata. Toaddressthis,
               Transformer Block Consider a transformer block at                weintroduce the Token-Disentangled (Tok-D) Transformer
               layer l, which includes a Multi-head Self Attention layer        block (see Figure 3), which enables differentiated process-
               (SelfAttn ), a Feed-forward network (FFN ), and a Layer          ing of source and target tokens through modulation. The
                        l                                  l
               Normoperation (LN ). For an input [xs,xt], where xs and          Tok-D Transformer uses an indicator variable (Œ¥), where
                                    l                 l   l          l
               xt represent the source and target tokens, the data flow as      Œ¥ = 1 for target tokens and Œ¥ = 0 for source tokens, to
                 l                                                              modulate tokens based on their origin. This mechanism ex-
               follows:
                                                                                tracts distinct style vectors and computes specific scale and
                                                                                bias parameters for each layer and token type, allowing for
                         s   t      s   t                s  t                   precise and adaptive token modulation.
                       [x ,x ] = [x ,x ] + SelfAttn ([x ,x ])          (5)
                         l   l      l   l            l   l  l
                         s   t      s   t                 s   t
                       [x ,x ] = [x ,x ] + FFN (LN ([x ,x ])).
                         l   l      l   l        l    l   l   l                    style = Linear(Embed(Œ¥))                             (6)
                                                                                Mod(x)=(1+œÉ)x+¬µ,where[œÉ,¬µ]=Linear(style)
                  Giventhebasicselfattentionbasedtransformerblocksin                 l             l       l         l   l          l
               LVSM.Attheendoftheoptimizationprocesstherearisesa                 [xs,xt] = Mods,t([xs,xt]) = [Mods(xs),Modt(xt)]
               need for all token outputs of a particular layer to be aligned      l   l         l    l   l          l   l       l  l
               since they are processed by the same set of weights. Hence,      Modulating the input of each transformer block improves
               LVSM inherently has a chance to infuse noise or atifacts         perforamnce.    Drawing inspiration from DiT [29], we
                                                                          28570
                extend this modulation to the Attention and MLP lay-               sample 2 generated views I
                                                                                                               src
                ers, achieving further improvements. This modulation is
                termed pre-modulation if applied before a layer and post-                            I   , C    ‚àºI ,C                        (9)
                                                                                                      src   src     gen    gen
                modulation if after. Pre-modulation includes both scaling
                and shifting, and post-modulation involves only scaling.              and their camera poses as the source images I       , C
                                                                                                                                       src   src
                                                                                   and utilize the conditioned image and its camera as the tar-
                                                                                   get I ,C . Sampling the source and target images this way
                     s  t         s,t   s   t                                           c   c
                   ÀÜ ÀÜ
                  [xl,xl] = Modl1 ([xl,xl])                               (7)      forces the transformer to always generate a realistic image,
                     s  t       s   t      s   t               ÀÜs ÀÜt               making our model robust to artifacts from synthetic data.
                  [x ,x ] = [x ,x ] + [œÉ ,œÉ ]‚äôSelfAttn([x ,x ])
                     l  l       l   l      l1  l1                l   l
                   ÀÜs ÀÜt          s,t   s   t
                  [xl,xl] = Modl2 ([xl,xl])                                        5. Experiments
                     s  t       s   t      s   t                  ÀÜs ÀÜt
                  [x ,x ] = [x ,x ] + [œÉ ,œÉ ]‚äôFFN (LN ([x ,x ]))
                     l  l       l   l      l2  l2         l    l   l   l
                                                                                   5.1. Implementation Details
                where ‚äô denotes element-wise multiplication which scales           Training details We perform all experiments on 8 H100
                the corresponding source and target tokens.                        GPUs. We use the AdamW optimizer with Œ≤ parameters
                   Our Tok-D transformer block enhances the distinction            0.9 and 0.95, and we use weight decay with a rate of 0.05
                between source and target tokens, as reflected in their dis-       for all layers except the normalization layers. Moreover, we
                tinct feature representations (Figure 6, Section 5.4). This        use a linear learning rate scheduler with with a peak learn-
                specialization highlights the superior representational ca-        ing rate of 2e‚àí4, and a warmup of 2500 iterations. In total,
                pacity of our model. Furthermore, when trained on syn-             all experiments have 100k training iterations. In addition,
                thetic data (Section 4.2), out-of-distribution artifacts can       weuseexponential moving averaging (EMA) with a rate of
                introduce quality disparities between source and target to-        0.99 for stabilizing the training process. Although previous
                kens. By leveraging its token-aware architecture, our model        works required gradient clipping for a stable training pro-
                demonstrates greater robustness to these artifacts, resulting      cess, our training processes were smooth without a need for
                in improved performance, as shown in Section 5.3.                  an explicit gradient clipping.
                4.2. Synthetic Data Generation & Training Scheme                   TrainingandEvaluationDatasetsForscene-levelsynthe-
                Training a naive transformer model with synthetic data can         sis model training, we use Re10K [48], ACID [24] and
                lead to degraded performance rather than improvement due           DL3DV [23] with their originally released train and test
                to two key factors: (1) The model struggles to distinguish         splits. We also perform an experiment where the model
                between tokens from source images and target images, al-           is trained together with a mix of all of these datasets. For
                lowing artifacts from one to propagate into the other dur-         scene-level synthesis, we follow LVSM and train using 2
                ing alignment. (2) The model is trained to generate novel          input views and test using 6 target views fed one at a time.
                views from sparse input views, and if the target is a syn-         ForDL3DVdatasetevaluation,wechoosethefarthestcam-
                thetic image with artifacts, it may learn a distribution bi-       era from a randomly selected target view as the input view.
                ased toward unrealistic images. While these issues might           The training and evaluation of DL3DV dataset for in dis-
                not arise with perfect synthetic data, in-practice synthetic       tribution metrics is done using 2 input views and 2 target
                datasets often contain noise, making the model vulnerable          views. For cross dataset testing, we use 2 input views and 6
                to errors through either mechanism. However, for image-to-         target views for DL3DV dataset. We use a batch size of 64
                multiviewsynthesismodelslikeCAT3D,weproposeasim-                   for our experiments.
                ple yet effective solution: assigning the conditioned image        SyntheticDataForgeneratingthesyntheticdataweusethe
                as the target view and the generated views as input views.         state-of-the-art 3D generation model CAT3D. CAT3D was
                   Formally let I ,C denote the input image and camera             trained using a single scene dataset Re10K and three object-
                                  c   c                                            based datasets: Objaverse [7], MVImgNet [45] and Co3D
                conditioning used for the multiview diffusion model. We            [31]. To create synthetic data, we use two variants: one with
                sample additional random spline camera trajectory poses            1 conditioning view and 7 generated views, and another
                Ctgt relative to this particular view, and use the state-of-       with3conditioningviewsand5generatedviews. Wematch
                the-art multi-view diffusion model CAT3D to generate the           the focal lengths of Re10K and DL3DV during generation.
                target views I    conditioned on the input conditioning and
                              src                                                  For the camera trajectory, we sample a random spline tra-
                target poses                                                       jectory with a random position rotation matrix, converting it
                              I    ‚àºDM(I |C ,C ,I )                       (8)      into ray mapsbeforepassingitintothenetwork. AsCAT3D
                               gen           gen   gen    c  c                     is originally trained with a resolution of 512, we convert the
                Here DMrepresents inferencing through the state of the art         imagesandcameraparameterstoaresolutionof256before
                diffusion model, After obtaining the generated views, we           passing them through our network.
                                                                             28571
                                                                                 Ours                                    GT
                Input Views               LVSM
               Figure 4. Qualitative results on in-distribution datasets. We illustrate the cases Tok-D transformer works better than LVSM. We notice
               that we obtain substantial improvement in cases where the novel views needs to reconstruct regions present only in one of the views as
               shown in the highlighted regions in the images. The results presented here are taken from our in-distribution trained model. We present
               two diffrent views to show that this problem is persistent across views.
               Table 1. Quantitative comparisons for in-distribution scene synthesis at 256 resolution. LVSM and our method are trained with a
               batch size of 64. LVSM results are taken from the original paper rather than our re-implementation. Our method outperforms the previous
               SOTAmethodacrossallexisiting datasets. (   ,  ,   ) denotes the first, second and third best results.
                          Method     Venue           RealEstate10k [48]                 ACID[24]                     DL3DV[23]
                                                PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì
                      GPNR[38]      CVPR‚Äô23      24.11     0.793     0.255      25.28     0.764     0.332        -         -         -
                    PixelSplat [4]  CVPR‚Äô24      25.89     0.858     0.142      28.14     0.839     0.533        -         -         -
                     MVSplat[6]     ECCV‚Äô25      26.39     0.869     0.128      28.25     0.843     0.144      17.54     0.529     0.402
                 DepthSplat [43]    CVPR‚Äô25      27.44     0.887      0.119       -         -         -        19.05     0.610     0.313
                      LVSM [18]     ICLR‚Äô25      28.89     0.894      0.108     29.19     0.836     0.095      19.91     0.600     0.273
                            Ours                 30.02     0.919     0.058      29.47     0.846     0.086      21.55     0.643     0.208
               5.2. Scene Synthesis                                              corresponding input image. As can be seen from rows 1
               We evaluate our method qualitatively and quantitatively           and 2, the reconstruction form LVSM fails to reconstruct
               for scene synthesis using very recent feed-forward meth-          objects present in only one of the views, whereas Tok-D
               ods GPNR, PixelSplat, MVSplat, DepthSplat and LVSM.               transformer can effectively reconstruct these regions.
               These methods were chosen because they outperform con-            5.3. Cross-Dataset Scene Synthesis
               ventional approaches in 2-view reconstruction. Quantita-
               tive results are shown in Table 1. We observe that Tok-D-         To analyze the generalization capacity of our method, we
               PlusoutperformsLVSMby1.2dBontheRe10Kevaluation                    evaluate our method trained with Re10K dataset on two dif-
               benchmarkwhenbothmodelsaretrainedwith8GPUs. Fur-                  ferent datasets: ACID and DL3DV [23]. ACID is a dataset
               thermore, despite using only 8 GPUs, our method still sur-        with aerial views similar to Re10K. DL3DV [23] is a di-
               passes LVSM trained with 64 GPUs by a margin of 0.2 dB.           verse dataset comprising natural scenes and various indoor
               Moreover we obtain an improvement of 1.6dB over LVSM              and outdoor settings. The scene geometry and appearance
               in a more diverse scene-level dataset, DL3DV [23] dataset         of DL3DV [23] is very different from Re10k. We test the
               aswell. Wealsoobservethatourperformanceimprovement                Re10KandACIDdatasetsataresolution of 256√ó265. For
               is 0.2 in ACID dataset. We emphasize that this happens be-        testing on DL3DV[23],wechoosearesolutionof256√ó448
               cause ACID has a relatively smaller training and testing set      to maintain the original aspect ratio in the DL3DV [23]
               and the dataset is generally clean and easier to reconstruct.     dataset and well as maintain consistent evaluation settings
               Wealsoprovide the corresponding qualitative comparisons           with DepthSplat.    We choose 2 source views and 6 tar-
               onRe10KandDL3DVdatasetinFigure4. Comparingthe                     get views for all of these datasets. Looking closely at the
               main results we find that our method usually outperforms          quantitative results on Table 1 and Table 2, we find that the
               LVSMwhenthegeneratedcontent is only visible in one of             modeltrainedonRe10Kunderperformedthein-distribution
               the source views. When the camera is far from both views          trained model by a small margin. The drop is higher in the
               and the information is present only in one of the views, our      case of DL3DV due to resolution and diversity differences
               method is still able to extract the relevant content from the     in the datasets. Next we add a small portion of synthetic
                                                                           28572
                      Input                     LVSM                                          Ours                                       GT
                 Figure 5. Out-of-distribution Evaluation: We evaluate our the version of our method fine-tuned on synthetic data and LVSM on DL3DV
                 andACID(i.e. out-of-distribution datasets). We also evaluate the model with resolutions that were not used during training. We notice that
                 LVSM‚Äôsvisual quality degrades when substantial camera motion reveals previously-occluded regions.
                 Table 2. Quantitative comparisons for scaling up with synthetic data. We evaluate LVSM and our method, which are both trained with
                 abatchsizeof64. AmixtureofsynthesizedDL3DVandRe10Kdataisusedforthesynthetictab. ForMVSplatandDepthSplatweinclude
                 the numbers reported in their papers
                           Method        Training Dataset            RealEstate10k [48]                  ACID[24]                       DL3DV[23]
                                      Re10K[48]    Synthetic   PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì
                      MVSplat[6]          ‚úì                      26.39     0.869      0.128      28.15     0.147      0.841      17.72      0.534     0.371
                   Depthsplat [43]        ‚úì                      27.44     0.887      0.119        -          -         -        18.90      0.640     0.317
                       LVSM[18]           ‚úì                      28.89     0.894      0.108      28.29     0.809      0.104      20.52      0.621      0.223
                       LVSM[18]           ‚úì            ‚úì         28.49     0.892      0.070      28.16     0.802      0.107      20.21      0.595     0.240
                              Ours        ‚úì                      30.02     0.910      0.058      29.31     0.838      0.091      21.18      0.652     0.205
                              Ours        ‚úì            ‚úì         29.97     0.920      0.058      29.37     0.839      0.091      21.27      0.657     0.202
                 data comprising about half the size of Re10K dataset and                   though the training objective is to reconstruct the target.
                 perform training with the new framework. We also retrain                   This causes inefficient usage of the transformer parameters
                 LVSM for the same setting. We find that LVSM‚Äôs perfor-                     to maintain the source information throughout the layers.
                 mance drops rather than improving when synthetic data is                   Moreover this also makes the model prone to noise in the
                 added. We emphasize that this arises due to the introduc-                  source data. However, with our Tok-D transformer there is
                 tion of artifacts during feature alignment. In contrast, we                no alignment and the source information is infused much
                 observe an improvement in quality on our method when a                     earlier, leaving more room for the transformer blocks to re-
                 small amount of synthetic data is added.                                   construct the target. Another important observation is that
                                                                                                                                    ¬®
                 5.4. Analysis and Discussion                                               although both source image and Plucker coordinates are fed
                 Visualization of source and target features. To visually                   as input to the source, the source tokens look similar to the
                                                                                               ¬®
                 illustrate the representation alignment problem mentioned                  Pluckercoordinates. Whereasinourcasetheimagecompo-
                 in the previous sections, we visualize the 3 channel PCA                   nents in the source PCA components leading to much more
                 of each transformer block output after unpatchifying for all               effective information extraction from each source token.
                 24 layers of LVSM and our method in Figure 6. The first                    Impact of additional real data. Incorporating synthetic
                 rowshowsthefirst 6 layer outputs, second row shows layer                   data into the training process facilitates the introduction of
                 6-12, and so on. We can see that for a particular scene the                diverse scenes and camera motion, enhancing model gener-
                 source and target layer tokens are aligned at all layers even              alizability. While the proposed Tok-D transformer demon-
                                                                                      28573
                  (a)
                  Target    (b)
                            Source        (c) LVSMTarget PCA              (d) LVSMSourcePCA                 (e) Ours Source PCA              (f) Ours Target PCA
                  Figure 6. A visualization of the principal components of transformer layer outputs for source and target of LVSM. The 24 images
                  in each subfigure show the layer output of each layer of the transformer. LVSM features for source and target images looks similar even
                                                                          ¬®                                                      ¬®
                  though the source is conditioned with image and Plucker coordinates and target is conditioned with Plucker coordinates alone. This leads
                  to inefficient transformer usage requiring explicit alignment of source and target features across different layers
                  Table 3. Ablation studies on scaling up with more real data. Although including synthetic data in training is helpful for improving
                  quality, including additional real data significantly improves reconstruction quality.
                        Method              Training Dataset                   RealEstate10k [48]                ACID[24]                      DL3DV[23]
                                  Re10K[48]+Synthetic      DL3DV[23]     PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì PSNR‚Üë SSIM‚Üë LPIPS‚Üì
                    LVSM[18]                ‚úì                              28.49     0.892     0.070      28.16     0.802     0.107      20.21     0.595     0.240
                    LVSM[18]                ‚úì                   ‚úì          28.10     0.892     0.073      28.79     0.826     0.096      21.37     0.665     0.196
                          Ours              ‚úì                              29.97     0.920     0.058      29.37     0.839     0.091      21.27     0.657     0.202
                          Ours              ‚úì                   ‚úì          29.78     0.917    0.0604      30.13     0.857     0.082      23.14     0.726     0.156
                  Table 4. Ablation analysis We analyze the performance improve-                Table 5. Effect of EMA on runtime performance and quality.
                  ment of our design choices. Pre and Post demonstrate the effects              Comparison performed on Re10k.
                  of including or not including pre/post-modulation.                               Method   Train  Render GFLOPs        NoEMA               With EMA
                    Pre   Post   Whole     Attn   MLP PSNR‚Üë SSIM‚Üë LPIPS‚Üì                                     (ms)   (ms)           PSNR SSIM LPIPS PSNR SSIM LPIPS
                                                           28.50      0.893     0.070            LVSM-1024  706.1  171.6  2896.88  27.68  0.88  0.077  28.65  0.90  0.070
                                                                                                    Ours    734.6  174.4  2900.78  28.75  0.90  0.064  30.02  0.92  0.058
                     ‚úì              ‚úì                      29.69      0.911     0.063           Table 6. Effect of adding more source views. Our method works
                     ‚úì                      ‚úì      ‚úì       28.51      0.894     0.070           well as additional source views are introduced.
                     ‚úì     ‚úì                ‚úì      ‚úì       30.02      0.918     0.058
                                                                                                 Method         2views               4views              8views
                  strates reduced sensitivity to synthetic data artifacts and in-                         PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS
                  creased generative diversity, its photorealistic reconstruc-                     Ours   30.02  0.92  0.058   31.51  0.94  0.048  33.09  0.94   0.042
                  tion performance remains comparabletothebaselinemodel                         consistency, we show the results of our model and our re-
                  trained solely on real data. To investigate the impact of                     implementation of LVSM with 1024 channels trained with
                  augmenting the training dataset with additional real data,                    and without EMA in Table 5.
                  we integrated the DL3DV dataset into the existing exper-                      Impact of number of source frames. Our model scales
                  imental setup. This modification resulted in a significant                    with the number of input views and results in better re-
                  improvement in photorealistic reconstruction, as evidenced                    construction quality when more input views are fed to the
                  byasubstantialincreaseinPSNRontheACIDdataset. Fur-                            model to the model as presented in Table 6.
                  thermore, the relative performance gains observed with our                    6. Conclusion
                  model,comparedtoLVSM,wereconsiderablygreater,sug-
                  gesting a reduced susceptibility to noise.                                    In this paper, we introduce a new approach to scaling up
                  5.5. Ablation Studies                                                         NVS by addressing two key limitations in existing mod-
                                                                                                els:  efficiency and diversity.        To enhance the efficiency
                  Weanalyzetheimpactofvarious design choices in the net-                        of transformer-based NVS models, we propose the Token-
                  work. Specifically, we examinethreeaspects: (1) Theeffect                     Disentangled (Tok-D) Transformer, which reduces redun-
                  of modulation in different parts of the network, (2) The role                 dancies and improves data efficiency, enabling higher re-
                  of EMAinperformance, (3) Number of input views.                               construction quality with less compute. Additionally, the
                  Impact of modulation at different locations of Tok-D                          Tok-D Transformer mitigates training artifacts through its
                  transformer. We examine the effect of modulating differ-                      disentangling property, allowing for effective scaling us-
                  ent parts of the network. For this, we consider four differ-                  ing synthetic data. Incorporating synthetic data into train-
                  ent cases. We present the corresponding results in Table 4.                   ing significantly improves cross-dataset performance com-
                  Havingacommonmodulationpremodulationworkedbetter                              pared to existing models. By integrating the Tok-D Trans-
                  than separate premodulation for both layers.                                  former and synthetic data, we achieve state-of-the-art re-
                  Impact of EMA. We also observe that performing Expo-                          sults across three large-scale NVS benchmarks, surpassing
                  nential moving average (EMA) [12] during training results                     previous methods with lower computational cost and by a
                  in a performance boost for the base model. For the sake of                    substantial margin.
                                                                                         28574
                References                                                          [13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
                                                                 ¬®                        QifengChen. Latentvideodiffusionmodelsforhigh-fidelity
                 [1] BenjaminAttal,Jia-BinHuang,MichaelZollhofer,Johannes                 long video generation.  arXiv preprint arXiv:2211.13221,
                     Kopf, and Changil Kim. Learning neural light fields with             2022. 2
                     ray-spaceembedding. InProceedingsoftheIEEE/CVFCon-             [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
                     ference on Computer Vision and Pattern Recognition, pages            fusion probabilistic models. Advances in neural information
                     19819‚Äì19829, 2022. 2                                                 processing systems, 33:6840‚Äì6851, 2020. 2
                 [2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P        [15] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,
                     Srinivasan, and Peter Hedman.      Zip-nerf:  Anti-aliased           DifanLiu,FengLiu,KalyanSunkavalli,TrungBui,andHao
                     grid-based neural radiance fields.  In Proceedings of the            Tan. LRM: Large reconstruction model for single image to
                     IEEE/CVF International Conference on Computer Vision,                3d. In The Twelfth International Conference on Learning
                     pages 19697‚Äì19705, 2023. 2                                           Representations, 2024. 2, 3
                 [3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.      [16] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and
                     Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-           ShenghuaGao. 2dgaussiansplattingforgeometricallyaccu-
                     based neural radiance fields. ICCV, 2023. 3                          rate radiance fields. In Special Interest Group on Computer
                 [4] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and            GraphicsandInteractiveTechniquesConferenceConference
                     Vincent Sitzmann. pixelsplat: 3d gaussian splats from image          Papers, page 1‚Äì11. ACM, 2024. 3
                     pairs for scalable generalizable 3d reconstruction. In Pro-    [17] Hanwen Jiang, Zexiang Xu, Desai Xie, Ziwen Chen, Haian
                     ceedings of the IEEE/CVF conference on computer vision               Jin, Fujun Luan, Zhixin Shu, Kai Zhang, Sai Bi, Xin Sun, Ji-
                     and pattern recognition, pages 19457‚Äì19467, 2024. 2, 3, 6            uxiang Gu, Qixing Huang, Georgios Pavlakos, and Hao Tan.
                 [5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and               Megasynth: Scaling up 3d scene reconstruction with syn-
                     Hao Su. Tensorf: Tensorial radiance fields. In European              thesized data. In Proceedings of the IEEE/CVF Conference
                     Conference on Computer Vision (ECCV), 2022. 2, 3                     onComputerVisionandPatternRecognition (CVPR), pages
                 [6] YuedongChen,HaofeiXu,ChuanxiaZheng,BohanZhuang,                      16441‚Äì16452, 2025. 3
                     Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei      [18] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi,
                     Cai. Mvsplat: Efficient 3d gaussian splatting from sparse            Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang
                     multi-view images. In European Conference on Computer                Xu. LVSM: A large view synthesis model with minimal 3d
                     Vision, pages 370‚Äì386. Springer, 2024. 6, 7                          inductive bias. In The Thirteenth International Conference
                 [7] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,             onLearning Representations, 2025. 2, 3, 6, 7, 8
                     Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana                                                                      ¬®
                                                                                    [19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler,
                     Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:              and George Drettakis. 3d gaussian splatting for real-time
                     A universe of annotated 3d objects.     In Proceedings of            radiance field rendering. ACM Trans. Graph., 42(4):139‚Äì1,
                     the IEEE/CVF Conference on Computer Vision and Pattern               2023. 2, 3
                     Recognition, pages 13142‚Äì13153, 2023. 5                        [20] Shakiba Kheradmand, Delio Vicini, George Kopanas,
                 [8] John Flynn, Michael Broxton, Lukas Murmann, Lucy Chai,               Dmitry Lagun, Kwang Moo Yi, Mark Matthews, and An-
                                         ¬¥
                     Matthew DuVall, Clement Godard, Kathryn Heal, Srinivas               drea Tagliasacchi. Stochasticsplats: Stochastic rasterization
                     Kaza,StephenLombardi,XuanLuo,etal.Quark: Real-time,                  for sorting-free 3d gaussian splatting, 2025. 3
                     high-resolution, and general neural view synthesis.  ACM       [21] DiederikPKingma,MaxWelling,etal. Auto-encodingvari-
                     Transactions on Graphics (TOG), 43(6):1‚Äì20, 2024. 3                  ational bayes, 2013. 3
                 [9] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong          [22] Marc Levoy and Pat Hanrahan. Light field rendering. In
                     Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:                Proceedings of the 23rd Annual Conference on Computer
                     Radiance fields without neural networks. In Proceedings of           Graphics and Interactive Techniques, page 31‚Äì42, New
                     the IEEE/CVF conference on computer vision and pattern               York, NY, USA, 1996. Association for Computing Machin-
                     recognition, pages 5501‚Äì5510, 2022. 3                                ery. 2
                [10] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur        [23] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin,
                     Brussee, RicardoMartinBrualla,PratulSrinivasan,Jonathan              Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu,
                     Barron, and Ben Poole. Cat3d: Create anything in 3d with             et al.  Dl3dv-10k: A large-scale scene dataset for deep
                     multi-view diffusion models. Advances in Neural Informa-             learning-based 3d vision. In Proceedings of the IEEE/CVF
                     tion Processing Systems, 37:75468‚Äì75494, 2024. 2, 4                  Conference on Computer Vision and Pattern Recognition,
                [11] Steven J. Gortler, Radek Grzeszczuk, Richard Szeliski, and           pages 22160‚Äì22169, 2024. 2, 5, 6, 7, 8
                     Michael F. Cohen. The lumigraph. In Proceedings of the         [24] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh
                     23rd Annual Conference on Computer Graphics and Inter-               Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite na-
                     active Techniques, page 43‚Äì54, New York, NY, USA, 1996.              ture: Perpetual view generation of natural scenes from a sin-
                     Association for Computing Machinery. 2                               gle image. In Proceedings of the IEEE/CVF International
                [12] David Haynes, Steven Corns, and Ganesh Kumar Venayag-                Conference on Computer Vision, pages 14458‚Äì14467, 2021.
                     amoorthy. An exponential moving average algorithm. In                2, 5, 6, 7, 8
                     2012 IEEE Congress on Evolutionary Computation, pages          [25] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and
                     1‚Äì8. IEEE, 2012. 8                                                   Christian Theobalt. Neural sparse voxel fields. Advances
                                                                               28575
                        inNeuralInformationProcessingSystems,33:15651‚Äì15663,                     [37] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
                        2020. 2, 3                                                                      hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
                  [26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,                             generative modeling through stochastic differential equa-
                        Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:                          tions.  In International Conference on Learning Represen-
                        Representing scenes as neural radiance fields for view syn-                     tations, 2021. 2
                        thesis. In European conference on computer vision, 2020. 2,              [38] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and
                        3                                                                               AmeeshMakadia. Generalizable patch-based neural render-
                                     ¬®                                                                  ing.  In European Conference on Computer Vision, pages
                  [27] Thomas Muller, Alex Evans, Christoph Schied, and Alexan-
                        der Keller. Instant neural graphics primitives with a mul-                      156‚Äì174. Springer, 2022. 2, 3, 6
                        tiresolution hash encoding. ACM transactions on graphics                 [39] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
                        (TOG), 41(4):1‚Äì15, 2022. 2, 3                                                   Komura, and Wenping Wang. Neus: Learning neural im-
                  [28] Alexander Quinn Nichol and Prafulla Dhariwal. Improved                           plicit surfaces by volume rendering for multi-view recon-
                        denoising diffusion probabilistic models.        In International               struction. Advances in Neural Information Processing Sys-
                        conference on machine learning, pages 8162‚Äì8171. PMLR,                          tems, 34:27171‚Äì27183, 2021. 2
                        2021. 2                                                                  [40] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P
                  [29] William Peebles and Saining Xie. Scalable diffusion models                       Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo
                        with transformers. In Proceedings of the IEEE/CVF inter-                        Martin-Brualla, NoahSnavely,andThomasFunkhouser. Ibr-
                        national conference on computer vision, pages 4195‚Äì4205,                        net: Learning multi-view image-based rendering. In Pro-
                        2023. 4                                                                         ceedings of the IEEE/CVF conference on computer vision
                  [30] Lukas Radl, Michael Steiner, Mathias Parger, Alexan-                             and pattern recognition, pages 4690‚Äì4699, 2021. 3
                        der Weinrauch, Bernhard Kerbl, and Markus Steinberger.                   [41] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi
                        StopThePop: Sorted Gaussian Splatting for View-Consistent                       Zheng,JonathanTBarron,andAleksanderHolynski. Cat4d:
                        Real-time Rendering.       ACM Transactions on Graphics, 4                      Create anything in 4d with multi-view video diffusion mod-
                        (43), 2024. 3                                                                   els.  In Proceedings of the Computer Vision and Pattern
                  [31] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,                           Recognition Conference, pages 26057‚Äì26068, 2025. 3
                        Luca Sbordone, Patrick Labatut, and David Novotny. Com-                  [42] Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, Yi
                        mon objects in 3d: Large-scale learning and evaluation                          Zhou, Soren Pirk, Arie Kaufman, Xin Sun, and Hao Tan.
                        of real-life 3d category reconstruction.      In Proceedings of                 LRM-zero: Training large reconstruction models with syn-
                        the IEEE/CVF international conference on computer vision,                       thesized data. In The Thirty-eighth Annual Conference on
                        pages 10901‚Äì10911, 2021. 5                                                      Neural Information Processing Systems, 2024. 3
                  [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,                         [43] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann
                                                 ¬®                                                      Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys.
                        Patrick Esser, and Bjorn Ommer.          High-resolution image                  Depthsplat: Connectinggaussiansplattinganddepth. InPro-
                        synthesis with latent diffusion models. In Proceedings of                       ceedings of the IEEE/CVF Conference on Computer Vision
                        the IEEE/CVF conference on computer vision and pattern                          andPatternRecognition(CVPR),pages16453‚Äì16463,2025.
                        recognition, pages 10684‚Äì10695, 2022. 2                                         2, 6, 7
                  [33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala                       [44] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-
                        Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,                              umerendering of neural implicit surfaces, 2021. 2
                        RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,                        [45] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu,
                        et al. Photorealistic text-to-image diffusion models with deep                  Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu,
                        language understanding.       Advances in neural information                    Zhangyang Xiong, Tianyou Liang, et al.           Mvimgnet: A
                        processing systems, 35:36479‚Äì36494, 2022. 2                                     large-scale dataset of multi-view images. In Proceedings of
                  [34] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs                                the IEEE/CVF conference on computer vision and pattern
                        Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario                          recognition, pages 9150‚Äì9161, 2023. 5
                            Àá ¬¥
                        Lucic, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene                [46] KaiZhang,SaiBi,HaoTan,YuanboXiangli,NanxuanZhao,
                        representation transformer: Geometry-free novel view syn-                       Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large recon-
                        thesis through set-latent scene representations. In Proceed-                    struction model for 3d gaussian splatting. European Confer-
                        ings of the IEEE/CVF Conference on Computer Vision and                          ence on Computer Vision, 2024. 3
                        Pattern Recognition, pages 6229‚Äì6238, 2022. 3
                  [35] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,                  [47] Chaoyi Zhou, Xi Liu, Feng Luo, and Siyu Huang. Latent
                        andXiaoYang. MVDream: Multi-viewdiffusionfor3dgen-                              radiance fields with 3d-aware 2d representations.         In The
                        eration. In The Twelfth International Conference on Learn-                      Thirteenth International Conference on Learning Represen-
                        ing Representations, 2024. 2                                                    tations, 2025. 3
                  [36] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh                     [48] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
                        Tenenbaum, and Fredo Durand. Light field networks: Neu-                         and Noah Snavely. Stereo magnification: learning view syn-
                        ral scene representations with single-evaluation rendering.                     thesis using multiplane images. ACM Trans. Graph., 37(4),
                        Advances in Neural Information Processing Systems, 34:                          2018. 2, 5, 6, 7, 8
                        19313‚Äì19325, 2021. 2
                                                                                           28576
