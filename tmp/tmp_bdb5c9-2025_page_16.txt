                         Preprint.
                         for both similarity-based retrieval and continual adaptation He et al. (2024a). MemGPT introduced
                         a hierarchical memory system with a working (RAM-like) memory and a larger archival memory,
                         simulating long-term information management in a more structured fashion (Packer et al., 2024).
                         Parameter-Free Test-time Learning.  Test-time learning (sometimes referred to as online learning)
                         describes methods that update predictions at inference time Bottou & LeCun (2003; 2005). However,
                         trends towards (1) massive model sizes and (2) black box models accessed through APIs make
                         continually updating parameters computationally impractical or downright impossible. In light of
                         this, LLM test-time learning research sought to pursue parameter-free adaptation by using LLMs’
                         remarkable capacity for instruction following and self-reflection. Methods such as Reflexion (Shinn
                         et al., 2023), Self-Refine (Madaan et al., 2023), and Self-RAG (Asai et al., 2023), inter alia attempt
                         to correct mistakes at test-time by way of self-reflection. Follow-up work (Huang et al., 2024; Kamoi
                         et al., 2024) finds that self-correction methods depend on the availability of some extrinsic feedback or
                         verification mechanism to reliably improve end performance and that these methods produce dubious
                         results absent these grounded sources of feedback. Still, self-reflection remains a key component of
                         self-evolving agents (Gao et al., 2025).
                         MemoryforAgenticSkillAcquisition.       Memoryalsoplaysacrucialrole in LLM-based agents.
                         Park et al. (2023) structured memory as a stream of episodic observations and distilled high-level
                         summaries from them to guide decision-making. Contextual Experience Replay discretizes trajecto-
                         ries into “experiences” (containing environment dynamics from a trajectory and skills storing actions
                         from said trajectory) and retrieves relevant blocks to guide new episodes (Liu et al., 2025). HiAgent
                         organizes working memory (within a single query) hierarchically by subgoals, summarizing and
                         replacing traces to improve long-horizon efficiency (Hu et al., 2024). External-memory systems
                         such as Mem0 and MemP manage a persistent store with explicit add–update–prune operations
                         (MemPcastscontrol as an MDP), yielding gains on dialogue and tool-use/planning tasks (Chhikara
                         et al., 2025; Fang et al., 2025). MemP in particular is a concurrent work that also demonstrates the
                         efficacy of abstraction, although it targets rewriting action sequences as step sequences in explicitly
                         agentic settings, contrasting with ArcMemo’s general problem-solving focus. Voyager uses LLMs to
                         autonomously acquire and store skills while exploring Minecraft, with a growing library of reusable
                         code snippets written by the agent itself Wang et al. (2023a). Other systems, like the ResearchAgent,
                         use memory initialized from domain-specific corpora, such as scientific papers, to ground query
                         responses in high-utility prior knowledge (Baek et al., 2024).
                                                                     16
