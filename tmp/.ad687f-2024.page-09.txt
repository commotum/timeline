                  7 Conclusion                                            References
                 We introduce RESONANCE ROPE, a novel en-                 Chenxin An, Shansan Gong, Ming Zhong, Mukai Li,
                  hancement of RoPE that focuses on minimizing               Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023.
                  the interpolation of RoPE features for OOD posi-           L-eval: Instituting standardized evaluation for long
                                                                             context language models. CoRR, abs/2307.11088.
                  tions, thereby reducing the generalization gap and
                  improving LLM’s performance on train-short-test-        Zhangir Azerbayev. 2022. zhangir-azerbayev/proof-
                  long (TSTL) scenarios. Additionally, we present            pile.
                  a novel synthetic benchmark, POSGEN, which              Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
                  provides a fine-grained analysis of the model’s            Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
                 TSTL performance regarding various token de-                Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
                  pendency patterns. Extensive experiments on our            and Juanzi Li. 2023. Longbench: A bilingual, mul-
                  proposed POSGEN and two LLM-based evalua-                  titask benchmark for long context understanding.
                                                                             CoRR,abs/2308.14508.
                  tions demonstrate RESONANCE ROPE’s efficacy
                  in identifying OOD positions and its compatibil-        bloc97.2023. NTK-AwareScaledRoPEallowsLLaMA
                  ity with current RoPE scaling strategies. Future           models to have extended (8k+) context size without
                 workincludes exploring RESONANCE ROPE’s per-                any fine-tuning and minimal perplexity degradation.
                  formance on other foundational models, and the          ShouyuanChen,ShermanWong,LiangjianChen,and
                  identification of more optimal wavelength combi-           YuandongTian. 2023. Extending context window of
                  nations for RoPE features.                                 large language models via positional interpolation.
                                                                             CoRR,abs/2306.15595.
                  Limitations                                             Tri Dao. 2023. Flashattention-2: Faster attention with
                  Ourproposed RESONANCE ROPE focusonreduc-                   better parallelism and work partitioning.  CoRR,
                                                                             abs/2307.08691.
                  ing the interpolation of only RoPE’s pre-critical
                  dimensions on OOD positions. However, this              LuyangHuang,ShuyangCao,NikolausNovaParulian,
                  method does not solve the extrapolation issue on           HengJi, and Lu Wang. 2021. Efficient attentions for
                  RoPE’s post-critical dimensions, which has been            long document summarization. In Proceedings of
                                                                             the 2021 Conference of the North American Chap-
                  shown to be also detrimental to LLM’s length               ter of the Association for Computational Linguistics:
                  extrapolation performance. Thus, the technique             Human Language Technologies, pages 1419–1436.
                  of RESONANCE ROPE needstobecombinedwith                    Association for Computational Linguistics.
                  another RoPE scaling method that can reduce ex-         Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
                  trapolation on RoPE’s post-critical dimensions,            sch, Chris Bamford, Devendra Singh Chaplot, Diego
                  e.g., YaRN, to achieve the full potential of LLM in        de Las Casas, Florian Bressand, Gianna Lengyel,
                 TSTLscenarios. Such combination has been our                Guillaume Lample, Lucile Saulnier, Lélio Re-
                  focus in Section 6.2.                                      nard Lavaud, Marie-Anne Lachaux, Pierre Stock,
                                                                             Teven Le Scao, Thibaut Lavril, Thomas Wang, Timo-
                    Secondly, applying LLMs to long text sequences           thée Lacroix, and William El Sayed. 2023. Mistral
                  requires considerations of both performance and            7b. CoRR, abs/2310.06825.
                  efficiency due to the super-linear complexity of        Albert Q. Jiang, Alexandre Sablayrolles, Antoine
                 Transformers w.r.t input length. As an improve-             Roux, Arthur Mensch, Blanche Savary, Chris Bam-
                  ment of the position embeddings, we focus only             ford, Devendra Singh Chaplot, Diego de Las Casas,
                  on improving Transformers’ performance in TSTL             Emma Bou Hanna, Florian Bressand, Gianna
                  scenarios. An interesting future direction would           Lengyel, Guillaume Bour, Guillaume Lample,
                  be to apply RESONANCE ROPE to efficient Trans-             Lélio Renard Lavaud, Lucile Saulnier, Marie-
                                                                             AnneLachaux,Pierre Stock, Sandeep Subramanian,
                  formers for both performance and efficiency en-            Sophia Yang, Szymon Antoniak, Teven Le Scao,
                  hancements.                                                Théophile Gervet, Thibaut Lavril, Thomas Wang,
                    Lastly, benchmarkingLLMsisstillanopenques-               TimothéeLacroix, and William El Sayed. 2024. Mix-
                  tion, as there is currently no benchmark to thor-          tral of experts. CoRR, abs/2401.04088.
                  oughly test the performance of LLMs, especially         Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan
                  on long-sequence tasks. We expect that a more              Natesan Ramamurthy, Payel Das, and Siva Reddy.
                  comprehensive long-text benchmark would further            2023. The impact of positional encoding on length
                  improve the validity of the experiment results.            generalization in transformers. In Advances in Neu-
                                                                             ral Information Processing Systems, volume 36,
                                                                             pages 24892–24928.
                                                                      594
