                          Preprint.
                          Figure 1:   Instance-Level vs. Abstract Concepts Example. Each ARC-AGI Chollet (2019)
                          puzzle requires inferring the transformation rule for a set of input/output pixel grids. Here, Puzzle
                          1 instantiates (A ∧ B) ⇒ C and Puzzle 2 instantiates D ⇒ E. The target puzzle is solved by
                          recombining these ideas (B ⇒ E,D ⇒ C). Instance-level memory tends to store fully composed
                          rules, coupling A with B,C, and so on. Transferring to the target then demands both ignoring A and
                          disentangling/reordering B,C with D,E. Abstract memory instead stores A,B,C,D,E as separate,
                          modular concepts, making them easier to recognize and reassemble in new contexts.
                          Weintroduce an abstract concept-level memory framework to support compositional reasoning for
                          all subsequent queries. We emphasize (1) abstract concepts that are more general and separated
                          from their original context to be useful across a larger set of future problems, and (2) modular
                          concepts that directly promote recombination with other ideas that can be easily built on with new
                          experiences or curated for a target problem. We distill solution traces (both from the system itself or
                          from external sources) into reusable, modular abstractions stored in natural language. At inference
                          time, relevant concepts are selected and integrated into context, enabling test-time continual learning
                          without expensive weight updates. Our design encapsulates two primary operations: (i) writing to
                          memory: formatting concepts for abstraction and generality; and (ii) reading from memory: selecting
                          a subset of concepts for the current problem. We title our method ArcMemo— Abstract Reasoning
                          Composition Memory.
                          Asillustrated in Figure 1, instance-level memories often capture an entire solution pattern tied closely
                          to its original problem (e.g., the joint use of ideas A,B,C in Seen Puzzle 1). Such overspecified
                          entries are less likely to recur in future problems, and even when partially relevant to the Target
                          Puzzle, the agent must still disentangle useful pieces like B from the original bundle. In contrast,
                          abstract concepts are stored individually with fewer contextual assumptions, making them easier to
                          recognize, adapt, and recombine across superficially different puzzles.
                          Weevaluate on ARC-AGI, where simple pixel-grid operations compose into a vast task space, and
                          solving tasks requires compositional reasoning rather than memorizing individual patterns, making it
                          a natural testbed for ArcMemo. On ARC-AGI-1, our method improves the official score from 55.17
                          to 59.33 (+7.5% relative gain over a strong no-memory baseline) and is the only memory design
                          wefind to outperform the baseline at all inference scales. Our experiments confirm that continually
                          updating memory at evaluation time yields a score improvement that emerges with inference scale
                          via retries: memory updates triggered from a previous pass over the test set may enable new solves in
                          a subsequent pass. Finally, we observe that selecting a subset of memories to include for a particular
                          problem improves performance and reduces token cost, indicating the selection operation is essential
                          beyond allowing memory to grow continually.
                          2    RELATED WORK
                          Ourapproach draws on several threads of research in augmenting LLMs with memory but contrasts
                          in (1) target applications (reasoning-intensive vs. knowledge-intensive tasks), (2) the underlying
                                                                        2
