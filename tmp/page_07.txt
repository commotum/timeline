                                             Setting                                                         AIME24                 AMC23 MATH500 Olympiad College                                                                   Minerva                Avg.
                                                                                                                                                                                         Bench                    Math                  Math
                                             Pass@1                                                              11.2                   47.8                     73.0                      38.0                    38.6                  37.2               41.0
                                             Maj@8                                                               20.0                   57.5                     79.6                      47.0                    41.5                  42.7               48.0
                                             Pass@8(Oracle)                                                      33.3                   82.5                     88.8                      58.5                    47.5                  57.7               61.4
                                             Math-Shepherd-7B                                                    16.7                   42.5                     76.0                      42.0                    37.0                  39.3               42.3
                                             Math-PSA-7B                                                         20.0                   55.0                     80.8                      47.5                    39.5                  40.1               47.2
                                             RLHFlow-Mistral-8B                                                  10.0                   55.0                     76.8                      42.0                    39.5                  37.1               43.4
                                             RLHFlow-DeepSeek-8B                                                 13.3                   57.5                     76.2                      40.0                    39.0                  39.7               44.3
                                             Llemma-PRM800K-7B                                                   10.0                   52.5                     76.6                      42.5                    39.0                  42.7               43.9
                                             Skywork-PRM-7B                                                      16.7                   55.0                     81.2                      44.0                    40.5                  44.5               47.0
                                             ReasonEval-7B                                                        6.7                   55.0                     75.2                      41.0                    40.0                  40.4               43.1
                                             Qwen2.5-Math-7B-PRM800K                                             13.3                   57.5                     80.0                      44.5                    43.5                  43.0               47.7
                                             ⋆R-PRM-7B-DPO                                                       20.0                   62.5                     82.2                      48.0                    41.0                  44.1               49.6
                                     Table 4: Performance comparison on the Best-of-8 strategy of the policy model Qwen2.5-7B-Instruct, where
                                     superior performance indicates a more accurate reward from the PRM and consequently, more effective guidance.
                                             85
                                                                82.0                                                             R-PRM-SFT                             70                                                                                 R­PRM­DPO
                                                             81.9                                                                R-PRM-DPO
                                             80          80.7                      80.1                                          R-PRM-DPO-Iter1
                                                                               78.8                                              R-PRM-DPO-Iter2                                                                     R­PRM­SFT
                                                     77.2                  76.9
                                             75                                                                                                                        60                                                                           LLaMA3.3­70B­Instruct
                                                                                                                                          74.1
                                                                                                                                      72.6
                                                                        71.6                                                                                                                            Qwen2.5­PRM800K
                                            e70                                                      70.1                          70.4
                                                                                                  67.2                                                                 50
                                             65                                                                                65.2
                                            F1 Scor                                           63.8                      64.1                                                                                                                        Math­Shepherd
                                                                                                                    62.6                                            F1 Score
                                                                                                                                                                                                                                                    RLHFlow­DeepSeek
                                             60                                           59.6                  60.1                                                   40                                                                           RLHFlow­Mistral
                                                                                                                                                                                                                                                    Llemma­PRM800K
                                             55                                                                                                                                                                                                     Qwen2.5­PRM800K
                                                                                                             52.3                                                      30                                                                           ReasonEval
                                             50                                                                                                                                                                                                     R­PRM
                                                         GSM8K              MATH          OlympiadBench        OmniMATH           Average
                                                                                           Datasets                                                                       0            100           200           300           400          500           600           700
                                                                                                                                                                                                        Training Data Amount (K)
                                     Figure 2: Performance of R-PRM on ProcessBench
                                     across self-improving iterations.                                                                                         Figure 3: Average F1 score on ProcessBench with dif-
                                                                                                                                                               ferent training data scales.
                                     of policy model.
                                                                                                                                                               erative DPO progressively enhances the model’s
                                     5 Analysis                                                                                                                reasoning capabilities for tackling more challeng-
                                     In this section, we further delve into our R-PRM’s                                                                        ing evaluation tasks.
                                     core strengths: impressive self-evolution, remark-                                                                        5.2          Effective Data Scaling
                                     able data efficiency, and efficient inference-time
                                     scaling, culminating in a case study that demon-                                                                          Figure 3 visualizes the F1 performance on Process-
                                     strates its practical efficacy.                                                                                           Bench versus the data scale. With 12.8k training
                                                                                                                                                               samples, our R-PRM already surpasses most open-
                                     5.1          Self-Improve Further Boosts Performance                                                                      source PRMs. Notably, with only 64k samples, R-
                                     Toinvestigate the potential for self-improvement,                                                                         PRMoutperformsQwen2.5-Math-7B-PRM800K
                                     weexplore an iterative refinement strategy. In the                                                                        (trained on 265k samples) by 3.6 points. Further
                                     first iteration, we employ the R-PRM-DPO model                                                                            scaling to the full 285k-sample dataset yields con-
                                     to sample trajectories from the PRM800K dataset,                                                                          tinued gains, reaching an F1 score of 65.2, clearly
                                     from which we construct a new preference dataset                                                                          demonstrating the strong data efficiency and scala-
                                     of 30k examples. Fine-tuning on this data for one                                                                         bility of our approach.
                                     epoch yields R-PRM-DPO-Iter1. As shown in Fig-                                                                                  Moreover, our proposed meta-optimization,
                                     ure 2, this iterative process leads to consistent per-                                                                    without requiring any additional labeled data, fur-
                                     formance gains across all benchmarks. Notably,                                                                            ther boosts performance to an impressive F1 score
                                     R-PRM-DPO-Iter1improvestheperformanceon                                                                                   of 70.4. Even more notably, R-PRM also exceeds
                                     OlympiadBench by 3.4 points to 67.2 and achieves                                                                          the Llama3.3-70B-Instruct model used for cold-
                                     an average improvement of 2.2 points on Process-                                                                          start data construction, demonstrating that our ap-
                                     Bench. A second iteration (R-PRM-DPO-Iter2)                                                                               proach is not merely a distillation of the teacher
                                     continues this upward trend, demonstrating that it-                                                                       modelbutameaningfuladvancementbeyondit.
                                                                                                                                                     13456
