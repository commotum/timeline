                                  4        H. Wang et al.
                                  attention and convolution. State-of-the-art results on video action recognition
                                  tasks [17] are also achieved in this way. On semantic segmentation, self-attention
                                  is developed as a context aggregation module that captures multi-scale con-
                                  text [39,26,98,95]. Eﬃcient attention methods are proposed to reduce its com-
                                  plexity [73,39,53]. Additionally, CNNs augmented with non-local means [9] are
                                  showntobemorerobusttoadversarialattacks[86].Besidesdiscriminative tasks,
                                  self-attention is also applied to generative modeling of images [91,8,32]. Recently,
                                  [65,37] show that self-attention layers alone could be stacked to form a fully
                                  attentional model by restricting the receptive ﬁeld of self-attention to a local
                                  square region. Encouraging results are shown on both image classiﬁcation and
                                  object detection. In this work, we follow this direction of research and propose
                                  a stand-alone self-attention model with large or global receptive ﬁeld, making
                                  self-attention models non-local again. Our models are evaluated on bottom-up
                                  panoptic segmentation and show signiﬁcant improvements.
                                  3    Method
                                  We begin by formally introducing our position-sensitive self-attention mecha-
                                  nism. Then, we discuss how it is applied to axial-attention and how we build
                                  stand-alone Axial-ResNet and Axial-DeepLab with axial-attention layers.
                                  3.1    Position-Sensitive Self-Attention
                                  Self-Attention: Self-attention mechanism is usually applied to vision models
                                  as an add-on to augment CNNs outputs [84,91,39]. Given an input feature map
                                  x∈Rh×w×din with height h, width w, and channels din, the output at position
                                  o = (i,j), yo ∈ Rdout, is computed by pooling over the projected input as:
                                                               y = Xsoftmax (qTk )v                                   (1)
                                                                o                 p  o  p   p
                                                                     p∈N
                                  where N is the whole location lattice, and queries qo = WQxo, keys ko = WKxo,
                                  values v = W x are all linear projections of the input x ∀o ∈ N. W ,W                 ∈
                                           o      V o                                             o             Q    K
                                    d ×d                 d   ×d
                                  R q    in and W ∈R out        in are all learnable matrices. The softmax denotes a
                                                   V                                                         p
                                  softmax function applied to all possible p = (a,b) positions, which in this case
                                  is also the whole 2D lattice.
                                      This mechanism pools values v globally based on aﬃnities xTWTW x ,
                                                                         p                                  o   Q K p
                                  allowing us to capture related but non-local context in the whole feature map,
                                  as opposed to convolution which only captures local relations.
                                                                                                             2 2
                                      However, self-attention is extremely expensive to compute (O(h w )) when
                                  the spatial dimension of the input is large, restricting its use to only high levels of
                                  a CNN (i.e., downsampled feature maps) or small images. Another drawback is
                                  that the global pooling does not exploit positional information, which is critical
                                  to capture spatial structures or shapes in vision tasks.
                                      These two issues are mitigated in [65] by adding local constraints and po-
                                  sitional encodings to self-attention. For each location o, a local m × m square
