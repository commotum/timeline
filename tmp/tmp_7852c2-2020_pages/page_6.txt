                                                                            6                  H. Wang et al.
                                                                                                              Ì†µ„åµ        Ì†µ„åµ√óÌ†µ„åµ√ó16                                                            Ì†µ„åµ√óÌ†µ„åµ√ó16              Ì†µ„åµ        Ì†µ„åµ√óÌ†µ„åµ√ó16
                                                                                                                Ì†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµ            Ì†µ„åµ√ó(Ì†µ„åµ√ó16)                           Ì†µ„åµ/                          Ì†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµÌ†µ„åµ                    Ì†µ„åµ√ó(Ì†µ„åµ√ó16)
                                                                                                                 Ì†µ„åµ√ó(Ì†µ„åµ√óÌ†µ„åµ)                                               Ì†µ„åµ√ó16√óÌ†µ„åµ Ì†µ„åµ√ó(Ì†µ„åµ√óÌ†µ„åµ)                            Ì†µ„åµ√ó(Ì†µ„åµ√óÌ†µ„åµ)
                                                                                                                                                                               Ì†µ„åµ*                                                                  Ì†µ„åµ+
                                                                                    Ì†µ„åµ√ó(Ì†µ„åµ√ó8)                                                                             Ì†µ„åµ√ó8√óÌ†µ„åµÌ†µ„åµ√ó(Ì†µ„åµ√ó8)                                                     Ì†µ„åµ√ó8√óÌ†µ„åµ
                                                                                      Ì†µ„åµ√óÌ†µ„åµ√ó8                                Ì†µ„åµ√ó(8√óÌ†µ„åµ)                                                    Ì†µ„åµ√óÌ†µ„åµ√ó8                               Ì†µ„åµ√ó(8√óÌ†µ„åµ)
                                                                                             Ì†µ„åµ : 1√ó1                        Ì†µ„åµ√óÌ†µ„åµ√ó8                                                            Ì†µ„åµ : 1√ó1                        Ì†µ„åµ√óÌ†µ„åµ√ó8
                                                                                               #                    Ì†µ„åµ : 1√ó1                        Ì†µ„åµ : 1√ó1                                       #                   Ì†µ„åµ : 1√ó1                        Ì†µ„åµ : 1√ó1
                                                                                                                      '                                (                                                                  '                               (
                                                                                                              Ì†µ„åµÌ†µ„åµ√óÌ†µ„åµ√ó128                                                                                         Ì†µ„åµÌ†µ„åµ√óÌ†µ„åµ√ó128
                                                                            Fig.1. A non-local block (left) vs. our position-sensitive axial-attention applied along
                                                                            the width-axis (right). ‚Äú‚äó‚Äù denotes matrix multiplication, and ‚Äú‚äï‚Äù denotes element-
                                                                            wise sum. The softmax is performed on the last axis. Blue boxes denote 1 √ó 1 convo-
                                                                            lutions, and red boxes denote relative positional encoding. The channels d                                                                                                         = 128,
                                                                                                                                                                                                                                                         in
                                                                            d =8, and d                            =16is what we use in the Ô¨Årst stage of ResNet after ‚Äòstem‚Äô
                                                                               q                           out
                                                                            3.2            Axial-Attention
                                                                            Thelocal constraint, proposed by the stand-alone self-attention models [65], sig-
                                                                            niÔ¨Åcantly reduces the computational costs in vision tasks and enables building
                                                                            fully self-attentional model. However, such constraint sacriÔ¨Åces the global con-
                                                                            nection, making attention‚Äôs receptive Ô¨Åeld no larger than a depthwise convolution
                                                                            with the same kernel size. Additionally, the local self-attention, performed in lo-
                                                                            cal square regions, still has complexity quadratic to the region length, introduc-
                                                                            ing another hyper-parameter to trade-oÔ¨Ä between performance and computation
                                                                            complexity. In this work, we propose to adopt axial-attention [39,32] in stand-
                                                                            alone self-attention, ensuring both global connection and eÔ¨Écient computation.
                                                                            SpeciÔ¨Åcally, we Ô¨Årst deÔ¨Åne an axial-attention layer on the width-axis of an image
                                                                            as simply a one dimensional position-sensitive self-attention, and use the similar
                                                                            deÔ¨Ånition for the height-axis. To be concrete, the axial-attention layer along the
                                                                            width-axis is deÔ¨Åned as follows.
                                                                                                  y =                  X softmax (qTk +qTrq                                                         +kTrk )(v +rv )                                                    (4)
                                                                                                     o                                                     p      o     p           o p‚àío                    p p‚àío                p           p‚àío
                                                                                                               p‚ààN1√óm(o)
                                                                            One axial-attention layer propagates information along one particular axis. To
                                                                            capture global information, we employ two axial-attention layers consecutively
                                                                            for the height-axis and width-axis, respectively. Both of the axial-attention layers
                                                                            adopt the multi-head attention mechanism, as described above.
                                                                                     Axial-attention reduces the complexity to O(hwm). This enables global re-
                                                                            ceptive Ô¨Åeld, which is achieved by setting the span m directly to the whole
                                                                            input features. Optionally, one could also use a Ô¨Åxed m value, in order to reduce
                                                                            memory footprint on huge feature maps.
