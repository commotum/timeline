                                                                                                                                                                            Axial-DeepLab                          7
                                                                                  Conv                                   Concat        Ì†µ„åµ                    Ì†µ„åµ    Concat     Ì†µ„åµ     Conv
                                                                                   1√ó1                              (Ì†µ„åµ√óÌ†µ„åµ√ó16)√ó8                                                     1√ó1
                                                                              Ì†µ„åµ√óÌ†µ„åµ√ó128                                Ì†µ„åµ√óÌ†µ„åµ√ó128                               (Ì†µ„åµ√óÌ†µ„åµ√ó16)√ó8              Ì†µ„åµ√óÌ†µ„åµ√ó256
                                                                                              Multi-Head Attention                      Multi-Head Attention     Ì†µ„åµ√óÌ†µ„åµ√ó128
                                                                                                   Height-Axis                                Width-Axis
                                                                      Ì†µ„åµ√óÌ†µ„åµ√ó256                                                                                                                Ì†µ„åµ√óÌ†µ„åµ√ó256
                                                            Fig.2. An axial-attention block, which consists of two axial-attention layers operating
                                                            along height- and width-axis sequentially. The channels d                                                   =128, d               =16 is what
                                                                                                                                                                    in                  out
                                                            we use in the Ô¨Årst stage of ResNet after ‚Äòstem‚Äô. We employ N = 8 attention heads
                                                                   Axial-ResNet: To transform a ResNet [31] to an Axial-ResNet, we replace
                                                            the 3 √ó 3 convolution in the residual bottleneck block by two multi-head axial-
                                                            attention layers (one for height-axis and the other for width-axis). Optional
                                                            striding is performed on each axis after the corresponding axial-attention layer.
                                                            Thetwo1√ó1convolutionsarekepttoshuÔ¨Ñethefeatures. This forms our (resid-
                                                            ual) axial-attention block, as illustrated in Fig. 2, which is stacked multiple times
                                                            to obtain Axial-ResNets. Note that we do not use a 1√ó1 convolution in-between
                                                            the two axial-attention layers, since matrix multiplications (W ,W ,W ) fol-
                                                                                                                                                                                      Q        K         V
                                                            low immediately. Additionally, the stem (i.e., the Ô¨Årst strided 7√ó7 convolution
                                                            and 3√ó3 max-pooling) in the original ResNet is kept, resulting in a conv-stem
                                                            model where convolution is used in the Ô¨Årst layer and attention layers are used
                                                            everywhere else. In conv-stem models, we set the span m to the whole input from
                                                            the Ô¨Årst block, where the feature map is 56√ó56.
                                                                   In our experiments, we also build a full axial-attention model, called Full
                                                            Axial-ResNet, which further applies axial-attention to the stem. Instead of de-
                                                            signing a special spatially-varying attention stem [65], we simply stack three
                                                            axial-attention bottleneck blocks. In addition, we adopt local constraints (i.e., a
                                                            local m√ómsquareregion as in [65]) in the Ô¨Årst few blocks of Full Axial-ResNets,
                                                            in order to reduce computational cost.
                                                                   Axial-DeepLab:TofurtherconvertAxial-ResNettoAxial-DeepLabforseg-
                                                            mentation tasks, we make several changes as discussed below.
                                                                   Firstly, to extract dense feature maps, DeepLab [12] changes the stride and
                                                            atrous rates of the last one or two stages in ResNet [31]. Similarly, we remove the
                                                            stride of the last stage but we do not implement the ‚Äòatrous‚Äô attention module,
                                                            since our axial-attention already captures global information for the whole input.
                                                            In this work, we extract feature maps with output stride (i.e., the ratio of input
                                                            resolution to the Ô¨Ånal backbone feature resolution) 16. We do not pursue output
                                                            stride 8, since it is computationally expensive.
                                                                   Secondly, we do not adopt the atrous spatial pyramid pooling module (ASPP)
                                                            [13,14], since our axial-attention block could also eÔ¨Éciently encode the multi-
                                                            scale or global information. We show in the experiments that our Axial-DeepLab
                                                            without ASPP outperforms Panoptic-DeepLab [19] with and without ASPP.
