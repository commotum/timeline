                                                                                    Axial-DeepLab      3
                               – Our Axial-DeepLab improves signiﬁcantly over bottom-up state-of-the-art
                                 onCOCO,achievingcomparableperformanceoftwo-stagemethods.Wealso
                                 surpasspreviousstate-of-the-artmethodsonMapillaryVistasandCityscapes.
                              2   Related Work
                              Top-down panoptic segmentation: Most state-of-the-art panoptic segmen-
                              tation models employ a two-stage approach where object proposals are ﬁrstly
                              generatedfollowedbysequentialprocessingofeachproposal.Werefertosuchap-
                              proachesastop-downorproposal-basedmethods.MaskR-CNN[30]iscommonly
                              deployed in the pipeline for instance segmentation, paired with a light-weight
                              stuﬀ segmentation branch. For example, Panoptic FPN [44] incorporates a se-
                              mantic segmentation head to Mask R-CNN [30], while Porzi et al. [68] append a
                              light-weight DeepLab-inspired module [13] to the multi-scale features from FPN
                              [55]. Additionally, some extra modules are designed to resolve the overlapping
                              instance predictions by Mask R-CNN. TASCNet [49] and AUNet [52] propose
                              a module to guide the fusion between ‘thing’ and ‘stuﬀ’ predictions, while Liu
                              et al. [61] adopt a Spatial Ranking module. UPSNet [87] develops an eﬃcient
                              parameter-free panoptic head for fusing ‘thing’ and ‘stuﬀ’, which is further ex-
                              plored by Li et al. [50] for end-to-end training of panoptic segmentation models.
                              AdaptIS [77] uses point proposals to generate instance masks.
                                 Bottom-uppanoptic segmentation:Incontrasttotop-downapproaches,
                              bottom-up or proposal-free methods for panoptic segmentation typically start
                              with the semantic segmentation prediction followed by grouping ‘thing’ pixels
                              into clusters to obtain instance segmentation. DeeperLab [89] predicts bound-
                              ing box four corners and object centers for class-agnostic instance segmentation.
                              SSAP [28] exploits the pixel-pair aﬃnity pyramid [60] enabled by an eﬃcient
                              graph partition method [43]. BBFNet [7] obtains instance segmentation results
                              by Watershed transform [81,4] and Hough-voting [5,48]. Recently, Panoptic-
                              DeepLab [19], a simple, fast, and strong approach for bottom-up panoptic seg-
                              mentation, employs a class-agnostic instance segmentation branch involving a
                              simple instance center regression [42,79,63], coupled with DeepLab semantic
                              segmentation outputs [12,14,15]. Panoptic-DeepLab has achieved state-of-the-
                              art results on several benchmarks, and our method builds on top of it.
                                 Self-attention: Attention, introduced by [3] for the encoder-decoder in a
                              neural sequence-to-sequence model, is developed to capture correspondence of
                              tokens between two sequences. In contrast, self-attention is deﬁned as applying
                              attention to a single context instead of across multiple modalities. Its ability
                              to directly encode long-range interactions and its parallelizability, has led to
                              state-of-the-art performance for various tasks [80,38,25,66,72,24,53]. Recently,
                              self-attention has been applied to computer vision, by augmenting CNNs with
                              non-local or long-range modules. Non-local neural networks [84] show that self-
                              attention is an instantiation of non-local means [9] and achieve gains on many
                              vision tasks such as video classiﬁcation and object detection. Additionally, [17,6]
                              show improvements on image classiﬁcation by combining features from self-
