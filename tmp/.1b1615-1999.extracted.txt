

===== Page 1 =====
                                  Artiﬁcial Intelligence 112 (1999) 181–211
                           BetweenMDPsandsemi-MDPs:
                         Aframeworkfortemporalabstraction
                               in reinforcement learning
                         Richard S. Suttona,∗, Doina Precupb, Satinder Singha
                           a AT&TLabs.-Research, 180 Park Avenue, Florham Park, NJ 07932, USA
                       b Computer Science Department, University of Massachusetts, Amherst, MA 01003, USA
                                      Received 1 December 1998
                  Abstract
                   Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key,
                  longstanding challenges for AI. In this paper we consider how these challenges can be addressed
                  within the mathematical framework of reinforcement learning and Markov decision processes
                  (MDPs). We extend the usual notion of action in this framework to include options—closed-loop
                  policies for taking action over a period of time. Examples of options include picking up an object,
                  going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches
                  and joint torques. Overall, we show that options enable temporally abstract knowledge and action
                  to be included in the reinforcement learning framework in a natural and general way. In particular,
                  weshowthat options may be used interchangeably with primitive actions in planning methods such
                  as dynamic programming and in learning methods such as Q-learning. Formally, a set of options
                  deﬁnedoveranMDPconstitutesasemi-Markovdecisionprocess(SMDP),andthetheoryofSMDPs
                  provides the foundation for the theory of options. However, the most interesting issues concern the
                  interplay between the underlying MDPandtheSMDPandarethusbeyond SMDPtheory.Wepresent
                  results for three such cases: (1) we show that the results of planning with options can be used during
                  execution to interrupt options and thereby perform even better than planned, (2) we introduce new
                  intra-option methods that are able to learn about an option from fragments of its execution, and
                  (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these
                  results have precursors in the existing literature; the contribution of this paper is to establish them
                  in a simpler and more general setting with fewer changes to the existing reinforcement learning
                  framework. In particular, we show that these results can be obtained without committing to (or ruling
                  out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-
                  utility problem.  1999 Published by Elsevier Science B.V. All rights reserved.
                   ∗Corresponding author.
                  0004-3702/99/$ – see front matter  1999 Published by Elsevier Science B.V. All rights reserved.
                  PII:S0004-3702(99)00052-1


===== Page 2 =====
               182        R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
               Keywords: Temporal abstraction; Reinforcement learning; Markov decision processes; Options; Macros;
               Macroactions; Subgoals; Intra-option learning; Hierarchical planning; Semi-Markov decision processes
               0. Introduction
                Humandecision making routinely involves choice among temporally extended courses
               of action over a broad range of time scales. Consider a traveler deciding to undertake a
               journey to a distant city. To decide whether or not to go, the beneﬁts of the trip must be
               weighedagainsttheexpense.Havingdecidedtogo,choicesmustbemadeateachleg,e.g.,
               whether to ﬂy or to drive, whether to take a taxi or to arrange a ride. Each of these steps
               involves foresight and decision, all the way down to the smallest of actions. For example,
               just to call a taxi may involve ﬁnding a telephone, dialing each digit, and the individual
               muscle contractions to lift the receiver to the ear. How can we understand and automate
               this ability to work ﬂexibly with multiple overlapping time scales?
                Temporal abstraction has been explored in AI at least since the early 1970’s,
               primarily within the context of STRIPS-style planning [18,20,21,29,34,37,46,49,51,60,
               76]. Temporal abstraction has also been a focus and an appealing aspect of qualitative
               modeling approaches to AI [6,15,33,36,62]and has been explored in robotics and control
               engineering [1,7,9,25,39,61]. In this paper we consider temporal abstraction within the
               framework of reinforcement learning and Markov decision processes (MDPs). This
               framework has become popular in AI because of its ability to deal naturally with
               stochastic environments and with the integration of learning and planning [3,4,13,22,64].
               Reinforcement learning methods have also proven effective in a number of signiﬁcant
               applications [10,42,50,70,77].
                MDPsastheyareconventionallyconceiveddonotinvolvetemporalabstractionortem-
               porally extended action. They are based on a discrete time step: the unitary action taken
               at time t affects the state and reward at time t + 1. There is no notion of a course of
               action persisting over a variable period of time. As a consequence, conventional MDP
               methodsare unable to take advantage of the simplicities and efﬁciencies sometimes avail-
               able at higher levels of temporal abstraction. On the other hand, temporal abstraction can
               be introduced into reinforcement learning in a variety of ways [2,8,11,12,14,16,19,26,28,
               31,32,38,40,44,45,53,56,57,59,63,68,69,71,73,78–82]. In the present paper we generalize
               and simplify many of these previous and co-temporaneousworks to form a compact, uni-
               ﬁed framework for temporal abstraction in reinforcement learning and MDPs. We answer
               the question “What is the minimal extension of the reinforcementlearning framework that
               allows a general treatment of temporally abstract knowledge and action?” In the second
               part of the paper we use the new framework to develop new results and generalizations of
               previousresults.
                One of the keys to treating temporal abstraction as a minimal extension of the
               reinforcement learning framework is to build on the theory of semi-Markov decision
               processes (SMDPs), as pioneered by Bradtke and Duff [5], Mahadevan et al. [41], and
               Parr [52]. SMDPs are a special kind of MDP appropriate for modeling continuous-time
               discrete-event systems. The actions in SMDPs take variable amounts of time and are
               intended to model temporally-extended courses of action. The existing theory of SMDPs


===== Page 3 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       183
                                      speciﬁes how to model the results of these actions and how to plan with them. However,
                                      existing SMDP work is limited because the temporally extended actions are treated as
                                      indivisible and unknown units. There is no attempt in SMDP theory to look inside the
                                      temporally extended actions, to examine or modify their structure in terms of lower-level
                                      actions. As we have tried to suggest above, this is the essence of analyzing temporally
                                      abstract actions in AI applications: goal directed behavior involves multiple overlapping
                                      scales at which decisions are made and modiﬁed.
                                         In this paper we explore the interplay between MDPs and SMDPs. The base problem
                                      weconsideris that of a conventionaldiscrete-time MDP,1 but we also consider coursesof
                                      actionwithintheMDPwhoseresultsarestatetransitionsofextendedandvariableduration.
                                      Weusethetermoptions2 forthesecoursesofaction,which includeprimitiveactions as a
                                      special case. Any ﬁxed set of options deﬁnes a discrete-time SMDP embedded within the
                                      originalMDP,assuggestedbyFig.1.Thetoppanelshowsthestatetrajectoryoverdiscrete
                                      time of an MDP, the middle panel shows the larger state changes over continuous time of
                                      an SMDP, and the last panel shows how these two levels of analysis can be superimposed
                                      throughtheuseofoptions.InthiscasetheunderlyingbasesystemisanMDP,withregular,
                                      single-step transitions, while the options deﬁne potentially larger transitions, like those of
                                      an SMDP,thatmaylast for a number of discrete steps. All the usual SMDP theory applies
                                      to the superimposed SMDP deﬁned by the options but, in addition, we have an explicit
                                      interpretation of them in terms of the underlying MDP. The SMDP actions (the options)
                                      are no longer black boxes, but policies in the base MDP which can be examined, changed,
                                      learned, and planned in their own right.
                                         The ﬁrst part of this paper (Sections 1–3) develops these ideas formally and more
                                      fully. The ﬁrst two sections review the reinforcement learning framework and present its
                                      generalizationtotemporallyextendedaction.Section3focusesonthelinktoSMDPtheory
                                      and illustrates the speedups in planning and learning that are possible through the use
                                      of temporal abstraction. The rest of the paper concerns ways of going beyond an SMDP
                                      analysisofoptionstochangeorlearntheirinternalstructureintermsoftheMDP.Section4
                                      considers the problem of effectively combining a given set of options into a single overall
                                      policy. For example, a robot may have pre-designed controllers for servoing joints to
                                      positions, picking up objects, and visual search, but still face a difﬁcult problem of how
                                      to coordinate and switch between these behaviors [17,32,35,39,40,43,61,79]. Sections 5
                                      and6concernintra-optionlearning—lookinginsideoptionstolearnsimultaneouslyabout
                                      all options consistent with each fragment of experience. Finally, in Section 7 we illustrate
                                      a notion of subgoal that can be used to improve existing options and learn new ones.
                                        1 In fact, the base system could itself be an SMDP with only technical changes in our framework, but this would
                                      be a larger step away from the standard framework.
                                        2 This term may deserve some explanation. In previous work we have used other terms including “macro-
                                      actions”, “behaviors”, “abstract actions”, and “subcontrollers” for structures closely related to options. We
                                      introduce a new term to avoid confusion with previous formulations and with informal terms. The term “options”
                                      is meant as a generalization of “actions”, which we use formally only for primitive choices. It might at ﬁrst
                                      seem inappropriate that “option” does not connote a course of action that is non-primitive, but this is exactly our
                                      intention. We wish to treat primitive and temporally extended actions similarly, and thus we prefer one name for
                                      both.


===== Page 4 =====
                                 184                    R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                                 Fig. 1. The state trajectory of an MDP is made up of small, discrete-time transitions, whereas that of an SMDP
                                 comprises larger, continuous-time transitions. Options enable an MDP trajectory to be analyzed in either way.
                                 1. The reinforcementlearning (MDP) framework
                                    In this section we brieﬂy review the standard reinforcement learning framework of
                                 discrete-time, ﬁnite Markov decision processes,orMDPs, which forms the basis for our
                                 extension to temporally extended courses of action. In this framework, a learning agent
                                 interacts with an environment at some discrete, lowest-level time scale, t = 0,1,2,....On
                                 each time step, t, the agent perceives the state of the environment, s ∈ S, and on that
                                                                                                                t
                                 basis chooses a primitive action, a ∈ A . In response to each action, a , the environment
                                                                       t     st                                 t
                                 producesonestep later a numerical reward, r          , and a next state, s    . It is convenient to
                                                                                   t+1                      t+1
                                 suppress the differences in available actions across states whenever possible; we let A =
                                 S A denotetheunionoftheactionsets.IfS andA,areﬁnite,thentheenvironment’s
                                    s∈S   s
                                 transition dynamics can be modeled by one-step state-transition probabilities,
                                         a                 0                	
                                        p 0 =Pr s       =s s =s, a =a ,
                                         ss         t+1         t       t
                                 andone-stepexpectedrewards,
                                         a                           	
                                        r =E r        s =s, a =a ,
                                         s        t+1   t        t
                                 for all s,s0 ∈ S and a ∈ A . These two sets of quantities together constitute the one-step
                                                              s
                                 modeloftheenvironment.
                                    Theagent’s objective is to learn a Markov policy, a mapping from states to probabilities
                                 of taking each available primitive action, π :S ×A →[0,1], that maximizes the expected
                                 discounted future reward from each state s:
                                          π                             2                      	
                                        V (s)=E r          +γr +γ r +···s =s,π                                                  (1)
                                                     t+1       t+2       t+3       	 t
                                               =E r        +γVπ(s        )s =s,π
                                                      t+1           t+1    t            
                                               = Xπ(s,a) ra+γXpa0Vπ(s0) ,                                                        (2)
                                                                  s            ss
                                                  a∈A                     s0
                                                      s
                                 where π(s,a)is the probability with which the policy π chooses action a ∈ A in state s,
                                                                                                                        s
                                 andγ ∈[0,1]isadiscount-rateparameter.Thisquantity,Vπ(s),iscalledthevalueofstate


===== Page 5 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       185
                                      s under policy π,andVπ is called the state-value function for π.Theoptimal state-value
                                      function gives the value of each state under an optimal policy:
                                              V∗(s)=maxVπ(s)                                                                                         (3)
                                                           π                                                 	
                                                      =maxE r             +γV∗(s          )s =s, a =a
                                                         a∈A         t+1             t+1      t         t
                                                              s                             
                                                      =max ra+γXpa0V∗(s0) .                                                                          (4)
                                                         a∈A       s              ss
                                                              s             s0
                                      Anypolicythatachievesthemaximumin(3)isbydeﬁnitionanoptimalpolicy.Thus,given
                                      V∗,anoptimalpolicyiseasily formedbychoosingineachstate s any actionthat achieves
                                      the maximumin (4). Planning in reinforcementlearning refers to the use of models of the
                                      environmentto computevalue functionsand thereby to optimize or improvepolicies. Par-
                                      ticularly useful in this regard are Bellman equations, such as (2) and (4), which recursively
                                      relate value functions to themselves. If we treat the values, Vπ(s) or V∗(s), as unknowns,
                                      then a set of Bellman equations, for all s ∈ S, forms a system of equations whose unique
                                      solution is in fact Vπ or V∗ as given by (1) or (3). This fact is key to the way in which all
                                      temporal-differenceand dynamic programmingmethodsestimate value functions.
                                         There are similar value functions and Bellman equations for state-action pairs, rather
                                      than for states, which are particularly important for learning methods. The value of taking
                                                                                                   π(s,a), is the expected discounted future
                                      action a in state s under policy π, denoted Q
                                      reward starting in s,takinga, and henceforth following π:
                                                π                                      2                                   	
                                              Q (s,a)=E r              +γr +γ r                +···s =s,a =a,π
                                                                  t+1X t+2                t+3            t         t
                                                          =ra+γ            pa 0Vπ(s0)
                                                              s              ss
                                                                        s0
                                                               a      X a X 0 0 π 0 0
                                                          =r +γ            p 0        π(s,a)Q (s ,a).
                                                              s              ss
                                                                        s0        a0
                                      This is known as the action-valuefunction for policy π.Theoptimal action-valuefunction
                                      is
                                                ∗                    π
                                              Q (s,a)=maxQ (s,a)
                                                              π       X
                                                              a              a           ∗   0   0
                                                         =r +γ             p 0maxQ (s ,a).
                                                              s              ss   a0
                                                                       s0
                                         Finally, many tasks are episodic in nature, involving repeated trials, or episodes, each
                                      endingwith a reset to a standard state or state distribution. Episodic tasks include a special
                                      terminalstate; arrivingin this state terminates the current episode. The set of regular states
                                      plus the terminal state (if there is one) is denoted S+. Thus, the s0 in pa 0 in general ranges
                                                       +                                                                        ss
                                      overtheset S        rather than just S as stated earlier. In an episodic task, values are deﬁned by
                                      the expected cumulativerewardup until termination rather than over the inﬁnite future (or,
                                      equivalently, we can consider the terminal state to transition to itself forever with a reward
                                      of zero). There are also undiscounted average-reward formulations, but for simplicity we
                                      do not consider them here. For more details and background on reinforcement learning
                                      see [72].


===== Page 6 =====
                                  186                    R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                                  2. Options
                                     Asmentionedearlier,weusethetermoptionsforourgeneralizationofprimitiveactions
                                  to include temporally extended courses of action. Options consist of three components: a
                                  policy π :S × A →[0,1], a termination condition β:S+ →[0,1], and an initiation set
                                  I ⊆S. An option hI,π,βi is available in state s if and only if s ∈ I. If the option is
                                                                                           t                  t
                                  taken, then actions are selected according to π until the option terminates stochastically
                                  according to β. In particular, a Markov option executes as follows. First, the next action
                                  a is selected according to probability distribution π(s ,·). The environment then makes a
                                   t                                                           t
                                  transition to state s   , wherethe optioneither terminates, with probability β(s            ),orelse
                                                       t+1                                                                t+1
                                  continues,determininga          accordingtoπ(s        ,·), possibly terminatingin s       according
                                                          3 t+1                     t+1                                 t+2
                                  to β(s    ), and so on.    Whentheoptionterminates,theagenthastheopportunitytoselect
                                        t+2
                                  anotheroption.Forexample,anoptionnamedopen-the-doormightconsistofapolicy
                                  for reaching, grasping and turning the door knob, a termination condition for recognizing
                                  that the door has been opened, and an initiation set restricting consideration of open-
                                  the-doortostatesinwhichadoorispresent.Inepisodictasks,terminationofanepisode
                                  also terminates the current option (i.e., β maps the terminal state to 1 in all options).
                                     The initiation set and termination condition of an option together restrict its range of
                                  application in a potentially useful way. In particular, they limit the range over which the
                                  option’spolicyneedstobedeﬁned.Forexample,ahandcraftedpolicyπ foramobilerobot
                                  to dock with its battery charger might be deﬁned only for states I in which the battery
                                  charger is within sight. The termination condition β could be deﬁned to be 1 outside
                                  of I and when the robot is successfully docked. A subpolicy for servoing a robot arm
                                  to a particular joint conﬁguration could similarly have a set of allowed starting states, a
                                  controllerto be appliedto them,andaterminationconditionindicatingthateitherthetarget
                                  conﬁguration has been reached within some tolerance or that some unexpected event has
                                  taken the subpolicy outside its domain of application. For Markov options it is natural to
                                  assumethatallstateswhereanoptionmightcontinuearealsostateswheretheoptionmight
                                  be taken (i.e., that {s: β(s)<1}⊆I). In this case, π need only be deﬁned over I rather
                                  than over all of S.
                                     Sometimes it is useful for options to “timeout”, to terminate after some period of time
                                  has elapsed even if they have failed to reach any particular state. This is not possible
                                  with Markov options because their termination decisions are made solely on the basis
                                  of the current state, not on how long the option has been executing. To handle this and
                                  other cases of interest we allow semi-Markov options, in which policies and termination
                                  conditions may make their choices dependent on all prior events since the option was
                                  initiated. In general, an option is initiated at some time, say t, determines the actions
                                  selected for some numberof steps, say k, and then terminatesin s             . At each intermediate
                                                                                                           t+k
                                  time τ,t 6 τ<t+k, the decisions of a Markov option may depend only on s , whereas
                                                                                                                           τ
                                  the decisions of a semi-Markov option may depend on the entire preceding sequence
                                  s ,a,r      ,s    ,a    ,...,r ,s , but not on events prior to s (or after s ). We call this
                                   t  t   t+1   t+1   t+1        τ   τ                                  t             τ
                                  sequence the history from t to τ and denote it by h . We denote the set of all histories
                                                                                              tτ
                                    3 The termination condition β plays a role similar to the β in β-models [71], but with an opposite sense. That
                                  is, β(s)in this paper corresponds to 1 −β(s)in [71].


===== Page 7 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       187
                                      by Ω. In semi-Markov options, the policy and termination condition are functions of
                                      possible histories, that is, they are π :Ω × A →[0,1] and β:Ω →[0,1]. Semi-Markov
                                      options also arise if options use a more detailed state representation than is available
                                      to the policy that selects the options, as in hierarchical abstract machines [52,53] and
                                      MAXQ[16]. Finally, note that hierarchical structures, such as options that select other
                                      options, can also give rise to higher-level options that are semi-Markov (even if all the
                                      lower-level options are Markov). Semi-Markov options include a very general range of
                                      possibilities.
                                         Given a set of options, their initiation sets implicitly deﬁne a set of available options
                                      O foreach state s ∈S.TheseO are much like the sets of available actions, A . We can
                                         s                                       s                                                           s
                                      unify these two kinds of sets by noting that actions can be considered a special case of
                                      options. Each action a corresponds to an option that is available whenever a is available
                                      (I ={s: a ∈ A }), that always lasts exactly one step (β(s)= 1,∀s ∈ S), and that selects
                                                          s
                                      a everywhere(π(s,a)=1,∀s ∈I). Thus,we can considerthe agent’s choice at each time
                                      to be entirely among options, some of which persist for a single time step, others of which
                                      are temporally extended.The formerwe refer to as single-step or primitive options and the
                                      latter as multi-step options. Just as in the case of actions, it is convenient to suppress the
                                      differences in available options across states. We let O = S                        O denote the set of all
                                                                                                                     s∈S     s
                                      available options.
                                         Ourdeﬁnitionof options is crafted to make them as much like actions as possible while
                                      adding the possibility that they are temporally extended. Because options terminate in a
                                      welldeﬁnedway,wecanconsidersequencesoftheminmuchthesamewayasweconsider
                                      sequences of actions. We can also consider policies that select options instead of actions,
                                      and we can model the consequences of selecting an option much as we model the results
                                      of an action. Let us consider each of these in turn.
                                         Given any two options a and b, we can consider taking them in sequence, that is, we
                                      can consider ﬁrst taking a until it terminates, and then b until it terminates (or omitting b
                                      altogetherif a terminatesin a state outsideof b’s initiation set). We say that the two options
                                      are composed to yield a new option, denoted ab, corresponding to this way of behaving.
                                      The composition of two Markov options will in general be semi-Markov, not Markov,
                                      because actions are chosen differently before and after the ﬁrst option terminates. The
                                      compositionof two semi-Markovoptions is always another semi-Markovoption. Because
                                      actions are special cases of options, we can also compose them to produce a deterministic
                                      action sequence, in other words, a classical macro-operator.
                                         Moreinteresting for our purposes are policies over options. When initiated in a state s ,
                                                                                                                                                       t
                                      the Markov policy over options µ:S ×O →[0,1] selects an option o ∈O according to
                                                                                                                                      st
                                      probability distribution µ(s ,·). The option o is then taken in s , determining actions until
                                                                         t                                            t
                                      it terminates in s        , at which time a new option is selected, according to µ(s                     ,·),and
                                                            t+k                                                                            t+k
                                      so on. In this way a policy over options, µ, determines a conventional policy over actions,
                                      or ﬂat policy, π = ﬂat(µ). Henceforth we use the unqualiﬁed term policy for policies over
                                      options, which include ﬂat policies as a special case. Note that even if a policy is Markov
                                      and all of the options it selects are Markov, the corresponding ﬂat policy is unlikely to be
                                      Markovif any of the options are multi-step (temporally extended). The action selected by
                                      the ﬂat policy in state s        depends not just on s but on the option being followed at that
                                                                     τ                             τ
                                      time, and this dependsstochastically on the entire history h                  since the policy was initiated
                                                                                                                 tτ


===== Page 8 =====
                                    188                       R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                                    at time t.4 By analogy to semi-Markov options, we call policies that depend on histories
                                    in this way semi-Markov policies. Note that semi-Markov policies are more specialized
                                    than nonstationary policies. Whereas nonstationary policies may depend arbitrarily on all
                                    precedingevents,semi-Markovpoliciesmaydependonlyoneventsbacktosomeparticular
                                    time. Their decisions must be determined solely by the event subsequence from that time
                                    to the present, independent of the events preceding that time.
                                        These ideas lead to natural generalizations of the conventional value functions for a
                                    givenpolicy. We deﬁne the value of a state s ∈ S under a semi-Markovﬂat policy π as the
                                    expectedreturn given that π is initiated in s:
                                              π     def                         2                         	
                                            V (s)=E r            +γr +γ r +···E(π,s,t) ,
                                                            t+1        t+2         t+3
                                    where E(π,s,t) denotes the event of π being initiated in s at time t.Thevalueofa
                                    state under a general policy µ can then be deﬁned as the value of the state under the
                                                                         µ       def   ﬂat(µ)
                                    corresponding ﬂat policy: V (s)=                 V        (s),foralls ∈ S. Action-value functions
                                                                                                    µ
                                    generalize to option-value functions. We deﬁne Q (s,o), the value of taking option o
                                    in state s ∈ I under policy µ,as
                                              µ        def                         2                           	
                                            Q (s,o)=E r             +γr +γ r               +···E(oµ,s,t) ,                                    (5)
                                                                t+1       t+2         t+3
                                    where oµ,thecomposition of o and µ, denotes the semi-Markov policy that ﬁrst follows
                                    o until it terminates and then starts choosing according to µ in the resultant state. For
                                    semi-Markov options, it is useful to deﬁne E(o,h,t),theeventofo continuing from h
                                    at time t,whereh is a history ending with s . In continuing, actions are selected as if the
                                                                                           t
                                    historyhadprecededs .Thatis,a isselectedaccordingtoo(h,·),ando terminatesatt+1
                                                                t            t
                                    with probability β(ha r          s    );ifo doesnotterminate,then a               is selected accordingto
                                                               t t+1 t+1                                          t+1
                                    o(ha r       s    ,·), and so on. With this deﬁnition, (5) also holds where s is a history rather
                                          t  t+1 t+1
                                    than a state.
                                        This completes our generalization to temporal abstraction of the concept of value
                                    functions for a given policy. In the next section we similarly generalize the concept of
                                    optimal value functions.
                                    3. SMDP(option-to-option)methods
                                        Optionsarecloselyrelatedtothe actionsina special kindofdecision problemknownas
                                    a semi-Markov decision process,orSMDP (e.g., see [58]). In fact, any MDP with a ﬁxed
                                    set of options is an SMDP, as we state formally below. Although this fact follows more
                                    or less immediately from deﬁnitions, we present it as a theorem to highlight it and state
                                    explicitly its conditions and consequences:
                                    Theorem1 (MDP+Options=SMDP).ForanyMDP,andanysetofoptionsdeﬁnedon
                                    that MDP, the decision process that selects only among those options, executing each to
                                    termination, is an SMDP.
                                      4 For example, the options for picking up an object and putting down an object may specify different actions in
                                    the same intermediate state; which action is taken depends on which option is being followed.


===== Page 9 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       189
                                      Proof (Sketch). An SMDP consists of
                                         (1) a set of states,
                                         (2) a set of actions,
                                         (3) for each pair of state and action, an expected cumulative discounted reward, and
                                         (4) a well-deﬁned joint distribution of the next state and transit time.
                                      In our case, the set of states is S, and the set of actions is the set of options. The expected
                                      reward and the next-state and transit-time distributions are deﬁned for each state and
                                      option by the MDP and by the option’s policy and termination condition, π and β.These
                                      expectations and distributions are well deﬁned because MDPs are Markov and the options
                                      are semi-Markov; thus the next state, reward, and time are dependent only on the option
                                      and the state in which it was initiated. The transit times of options are always discrete, but
                                      this is simply a special case of the arbitrary real intervals permitted in SMDPs.                       2
                                         This relationship among MDPs, options, and SMDPs provides a basis for the theory of
                                      planning and learning methods with options. In later sections we discuss the limitations
                                      of this theory due to its treatment of options as indivisible units without internal structure,
                                      but in this section we focus on establishing the beneﬁts and assurances that it provides. We
                                      establishtheoreticalfoundationsandthensurveySMDPmethodsforplanningandlearning
                                      withoptions.Althoughourformalismisslightlydifferent,theseresultsareinessencetaken
                                      oradaptedfrompriorwork(includingclassicalSMDPworkand[5,44,52–57,65–68,71,74,
                                      75]). A result very similar to Theorem 1 was proved in detail by Parr [52]. In Sections 4–7
                                      wepresentnewmethodsthatimproveoverSMDPmethods.
                                         Planning with options requires a model of their consequences. Fortunately, the
                                      appropriate form of model for options, analogous to the ra and pa 0 deﬁned earlier for
                                                                                                                   s         ss
                                      actions, is known from existing SMDP theory. For each state in which an option may be
                                      started, this kind of model predicts the state in which the option will terminate and the total
                                      rewardreceivedalongtheway.Thesequantitiesarediscountedinaparticularway.Forany
                                      option o,letE(o,s,t)denote the event of o being initiated in state s at time t. Then the
                                      reward part of the model of o for any state s ∈ S is
                                               o                                     k−1                    	
                                              r =E r          +γr +···+γ                   r     E(o,s,t) ,                                         (6)
                                               s          t+1        t+2                    t+k
                                      where t + k is the random time at which o terminates. The state-prediction part of the
                                      modelofoforstate s is
                                                        ∞
                                              po 0 =Xp(s0,k)γk,                                                                                      (7)
                                                ss
                                                       k=1
                                      for all s0 ∈ S,wherep(s0,k)is the probability that the option terminates in s0 after k steps.
                                      Thus, po 0 is a combination of the likelihood that s0 is the state in which o terminates
                                                ss
                                      together with a measure of how delayed that outcome is relative to γ. We call this kind of
                                      model a multi-time model [54,55] because it describes the outcome of an option not at a
                                      single time but at potentially many different times, appropriately combined.5
                                        5 Note that this deﬁnition of state predictions for options differs slightly from that given earlier for actions.
                                      Underthenewdeﬁnition, the model of transition from state s to s0 for an action a is not simply the corresponding
                                      transition probability, but the transition probability times γ. Henceforth we use the new deﬁnition given by (7).


===== Page 10 =====
                                                          190                                     R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                                                               Using multi-time models we can write Bellman equations for general policies and
                                                          options. For any Markov policy µ, the state-value function can be written
                                                                         µ                                               k−1                     k     µ                                   	
                                                                      V (s)=E r                        +···+γ                    r        +γ V (s                    )E(µ,s,t)
                                                                                                t+1                                t+k                         t+k
                                                                                             (wherek is the duration of the ﬁrst option selected by µ)
                                                                                         X                       o         X o µ 0 
                                                                                   =              µ(s,o) r +                        p 0V (s ) ,                                                                                     (8)
                                                                                                                    s                  ss
                                                                                        o∈Os                                  s0
                                                          whichisaBellmanequationanalogousto(2).ThecorrespondingBellmanequationforthe
                                                          value of an option o in state s ∈ I is                                                                             
                                                                          µ                                                   k−1                     k     µ                                  	
                                                                      Q (s,o)=E r                           +···+γ                    r        +γ V (s                    ) E(o,s,t) ,
                                                                                                 t+1                                   t+k                         t+k
                                                                                        =E r                 +···+γk−1r
                                                                                                     t+1                                 t+k                                                 
                                                                                                             k X                             0      µ                 0   
                                                                                                     +γ                    µ(s          ,o)Q (s                  ,o)E(o,s,t)
                                                                                                                  0               t+k                      t+k
                                                                                                                o ∈Os
                                                                                               o       X o X                              0     0      µ 0 0
                                                                                        =r +                   p 0                 µ(s ,o)Q (s ,o).                                                                                 (9)
                                                                                               s                  ss
                                                                                                         s0            o0∈Os0
                                                          Notethatall these equationsspecialize to those given earlier in the special case in which µ
                                                                                                                                                                                                       µ(s,o)=Voµ(s).
                                                          is a conventionalpolicy and o is a conventionalaction. Also note that Q
                                                               Finally, there are generalizations of optimal value functions and optimal Bellman
                                                          equationstooptionsandtopoliciesoveroptions.Ofcoursetheconventionaloptimalvalue
                                                                                 ∗               ∗
                                                          functionsV andQ arenotaffectedbytheintroductionofoptions;onecanultimatelydo
                                                          just as well with primitive actions as one can with options. Nevertheless, it is interesting
                                                          to know how well one can do with a restricted set of options that does not include all the
                                                          actions. For example, in planning one might ﬁrst consider only high-level options in order
                                                          to ﬁnd an approximate plan quickly. Let us denote the restricted set of options by O and
                                                          the set of all policies selecting only from options in O by Π(O). Then the optimal value
                                                          function given that we can select only from O is
                                                                         ∗         def                      µ
                                                                      VO(s) = max V (s)
                                                                                         µ∈Π(O)                                                                                   
                                                                                                                                    k−1                     k     ∗                                 	
                                                                                   =maxE r                        +···+γ                    r        +γ V (s                    ) E(o,s,t)
                                                                                         o∈Os              t+1                                t+k                 O t+k
                                                                                              (wherek is the duration of o when taken in s)
                                                                                   =maxro+Xpo0V∗(s0)                                                                                                                            (10)
                                                                                         o∈O           s                  ss      O
                                                                                                s                s0
                                                                                                                   k     ∗      0                 	
                                                                                   =maxE r+γ VO(s)E(o,s) ,                                                                                                                       (11)
                                                                                         o∈Os
                                                          where E(o,s) denotes option o being initiated in state s. Conditional on this event are the
                                                          usualrandomvariables:s0 isthestate inwhich o terminates, r is thecumulativediscounted
                                                          reward along the way, and k is the number of time steps elapsing between s and s0.The
                                                          value functions and Bellman equations for optimal option values are


===== Page 11 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       191
                                                ∗         def              µ
                                              Q (s,o) = max Q (s,o)
                                                O             µ∈Π(O)                                              
                                                                                    k−1            k   ∗                     	
                                                           =E r         +···+γ           r     +γ V (s           ) E(o,s,t)
                                                                   t+1                    t+k           O t+k
                                                                  (wherek is the duration of o from s)                            
                                                                 n                   k−1             k             ∗            0             o
                                                           =E r         +···+γ           r     +γ max Q (s ,o)E(o,s,t) ,
                                                                   t+1                     t+k        o0∈Os        O t+k
                                                                     X                                       t+k
                                                                o           o             ∗    0   0
                                                           =r +           p 0 max Q (s ,o)
                                                               s            ss o0∈O 0     O
                                                                      s0             s
                                                                 n                                         o
                                                                          k           ∗    0   0 
                                                           =E r+γ max Q (s,o)E(o,s) ,                                                              (12)
                                                                           o0∈Os0     O
                                      where r, k,ands0 are again the reward, number of steps, and next state due to taking
                                      o∈O .
                                              s
                                                                                                                                    ∗
                                         Given a set of options, O, a corresponding optimal policy, denoted µ , is any policy
                                                                                    ∗                                               O
                                                          ∗                        µ             ∗                                  ∗
                                      that achieves VO, i.e., for which V O(s) = VO(s) in all states s ∈ S.IfVO and models of
                                      the optionsare known,thenoptimalpolicies can beformedbychoosingin anyproposition
                                                                                                             ∗ is known, then optimal policies
                                      amongthemaximizingoptionsin (10) or (11). Or, if Q
                                                                                                             O
                                      can be found without a model by choosing in each state s in any proportion among the
                                                                   ∗                   0   ∗       0
                                      options o for which Q (s,o) = max Q (s,o ). In this way, computing approximations
                                            ∗        ∗             O                  o    O
                                      to V    or Q      becomekeygoalsofplanningandlearningmethodswith options.
                                           O         O
                                      3.1. SMDPplanning
                                         With these deﬁnitions, an MDP together with the set of options O formally comprises
                                      an SMDP, and standard SMDP methods and results apply. Each of the Bellman equations
                                      for options, (8), (9), (10), and (12), deﬁnes a system of equations whose unique solution
                                      is the corresponding value function. These Bellman equations can be used as update rules
                                      in dynamic-programming-likeplanningmethodsforﬁndingthevaluefunctions.Typically,
                                                                                                                           ∗           ∗
                                      solution methodsforthis problemmaintainan approximationof V (s) or Q (s,o) forall
                                                                                                                          O            O
                                      states s ∈ S and all options o ∈ O . For example, synchronous value iteration (SVI) with
                                                                                  s
                                      options starts with an arbitrary approximation V0 to V∗ and then computes a sequence of
                                      newapproximations{V } by                                             O
                                                                    k
                                                                o      X o                0 
                                              Vk(s)= max r +                 p 0Vk−1(s )                                                            (13)
                                                         o∈O      s            ss
                                                              s        s0∈S
                                      for all s ∈ S. The option-value form of SVI starts with an arbitrary approximation Q to
                                                                                                                                                    0
                                         ∗
                                      Q andthencomputesasequenceofnewapproximations{Q }by
                                         O                                                                            k
                                                              o    X o                          0   0
                                              Q (s,o)=r +                p 0 max Q           (s ,o)
                                                k             s            ss o0∈O 0     k−1
                                                                   s0∈S             s
                                      for all s ∈ S and o ∈ O . Note that these algorithms reduce to the conventional value
                                                                      s
                                      iteration algorithms in the special case that O = A. Standard results from SMDP theory
                                      guarantee that these processes converge for general semi-Markov options: lim                                 V =
                                         ∗                            ∗                                                                     k→∞ k
                                      V andlim              Q =Q ,foranyO.
                                        O            k→∞ k            O


===== Page 12 =====
                  192         R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                  Fig. 2. The rooms example is a gridworld environment with stochastic cell-to-cell actions and room-to-room
                  hallway options. Two of the hallway options are suggested by the arrows labeled o and o . The labels G and
                                                         1   2      1
                  G2 indicate two locations used as goals in experiments described in the text.
                   Theplans(policies)foundusingtemporallyabstractoptionsareapproximateinthesense
                  that theyachieveonlyV∗,whichmaybelessthanthemaximumpossible,V∗.Ontheother
                                O                                     ∗
                  hand, if the models used to ﬁnd them are correct, then they are guaranteed to achieve VO.
                  Wecall this the value achievement property of planning with options. This contrasts with
                  planning methods that abstract over state space, which generally cannot be guaranteed to
                  achieve their planned values even if their models are correct.
                   As a simple illustration of planning with options, consider the rooms example,a
                  gridworldenvironmentoffourroomsasshowninFig.2.Thecellsofthegridcorrespondto
                  thestates of theenvironment.Fromanystatetheagentcanperformoneoffouractions,up,
                  down,leftorright,whichhaveastochastic effect. With probability 2/3, the actions
                  cause the agent to move one cell in the corresponding direction, and with probability 1/3,
                  the agent moves instead in one of the other three directions, each with probability 1/9. In
                  either case, if the movementwould take the agent into a wall then the agent remains in the
                  samecell. For now we consider a case in which rewards are zero on all state transitions.
                   In each of the four rooms we provide two built-in hallway options designed to take the
                  agent from anywhere within the room to one of the two hallway cells leading out of the
                  room. A hallway option’s policy π follows a shortest path within the room to its target
                  hallway while minimizing the chance of stumbling into the other hallway. For example,
                  the policy for one hallway option is shown in Fig. 3. The termination condition β(s) for
                  each hallway option is zero for states s within the room and 1 for states outside the room,
                  including the hallway states. The initiation set I comprises the states within the room plus
                  thenon-targethallwaystateleadingintotheroom.Notethattheseoptionsaredeterministic
                  andMarkov,andthatanoption’spolicyisnotdeﬁnedoutsideofitsinitiationset.Wedenote
                  the set of eight hallway options by H. For each option o ∈ H, we also provide a priori its
                  accurate model, ro and po 0,foralls ∈ I and s0 ∈ S (assuming there is no goal state, see
                            s    ss
                  below).Notethatalthoughthetransitionmodelspo 0 are nominallylarge(order|I|×|S|),
                                               ss


===== Page 13 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       193
                                                                Fig. 3. The policy underlying one of the eight hallway options.
                                      Fig. 4. Value functions formed over iterations of planning by synchronous value iteration with primitive
                                      options (above) and with multi-step hallway options (below). The hallway options enabled planning to proceed
                                      room-by-room rather than cell-by-cell. The area of the disk in each cell is proportional to the estimated value of
                                      the state, where a disk that just ﬁlls a cell represents a value of 1.0.
                                      in fact they are sparse, and relatively little memory (order |I|×2) is actually needed to
                                      hold the nonzero transitions from each state to the two adjacent hallway states.6
                                         Nowconsiderasequenceofplanningtasksfornavigatingwithinthegridtoadesignated
                                      goal state, in particular, to the hallway state labeled G in Fig. 2. Formally, the goal state
                                                                                                            1
                                      is a state from which all actions lead to the terminal state with a reward of +1. Throughout
                                      this paper we discount with γ =0.9 in the rooms example.
                                         As a planning method, we used SVI as given by (13), with various sets of options
                                      O. The initial value function V0 was 0 everywhere except the goal state, which was
                                      initialized to its correct value, V0(G1) = 1, as shown in the leftmost panels of Fig. 4.
                                        6 The off-target hallway states are exceptions in that they have three possible out-comes: the target hallway,
                                      themselves, and the neighboring state in the off-target room.


===== Page 14 =====
               194        R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
               This ﬁgure contrasts planning with the original actions (O = A) and planning with the
               hallway options and not the original actions (O = H). The upper part of the ﬁgure shows
               the value function after the ﬁrst two iterations of SVI using just primitive actions. The
               region of accurately valued states moved out by one cell on each iteration, but after two
               iterations most states still had their initial arbitrary value of zero. In the lower part of
               the ﬁgure are shown the corresponding value functions for SVI with the hallway options.
               In the ﬁrst iteration all states in the rooms adjacent to the goal state became accurately
               valued, and in the second iteration all the states became accurately valued. Although the
               values continued to change by small amounts over subsequent iterations, a complete and
               optimal policy was known by this time. Rather than planning step-by-step, the hallway
               optionsenabledtheplanningtoproceedatahigherlevel,room-by-room,andthusbemuch
               faster.
                This example is a particularly favorable case for the use of multi-step options because
               the goal state is a hallway, the target state of some of the options. Next we consider a case
               in which there is no such coincidence, in which the goal lies in the middle of a room, in
               the state labeled G2 in Fig. 2. The hallway options and their models were just as in the
               previous experiment. In this case, planning with (models of) the hallway options alone
               could never completely solve the task, because these take the agent only to hallways and
               thus never to the goal state. Fig. 5 shows the value functions found over ﬁve iterations of
               SVIusingboththehallwayoptionsandthe primitiveoptionscorrespondingto the actions
               (i.e., using O = A ∪ H). In the ﬁrst two iterations, accurate values were propagated from
               G2byonecellperiterationbythemodelscorrespondingtotheprimitiveoptions.Aftertwo
               iterations, however, the ﬁrst hallway state was reached, and subsequently room-to-room
               planning using the multi-step hallway options dominated. Note how the state in the lower
               Fig. 5. An example in which the goal is different from the subgoal of the hallway options. Planning here was by
               SVI with options O = A∪H. Initial progress was due to the models of the primitive options (the actions), but
               by the third iteration room-to-room planning dominated and greatly accelerated planning.


===== Page 15 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       195
                                      right cornerwas givenanonzerovalueduringiterationthree.Thisvaluecorrespondstothe
                                      plan of ﬁrst going to the hallway state above and then down to the goal; it was overwritten
                                      by a larger value corresponding to a more direct route to the goal in the next iteration.
                                      Because of the multi-step options, a close approximation to the correct value function was
                                      found everywhere by the fourth iteration; without them only the states within three steps
                                      of the goal would have been given non-zero values by this time.
                                         Wehaveused SVI in this example because it is a particularly simple planning method
                                      which makes the potential advantage of multi-step options clear. In large problems, SVI
                                      is impractical because the number of states is too large to complete many iterations, often
                                      not even one. In practice it is often necessary to be very selective about the states updated,
                                      the options considered, and even the next states considered. These issues are not resolved
                                      by multi-step options, but neither are they greatly aggravated. Options provide a tool for
                                      dealing with them more ﬂexibly.
                                         Planning with options is not necessarily more complex than planning with actions. For
                                      example, in the ﬁrst experiment described above there were four primitive options and
                                      eight hallway options, but in each state only two hallway options needed to be considered.
                                      In addition, the models of the primitive options generated four possible successors with
                                      non-zero probability whereas the multi-step options generated only two. Thus planning
                                      with the multi-step options was actually computationally cheaper than conventional SVI
                                      in this case. In the second experiment this was not the case, but the use of multi-step
                                      options did not greatly increase the computational costs. In general, of course, there is no
                                      guaranteethatmulti-stepoptionswillreducetheoverallexpenseofplanning.Forexample,
                                      Hauskrecht et al. [26] have shown that adding multi-step options may actually slow SVI
                                      if the initial value function is optimistic. Research with deterministic macro-operators has
                                      identiﬁed a related “utility problem” when too many macros are used (e.g., see [20,23,
                                      24,47,76]). Temporal abstraction provides the ﬂexibility to greatly reduce computational
                                      complexity, but can also have the opposite effect if used indiscriminately. Nevertheless,
                                      these issues are beyond the scope of this paper and we do not consider them further.
                                      3.2. SMDPvaluelearning
                                         Theproblemofﬁndinganoptimalpolicyoveraset ofoptions O can also be addressed
                                      by learning methods. Because the MDP augmented by the options is an SMDP, we
                                      can apply SMDP learning methods [5,41,44,52,53]. Much as in the planning methods
                                      discussed above, each option is viewed as an indivisible, opaque unit. When the execution
                                      of option o is started in state s, we next jump to the state s0 in which o terminates. Based
                                      onthis experience,an approximateoption-valuefunction Q(s,o) is updated.For example,
                                      the SMDP version of one-step Q-learning [5], which we call SMDP Q-learning, updates
                                      after each option termination by
                                                                           h        k              0   0              i
                                              Q(s,o)←Q(s,o)+α r+γ max Q(s ,o)−Q(s,o) ,
                                                                                      o0∈Os0
                                      where k denotes the number of time steps elapsing between s and s0, r denotes the
                                      cumulativediscountedrewardoverthistime,anditisimplicitthatthestep-sizeparameterα
                                      maydependarbitrarilyonthestates,option,andtimesteps.TheestimateQ(s,o)converges


===== Page 16 =====
                           196                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                           Fig. 6. Performance of SMDP Q-learning in the rooms example with various goals and sets of options. After
                           100 episodes, the data points are averages over groups of 10 episodes to make the trends clearer. The step size
                           parameter was optimized to the nearest power of 2 for each goal and set of options. The results shown used
                           α=1/8inallcases except that with O =H and G1 (α =1/16), and that with O =A∪H and G2 (α =1/4).
                                ∗
                           to Q (s,o) for all s ∈ S and o ∈ O under conditions similar to those for conventional Q-
                                O
                           learning [52], from which it is easy to determine an optimal policy as described earlier.
                              Asanillustration, we applied SMDP Q-learning to the rooms example with the goal at
                           G1 and at G2 (Fig. 2). As in the case of planning, we used three different sets of options,
                           A,H,andA∪H.Inallcases,optionswereselectedfromthesetaccordingtoanε-greedy
                           method.Thatis, options were usually selected at random from among those with maximal
                           option value (i.e., o was such that Q(s ,o) = max      Q(s ,o)), but with probability
                                              t                  t  t       o∈Os     t
                                                                                t
                           ε the option was instead selected randomly from all available options. The probability of
                           randomaction, ε,was0.1 in all our experiments. The initial state of each episode was in
                           the upper-left corner. Fig. 6 shows learning curves for both goals and all sets of options.
                           In all cases, multi-step options enabled the goal to be reached much more quickly, even
                           on the very ﬁrst episode. With the goal at G1, these methods maintained an advantage
                           over conventional Q-learning throughout the experiment, presumably because they did
                           less exploration. The results were similar with the goal at G2, except that the H method
                           performedworsethantheothersinthelongterm.Thisisbecausethebestsolutionrequires
                           several steps of primitive options (the hallway options alone ﬁnd the best solution running
                           betweenhallwaysthatsometimesstumblesuponG2).Forthesamereason,theadvantages
                           of the A∪H methodoverthe A methodwerealso reduced.
                           4. Interrupting options
                              SMDPmethodsapply to options, but only when they are treated as opaque indivisible
                           units. More interesting and potentially more powerful methods are possible by looking
                           inside options or by altering their internal structure, as we do in the rest of this paper. In
                           this section we take a ﬁrst step in altering options to make them more useful. This is the
                           area where working simultaneously in terms of MDPs and SMDPs is most relevant. We
                           cananalyzeoptionsintermsoftheSMDPandthenusetheirMDPinterpretationtochange
                           themandproduceanewSMDP.


===== Page 17 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       197
                                         In particular, in this section we consider interrupting options before they would
                                      terminate naturally according to their termination conditions. Note that treating options
                                      as indivisible units, as SMDP methods do, is limiting in an unnecessary way. Once an
                                      option has been selected, such methods require that its policy be followed until the option
                                                                                                                                   µ
                                      terminates. Suppose we have determined the option-value function Q (s,o) for some
                                      policy µ and for all state-option pairs s, o that could be encountered while following
                                      µ. This function tells us how well we do while following µ, committing irrevocably to
                                      each option, but it can also be used to re-evaluate our commitment on each step. Suppose
                                      at time t we are in the midst of executing option o.Ifo is Markov in s , then we can
                                                                                                                                      t
                                                                                                         µ(s ,o), to the value of interrupting o
                                      comparethe value of continuing with o,whichisQ                         t        P
                                                                                                             µ                          µ
                                      and selecting a new option according to µ,whichisV (s) =                            q µ(s,q)Q (s,q).Ifthe
                                      latter is more highly valued, then why not interrupt o and allow the switch? If these were
                                      simpleactions,theclassicalpolicyimprovementtheorem[27]wouldassureusthatthenew
                                      wayofbehavingisindeedbetter.Hereweprovethegeneralizationtosemi-Markovoptions.
                                      The ﬁrst empirical demonstration of this effect—improved performance by interrupting
                                      a temporally extended substep based on a value function found by planning at a higher
                                      level—may have been by Kaelbling [31]. Here we formally prove the improvement in a
                                      moregeneralsetting.
                                         In the following theorem we characterize the new way of behaving as following a
                                      policy µ0 that is the same as the original policy, µ, but over a new set of options;
                                      µ0(s,o0) = µ(s,o),foralls ∈ S. Each new option o0 is the same as the corresponding
                                      old option o except that it terminates whenever switching seems better than continuing
                                                          µ                                                          0      0
                                      according to Q . In other words, the termination condition β of o is the same as that of
                                                           0                 µ              µ                              0
                                      o except that β (s) = 1ifQ (s,o) < V (s). We call such a µ an interrupted policy
                                      of µ. The theorem is slightly more general in that it does not require interruption at
                                                                                                                                            µ
                                      each state in which it could be done. This weakens the requirement that Q (s,o) be
                                      completely known. A more important generalization is that the theorem applies to semi-
                                      Markovoptions rather than just Markov options. This generalization may make the result
                                      less intuitively accessible on ﬁrst reading. Fortunately, the result can be read as restricted
                                      to the Markov case simply by replacing every occurrence of “history” with “state” and set
                                      of histories, Ω, with set of states, S.
                                      Theorem2 (Interruption). For any MDP, any set of options O, and any Markov policy
                                      µ:S×O→[0,1],deﬁneanewset of options, O0, with a one-to-one mapping between
                                      the two option sets as follows: for every o =hI,π,βi∈O we deﬁne a corresponding
                                        0              0      0             0
                                      o =hI,π,βi∈O,whereβ =β exceptthatforanyhistoryhthatendsinstates andin
                                                 µ              µ                                    0
                                      whichQ (h,o)<V (s),wemaychoosetosetβ (h)=1.Anyhistorieswhosetermination
                                      conditions are changed in this way are called interrupted histories. Let the interrupted
                                      policy µ0 be such that for all s ∈ S, and for all o0 ∈ O0, µ0(s,o0) = µ(s,o),whereo is the
                                      option in O corresponding to o0.Then
                                                   0
                                          (i) Vµ (s) >Vµ(s) for all s ∈S.
                                         (ii) If from state s ∈ S there is a non-zero0probability of encountering an interrupted
                                               history upon initiating µ0 in s,thenVµ (s) > Vµ(s).


===== Page 18 =====
                                                          198                                     R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                                                          Proof. Shortly we show that, for an arbitrary start state s, executing the option given by
                                                          the interrupted policy µ0 and then following policy µ thereafter is no worse than always
                                                          following policy µ. In other words, we show that the following inequality holds:
                                                                      X 0                 0    o0         X o0 µ 0 
                                                                             µ(s,o) r +                            p 0V (s )
                                                                                                  s                   ss
                                                                        o0                                    s0                                                
                                                                          >Vµ(s)=Xµ(s,o) ro+Xpo0Vµ(s0) .                                                                                                                          (14)
                                                                                                                            s                  ss
                                                                                                    o                                 s0
                                                          If this is true, then we can use it to expand the left-hand side, repeatedly replacing every
                                                          occurrenceofVµ(x)ontheleftbythecorrespondingP 0µ0(x,o0)[ro0+P 0po0 0Vµ(x0)].
                                                                                                                                              0                       o          0             x            x      xx
                                                          In the limit, the left-hand side becomes Vµ , proving that Vµ > Vµ.
                                                                                                                                                                           0         0
                                                               Toprovetheinequality in (14), we note that for all s,µ (s,o ) = µ(s,o),andshowthat
                                                                      ro0 +Xpo00Vµ(s0)>ro+Xpo0Vµ(s0)                                                                                                                              (15)
                                                                       s                   ss                        s                  ss
                                                                                   s0                                           s0
                                                          as follows. Let Γ denote the set of all interrupted histories: Γ ={h ∈ Ω: β(h)6= β0(h)}.
                                                          Then,
                                                                        o0       X o0 µ 0                                           k     µ 0                0                     	
                                                                      r     +           p 0V (s )=E r+γ V (s )E(o,s),h∈/Γ
                                                                       s                   ss
                                                                                   s0                                                                                                   	
                                                                                                                    +E r+γkVµ(s0)E(o0,s),h∈Γ
                                                          where s0, r,andk are the next state, cumulative reward, and number of elapsed steps
                                                          following option o from s,andwhereh is the history from s to s0. Trajectories that end
                                                          because of encountering a history not in Γ never encounter a history in Γ , and therefore
                                                          also occur with the same probability and expected reward upon executing option o in state
                                                          s. Therefore, if we continue the trajectories that end because of encountering a history in
                                                          Γ with option o until termination and thereafter follow policy µ,weget
                                                                                      k     µ 0                0                     	
                                                                      E r +γ V (s )E(o,s),h∈/Γ                                                                                                    
                                                                                                0              k     µ 0                               0               k     µ                        0                    	
                                                                               +E β(s) r+γ V (s ) + 1−β(s) r+γ Q (h,o) E(o,s),h∈Γ
                                                                          =ro+Xpo0Vµ(s0),
                                                                                 s                  ss
                                                                                            s0
                                                          becauseoption o is semi-Markov.This proves(14) because for all h ∈Γ,
                                                                          µ                      µ 0
                                                                      Q (h,o)6V (s ).
                                                                                                                                               µ                      µ 0
                                                          Notethatstrict inequality holds in (15) if Q (h,o) < V (s ) for at least one history h ∈ Γ
                                                          that ends a trajectory generated by o0 with non-zero probability.                                                                2
                                                               As one application of this result, consider the case in which µ is an optimal policy
                                                          for some given set of Markov options O. We have already discussed how we can, by
                                                                                                                                                                                      ∗                ∗
                                                          planning or learning, determine the optimal value functions V                                                                   and Q             and from them
                                                                                                  ∗                                                                                  O                 O
                                                          the optimal policy µO that achieves them. This is indeed the best that can be done without


===== Page 19 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       199
                                      changingO,thatis,intheSMDPdeﬁnedbyO,butlessthanthebestpossibleachievablein
                                      the MDP, which is V∗ =V∗. But of course we typically do not wish to work directly with
                                                                        A
                                      the (primitive) actions A because of the computational expense. The interruption theorem
                                      gives us a way of improving over µ∗ with little additional computation by stepping
                                                                                        O
                                      outsideO.Thatis,ateachstepweinterruptthecurrentoptionandswitchtoanynewoption
                                                                                             ∗
                                      that is valued more highly according to Q . Checking for such options can typically be
                                                                                             O
                                      done at vastly less expense per time step than is involved in the combinatorial process of
                                                        ∗
                                      computing Q . In this sense, interruption gives us a nearly free improvement over any
                                                        O                                                 ∗
                                      SMDPplanningorlearningmethodthatcomputesQ asanintermediatestep.
                                                                                                          O
                                         In the extreme case, we might interrupt on every step and switch to the greedy option—
                                                                                                                                   ∗
                                      the option in that state that is most highly valued according to Q                               (as in polling
                                                                                                                                   O
                                      execution [16]). In this case, options are never followed for more than one step, and they
                                                                                                                                                 ∗ ,the
                                      might seem superﬂuous. However, the options still play a role in determining Q
                                                                                                                                                 O
                                      basis on whichthegreedyswitchesaremade,andrecallthatmulti-stepoptionsmayenable
                                         ∗                                                     ∗
                                      Q tobefoundmuchmorequickly than Q could (Section 3). Thus, even if multi-step
                                         O
                                      optionsareneveractuallyfollowedformorethanonesteptheycanstillprovidesubstantial
                                      advantagesin computationand in our theoretical understanding.
                                         Fig. 7 shows a simple example. Here the task is to navigate from a start location to a
                                      goallocationwithinacontinuoustwo-dimensionalstate space.Theactionsaremovements
                                      of 0.01 in any direction from the current state. Rather than work with these low-level
                                      actions, inﬁnite in number, we introduce seven landmark locations in the space. For each
                                      landmarkwedeﬁneacontrollerthattakesustothelandmarkinadirectpath(cf.[48]).Each
                                      controller is only applicable within a limited range of states, in this case within a certain
                                      distanceofthecorrespondinglandmark.Eachcontrollerthendeﬁnesanoption:thecircular
                                      region around the controller’s landmark is the option’s initiation set, the controller itself is
                                      thepolicy,andarrivalatthetargetlandmarkistheterminationcondition.Wedenotetheset
                                      of seven landmark options by O. Any action within 0.01 of the goal location transitions to
                                      the terminal state, the discount rate γ is 1, and the reward is −1 on all transitions, which
                                      makesthisa minimum-timetask.
                                         One of the landmarks coincides with the goal, so it is possible to reach the goal while
                                      picking only from O. The optimal policy within O runs from landmark to landmark, as
                                      shown by the thin line in the upper panel of Fig. 7. This is the optimal solution to the
                                      SMDPdeﬁnedbyOandisindeedthebestthatonecandowhilepickingonlyfromthese
                                      options. But of course one can do better if the options are not followed all the way to each
                                      landmark. The trajectory shown by the thick line in Fig. 7 cuts the corners and is shorter.
                                      This is the interrupted policy with respect to the SMDP-optimal policy. The interrupted
                                      policy takes 474 steps from start to goal which, while not as good as the optimal policy in
                                      primitive actions (425 steps), is much better, for nominal additional cost, than the SMDP-
                                                                                                                                               0
                                      optimalpolicy,whichtakes600steps.Thestate-valuefunctions,Vµ =V∗ andVµ forthe
                                                                                                                                   O
                                      two policies are shown in the lower part of Fig. 7. Note how the values for the interrupted
                                      policy are everywhere greater than the values of the original policy. A related but larger
                                      applicationoftheinterruptionideatomissionplanningforuninhabitedairvehiclesisgiven
                                      in [75].
                                         Fig. 8 shows results for an example using controllers/options with dynamics. The task
                                      here is to move a mass along one dimension from rest at position 0 to rest at position 2,


===== Page 20 =====
               200        R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
               Fig. 7. Using interruption to improve navigation with landmark-directed controllers. The task (top) is to navigate
               from S to G in minimum time using options based on controllers that run each to one of seven landmarks (the
               black dots.) The circles show the region around each landmark within which the controllers operate. The thin line
               shows the SMDP solution, the optimal behavior that uses only these controllers without interrupting them, and
               the thick line shows the corresponding solution with interruption, which cuts the corners. The lower two panels
               show the state-value functions for the SMDP and interrupted solutions.
               again in minimum time. There is no option that takes they system all the way from 0
               to 2, but we do have an option that takes it from 0 to 1 and another option that takes it
               from any position greater than 0.5 to 2. Both options control the system precisely to its
               target position and to zero velocity, terminating only when both of these are correct to
               within ε = 0.0001. Using just these options, the best that can be done is to ﬁrst move
               precisely to rest at 1, using the ﬁrst option, then re-accelerate and move to 2 using
               the second option. This SMDP-optimal solution is much slower than the corresponding
               interrupted solution, as shown in Fig. 8. Because of the need to slow down to near-zero
               velocity at 1, it takes over 200 time steps, whereas the interrupted solution takes only 121
               steps.


===== Page 21 =====
                                                                   R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                           201
                                        Fig. 8. Phase-space plot of the SMDP and interrupted policies in a simple dynamical task. The system is a mass
                                        moving in one dimension: x        =x +˙x       , x˙   =˙x +a −0.175x˙ where x isthe position, x˙ the velocity,
                                                                     t+1      t    t+1    t+1     t     t          t          t                  t
                                        0.175 a coefﬁcient of friction, and the action a an applied force. Two controllers are provided as options, one
                                                                                      ∗   t                    ∗
                                        that drives the position to zero velocity at x  =1andtheother to x =2. Whichever option is being followed at
                                        time t, its target position x∗ determines the action taken, according to a = 0.01(x∗ −x ).
                                                                                                                    t                t
                                        5. Intra-option model learning
                                           In this section we introduce a new method for learning the model, ro and po 0,ofan
                                                                                                                                        s          ss
                                        option o, given experience and knowledge of o (i.e., of its I, π,andβ). Our method
                                        requires that π be deterministic and that the option be Markov. For a semi-Markov option,
                                        the only general approach is to execute the option to termination many times in each state
                                        s, recording in each case the resultant next state s0, cumulative discounted reward r,and
                                        elapsed time k. These outcomes are then averaged to approximate the expected values for
                                        ro and po 0 given by (6) and (7). For example, an incremental learning rule for this could
                                         s         ss
                                        update its model after each execution of o by
                                                  o      o               o
                                               br   =br +α[r −br ],                                                                                       (16)
                                                 s      s               s
                                        and
                                                  o        o        k 0           o 
                                                pb   =pb +α γ δsx −pb ,                                                                                   (17)
                                                  sx      sx                       sx
                                        for all x ∈ S+,whereδs0x =1ifs0 =x andis 0 else, and where the step-size parameter, α,
                                        maybeconstantormaydependonthestate,option,andtime.Forexample,ifα is1divided
                                        by the number of times that o has been experienced in s, then these updates maintain the
                                        estimates as sampleaveragesoftheexperiencedoutcomes.Howevertheaveragingisdone,
                                        wecalltheseSMDPmodel-learningmethodsbecause,likeSMDPvalue-learningmethods,
                                        they are based on jumping from initiation to termination of each option, ignoring what
                                        happensalongtheway.Inthe special case in which o is a primitive option, SMDP model-
                                        learning methods reduce to those used to learn conventionalone-step models of actions.
                                           One disadvantage of SMDP model-learning methods is that they improve the model
                                        of an option only when the option terminates. Because of this, they cannot be used for
                                        nonterminating options and can only be applied to one option at a time—the one option


===== Page 22 =====
                                                          202                                     R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                                                          that is executing at that time. For Markov options, special temporal-difference methods
                                                          can be used to learn usefully about the model of an option before the option terminates.
                                                          Wecallthese intra-option methods because they learn about an option from a fragment of
                                                          experience “within” the option. Intra-option methods can even be used to learn about an
                                                          option without ever executing it, as long as some selections are made that are consistent
                                                          with the option. Intra-option methods are examples of off-policy learning methods [72]
                                                          becausetheylearnabouttheconsequencesofonepolicywhileactuallybehavingaccording
                                                          to another. Intra-option methods can be used to simultaneously learn models of many
                                                          different options from the same experience. Intra-option methods were introduced in [71],
                                                          but only for a prediction problem with a single unchanging policy, not for the full control
                                                          case we consider here and in [74].
                                                               Just as there are Bellman equationsforvaluefunctions,thereare also Bellmanequations
                                                          for models of options. Consider the intra-option learning of the model of a Markov option
                                                          o=hI,π,βi.Thecorrectmodelofo isrelatedtoitself by
                                                                        o        X                                                       0   o	
                                                                      r =                 π(s,a)E r +γ 1−β(s) r 0
                                                                       s                                                                         s
                                                                                a∈A
                                                                                       s
                                                                                     (wherer and s0 are the reward and next state
                                                                                     given that action a is taken in state s)
                                                                                 X                       a          X a                             0   o
                                                                           =              π(s,a) r +                        p 0 1−β(s) r 0 ,
                                                                                                            s                  ss                           s
                                                                                a∈A                                    s0
                                                                                       s
                                                          and
                                                                         o          X                                               0   o                    0      0  	
                                                                      p =                   π(s,a)γE 1−β(s) p 0 +β(s)δsx
                                                                         sx                                                                  s x
                                                                                  a∈A
                                                                                          s                X
                                                                                    X                                 a                     0   o                    0      0  
                                                                              =             π(s,a)                 p 0 1−β(s) p 0 +β(s)δsx ,
                                                                                                                      ss                             s x
                                                                                  a∈A                        s0
                                                                                          s
                                                          for all s,x ∈ S. How can we turn these Bellman-like equations into update rules for
                                                          learning the model? First consider that action a is taken in s , and that the way it was
                                                                                                                                                          t                           t
                                                          selected is consistent with o =hI,π,βi,thatis,thatat was selected with the distribution
                                                          π(s,·). Then the Bellman equations above suggest the temporal-differenceupdate rules
                                                                 t
                                                                         o           o                                                      o                 o
                                                                     br     ←br +α r                       +γ 1−β(s )br                                 −br                                                                       (18)
                                                                       st           st              t+1                             t+1         st+1           st
                                                          and
                                                                      pbo ←pbo +αγ 1−β(s                                         )pbo            +γβ(s )δ                           −pbo ,                                     (19)
                                                                         stx            stx                                 t+1          st+1x                   t+1        st+1x            stx
                                                          for all x ∈ S+,wherepbo0 andbr o are the estimates of po 0 and ro, respectively, and α is
                                                                                                         ss               s                                               ss              s
                                                          a positive step-size parameter. The method we call one-step intra-option model learning
                                                          applies these updates to every option consistent with every action taken, at. Of course,
                                                          this is just the simplest intra-option model-learning method. Others may be possible using
                                                          eligibility traces and standard tricks for off-policy learning (as in [71]).
                                                               Asanillustration,considermodellearningintheroomsexampleusingSMDPandintra-
                                                          optionmethods.Asbefore,weassumethattheeighthallwayoptionsaregiven,butnowwe


===== Page 23 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       203
                                      Fig. 9. Model learning by SMDP and intra-option methods. Shown are the average and maximum over I of the
                                      absolute errors between the learned and true models, averaged over the eight hallway options and 30 repetitions
                                      of the whole experiment. The lines labeled ‘SMDP 1/t’ are for the SMDP method using sample averages; all the
                                      others used α = 1/4.
                                      assumethattheirmodelsarenotgivenandmustbelearned.Inthisexperiment,therewards
                                      were selected according to a normal probability distribution with a standard deviation
                                      of 0.1 and a mean that was different for each state–action pair. The means were selected
                                      randomlyatthebeginningofeachrununiformlyfromthe[−1,0]interval.Experiencewas
                                      generatedbyselectingrandomlyineachstateamongthetwopossibleoptionsandfourpos-
                                      sible actions, with no goal state. In the SMDP model-learning method, Eqs. (16) and (17)
                                      were applied whenever an option terminated, whereas, in the intra-option model-learning
                                      method, Eqs. (18) and (19) were applied on every step to all options that were consistent
                                      with the action taken on that step. In this example, all options are deterministic, so consis-
                                      tencywiththeactionselectedmeanssimplythattheoptionwouldhaveselectedthataction.
                                         For each method, we tried a range of values for the step-size parameter, α =
                                      1/2,1/4,1/8,and1/16.ResultsareshowninFig.9forthevaluethatseemedtobebestfor
                                      each method, which happened to be α =1/4 in all cases. For the SMDP method, we also
                                      show results with the step-size parameter set such that the model estimates were sample
                                      averages, which should give the best possible performance of this method (these lines are
                                      labeled1/t).Theﬁgureshowstheaverageandmaximumerrorsoverthestate–optionspace
                                      for each method, averaged over the eight options and 30 repetitions of the experiment. As
                                      expected, the intra-option method was able to learn signiﬁcantly faster than the SMDP
                                      methods.
                                      6. Intra-option value learning
                                         Weturn now to the intra-option learning of option values and thus of optimal policies
                                      over options. If the options are semi-Markov, then again the SMDP methods described in
                                      Section 3.2 may be the only feasible methods; a semi-Markov option must be completed
                                      before it can be evaluated. But if the options are Markov and we are willing to look inside
                                      them, then we can consider intra-option methods. Just as in the case of model learning,


===== Page 24 =====
                            204                 R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                            intra-option methodsfor value learning are potentially more efﬁcient than SMDP methods
                            becausethey extract more training examples from the same experience.
                                                                                      ∗
                               For example, suppose we are learning to approximate Q (s,o) and that o is Markov.
                                                                                      O
                            Basedonanexecutionofofromt tot+k,SMDPmethodsextractasingletrainingexample
                                  ∗ (s,o). But because o is Markov, it is, in a sense, also initiated at each of the steps
                            for Q
                                  O                                                to s   are also valid experiences
                            betweent andt+k.Thejumpsfromeachintermediates
                                                                                  i   t+k ∗ (s ,o). Or consider an
                            with o, experiences that can be used to improve estimates of Q    i
                                                                                           O
                            option that is very similar to o and which would have selected the same actions, but which
                            would have terminated one step later, at t + k + 1 rather than at t + k. Formally this is
                            a different option, and formally it was not executed, yet all this experience could be used
                            for learning relevant to it. In fact, an option can often learn something from experience
                            that is only slightly related (occasionally selecting the same actions) to what would be
                            generatedby executingthe option. This is the idea of off-policytraining—to make full use
                            of whatever experience occurs to learn as much as possible about all options irrespective
                            of their role in generating the experience. To make the best use of experience we would
                            like off-policy and intra-option versions of value-learning methods such as Q-learning.
                               It is convenient to introduce new notation for the value of a state–option pair given that
                            the option is Markov and executing upon arrival in the state:
                                    ∗                    ∗                    ∗     0
                                  U (s,o)= 1−β(s) Q (s,o)+β(s)maxQ (s,o).
                                    O                     O             o0∈O   O
                                                                                     ∗
                            Then we can write Bellman-like equations that relate Q (s,o) to expected values of
                              ∗   0            0                                     O
                            UO(s ,o),wheres is the immediate successor to s after initiating Markov option o =
                            hI,π,βi in s:     X
                                    ∗                              ∗  0       	
                                  Q (s,o)=         π(s,a)E r +γU (s ,o)s,a
                                    O         a∈A                   O
                                                  s                            
                                           = Xπ(s,a) ra+Xpa0U∗(s0,o) ,                                        (20)
                                                            s        ss  O
                                              a∈A                s0
                                                  s
                            where r is the immediate reward upon arrival in s0. Now consider learning methods based
                            on this Bellman equation. Suppose action a is taken in state s to produce next state s
                                                                      t                 t                      t+1
                            and reward r    ,andthata was selected in a way consistent with the Markov policy
                                         t+1            t
                            π of an option o =hI,π,βi. That is, suppose that at was selected according to the
                            distribution π(s ,·). Then the Bellman equation above suggests applying the off-policy
                                            t
                            one-step temporal-differenceupdate:
                                                                                        
                                  Q(s ,o)←Q(s ,o)+α r           +γU(s ,o) −Q(s ,o),                           (21)
                                      t          t          t+1         t+1          t
                            where
                                                                               0
                                  U(s,o)= 1−β(s) Q(s,o)+β(s)maxQ(s,o ).
                                                                     o0∈O
                            The method we call one-step intra-option Q-learning applies this update rule to every
                            option o consistent with every action taken, at. Note that the algorithm is potentially
                            dependent on the order in which options are updated because, in each update, U(s,o)
                            depends on the current values of Q(s,o) for other options o0. If the options’ policies are


===== Page 25 =====
                                                                                                  R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                                                                    205
                                                          deterministic,then the conceptofconsistencyaboveisclear,andforthiscasewecanprove
                                                          convergence.Extensionsto stochastic options are a topic of current research.
                                                          Theorem3 (Convergenceof intra-option Q-learning). For any set of Markov options, O,
                                                          with deterministic policies, one-step intra-option Q-learning converges with probability 1
                                                                                                                 ∗
                                                          to the optimal Q-values, Q , for every option regardless of what options are executed
                                                                                                                 O
                                                          during learning, provided that every action gets executed in every state inﬁnitely often.
                                                                                                                                                                      0     0
                                                          Proof (Sketch). On experiencing the transition, (s,a,r,s), for every option o that picks
                                                          action a in state s, intra-option Q-learning performs the following update:
                                                                                                                               0                   0                           
                                                                      Q(s,o)←Q(s,o)+α(s,o) r +γU(s,o)−Q(s,o) .
                                                          Our result follows directly from Theorem 1 of [30] and the observation that the expected
                                                                                                                    0                 0
                                                          value of the update operator r +γU(s,o)yields a contraction, proved below:
                                                                           0                    0       	           ∗             
                                                                                                                                   
                                                                       E r +γU(s,o) −Q (s,o)
                                                                                          X                          O                           
                                                                                a                    a           0                 ∗             
                                                                                                                                                 
                                                                          = r +                    p 0U(s,o)−Q (s,o)
                                                                                s                    ss                            O             
                                                                                            s0                                                                              
                                                                                a         X a                    0               a       X a ∗ 0 
                                                                                                                                                                            
                                                                          = r +                    p 0U(s,o)−r −                                  p 0U (s ,o)
                                                                                s                    ss                          s                  ss      O               
                                                                                            s0                                             s0
                                                                               X a h                               0             0                 ∗       0       
                                                                          6             p 0 1−β(s) Q(s ,o)−Q (s ,o)
                                                                                    0      ss                                                       O
                                                                                    s                                                                                            i
                                                                                                              0                     0     0                     ∗       0     0    
                                                                                                   +β(s)maxQ(s ,o)−maxQ (s ,o) 
                                                                               X                                  o0∈O                            o0∈O          O                  
                                                                                          a                      00     00            ∗       00     00  
                                                                          6            p 0maxQ(s ,o )−Q (s ,o )
                                                                                   0      ss s00,o00                                   O
                                                                                 s                                                           
                                                                                                     00     00            ∗       00     00  
                                                                          6γmax Q(s ,o )−Q (s ,o ) .                                                       2
                                                                                   s00,o00                                 O
                                                               As an illustration, we applied this intra-option method to the rooms example, this time
                                                          with the goal in the rightmost hallway, cell G1 in Fig. 2. Actions were selected randomly
                                                          with equal probability from the four primitives. The update (21) was applied ﬁrst to the
                                                          primitive options, then to any of the hallway options that were consistent with the action.
                                                          Thehallwayoptionswereupdatedinclockwiseorder,startingfromanyhallwaysthatfaced
                                                          up from the current state. The rewards were the same as in the experiment in the previous
                                                          section.Fig.10showslearningcurvesdemonstratingtheeffectivelearningofoptionvalues
                                                          without ever selecting the corresponding options.
                                                               Intra-optionversionsofotherreinforcementlearningmethodssuchasSarsa,TD(λ),and
                                                          eligibility-trace versions of Sarsa and Q-learning should be straightforward,althoughthere
                                                          has been no experience with them. The intra-option Bellman equation (20) could also be
                                                          used for intra-option sample-based planning.


===== Page 26 =====
                           206               R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                           Fig. 10. The learning of option values by intra-option methods without ever selecting the options. Experience was
                           generated by selecting randomly among actions, with the goal at G1. Shown on the left is the value of the greedy
                           policy, averaged over all states and 30 repetitions of the experiment, as compared with the value of the optimal
                           policy. The right panel shows the learned option values for state G2 approaching their correct values.
                           7. Subgoals for learning options
                             Perhaps the most important aspect of working between MDPs and SMDPs is that the
                           options making up the SMDP actions may be changed. We have seen one way in which
                           this can be done by changing their termination conditions. Perhaps more fundamentalthan
                           that is changingtheirpolicies,whichweconsiderbrieﬂyinthissection.Itisnaturaltothink
                           of options as achieving subgoals of some kind, and to adapt each option’s policy to better
                           achieve its subgoal. For example, if the option is open-the-door, then it is natural to
                           adaptits policyovertimetomakeitmoreeffectiveandefﬁcientatopeningthedoor,which
                           may make it more generally useful. It is possible to have many such subgoals and learn
                           aboutthemeachindependentlyusinganoff-policylearningmethodsuchasQ-learning,as
                           in [17,31,38,66,78].In this section we develop this idea within the options framework and
                           illustrate it by learning the hallway options in the rooms example. We assume the subgoals
                           are given and do not address the larger question of the source of the subgoals.
                             Asimplewaytoformulateasubgoalforanoptionistoassignaterminalsubgoalvalue,
                           g(s), to each state s in a subset of states G ⊆ S. These values indicate how desirable it is
                           for the option to terminate in each state in G. For example, to learn a hallway option in the
                           rooms task, the target hallway might be assigned a subgoal value of +1 while the other
                           hallway and all states outside the room might be assigned a subgoal value of 0. Let O
                                                                                                          g
                           denote the set of options that terminate only and always in G (i.e., for which β(s)= 0for
                           s/∈ G and β(s)=1fors ∈G). Given a subgoal-value function g:G →R, one can deﬁne
                           a new state-value function, denoted Vo(s), for options o ∈ O , as the expected value of
                                                             g                    g
                           the cumulative reward if option o is initiated in state s, plus the subgoal value g(s0) of the
                           state s0 in which it terminates, both discounted appropriately. Similarly, we can deﬁne a
                                                    o          ao
                           newaction-valuefunction Q (s,a)=V    (s) for actions a ∈ A and options o ∈ O .
                                                    g         g                    s                 g
                             Finally, we can deﬁne optimal value functions for any subgoal g:
                                  ∗           o             ∗              o
                                V (s)= maxV (s) and Q (s,a)= maxQ (s,a).
                                 g      o∈O   g             g       o∈O    g
                                           g                            g


===== Page 27 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       207
                                      Fig. 11. Learning subgoal-achieving hallway options under random behavior. Shown on the left is the error
                                                               ∗
                                      between Q (s,a)andQ (s,a)averagedovers ∈I,a ∈A,and30repetitions. Therightpanelshowsthelearned
                                                 g             g
                                      state values (maximum over action values) for two options at state G2 approaching their correct values.
                                      Finding an option that achieves these maximums (an optimal option for the subgoal) is
                                      then a well deﬁned subtask. For Markov options, this subtask has Bellman equations and
                                      methods for learning and planning just as in the original task. For example, the one-step
                                                                                                                             ∗
                                      tabular Q-learning method for updating an estimate Q (s ,a) of Q (s ,a) is
                                                                                                           g   t   t         g   t   t
                                              Q (s ,a)←Q (s ,a)+αhr                       +γmaxQ (s             ,a)−Q (s ,a)i,
                                                g   t    t        g   t   t          t+1          a     g   t+1            g   t   t
                                                                if s    ∈/ G,
                                                                    t+1                                                 
                                              Q (s ,a)←Q (s ,a)+α r                       +γg(s )−Q (s ,a) ,
                                                g   t    t        g   t   t          t+1           t+1         g   t   t
                                                                if s    ∈G.
                                                                    t+1
                                         As a simple example, we applied this method to learn the policies of the eight hallway
                                      optionsintheroomsexample.Eachoptionwasassignedsubgoalvaluesof+1forthetarget
                                      hallwayand0forallstates outsidetheoption’sroom,includingtheoff-targethallway.The
                                      initial state was that in the upper left corner, actions were selected randomly with equal
                                      probability, and there was no goal state. The parameters were γ = 0.9andα = 0.1. All
                                      rewards were zero. Fig. 11 shows the learned values for the hallway subgoals reliably
                                      approachingtheir ideal values.
                                      8. Conclusion
                                         Representing knowledge ﬂexibly at multiple levels of temporal abstraction has the
                                      potential to greatly speed planning and learning on large problems. We have introduced
                                      a framework for doing this within the context of reinforcement learning and MDPs. This
                                      context enables us to handle stochastic environments, closed-loop policies, and goals
                                      in a more general way than has been possible in classical AI approaches to temporal
                                      abstraction. Our framework is also clear enough to be learned, used, and interpreted
                                      mechanically,aswehaveshownbyexhibitingsimpleproceduresforlearningandplanning
                                      with options, for learning models of options, and for creating new options from subgoals.


===== Page 28 =====
               208        R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
                ThefoundationofthetheoryofoptionsisprovidedbytheexistingtheoryofSMDPsand
               associated learning methods. The fact that each set of options deﬁnes an SMDP provides
               a rich set of planning and learning methods, convergence theory, and an immediate,
               natural, and general way of analyzing mixtures of actions at different time scales. This
               theory offers a lot, but still the most interesting cases are beyond it because they involve
               interrupting, constructing, or otherwise decomposing options into their constituent parts.
               It is the intermediate ground between MDPs and SMDPs that seems richest in possibilities
               for new algorithms and results. In this paper we have broken this ground and touched on
               manyoftheissues,butthereisfarmorelefttobedone.Keyissuessuchastransferbetween
               subtasks,thesourceofsubgoals,andintegrationwithstateabstractionremainincompletely
               understood. The connection between options and SMDPs provides only a foundation for
               addressing these and other issues.
                Finally, although this paper has emphasized temporally extended action, it is interesting
               to note that there may be implications for temporally extended perception as well. It is
               now common to recognize that action and perception are intimately linked. To see the
               objects in a room is not so much to label or locate them as it is to know what opportunities
               they afford for action: a door to open, a chair to sit on, a book to read, a person to talk
               to. If the temporally extended actions are modeled as options, then perhaps the models of
               the options correspond well to these perceptions. Consider a robot learning to recognize
               its battery charger. The most useful concept for it is the set of states from which it can
               successfully dock with the charger, and this is exactly what would be produced by the
               modelofadockingoption.Thesekindsofaction-orientedconceptsare appealingbecause
               theycanbetestedandlearnedbytherobotwithoutexternalsupervision,aswehaveshown
               in this paper.
               Acknowledgement
                The authors gratefully acknowledge the substantial help they have received from many
               colleagues who have shared their related results and ideas with us over the long period
               during which this paper was in preparation, especially Amy McGovern, Ron Parr, Tom
               Dietterich, Andrew Fagg, B. Ravindran, Manfred Huber, and Andy Barto. We also thank
               Leo Zelevinsky, Csaba Szepesvári, Paul Cohen, Robbie Moll, Mance Harmon, Sascha
               Engelbrecht, and Ted Perkins. This work was supported by NSF grant ECS-9511805 and
               grantAFOSR-F49620-96-1-0254,bothtoAndrewBartoandRichardSutton.DoinaPrecup
               also acknowledges the support of the Fulbright foundation. Satinder Singh was supported
               by NSF grant IIS-9711753. An earlier version of this paper appeared as University of
               Massachusetts Technical Report UM-CS-1998-074.
               References
                [1] E.G. Araujo, R.A. Grupen, Learning control composition in a complex environment, in: Proc. 4th
                 International Conference on Simulation of Adaptive Behavior, 1996, pp. 333–342.
                [2] M. Asada, S. Noda, S. Tawaratsumida, K. Hosada, Purposive behavior acquisition for a real robot by vision-
                 based reinforcement learning, Machine Learning 23 (1996) 279–303.


===== Page 29 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       209
                                       [3] A.G. Barto, S.J. Bradtke, S.P. Singh, Learning to act using real-time dynamic programming, Artiﬁcial
                                           Intelligence 72 (1995) 81–138.
                                       [4] C. Boutilier, R.I. Brafman, C. Geib, Prioritized goal decomposition of Markov decision processes: Toward
                                           a synthesis of classical and decision theoretic planning, in: Proc. IJCAI-97, Nagoya, Japan, 1997, pp. 1162–
                                           1165.
                                       [5] S.J. Bradtke, M.O. Duff, Reinforcement learning methods for continuous-time Markov decision problems,
                                           in: Advances in Neural Information Processing Systems 7, MIT Press, Cambridge, MA, 1995, pp. 393–400.
                                       [6] R.I. Brafman, M. Tennenholtz, Modeling agents as qualitative decision makers, Artiﬁcial Intelligence 94 (1)
                                           (1997) 217–268.
                                       [7] R.W. Brockett, Hybrid models for motion control systems, in: Essays in Control: Perspectives in the Theory
                                           and its Applications, Birkhäuser, Boston, MA, 1993, pp. 29–53.
                                       [8] L. Chrisman, Reasoning about probabilistic actions at multiple levels of granularity, in: Proc. AAAI Spring
                                           Symposium: Decision-Theoretic Planning, Stanford University, 1994.
                                       [9] M. Colombetti, M. Dorigo, G. Borghi, Behavior analysis and training: A methodology for behavior
                                           engineering, IEEE Trans. Systems Man Cybernet. Part B 26 (3) (1996) 365–380.
                                      [10] R.H. Crites, A.G. Barto, Improving elevator performance using reinforcement learning, in: Advances in
                                           Neural Information Processing Systems 8, MIT Press, Cambridge, MA, 1996, pp. 1017–1023.
                                      [11] P. Dayan, Improving generalization for temporal difference learning: The successor representation, Neural
                                           Computation 5 (1993) 613–624.
                                      [12] P. Dayan, G.E. Hinton, Feudal reinforcement learning, in: Advances in Neural Information Processing
                                           Systems 5, Morgan Kaufmann, San Mateo, CA, 1993, pp. 271–278.
                                      [13] T. Dean, L.P. Kaelbling, J. Kirman, A. Nicholson, Planning under time constraints in stochastic domains,
                                           Artiﬁcial Intelligence 76 (1–2) (1995) 35–74.
                                      [14] T. Dean, S.-H. Lin, Decomposition techniques for planning in stochastic domains, in: Proc. IJCAI-95,
                                           Montreal, Quebec, Morgan Kaufmann, San Mateo, CA, 1995, pp. 1121–1127. See also Technical Report
                                           CS-95-10, Brown University, Department of Computer Science, 1995.
                                      [15] G.F. DeJong, Learning to plan in continuous domains, Artiﬁcial Intelligence 65 (1994) 71–141.
                                      [16] T.G. Dietterich, The MAXQ method for hierarchical reinforcement learning, in: Machine Learning: Proc.
                                           15th International Conference, Morgan Kaufmann, San Mateo, CA, 1998, pp. 118–126.
                                      [17] M. Dorigo, M. Colombetti, Robot shaping: Developing autonomous agents through learning, Artiﬁcial
                                           Intelligence 71 (1994) 321–370.
                                      [18] G.L.Drescher, MadeUpMinds:AConstructivist ApproachtoArtiﬁcialIntelligence, MITPress,Cambridge,
                                           MA,1991.
                                      [19] C. Drummond, Composing functions to speed up reinforcement learning in a changing world, in: Proc. 10th
                                           European Conference on Machine Learning, Springer, Berlin, 1998.
                                      [20] O. Etzioni, Why PRODIGY/EBL works, in: Proc. AAAI-90, Boston, MA, MIT Press, Cambridge, MA,
                                           1990, pp. 916–922.
                                      [21] R.E. Fikes, P.E. Hart, N.J. Nilsson, Learning and executing generalized robot plans, Artiﬁcial Intelligence 3
                                           (1972) 251–288.
                                      [22] H. Geffner, B. Bonet, High-level planning and control with incomplete information using POMDPs, in:
                                           Proc. AIPS-98 Workshop on Integrating Planning, Scheduling and Execution in Dynamic and Uncertain
                                           Environments, 1998.
                                      [23] J. Gratch, G. DeJong, A statistical approach to adaptive problem solving, Artiﬁcial Intelligence 88 (1–2)
                                           (1996) 101–161.
                                      [24] R. Greiner, I. Jurisica, A statistical approach to solving the EBL utility problem, in: Proc. AAAI-92, San
                                           Jose, CA, 1992, pp. 241–248.
                                      [25] R.L. Grossman, A. Nerode, A.P. Ravn, H. Rischel, Hybrid Systems, Springer, New York, 1993.
                                      [26] M. Hauskrecht, N. Meuleau, C. Boutilier, L.P. Kaelbling, T. Dean, Hierarchical solution of Markov decision
                                           processes using macro-actions, in: Uncertainty in Artiﬁcial Intelligence: Proc. 14th Conference, 1998,
                                           pp. 220–229.
                                      [27] R. Howard, Dynamic Programming and Markov Processes, MIT Press, Cambridge, MA, 1960.
                                      [28] M. Huber, R.A. Grupen, A feedback control structure for on-line learning tasks, Robotics and Autonomous
                                           Systems 22 (3–4) (1997) 303–315.
                                      [29] G.A. Iba, A heuristic approach to the discovery of macro-operators, Machine Learning 3 (1989) 285–317.


===== Page 30 =====
               210        R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211
               [30] T. Jaakkola, M.I. Jordan, S. Singh, On the convergence of stochastic iterative dynamic programming
                 algorithms, Neural Computation 6 (6) (1994) 1185–1201.
               [31] L.P. Kaelbling, Hierarchical learning in stochastic domains: Preliminary results, in: Proc. 10th International
                 Conference on Machine Learning, Morgan Kaufmann, San Mateo, CA, 1993, pp. 167–173.
               [32] Zs. Kalmár, Cs. Szepesvári, A. Lörincz, Module based reinforcement learning: Experiments with a real
                 robot, Machine Learning 31 (1998) 55–85 and Autonomous Robots 5 (1998) 273–295 (special joint issue).
               [33] J. de Kleer, J.S. Brown, A qualitative physics based on conﬂuences, Artiﬁcial Intelligence 24 (1–3) (1984)
                 7–83.
               [34] R.E. Korf, Learning to Solve Problems by Searching for Macro-Operators, Pitman Publishers, Boston, MA,
                 1985.
               [35] J.R. Koza, J.P. Rice, Automatic programming of robots using genetic programming, in: Proc. AAAI-92, San
                 Jose, CA, 1992, pp. 194–201.
               [36] B.J. Kuipers, Commonsense knowledge of space: Learning from experience, in: Proc. IJCAI-79, Tokyo,
                 Japan, 1979, pp. 499–501.
               [37] J.E. Laird, P.S. Rosenbloom, A. Newell, Chunking in SOAR: Theanatomy of a general learning mechanism,
                 Machine Learning 1 (1986) 11–46.
               [38] L.-J. Lin, Reinforcement learning for robots using neural networks, Ph.D. Thesis, Carnegie Mellon
                 University, Technical Report CMU-CS-93-103, 1993.
               [39] P. Maes, R. Brooks, Learning to coordinate behaviors, in: Proc. AAAI-90, Boston, MA, 1990, pp. 796–802.
               [40] S. Mahadevan, J. Connell, Automatic programming of behavior-based robots using reinforcement learning,
                 Artiﬁcial Intelligence 55 (2–3) (1992) 311–365.
               [41] S. Mahadevan, N. Marchalleck, T. Das, A. Gosavi, Self-improving factory simulation using continuous-time
                 average-reward reinforcement learning, in: Proc. 14th International Conference on Machine Learning, 1997,
                 pp. 202–210.
               [42] P. Marbach, O. Mihatsch, M. Schulte, J.N. Tsitsiklis, Reinforcement learning for call admission control in
                 routing in integrated service networks, in: Advances in Neural Information Processing Systems 10, Morgan
                 Kaufmann, San Mateo, CA, 1998, pp. 922–928.
               [43] M.J. Mataric, Behavior-based control: Examples from navigation, learning, and group behavior, J. Experi-
                 ment. Theoret. Artiﬁcial Intelligence 9 (2–3) (1997) 323–336.
               [44] A. McGovern, R.S. Sutton, Macro-actions in reinforcement learning: An empirical analysis, Technical
                 Report 98-70, University of Massachusetts, Department of Computer Science, 1998.
               [45] N. Meuleau, M. Hauskrecht, K.-E.Kim,L. Peshkin, L.P.Kaelbling, T.Dean, C. Boutilier, Solving very large
                 weakly coupled Markov decision processes, in: Proc. AAAI-98, Madison, WI, 1998, pp. 165–172.
               [46] S. Minton, Learning Search Control Knowledge: An Explanation-Based Approach, Kluwer Academic,
                 Dordrecht, 1988.
               [47] S. Minton, Quantitative results concerning the utilty of explanation-based learning, Artiﬁcial Intelligence 42
                 (2–3) (1990) 363–391.
               [48] A.W. Moore, The parti-game algorithm for variable resolution reinforcement learning in multidimensional
                 spaces, in: Advances in Neural Information Processing Systems 6, MIT Press, Cambridge, MA, 1994,
                 pp. 711–718.
               [49] A. Newell, H.A. Simon, Human Problem Solving, Prentice-Hall, Englewood Cliffs, NJ, 1972.
               [50] J. Nie, S. Haykin, A Q-learning based dynamic channel assignment technique for mobile communication
                 systems, IEEE Transactions on Vehicular Technology, to appear.
               [51] N. Nilsson, Teleo-reactive programs for agent control, J. Artiﬁcial Intelligence Res. 1 (1994) 139–158.
               [52] R. Parr, Hierarchical control and learning for Markov decision processes, Ph.D. Thesis, University of
                 California at Berkeley, 1998.
               [53] R.Parr, S.Russell, Reinforcement learning with hierarchies ofmachines, in: Advances in Neural Information
                 Processing Systems 10, MIT Press, Cambridge, MA, 1998, pp. 1043–1049.
               [54] D. Precup, R.S. Sutton, Multi-time models for reinforcement learning, in: Proc. ICML’97 Workshop on
                 Modeling in Reinforcement Learning, 1997.
               [55] D. Precup, R.S. Sutton, Multi-time models for temporally abstract planning, in: Advances in Neural
                 Information Processing Systems 10, MIT Press, Cambridge, MA, 1998, pp. 1050–1056.
               [56] D. Precup, R.S. Sutton, S.P. Singh, Planning with closed-loop macro actions, in: Working Notes 1997 AAAI
                 Fall Symposium on Model-directed Autonomous Systems, 1997, pp. 70–76.


===== Page 31 =====
                                                                R.S. Sutton et al. / Artiﬁcial Intelligence 112 (1999) 181–211                       211
                                      [57] D. Precup, R.S. Sutton, S.P. Singh, Theoretical results on reinforcement learning with temporally abstract
                                           options, in: Proc. 10th European Conference on Machine Learning, Springer, Berlin, 1998.
                                      [58] M.L. Puterman, Markov Decision Problems, Wiley, New York, 1994.
                                      [59] M. Ring, Incremental development of complex behaviors through automatic construction of sensory-motor
                                           hierarchies, in: Proc. 8th International Conference on Machine Learning, Morgan Kaufmann, San Mateo,
                                           CA,1991, pp. 343–347.
                                      [60] E.D. Sacerdoti, Planning in a hierarchy of abstraction spaces, Artiﬁcial Intelligence 5 (1974) 115–135.
                                      [61] S. Sastry, Algorithms for design of hybrid systems, in: Proc. International Conference of Information
                                           Sciences, 1997.
                                      [62] A.C.C. Say, S. Kuru, Qualitative system identiﬁcation: Deriving structure from behavior, Artiﬁcial
                                           Intelligence 83 (1) (1996) 75–141.
                                      [63] J. Schmidhuber, Neural sequence chunkers, Technische Universität München, TR FKI-148-91, 1991.
                                      [64] R. Simmons,S.Koenig,Probabilistic robot navigation in partially observable environments, in: Proc. IJCAI-
                                           95, Montreal, Quebec, Morgan Kaufmann, San Mateo, CA, 1995, pp. 1080–1087.
                                      [65] S.P. Singh, Reinforcement learning with a hierarchy of abstract models, in: Proc. AAAI-92, San Jose, CA,
                                           MIT/AAAIPress,Cambridge, MA,1992, pp. 202–207.
                                      [66] S.P. Singh, Scaling reinforcement learning by learning variable temporal resolution models, in: Proc. 9th
                                           International Conference on Machine Learning, Morgan Kaufmann, San Mateo, CA, 1992, pp. 406–415.
                                      [67] S.P. Singh, The efﬁcient learning of multiple task sequences, in: Advances in Neural Information Processing
                                           Systems 4, Morgan Kaufmann, San Mateo, CA, 1992, pp. 251–258.
                                      [68] S.P. Singh, Transfer of learning by composing solutions of elemental sequential tasks, Machine Learning 8
                                           (3/4) (1992) 323–340.
                                      [69] S.P. Singh, A.G. Barto, R.A. Grupen, C.I. Connolly, Robust reinforcement learning in motion planning, in:
                                           Advances in Neural Information Processing Systems 6, Morgan Kaufmann, San Mateo, CA, 1994, pp. 655–
                                           662.
                                      [70] S.P. Singh, D. Bertsekas, Reinforcement learning for dynamic channel allocation in cellular telephone
                                           systems, in: Advances in Neural Information Processing Systems 9, MIT Press, Cambridge, MA, 1997,
                                           pp. 974–980.
                                      [71] R.S. Sutton, TD models: Modeling the world at a mixture of time scales, in: Proc. 12th International
                                           Conference on Machine Learning, Morgan Kaufmann, San Mateo, CA, 1995, pp. 531–539.
                                      [72] R.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction, MIT Press, Cambridge, MA, 1998.
                                      [73] R.S. Sutton, B. Pinette, The learning of world models by connectionist networks, in: Proc. 7th Annual
                                           Conference of the Cognitive Science Society, 1985, pp. 54–64.
                                      [74] R.S. Sutton, D. Precup, S. Singh, Intra-option learning about temporally abstract actions, in: Proc. 15th
                                           International Conference on Machine Learning, Morgan Kaufmann, San Mateo, CA, 1998, pp. 556–564.
                                      [75] R.S. Sutton, S. Singh, D. Precup, B. Ravindran, Improved switching among temporally abstract actions, in:
                                           Advances in Neural Information Processing Systems 11, MIT Press, Cambridge, MA, 1999, pp. 1066–1072.
                                      [76] M. Tambe, A. Newell, P. Rosenbloom, The problem of expensive chunks and its solution by restricting
                                           expressiveness, Machine Learning 5 (3) (1990) 299–348.
                                      [77] G.J. Tesauro, Temporal difference learning and TD-Gammon, Comm. ACM 38 (1995) 58–68.
                                      [78] T. Thrun, A. Schwartz, Finding structure in reinforcement learning, in: Advances in Neural Information
                                           Processing Systems 7, Morgan Kaufmann, San Mateo, CA, 1995, pp. 385–392.
                                      [79] M. Uchibe, M. Asada, K. Hosada, Behavior coordination for a mobile robot using modular reinforcement
                                           learning, in: Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems, 1996, pp. 1329–
                                           1336.
                                      [80] C.J.C.H. Watkins, Learning with delayed rewards, Ph.D. Thesis, Cambridge University, 1989.
                                      [81] M. Wiering, J. Schmidhuber, HQ-learning, Adaptive Behavior 6 (2) (1997) 219–246.
                                      [82] L.E. Wixson, Scaling reinforcement learning techniques via modularity, in: Proc. 8th International
                                           Conference on Machine Learning, Morgan Kaufmann, San Mateo, CA, 1991, pp. 368–372.
