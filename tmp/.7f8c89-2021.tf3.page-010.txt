                196                                                                                        M.-H. Guo, J.-X. Cai, Z.-N. Liu, et al.
                categories. For fair comparison, we use the same data               5     Conclusions
                processing method as Ref. [1]. Table 4 shows that                   In this paper, we propose a permutation-invariant
                our PCT achieves superior performance compared to                   point cloud transformer, which is suitable for learning
                the previous methods.                                               on unstructured point clouds with irregular domain.
                4.5     Computational requirements analysis                         The proposed oﬀset-attention and normalization
                We now consider the computational requirements                      mechanisms help to make our PCT eﬀective.
                of NPCT, SPCT, PCT, and several other methods                       Experiments show that PCT has good semantic
                by comparing the ﬂoating point operations required                  feature learning capability, and achieves state-of-
                (FLOPs) and number of parameters (Params) in                        the-art performance on several tasks, particularly
                Table 5. SPCT has the lowest memory requirements                    shape classiﬁcation, part segmentation, and normal
                with only 1.36M parameters and also puts a low load                 estimation.
                on the processor of only 1.82G FLOPs, yet delivers                     Transformer       has    already     revealed     powerful
                highly accurate results. These characteristics make it              capabilities given large amounts of training data.
                suitable for deployment on a mobile device. PCT                     At present, the available point cloud datasets are
                has best performance, yet modest computational                      very limited compared to image. In future, we will
                and memory requirements.              If we pursue higher           train it on larger datasets and study its advantages
                performance and ignore the amount of calculation                    and disadvantages with respect to other popular
                and parameters, we can add a neighbor embedding                     frameworks.        The encoder–decoder structure of
                layer in the input embedding module. The results                    Transformer supports more complex tasks, such as
                of 3-layer embedding PCT are shown in Tables 6                      point cloud generation and completion.                We will
                and 7.                                                              extend the PCT to further applications. Besides, we
                                                                                    will attempt more precise methods to approximate
                          Table 5 Computational resource requirements               Laplacian operation and complete oﬀset-attention.
                        Method               #Params     #FLOPs       Accuracy      Acknowledgements
                  PointNet [1]                3.47M       0.45G        89.2%        This work was supported by the National Natural
                  PointNet++(SSG) [21]        1.48M        1.68G       90.7%        Science Foundation of China (Project Number
                  PointNet++(MSG) [21]        1.74M        4.09G       91.9%        61521002) and the Joint NSFC–DFG Research
                  DGCNN[26]                   1.81M        2.43G       92.9%        Program (Project Number 61761136018).
                  NPCT                        1.36M        1.80G       91.0%
                  SPCT                        1.36M        1.82G       92.0%        References
                  PCT                         2.88M        2.32G       93.2%
                                                                                      [1] Charles, R. Q.; Hao, S.; Mo, K. C.; Guibas, L.
                                                                                          J. PointNet:    Deep learning on point sets for 3D
                Table 6 Comparison on the ModelNet40 classiﬁcation dataset. PCT-          classiﬁcation and segmentation. IN: Proceedings of the
                2L means PCT with 2 layer neighbor embedding and PCT-3L means             IEEE Conference on Computer Vision and Pattern
                PCT with 3 layer neighbor embedding. Accuracy means overall
                accuracy. P = points                                                      Recognition, 77–85, 2017.
                    Method          Input         #Points         Accuracy            [2] Tchapmi, L. P.; Choy, C. B.; Armeni, I.; Gwak, J.;
                    PCT-2L             P             1k             93.2%                 Savarese, S. SEGCloud: Semantic segmentation of
                    PCT-3L             P             1k             93.4%                 3D point clouds. In: Proceedings of the International
                                                                                          Conference on 3D Vision, 537–547, 2017.
                Table 7 Comparison on the ShaperNet part segmentation dataset. pIoU means part-average Intersection-over-Union. PCT-2L means PCT
                with 2 layer neighbor embedding and PCT-3L means PCT with 3 layer neighbor embedding
                 Method pIoU        air-   bag   cap    car   chair  ear-   guitar knife  lamp laptop motor- mug pistol rocket skate- table
                                   plane                            phone                                 bike                        board
                 PCT-2L 86.4        85.0  82.4   89.0  81.2   91.9   71.5    91.3  88.1   86.3    95.8    64.6   95.8   83.6   62.2    77.6   83.7
                 PCT-3L 86.6 85.3 84.5 89.4 81.0              91.7   78.6   91.5    87.5  85.8   96.0     70.6   95.6   82.8    60.9   76.6   83.7
