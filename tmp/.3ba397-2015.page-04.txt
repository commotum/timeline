                                    VGG-19              34-layer plain                   34-layer residual                       Residual Network. Based on the above plain network, we
                                       image                        image                        image                           insert shortcut connections (Fig. 3, right) which turn the
                      output                                                                                                     network into its counterpart residual version. The identity
                                    3x3 conv, 64
                      size: 224                                                                                                  shortcuts (Eqn.(1)) can be directly used when the input and
                                    3x3 conv, 64
                                                                                                                                 output are of the same dimensions (solid line shortcuts in
                                      pool, /2
                      output                                                                                                     Fig.3). Whenthedimensionsincrease(dottedlineshortcuts
                      size: 112
                                    3x3 conv, 128                                                                                in Fig. 3), we consider two options: (A) The shortcut still
                                    3x3 conv, 128               7x7 conv, 64, /2             7x7 conv, 64, /2                    performs identity mapping, with extra zero entries padded
                                      pool, /2                      pool, /2                     pool, /2
                      output                                                                                                     for increasing dimensions. This option introduces no extra
                      size: 56
                                    3x3 conv, 256                 3x3 conv, 64                 3x3 conv, 64                      parameter; (B) The projection shortcut in Eqn.(2) is used to
                                    3x3 conv, 256                 3x3 conv, 64                 3x3 conv, 64                      match dimensions (done by 1×1 convolutions). For both
                                    3x3 conv, 256                 3x3 conv, 64                 3x3 conv, 64                      options, when the shortcuts go across feature maps of two
                                    3x3 conv, 256                 3x3 conv, 64                 3x3 conv, 64                      sizes, they are performed with a stride of 2.
                                                                  3x3 conv, 64                 3x3 conv, 64                      3.4. Implementation
                                                                  3x3 conv, 64                 3x3 conv, 64
                                      pool, /2                  3x3 conv, 128, /2            3x3 conv, 128, /2                        Our implementation for ImageNet follows the practice
                      output 
                      size: 28
                                    3x3 conv, 512                3x3 conv, 128                3x3 conv, 128                      in [21, 41]. The image is resized with its shorter side ran-
                                    3x3 conv, 512                3x3 conv, 128                3x3 conv, 128                      domly sampled in [256,480] for scale augmentation [41].
                                    3x3 conv, 512                3x3 conv, 128                3x3 conv, 128                      A224×224cropisrandomlysampledfromanimageorits
                                    3x3 conv, 512                3x3 conv, 128                3x3 conv, 128                      horizontal ﬂip, with the per-pixel mean subtracted [21]. The
                                                                 3x3 conv, 128                3x3 conv, 128                      standardcoloraugmentationin[21]isused. Weadoptbatch
                                                                                                                                 normalization (BN) [16] right after each convolution and
                                                                 3x3 conv, 128                3x3 conv, 128
                                                                                                                                 before activation, following [16]. We initialize the weights
                                                                 3x3 conv, 128                3x3 conv, 128
                      output                                                                                                     as in [13] and train all plain/residual nets from scratch. We
                                      pool, /2                  3x3 conv, 256, /2            3x3 conv, 256, /2
                      size: 14                                                                                                   use SGD with a mini-batch size of 256. The learning rate
                                    3x3 conv, 512                3x3 conv, 256                3x3 conv, 256                      starts from 0.1 and is divided by 10 when the error plateaus,
                                    3x3 conv, 512                3x3 conv, 256                3x3 conv, 256                      andthemodelsaretrainedforupto60×104 iterations. We
                                    3x3 conv, 512                3x3 conv, 256                3x3 conv, 256                      use a weight decay of 0.0001 and a momentum of 0.9. We
                                    3x3 conv, 512                3x3 conv, 256                3x3 conv, 256                      donotusedropout [14], following the practice in [16].
                                                                 3x3 conv, 256                3x3 conv, 256                           In testing, for comparison studies we adopt the standard
                                                                 3x3 conv, 256                3x3 conv, 256                      10-crop testing [21]. For best results, we adopt the fully-
                                                                 3x3 conv, 256                3x3 conv, 256                      convolutional form as in [41, 13], and average the scores
                                                                 3x3 conv, 256                3x3 conv, 256                      at multiple scales (images are resized such that the shorter
                                                                 3x3 conv, 256                3x3 conv, 256                      side is in {224,256,384,480,640}).
                                                                 3x3 conv, 256                3x3 conv, 256
                                                                 3x3 conv, 256                3x3 conv, 256                      4. Experiments
                      output 
                                      pool, /2                  3x3 conv, 512, /2            3x3 conv, 512, /2
                       size: 7                                                                                                   4.1. ImageNet Classiﬁcation
                                                                 3x3 conv, 512                3x3 conv, 512
                                                                 3x3 conv, 512                3x3 conv, 512                           Weevaluate our method on the ImageNet 2012 classiﬁ-
                                                                                                                                 cationdataset[36]thatconsistsof1000classes. Themodels
                                                                 3x3 conv, 512                3x3 conv, 512
                                                                                                                                 are trained on the 1.28 million training images, and evalu-
                                                                 3x3 conv, 512                3x3 conv, 512                      ated on the 50k validation images. We also obtain a ﬁnal
                                                                 3x3 conv, 512                3x3 conv, 512                      result on the 100k test images, reported by the test server.
                      output 
                                      fc 4096                      avg pool                     avg pool
                       size: 1                                                                                                   Weevaluate both top-1 and top-5 error rates.
                                      fc 4096                       fc 1000                      fc 1000
                                      fc 1000                                                                                    Plain Networks. We ﬁrst evaluate 18-layer and 34-layer
                                                                                                                                 plain nets. The 34-layer plain net is in Fig. 3 (middle). The
                     Figure 3. Example network architectures for ImageNet. Left: the                                             18-layer plain net is of a similar form. See Table 1 for de-
                     VGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-                                                 tailed architectures.
                     dle: a plain network with 34 parameter layers (3.6 billion FLOPs).                                               TheresultsinTable2showthatthedeeper34-layerplain
                     Right: a residual network with 34 parameter layers (3.6 billion                                             net has higher validation error than the shallower 18-layer
                     FLOPs). Thedottedshortcutsincreasedimensions. Table1shows                                                   plain net. To reveal the reasons, in Fig. 4 (left) we com-
                     moredetails and other variants.                                                                             pare their training/validation errors during the training pro-
                                                                                                                                 cedure. We have observed the degradation problem - the
                                                                                                                           4
