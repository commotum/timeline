                             modeling architectures (see [59] and [58]) such as global/local softmax attention [27], or other
                             modernrecurrent models [28, 60, 61]. This simple formulation of sequence models provides us with
                             better understanding of their internal process and also a tool to simply compare their modeling power
                             based on their objective and optimization process. In the following, using step-by-step examples, we
                             discuss how this formulation can be applied to all components of a neural architecture (including its
                             optimization process in pre-training) and in fact, how a model is an integrated system of multi-level,
                             nested, and or parallel memories, each of which with its own context flow.
                             ASimpleExampleofMLPTraining. Westartwithasimpleexample, in which we aim to train
                             a 1-layer MLP (parameterized with W) for task T and on dataset Dtrain = {x1,...,x|Dtrain|} by
                             optimizing the objective L(·;·) with gradient descent. In this case, the training process is equivalent
                             to the following optimization problem:
                                                                  W∗=argmin L(W;D ),                                                 (2)
                                                                               W             train
                             whoseoptimization by gradient descent results in a weight update rule equivalent to:
                                            W =W −η ∇ L(W;x )                                                                        (3)
                                               t+1      t     t+1   W       t   t+1
                                                                      t
                                                   =W −η ∇ L(W;x )⊗x , where x                                  ∼D ,                 (4)
                                                        t     t+1   yt+1      t  t+1       t+1             t+1       train
                             where y       =Wx istheoutputofthemodelforinputx                      . Given this formulation, one can
                                      t+1         t+1                                          t+1
                             let u      = ∇       L(W ;x      ) and reformulate the backpropagation process as the solution to
                                   t+1       y         t   t+1
                                              t+1
                             an optimization problem on finding an optimal associative memory that maps input data points
                                            |Dtrain|
                             D      ={x}          to their corresponding u       =∇ L(W;x ).Thatis,weletM(·)=W ·
                                train    t                                  t+1       y        t   t+1                               t
                                            t=1                                        t+1
                             parametrizes the memory, and use dot-product similarity to measure the quality of Wt’s mapping
                             between x       and ∇      L(W ;x       ):
                                         t+1        y        t   t+1
                                                     t+1
                                                                                        1                2
                                              W =argmin ⟨Wx ,u ⟩+                            ∥W−W∥                                   (5)
                                                t+1          W         t+1   t+1     2η                t 2
                                                                                        t+1
                                                                                                    1                2
                                                     =argmin ⟨Wx ,∇            L(W ;x       )⟩ +         ∥W−W∥.                      (6)
                                                                       t   y         t   t+1                       t 2
                                                             W              t+1                   2η
                                                                                                    t+1
                              In the above formulation, u      =∇ L(W;x )canbeinterpretedasalocalsurprisesignalin
                                                           t+1      yt+1      t   t+1
                             representation space that quantifies the mismatch between the current output and the structure the
                             objective L(·;·) enforces. Therefore, this formulation translates the training phase of the model as a
                             process of acquiring effective memory that maps data samples to their Local Surprise Signal (LSS) in
                             representation space–defined as the mismatch between the current output and the structure enforced
                             bythe objective L(·;·). Accordingly, in this example, our model has a single gradient flow over the
                             data samples, which is only active over dataset Dtrain = {x1,...,x|D        |} and will be frozen for any
                             other data samples afterwards (a.k.a inference or test time).             train
                             Next, in the above example, we replace the gradient descent algorithm with its enhanced momentum-
                             based variant, resulting in an update rule of:
                                        W =W −m ,                                                                                    (7)
                                          t+1       t      t+1
                                        m =m−η ∇ L(W;x )=m −η ∇ L(W;x )⊗x .                                                          (8)
                                          t+1       t     t+1   W       t   t+1       t     t+1   y         t  t+1       t+1
                                                                  t                                t+1
                             In Equation 8, given the previous state of Equation 7 (at time t), the value of ∇         L(W ;x       ) or
                                                                                                                    Wt      t   t+1
                             similarly ∇      L(W ;x       ) are independent of recurrence in Equation 8 and so can be pre-computed
                                          yt+1     t   t+1
                             beforehand. To this end, we let u       =∇ L(W;x ),andsoEquation8canbereformulatedas:
                                                                t+1       W       t   t+1
                                                                            t
                                            W =W −m ,                                                                                (9)
                                               t+1      t      t+1
                                                                                                                2
                                            m =argmin −⟨m,∇ L(W;x )⟩+η                              ∥m−m∥                          (10)
                                               t+1                       W        t  t+1        t+1           t 2
                                                           m               t
                                                                                                                       2
                                                   =argmin −⟨mx             , ∇    L(W ;x       )⟩ +η      ∥m−m∥,                  (11)
                                                           m            t+1    yt+1      t  t+1        t+1           t 2
                             where the optimization problem in Equation 10 is equivalent to on step of gradient descent with
                             adaptive learning rate of η      .  Given these formulation, one can interpret the momentum term
                                                           t+1
                             as either: (1) a key-less associative memory that compress the gradients into its parameters, or
                             (2) an associative memory that learns how to map data points to their corresponding LSS-value.
                             Interestingly, this formulation reveals that gradient descent with momentum is indeed a two-level
                                                                                  5
