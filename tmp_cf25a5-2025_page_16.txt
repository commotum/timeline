                                                   Test-Time Learning for Large Language Models
                                                          Table 6. Components of AdaptEval.
                                        Category          Dataset             Sources
                                                          Geosignal           Geoscience knowledge base, etc.
                                        DomainBench       Agriculture-QA      Agriculture data
                                                          GenMedGPT-5k        ChatGPT-generated data
                                                          Wealth-alpaca lora  FiQAdata, etc.
                                                          Dolly-15k           Databricks data
                                        InstructionBench  Alpaca gpt4 en      GPT-4instruction data
                                                          InstructionWild     User instruction data
                                                          GSM8k               OpenAIdata
                                        ReasoningBench    MetaMathQA          Advancedmathdatasets
                                                          Logiqa              Civil service logic tests
              A.3. Test-Time Adaptation
              Test-time adaptation (TTA) aims to improve a model’s performance on unseen test data, which may undergo distribution
              shifts, by learning directly from the test data during the testing phase. Based on their reliance on backward propagation, we
              categorize the related TTA works into the following two groups for discussion.
              Backpropagation (BP)-Based TTA. A foundational approach in this category is Test-Time Training (TTT) proposed by
              (Sun et al., 2020). TTT involves training a source model using both supervised and self-supervised objectives during the
              training phase. At test time, the model is adapted using self-supervised objectives such as rotation prediction (Sun et al.,
              2020), contrastive learning (Liu et al., 2021; Bartler et al., 2022), or reconstruction learning (Gandelsman et al., 2022).
              To address scenarios where modifying the training process or accessing source data is not feasible, Fully TTA methods
              directly update pre-trained models during testing. These methods rely on unsupervised learning objectives, with entropy
              minimization (Wang et al., 2021; Niu et al., 2023) emerging as one of the most widely used techniques due to its simplicity
              and effectiveness. Entropy minimization encourages the model to produce confident predictions by reducing uncertainty
              in its output distribution. This approach effectively aligns predictions to a single class. Beyond entropy minimization,
              other unsupervised objectives, such as prediction consistency maximization (Zhang et al., 2022a; Fleuret et al., 2021) and
              feature distribution alignment (Mirza et al., 2023), have also been explored, enhancing the model’s ability to adapt to diverse
              test-time scenarios.
              Tofurtherenhancetheefficiencyofbackpropagation-basedTTA,recentresearcheffortshavefocusedontowprimaryaspects:
              (1) Sample Efficiency: As not all test samples contribute equally to adaptation. Several recent works (Niu et al., 2022a;
              2023; Shu et al., 2022; Lee et al., 2024) have introduced sample selection strategies to focus on reliable and non-redundant
              samples, reducing the noise in the gradient and the number of samples for TTA, thereby enhancing adaptation performance
              and efficiency. (2) Memory Efficiency: Addressing the memory-intensive nature of backpropagation, methods such as
              EcoTTA(Songetal., 2023) optimize parameter-efficient components during adaptation, while MECTA (Hong et al., 2023)
              reduces batch size to lower memory consumption. Additionally, MECTA introduces a domain-aware batch normalization
              layer to stabilize model updates, even with smaller batch sizes. Similar to the sample efficiency methods, we propose a
              Sample-Efficient Learning Strategy that uses a perplexity-based weighting scheme to prioritize high-perplexity test samples
              for backpropagation, ensuring efficient utilization of computational resources.
              Backpropagation-Free TTA. The development of BP-free TTA has seen significant progress, with early research focusing
              primarily on the adjustment of batch normalization (BN) layer statistics using test data. These methods primarily involved
              recalculating the mean and variance of BN layers based on testing data (Nado et al., 2020; Schneider et al., 2020). However,
              such approaches were limited to adapting BN layers, restricting their applicability to architectures that heavily rely on
              BN.Toovercomethese limitations, more generalized methodologies have been proposed to enhance the flexibility and
              effectiveness of BP-free TTA. For instance, reconstruction-based approaches focusing on input-level adaptation leverage
              advanced techniques like diffusion models to preprocess corrupted test images before prediction (Gao et al., 2023; Oh et al.,
              2025). Additionally, output-level adaptation methods have been developed, such as T3A (Iwasawa & Matsuo, 2021), which
              utilizes prototype-based classifier adjustment for adaptive predictions, and LAME (Boudiaf et al., 2022), which directly
              corrects the predicted logits. The recent advanced FOA (Niu et al., 2024) adapts models to unseen test samples without
                                                                        16
