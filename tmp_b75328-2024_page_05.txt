                                             100     92.9                      97.9                               30
                                            Accuracy80                                                            25      24.3   23.9
                                              60                                                                 Accuracy20                  15.3
                                              40                                                                  15                               1.4                1.0
                                                         26.7                     30.6                                                             1                  1
                                            Exact Match                                                           10                                            8.7
                                              20            3.6     4.3               2.9    3.2                 Exact Match 5
                                               0                0      0                  0      0
                                                             ST                     ST w/ II                       0        LT                ST               ST w/ II
                                                                   Architecture Type                                                    Architecture Type
                                            Abacus, OOD               FIRE, OOD               NoPE, OOD
                                            Abacus, 100+ OOD          FIRE, 100+ OOD          NoPE, 100+ OOD                      FIRE, OOD            NoPE, OOD
                                     Figure 3:       Left:     Mean exact match accuracy of three models of depth sixteen on size 20 data,
                                     varying the architecture and embeddings. Abacus Embeddings improve accuracy for addition over
                                     FIREandNoPEEmbeddings. Right: Meanexactmatchaccuracyofthree models of effective depth
                                     sixteen on size 40 data, varying over NoPE or FIRE embeddings and architectures. Recurrent looped
                                     transformer models improve accuracy for addition for both the FIRE and NoPE embeddings.
                                     Looped transformer (LT): Weight tied decoder layers, with input injection and progressive loss.
                                     Standard Transformer (ST): Stacked decoder only layers. Standard Transformer with Input Injection
                                     (ST w/ II): Standard Transformer with input features added to the hidden representation between each
                                     decoder layer.
                                     [2024] highlight this by training models with different random seeds, varying weight initialization
                                     and data input order seeds, showing the variance in the performance of these models can vary from
                                     near perfect on 100 digit addition to 0% accuracy at 90 digit addition.
                                     Toaddressthelimitationsoftransformersatrepresentingpositionalinformation, wedesignaspecially
                                     built positional embedding that encodes the location of each digit relative to the start of the current
                                     number. We call this Abacus Embeddings. We apply the same positional embedding to all digits of
                                     the same significance, providing an explicit signal that the model can use to align digits. We visually
                                                                                          3
                                     describe these embeddings in Figure 2.
                                     Wetakeinspiration from Randomized Embeddings [Ruoss et al., 2023] but instead of using random
                                     ascending indices to represent positions in a sample, we use consecutive ascending indices with a
                                     random starting position to allow for length generalization. Specifically, during training we give
                                     consecutive positional embeddings to each digit in a number, starting from a randomly chosen offset
                                     value from U[1,k], where k is a hyperparameter. Unless otherwise stated the default value for k
                                     in this study is 100 and show this can be varied in Appendix A.5. For example, if the input is 123,
                                     the positional encodings are β,β + 1,β + 2 where β ∼ U[1,100], which are then passed through a
                                     learned embedding matrix. The value sampled from U[1,k] is the same for all numbers in a batch,
                                     meaning all digits of the same significance obtain the same positional embedding. This training
                                     schemeallowsthemodeltoseeawiderangeofpositionalembeddings,evenwhentrainingsequences
                                     are short. At test time, each positional embedding begins from one, i.e. β = 1.
                                     AbacusEmbeddingsSolveAddition. AbacusEmbeddingsimprovegeneralizationperformance
                                     upto100digitsandbeyondforstandardtransformerarchitectures. In Figure 3 (left), we highlight the
                                     comparative boost Abacus Embeddings have over standard transformer architectures and embeddings
                                     for performing addition, taking the mean accuracy of three models in all cases. The accuracy results
                                     for the standard transformer models trained with FIRE and Abacus, tested both in-domain (ID) and
                                     out-of-domain (OOD), are also shown in Figure 1. Additionally, in Appendix A.6, we present similar
                                     2Dgrid plots for several other experiments that are depicted as bar charts in the main text. Zhou
                                     et al. [2024] find that operand lengths of up to forty digits are required during training for good
                                     generalization to 100 digit addition during testing (albeit not robustly). We find that with our Abacus
                                     Embeddings, we can achieve similar accuracy and larger extrapolation using a standard model with
                                     input injection trained on maximum operand sizes of 20 digits.
                                     As Abacus Embeddings are a variant of absolute positional embeddings, technically they cannot
                                     generalize beyond the relative positions seen during training. However the hyperparameter k that
                                          3In Appendix A.3, we motivate these embeddings further with experiments demonstrating their utility in
                                     solving a bitwise OR task.
                                                                                                        5
