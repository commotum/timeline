                            4      S. He et al.
                            28,36,55], developing multi-modal LLMs. Flamingo [2], BLIP-2 [31], mPLUG-
                            OWL[73],Otter[30], LLaVA [37] and MiniGPT-4 [83] first construct image-text
                            feature alignment followed by instruction tuning and achieve superior perfor-
                            mance. Recent studies have increasingly concentrated on integrating founda-
                            tional models with tasks that demand a refined understanding at the region or
                            pixel level. VisionLLM [61] introduces a versatile interaction interface for var-
                            ious vision-centric tasks via instruction tuning, albeit without fully leveraging
                            LLMs for intricate reasoning tasks. Kosmos-2 [43] has built substantial data of
                            groundedimage-textpairs,therebyembeddinggroundingcapabilitiesintoLLMs.
                            DetGPT [44] links the fixed multi-modal LLM with an open-vocabulary detec-
                            tor, facilitating user instruction-based detection tasks. This growing interest has
                            spurred further innovations, including GPT4RoI [78], LLaVA-grounding [76],
                            Ferret [75], LISA [28], GlaMM [50], PixelLM [52], Sphinx [32], each contributing
                            to the evolving landscape of instruction-based, multi-modal understanding.
                               Building on advancements of multi-modal large language models in 2D image
                            domain,thefieldiswitnessingaseamlesstransitioninto3Dspaces.PointLLM[70]
                            harnesses the prowess of large language models and trains with 3D point clouds
                            following the paradigm of LLaVA [37]. 3D-LLM [20] utilizes 2D foundation mod-
                            els to encode multi-view images of 3D point clouds. Point-Bind [15] aligns point
                            clouds with Image-Bind [14] and leverages ImageBind-LLM to reason multi-
                            modality input without 3D-instruction data training. GPT4Point [47] pioneers
                            in facilitating a unified approach towards 3D object understanding and gener-
                            ation, setting a new standard for versatility. However, these models primarily
                            concentrate on scene-level insights, often overlooking the intricate details at the
                            region, or point level. Contrasting with existing models, our research focuses on
                            two pivotal goals: 1) efÏciently inject segmentation capabilities into multi-modal
                            LLMs to conduct point-level understanding and 2) design a unified framework
                            for 3D point cloud segmentation via the reasoning ability of LLMs.
                            2.2   3D Point Cloud Segmentation
                            3D point cloud segmentation, a crucial task in computer vision, can be cat-
                            egorized into 3D semantic, instance, and panoptic segmentation. 3D semantic
                            segmentation [3,7,19,45,53,74,80] assigns each point in a 3D space to specific,
                            predefined classes. In contrast, 3D instance segmentation [25,27,54] goes a step
                            further by classifying each point and differentiating between distinct objects
                            of the same class. 3D panoptic segmentation [68,82] aims to group 3D points
                            according to their semantics and identities. Lately, more practical tasks have
                            emerged, such as 3D referring segmentation [1,4,17,23,67,79], which extends
                            referring expression segmentation [8,10,11,33–35] to 3D and segments a target
                            instance based on explicit linguistic descriptions, and 3D open-vocabulary seg-
                            mentation [12,40,42,57], designed to identify and segment unseen objects beyond
                            a fixed set of known categories.
                               Despite the significant progress made by transformer-based models: Mask3D
                            [54], SPFormer [56], MAFT [29], and OneFormer3D [27] in basic 3D segmenta-
                            tion tasks, their application in real-world scenarios requiring human language
