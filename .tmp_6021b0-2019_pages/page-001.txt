                       CLUTRR:ADiagnosticBenchmarkforInductiveReasoningfromText
                                               KoustuvSinha1,3,4, Shagun Sodhani 2,3, Jin Dong 1,3,
                                                    Joelle Pineau 1,3,4 and William L. Hamilton 1,3,4
                                              1 School of Computer Science, McGill University, Canada
                                                              2              ´              ´
                                                                Universite de Montreal, Canada
                                             3 Montreal Institute of Learning Algorithms (Mila), Canada
                                                  4 Facebook AI Research (FAIR), Montreal, Canada
                                               {koustuv.sinha, sshagunsodhani, jin.dong, jpineau, wlh}
                                    @{mail.mcgill.ca, gmail.com, mail.mcgill.ca, cs.mcgill.ca, cs.mcgill.ca}
                                             Abstract
                         The recent success of natural language under-
                         standing (NLU) systems has been troubled by
                         results highlighting the failure of these mod-
                         els to generalize in a systematic and robust
                         way. In this work, we introduce a diagnostic
                         benchmark suite, named CLUTRR, to clarify                          Figure 1: CLUTRR inductive reasoning task.
                         some key issues related to the robustness and
                         systematicity of NLU systems. Motivated by                       However, there are growing concerns regarding
                         classic work on inductive logic programming,                  the ability of NLU systems—and neural networks
                         CLUTRR requires that an NLU system infer                      moregenerally—to generalize in a systematic and
                         kinship relations between characters in short                 robust way (Bahdanau et al., 2019; Lake and Ba-
                         stories.   Successful performance on this task                roni, 2018; Johnson et al., 2017). For instance,
                         requires both extracting relationships between
                         entities, as well as inferring the logical rules              recent work has highlighted the brittleness of NLU
                         governing these relationships.       CLUTRR al-               systems to adversarial examples (Jia and Liang,
                         lows us to precisely measure a model’s abil-                  2017), as well as the fact that NLU models tend to
                         ity for systematic generalization by evaluat-                 exploit statistical artifacts in datasets, rather than
                         ing on held-out combinations of logical rules,                exhibiting true reasoning and generalization capa-
                         and it allows us to evaluate a model’s robust-                bilities (Gururangan et al., 2018; Kaushik and Lip-
                         ness by adding curated noise facts. Our em-                   ton, 2018). These ﬁndings have also dovetailed
                         pirical results highlight a substantial perfor-               with the recent dominance of large pre-trained lan-
                         mancegapbetweenstate-of-the-art NLU mod-
                         els (e.g., BERT and MAC) and a graph neu-                     guagemodels, such as BERT, on NLU benchmarks
                         ral network model that works directly with                    (Devlin et al., 2018; Peters et al., 2018), which sug-
                         symbolic inputs—with the graph-based model                    gest that the primary difﬁculty in these datasets is
                         exhibiting both stronger generalization and                   incorporating the statistics of the natural language,
                         greater robustness.                                           rather than reasoning.
                     1    Introduction                                                    Animportant challenge is thus to develop NLU
                                                                                       benchmarks that can precisely test a model’s capa-
                     Natural language understanding (NLU) systems                      bility for robust and systematic generalization. Ide-
                     have been extremely successful at reading compre-                 ally, we want language understanding systems that
                     hension tasks, such as question answering (QA)                    can not only answer questions and draw inferences
                     and natural language inference (NLI). An array of                 from text, but that can also do so in a systematic,
                     existing datasets are available for these tasks. This             logical, and robust way. While such reasoning ca-
                     includes datasets that test a system’s ability to ex-             pabilities are certainly required for many existing
                     tract factual answers from text (Rajpurkar et al.,                NLU tasks, most datasets combine several chal-
                     2016; Nguyen et al., 2016; Trischler et al., 2016;                lenges of language understanding into one, such as
                     Mostafazadeh et al., 2016; Su et al., 2016), as well              co-reference/entity resolution, incorporating world
                     as datasets that emphasize commonsense inference,                 knowledge, and semantic parsing—making it difﬁ-
                     such as entailment between sentences (Bowman                      cult to isolate and diagnose a model’s capabilities
                     et al., 2015; Williams et al., 2018).                             for systematic generalization and robustness.
                                                                                 4506
                                      Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
                                    and the 9th International Joint Conference on Natural Language Processing, pages 4506–4515,
                                                                                 c
                                     HongKong,China,November3–7,2019. 2019AssociationforComputationalLinguistics
