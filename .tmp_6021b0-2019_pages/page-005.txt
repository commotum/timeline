                  Table 1: Statistics of the AMT paraphrases. Jaccard       these two entities. This representation avoids the
                  wordoverlap is calculated within the templates of each    pitfall of revealing information about the answer in
                  individual clause of length k.                            the question (Kaushik and Lipton, 2018).
                    NumberofParaphrases                         # clauses   Representing entities. When generating stories,
                                                k = 1  1,868      20        entity names are randomly drawn from a set of 300
                                                k = 2  1,890      58        commongenderedEnglishnames. Thus,depend-
                                                k = 3  2,258      236       ing on each run, the entities are never the same.
                                                Total  6,016                This ensures that the entity names are simply place-
                    Unique Word Count                  3,797                holders and uncorrelated from the task.
                     Jaccard Word Overlap   Unigrams   0.201                3.5   Variants of CLUTRR
                                             Bigrams   0.0385
                                                                            ThemodularnatureofCLUTRRprovidesrichdi-
                                                                            agnostic capabilities for evaluating the robustness
                  collection of natural language templates that can be      and generalization abilities of neural language un-
                  programmatically recombined to generate stories           derstanding systems. We highlight some key diag-
                  with various properties.                                  nostic capabilities available via different variations
                  Dataset statistics. At the time of submission, we         of CLUTRR below. These diagnostic variations
                  have collected 6,016 unique paraphrases with an           correspond to the concrete datasets that we gener-
                  averageof19paraphrasesforeverypossiblelogical             ated in this work, and we describe the results on
                  clause of length k = 1,2,3. Table 1 contains sum-         these Datasets in Section 4.
                  marystatistics of the collected paraphrases. Over-        Systematic generalization.        Most prominently,
                  all, we found high linguistic diversity in the col-       CLUTRRallowsustoexplicitlyevaluateamodel’s
                  lected paraphrases. For instance, the average Jac-        ability for systematic generalization. In particular,
                  cardoverlapinunigramsbetweenpairsparaphrases              we rely on the following hold-out procedures to
                  corresponding to the same logical clause was only         test systematic generalization:
                  0.201 and only 0.0385 for bigrams. The Appendix           • During training, we hold out a subset of the col-
                  contains further examples of the paraphrases.                lected paraphrases, and we only use this held-out
                  Humanperformance. To get a sense of the data                 subset of paraphrases when generating the test
                  quality and difﬁculty involved in CLUTRR, we                 set. Thus, to succeed on CLUTRR, an NLU sys-
                  asked human annotators to solve the task for ran-            temmustexhibitlinguistic generalization and be
                  domexamplesoflengthk = 2,3,...,6. We found                   robust to linguistic variation at test time.
                  that time-constrained AMT annotators performed            • Wealsoholdoutasubsetofthelogical clauses
                  well (i.e., > 70%) accuracy for k ≤ 3 but struggled          during training (for clauses of length k > 2).3 In
                  with examples involving longer stories, achieving            other words, during training, the model sees all
                  40-50%accuracyfork > 3. However, trained an-                 logical rules but does not see all combinations of
                  notators with unlimited time were able to solve              these logical rules. Thus, in addition to linguistic
                  100%oftheexamples (Appendix 1.7), highlight-                 generalization, success on this task also requires
                  ing the fact that this task requires attention and           logical generalization.
                  involved reasoning, even for humans.
                                                                            • Lastly, as a more extreme form of both logical
                  3.4   Queryrepresentation and inference                      and linguistic generalization, we consider the
                  Representing the question. The AMT paraphras-                setting where the models are trained on stories
                  ing approach described above allows us to convert            generated from clauses of length ≤ k and evalu-
                  the set of supporting facts B to a natural language          ated on stories generated from larger clauses of
                                                C                              length > k. Thus, we explicitly test the ability
                  story, which can be used to predict the target rela-         for models to generalize on examples that re-
                  tion/query H . However, instead of converting the
                                C         ∗                                    quire more steps of reasoning that any example
                  target query, H = [α ], toanaturallanguageques-
                                  C                                            they encountered during training.
                  tion, we instead opt to represent the target query as
                  a K-wayclassiﬁcation task, where the two entities             3One should not holdout clauses from length k = 2 in
                  in the target relation are provided as input and the      order to allow models to learn the compositionality of all
                  goal is to classify the relation that holds between       possible binary predicates.
                                                                        4510
