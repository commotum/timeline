                  Our work. Inspired by the classic AI challenge           Reading comprehension benchmarks.              Many
                  of inductive logic programming (Quinlan, 1990)—          datasetshavebeenproposedtotestthereadingcom-
                  as well as the recently developed CLEVR dataset          prehension ability of NLP systems. This includes
                  for visual reasoning (Johnson et al., 2017)—we           the SQuAD (Rajpurkar et al., 2016), NewsQA
                  propose a semi-synthetic benchmark designed to           (Trischler et al., 2016), and MCTest (Richardson
                  explicitly test an NLU model’s ability for system-       et al., 2013) benchmarks that focus on factual
                  atic and robust logical generalization.                  questions; the SNLI (Bowman et al., 2015) and
                     Our benchmark suite—termed CLUTRR                     MultiNLI (Williams et al., 2018) benchmarks for
                  (Compositional Language Understanding and                sentence understanding; and the bABI tasks (We-
                  Text-based     Relational    Reasoning)—contains         ston et al., 2015), to name a few. Our primary
                  a large set of semi-synthetic stories involving          contribution to this line of work is the development
                  hypothetical families.    Given a story, the goal        of a carefully designed diagnostic benchmark to
                  is to infer the relationship between two family          evaluate model robustness and systematic general-
                  members, whose relationship is not explicitly            ization in the context of NLU.
                  mentioned (Figure 1). To solve this task, a learning     Question-answering with knowledge graphs.
                  agent must extract the relationships mentioned           Our work is also related to the domain of ques-
                  in the text, induce the logical rules governing          tion answering and reasoning in knowledge graphs
                  the kinship relationships (e.g., the transitivity of     (Das et al., 2018; Xiong et al., 2018; Hamilton
                  the sibling relation), and use these rules to infer      et al., 2018; Wang et al., 2018; Xiong et al., 2017;
                  the relationship between a given pair of entities.       Welbl et al., 2018; Kartsaklis et al., 2018), where
                  Crucially, the CLUTRR benchmark allows us                either the model is provided with a knowledge
                  to test a learning agent’s ability for systematic        graphtoperforminferenceoverorwherethemodel
                  generalization by testing on stories that contain        must infer a knowledge graph from the text it-
                  unseen combinations of logical rules. CLUTRR             self. However, unlike previous benchmarks in this
                  also allows us to precisely test for the various         domain—whicharegenerally transductive and fo-
                  forms of model robustness by adding different            cus on leveraging and extracting knowledge graphs
                  kinds of superﬂuous noise facts to the stories.          as a source of background knowledge about a ﬁxed
                     We compare the performance of several state-          set of entities—CLUTRRrequiresinductivelogical
                  of-the-art NLU systems on this task—including            reasoning, where every example requires reasoning
                  Relation Networks (Santoro et al., 2017), Compo-         over a new set of previously unseen entities.
                  sitional Attention Networks (Hudson and Manning,
                  2018) and BERT(Devlin et al., 2018). We ﬁnd that         3   BenchmarkDesign
                  the generalization ability of these NLU systems          In order to design an NLU benchmark that explic-
                  is substantially below that of a Graph Attention         itly tests inductive reasoning and systematic gen-
                                 ˇ     ´
                  Network(Velickovic et al., 2018), which is given         eralization, we build upon the classic ILP task of
                  direct access to symbolic representations of the sto-    inferring family (i.e., kinship) relations (Hinton
                  ries. Moreover, we ﬁnd that the robustness of the        et al., 1986; Muggleton, 1991; Lavrac and Dze-
                  NLUsystemsgenerally does not improve by train-                                                         ¨
                  ing on noisy data, whereas the GAT model is able         roski, 1994;KokandDomingos,2007;Rocktaschel
                  to effectively learn robust reasoning strategies by      and Riedel, 2017). For example, given the facts
                  training on noisy examples. Both of these results        that “Alice is Bob’s mother” and “Jim is Alice’s
                  highlight important open challenges for closing the      father”, one can infer with reasonable certainty
                  gap between machine reasoning models that work           that “Jim is Bob’s grandfather.” While this exam-
                  with unstructured text and models that are given         ple may appear trivial, it is a challenging task to
                  access to more structured input.                         design models that can learn from data to induce
                                                                           the logical rules necessary to make such inferences,
                  2   Related Work                                         and it is even more challenging to design models
                                                                           that can systematically generalize by composing
                  Wedrawinspirationfromtheclassicworkoninduc-              these induced rules.
                  tive logic programming (ILP), a long line of read-          Inspired by this classic task of logical induction
                  ing comprehension benchmarks in NLP, as well as          and reasoning, the CLUTRR benchmark requires
                  workcombininglanguage and knowledge graphs.              an NLUsystemtoinferandreasonaboutkinship
                                                                      4507
