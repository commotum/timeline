                                                                             (Q1) How do state-of-the-art NLU models com-
                                                                                   pare in terms of systematic generalization?
                                                                                   Canthese models generalize to stories with
                                                                                   unseen combinations of logical rules?
                                                                             (Q2) How does the performance of neural lan-
                                                                                   guage understanding models compare to a
                                                                                   graph neural network that has full access to
                    Figure 4: Noise generation procedures of CLUTRR.               graph structure underlying the stories?
                                                                             (Q3) Howrobust are these models to the addition
                  Robust Reasoning.        In addition to evaluating               of noise facts to a given story?
                  systematic generalization, the modular setup of
                  CLUTRRalsoallowsustodiagnosemodelrobust-                  4.1   Baselines
                  ness by adding noise facts to the generated narra-        Ourprimarybaselines are neural language under-
                  tives. Due to the controlled semi-synthetic nature        standing models that take unstructured text as in-
                  of CLUTRR,weareabletoprovideaprecisetax-                  put. We consider bidirectional LSTMs (Hochreiter
                  onomyofthekindsofnoisefactsthatcanbeadded                 and Schmidhuber, 1997; Cho et al., 2014) (with
                  (Figure 4). In order to structure this taxonomy, it is    and without attention), as well as recently pro-
                  important to recall that any set of supporting facts      posed models that aim to incorporate inductive
                  B generated by CLUTRR can be interpreted as
                    C                                                       biases towards relational reasoning: Relation Net-
                  a path, p , in the corresponding kinship graph G
                            C                                               works (RN) (Santoro et al., 2017) and Composi-
                  (Figure 2). Based on this interpretation, we view         tional Memory Attention Network (MAC) (Hud-
                  addingnoisefactsfromtheperspectiveofsampling              son and Manning, 2018). We also use the large
                  three different types of noise paths, p , from the
                                                            n               pretrained language model, BERT (Devlin et al.,
                  kinship graph G:                                          2018), as well as a modiﬁed version of BERT hav-
                  • Irrelevant facts: We add a path p , which has
                                                          n                 ing a trainable LSTM encoder on top of the pre-
                     exactly one shared end-point with p . In this way,
                                                          c                 trained BERT embeddings. All of these models
                     this is a distractor path, which contains facts that   (except BERT) were re-implemented in PyTorch
                     are connected to one of the entities in the target     1.0 (Paszke et al., 2017) and adapted to work with
                     relation, H , but do not provide any information
                                C                                           the CLUTRRbenchmark.
                     that could be used to help answer the query.              Since the underlying relations in the stories gen-
                  • Supporting facts: We add a path p , whose two
                                                          n                 erated by CLUTRR inherently form a graph, we
                     end-points are on the path p . The facts on this
                                                   C                        also experiment with a Graph Attention Network
                     path p are noise because they are not needed to
                           n                                                             ˇ     ´
                     answer the query, but they are supporting facts        (GAT)(Velickovic et al., 2018). Rather than taking
                     because they can, in principle, be used to con-        the textual stories as input, the GAT baseline re-
                     struct alternative (longer) reasoning paths that       ceives a structured graph representation of the facts
                     connect the two target entities.                       that underlie the story.
                  • Disconnected facts: We add paths which neither          Entity and query representations. We use the
                     originate nor end in any entity on p . These           various baseline models to encode the natural lan-
                                                              c             guagestory (or graph) into a ﬁxed-dimensional em-
                     disconnected facts involve entities and relations      bedding. With the exception of the BERT models,
                     that are completely unrelated to the target query.     we do not use pre-trained word embeddings and
                  4    Experiments                                          learn the word embeddings from scratch using end-
                                                                            to-end backpropagation. An important note, how-
                  Weevaluate several neural language understanding          ever, is that we perform Cloze-style anonymization
                  systems on the proposed CLUTRR benchmark to               (Hermannetal., 2015) of the entities (i.e., names)
                  surface the relative strengths and shortcomings of        in the stories, where each entity name is replaced
                  these models in the context of inductive reason-          bya@entity-kplaceholder,whichisrandomlysam-
                                                           4                pled from a small, ﬁxed pool of placeholder tokens.
                  ing and combinatorial generalization. We aim to
                  answer the following key questions:                       The embeddings for these placeholders are ran-
                                                                                                                           5
                                                                            domlyinitialized and ﬁxed during training.
                     4Code to reproduce all the results in this section will be
                  released at https://github.com/facebookresearch/clutrr/.      5See Appendix 1.5 for a comparison of placeholder em-
                                                                        4511
