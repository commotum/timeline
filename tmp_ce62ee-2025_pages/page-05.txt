                         TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
                dual-verification mechanism ensures both the diversity and             1) Limit total cell content to 30,000 characters to improve
                the reliability of the dataset, culminating in a high-quality          large model processing efficiency.
                benchmark for table reasoning tasks.                                   2) The proportion of empty cells does not exceed 70%,
                Multi-Turn Adversarial Generation with LLMs: To con-                   ensuring data usability and completeness.
                struct high-quality question sets for multi-step analysis in           3) Remove tables with multi-level nesting or complex, non-
                table reasoning settings, we propose a novel generation-               standard headers to simplify structure and ensure clear se-
                discrimination pipeline. This pipeline constructs multi-turn           mantics.
                QApairsfromreal-world tables, covering 6 subtasks with
                846curated samples: Multi-step Retrieval, Fact Checking,               QAPairCleaning. Toensuretheaccuracyandreliabilityof
                Operations, Correlation Analysis, Hypothesis Testing, and              task-specific QA data, we implement a multi-stage quality
                Conditional Calculation. By emulating the COT reason-                  control framework that combines LLM voting and manual
                ing and typical analyst workflows, this method significantly           review. The process includes:
                enhances the logical complexity and practical relevance                1) Candidate Model Inference. Three candidate LLMs are
                of generated samples, thereby increasing their evaluative              used to generate inferences for QA pairs (excluding rule-
                challenge. The process includes:                                       based data ), resulting in three sets of candidate answers.
                1) Construction of Complex Reasoning Chains: Based on                  2) Judge Model Voting & Comprehensive Scoring. A
                standard data analysis paradigms , each subtask is decom-              judge model is introduced to evaluate the the consistency of
                posed into a sequence of atomic-level analytical operations,           these candidate answers against the ground truth, using the
                such as field filtering, aggregation, and logical judgment.            following scoring rules: If all three candidate answers agree
                These reasoning chains are generated through random com-               and match the ground truth, the original answer is retained.
                binations that balance task diversity with procedural co-              If all three agree but differ from the ground truth, the original
                herence and executability, closely mimicking real-world                answer is replaced with their consensus. If two answers
                analytical thought processes.                                          agree and align with the ground truth, the original answer is
                2) Generation of multi-turn reverse questions: We adopt a              kept. In all other cases—including partial agreement or full
                hierarchical framework for question generation, producing              disagreement—the original answer is retained.
                paired long-form and short-form questions from the con-                3) Manual Review Intervention. For QA pairs scoring
                structed reasoning chains and source tables. Long-form                 below full agreement , a subset is sampled for expert review.
                questions simulate expert analyst workflow, emphasizing                Domain experts re-annotate these cases to ensure answer
                multi-step reasoning and planning. Short-form questions                accuracy.
                mimictheinquiry patterns of non-expert users, exhibiting
                greater linguistic variability and brevity . Two LLMs are              ThefinalQAdata,whethercollectedorgenerated,isdivided
                involved: one for generation and another as a discriminator,           into High-QualityandLow-Qualitycategories. Dataverified
                which evaluates semantic plausibility, logical consistency,            by rule-based generation, manual annotation, or unanimous
                and answerability. Only samples that pass the discrimina-              LLMagreementis labeled as High-Quality, while all oth-
                tor’s criteria for both formats are retained in the final dataset.     ers are considered Low-Quality. Through the multi-stage
                3) Answer Generation: For each table and its associated                quality control process, we have finalized 2.75 million high-
                question pair, multiple candidate answers are generated                quality data, significantly enhancing dataset integrity and
                using LLM. These candidates then undergo human review                  providing a solid foundation for reliable evaluation.
                to ensure the accuracy and reliability of the final accepted           Question Classification. In the process of integrating open-
                answers.                                                               source datasets, we observe that some QA items correspond
                                                                                       to multiple downstream tasks within the evaluation system.
                2.4. Data Cleaning                                                     Tomaintain task independence, we clearly define the evalu-
                Toensure the quality and reliability of the evaluation data,           ation criteria and task boundaries. We leverage the semantic
                wedesign a rigorous data cleaning pipeline with multi-level            understanding capabilities of LLMs to automatically clas-
                filtering and processing mechanisms to enhance data purity             sify high-quality data, ensuring precise task alignment and
                and consistency. The workflow comprises three key steps:               eliminating overlaps across subtasks.
                (1) Table Cleaning, (2) QA Pair Cleaning, and (3) Question             2.5. Dataset Summary
                Classification.
                Table Cleaning. To meet experimental requirements for                  Our final evaluation dataset comprises 7,790 high-quality
                both scale and quality, we systematically clean the raw                samples, covering every subtask across the 6 core capabili-
                tabular data based on the following criteria:                          ties. Samples are drawn from a thoroughly cleaned corpus
                                                                                    5
