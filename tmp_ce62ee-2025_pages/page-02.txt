                       TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
               capturing the multifaceted nature of real-world table analy-    ference modes to evaluate the genuine capabilities of
               sis. This narrow scope limits their ability to assess models    LLMs. Thebenchmarksupports Textual Chain-of-Thought
               in more complex and genuine scenarios.                          (TCoT) (Wei et al., 2022) for generating textual answers
               Second, current benchmarks lack diversity in inference          in tasks like table summarization and title generation. It
               paradigms and underutilize the advanced capabilities of         also incorporates Program-of-Thought (PoT) (Chen et al.,
               modernLLMs. Mostbenchmarksrelyonpromptengineer-                 2022), where models write and execute code to handle data
               ing to guide models in generating textual answers. However,     retrieval and numerical computations, ensuring accurate and
               realistic table reasoning tasks often require models to invoke  objective results. For complex, multi-step tasks, Interleaved
               tools, execute code, or engage in self-reflection to arrive at  Chain-of-Thought (ICoT) (Yao et al., 2023) combines text
               accurate conclusions. For example, computing the standard       andcodereasoning, enabling dynamic code execution, inter-
               deviation of a column through free-form text is error-prone,    mediate result summarization, and self-reflection for better
               whereas allowing code execution ensures precision. More-        performance on challenging problems.
               over, single-turn interactions are insufficient for complex,    Robustevaluation methods: To ensure reliable evaluation,
               multi-step tasks that demand iterative problem-solving and      wedesignseveral evaluation metrics tailored to the charac-
               reasoning. To better align with real-world applications,        teristics of table reasoning tasks. For text generation tasks
               benchmarks should support more diverse inference modes          like table summarization, we use BLEU and ROUGE as
               that enable tool usage, code execution, and self-reflection.    evaluation metrics. For data retrieval, computational and
               Finally, the popular evaluation metrics are often biased        morecomplextasks, we rely on a discriminant LLM to as-
               when assessing LLM performance over table reasoning             sess semantic similarity and correctness of answers against
               tasks. While data retrieval and computational tasks typically   groundtruth. Fortaskswithfixedoutputformats,weemploy
               require concise and precise answers, LLMs often provide         prompt engineering and custom rules to calculate accuracy
               detailed reasoning alongside the final result. Traditional      with precision.
               natural language similarity metrics, such as BLEU (Pap-         Tosumup,ourworkmakesthreekeycontributions to the
               ineni et al., 2002) and ROUGE (Lin, 2004), tend to penalize     LLMandtableminingcommunities: (1)anopen-sourced,
               models for deviations in reasoning structure or verbosity,      high-quality dataset with comprehensive task settings1; (2)
               even if the final answer is correct. This inconsistency under-  a robust evaluation framework integrating diverse inference
                                                                                     2
               scores the need for task-specific metrics that fairly evaluate  modes ; and (3) detailed benchmarking results for state-of-
               both the accuracy of the final answer and the quality of        the-art LLMs. By open-sourcing both the dataset and the
               intermediate reasoning.                                         evaluation framework, we aim to establish a new standard
               To address these critical challenges in evaluating LLMsâ€™        for comprehensive and reproducible research in Table rea-
               table reasoning capabilities, we propose a benchmark            soning. We believe this unified benchmark will not only
               TReB that offers a comprehensive, objective, and multi-         facilitate fair comparisons across models with various ar-
               dimensional evaluation system in this paper. By focusing on     chitectures and training strategies, but also encourage the
               high-quality datasets, comprehensive task settings, diverse     development of more robust, generalizable, and executable
               inference modes, and robust evaluation methods, TReB bet-       table reasoning systems.
               ter reflects real-world scenarios and thoroughly assesses       2. Dataset
               model capabilities:
               High-quality datasets: We provide a curated dataset com-        In this section, we first introduce the data constructed based
               bining public cleaned data, real-world web tables, and pre-     on our evaluation framework, then details the data construc-
               viously unavailable proprietary table question answering        tion process, including data collection, data generation and
               (TableQA) data. All the QA pairs are manually constructed       augmentation, data cleaning and dataset summary.
               to align with practical applications, and each table & QA
               pair undergo rigorous cleaning and validation to eliminate      2.1. Dataset Overview
               issues such as formatting errors, noisy data, and inaccura-     Existing evaluation methods for LLMs in table reason-
               cies.                                                           ing, comprehension, and processing are primarily based on
               Comprehensivetasksettings: To assess a wide spectrum            single-task datasets (see Table6 for details). Although some
               of table reasoning capabilities, we identify 6 core skills and  multi-task benchmarks exist, they remain limited in scope
               develop 26 corresponding subtasks, enabling a fine-grained      and fail to comprehensively assess the general reasoning
               and multi-dimensional assessment of model performance               1
               across diverse aspects of tabular data analysis.                     https://huggingface.co/datasets/JT-LM/
                                                                               JIUTIAN-TReB
               Diverse inference modes: We introduces diverse in-                  2https://github.com/JT-LM/jiutian-treb
                                                                            2
