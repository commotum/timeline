                        TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
               tables embedded in images. This would address real-world              Anadkat, S., et al. Gpt-4 technical report. arXiv preprint
               scenarios where tabular data is often presented in scanned            arXiv:2303.08774, 2023.
               documents, screenshots, or other image-based formats. De-
               veloping methods to evaluate model performance on such             Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
               tasks would significantly broaden the benchmark’s applica-            Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
               bility and relevance.                                                 Askell, A., et al. Language models are few-shot learners.
               ComplexExcelTablesandMulti-TableScenarios: An-                        Advances in neural information processing systems, 33:
               other important direction is the incorporation of more so-            1877–1901, 2020.
               phisticated datasets, including complex Excel tables and           Chai, L., Liu, S., Yang, J., Yin, Y., Jin, K., Liu, J.,
               multi-table reasoning tasks. These additions would enable             Sun, T., Zhang, G., Ren, C., Guo, H., et al. Mceval:
               the evaluation of models’ abilities to handle inter-table re-         Massively multilingual code evaluation. arXiv preprint
               lationships, perform advanced operations across multiple              arXiv:2406.07436, 2024.
               datasets, and answer questions of higher complexity. By
               simulatingreal-worldchallenges,thisextensionwouldallow             Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H., and
               for a more comprehensive assessment of model capabilities             Wang,W.Y. Hybridqa: Adataset of multi-hop question
               in practical table mining applications.                               answering over tabular and textual data. In Findings of
               EnhancedToolIntegration: Future work could also focus                 the Association for Computational Linguistics: EMNLP
               onextending the framework to evaluate models’ ability to              2020, pp. 1026–1036, 2020.
               integrate with external tools, such as databases, APIs, or         Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program
               advanced computational systems. This would enable the                 of thoughts prompting: Disentangling computation from
               benchmark to assess how effectively models can utilize ex-            reasoning for numerical reasoning tasks. arXiv preprint
               ternal resources to solve highly complex or domain-specific           arXiv:2211.12588, 2022.
               tasks that go beyond the limits of standalone reasoning.
               Release New LLMs: To ensure fairness, we did not in-               Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li,
               clude evaluations of our own models in this paper. In the             S., Zhou, X., and Wang, W. Y. Tabfact: A large-scale
               future, we plan to train and release our own LLMs with                dataset for table-based fact verification. In International
               table reasoning capabilities. Additionally, we will explore           Conference on Learning Representations (ICLR), 2023.
               howthebenchmarkcanguidemodeltrainingandcontribute                  Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Lang-
               to performance enhancement.                                           don, D., Moussa, R., Beane, M., Huang, T.-H., Routledge,
               7. Conclusion                                                         B. R., et al. Finqa: A dataset of numerical reasoning over
                                                                                     financial data. In Proceedings of the 2021 Conference on
               In this work, we propose a comprehensive benchmark TReB               Empirical Methods in Natural Language Processing, pp.
               to evaluate table reasoning capabilities of LLMs. It inte-            3697–3711, 2021.
               grates diverse datasets, 6 core capabilities, and 26 sub-tasks     Cheng, M., Mao, Q., Liu, Q., Zhou, Y., Li, Y., Wang, J., Lin,
               to provide a thorough and multi-dimensional assessment                J., Cao, J., and Chen, E. A survey on table mining with
               of model performance. It also incorporates multiple rea-              large language models: Challenges, advancements and
               soning modes (TCoT, PoT, ICoT) and employs a variety of               prospects. Authorea Preprints, 2025.
               evaluation metrics to ensure objectivity and reliability in
               benchmarking. The framework offers meaningful insights             Chiang, C.-H. and Lee, H.-Y. Can large language models
               into the strengths and weaknesses of existing LLMs, partic-           be an alternative to human evaluations?        In Proceed-
               ularly in addressing real-world tabular data challenges. By           ings of the 61st Annual Meeting of the Association for
               openly releasing both the dataset and evaluation framework,           Computational Linguistics (Volume 1: Long Papers), pp.
               weaimtoadvanceresearch in table reasoning and complex                 15607–15631, 2023.
               data analysis, fostering innovation and providing a solid
               foundation for the development of more capable and robust          Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
               models.                                                               Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,
                                                                                     R., et al. Training verifiers to solve math word problems.
               References                                                            arXiv preprint arXiv:2110.14168, 2021.
               Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,         DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capa-
                  Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,          bility in llms via reinforcement learning. arXiv preprint
                                                                                     arXiv:2501.12948, 2025.
                                                                               16
