                        TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
               across tasks. ROUGE-L assesses the textual similarity be-          4.2.2. PERFORMANCE ANALYSIS BY METRICS
               tweenthestudentanswerandthegroundtruthanswer,while                 Weanalyzemodelperformanceunderdifferent evaluation
               LLM-as-a-judge evaluates semantic similarity and answer            metrics. Overall, ROUGE-L and LLM-as-a-judge demon-
               accuracy. For LLM-as-a-judge, we utilize the Qwen2-72B-            strate a high degree of consistency in their evaluations. Al-
               Instruct (Yang et al., 2024a) model, which has been fully          though the absolute scores differ between the two metrics,
               trained with RLHF to align with human preferences. This            the ranking of models within each metric remains highly
               model is excluded from the evaluated models to ensure im-          similar. For instance, models such as QwQ-32B and Qwen3-
               partiality in the assessment. In addition, for tasks where         32Bachieve relatively high scores under both metrics, in-
               the answer contains a single numerical value, we calculate         dicating their strong performance across different evalua-
               accuracy as a separate evaluation metric to ensure absolute        tion approaches. Additionally, it is observed that scores
               precision in the assessment. Notably, since PoT produces           from ROUGE-L are generally lower, with an average of
               code as its output, we use the result of code execution as the     only 23.16, compared to an average of 48.62 for LLM-as-a-
               student answer. This means that if the code fails to execute,      judge. This discrepancy arises because ROUGE primarily
               the test sample is automatically assigned a score of zero.         focuses on surface-level lexical overlap and may fail to cap-
               Experimental Environment: We design standardized                   ture deeper semantic correctness, often penalizing valid an-
               prompt templates to implement distinct reasoning methods,          swers that use alternative phrasing or synonyms. In contrast,
               ensuring fairness in the evaluation process. Additionally,         LLM-as-a-judge relies on a more comprehensive evaluation
               we enforce strict formatting constraints on the outputs of         of semantic alignment, enabling it to better recognize the
               LLMsandextract the final answers to prevent any irrele-            correctness of diverse but valid answer expressions. This
               vant information from influencing the results. The evalua-         finding is further validated in the subsequent case study (in
               tion of all models was conducted using the Transformers            section 4.2.6).
               library and vLLM framework, leveraging multiple NVIDIA
               A800-80GBGPUsforacceleratedcomputation. Intotal, we                4.2.3. PERFORMANCE ANALYSIS BY MODEL TYPE
               summarize the experimental results of 26 LLMs across 26            Wecomparedtheperformanceofthefive types of LLMs in-
               tasks under 3 reasoning modes, resulting in 1,794 experi-          troducedinSection4.1.1. Generally, withinthesamemodel
               mental groups, with the average performance taken as the           series, larger models outperform their smaller counterparts.
               final result.                                                      For example, the Llama-70B model achieves better results
               4.2. Experimental Results                                          than the Llama-7B, and the Qwen-72B model outperforms
                                                                                  the Qwen-7B.Table4presentstheaveragescoresofthefive
               4.2.1. OVERALL PERFORMANCE ANALYSIS                                categories of LLMs across six table-reasoning capabilities,
               Wesummarizetheaverageperformanceofallmodelsacross                  evaluated using the LLM-as-a-judge metric. The results
               all tasks (as shown in Table 2 and Table 3). Table 2 reports       show that Deep Thinking LLMs achieve the highest over-
               the evaluation results based on ROUGE-L, while Table 3             all performance. Their advanced reasoning capabilities for
               presents the results using LLM-as-a-judge. Among all mod-          complex problems and self-reflection abilities enable them
               els, QwQ-32B achieves the highest overall score, demon-            to consistently achieve top scores across the table reasoning
               strating superior performance across all six table-reasoning       tasks. Following this category are General LLMs and Table
               capabilities and inference modes.                                  Reasoning Optimized LLMs, which perform well overall
                                                                                  but exhibit noticeable gaps compared to Deep Thinking
               Two trends can be observed from the results: (1) Models            LLMsinADA,tasksthatrequiremorecapabilities in table
               with higher scores in NLU and TU tend to perform better in         operations and data analysis.
               the other four capabilities. This is because TU focuses on         Code-optimized LLMs, while proficient in generating code,
               the ability to retrieve and comprehend content from tables,        donotperformwellintable reasoning tasks, likely due to
               while NLUreflects the modelâ€™s understanding of questions.          the gap between general code generation and the specific
               Bothofthemserveasfoundationalskills for more advanced              requirements of table reasoning-related code generation.
               capabilities. (2) The datasets of Advanced Data Analysis           Additionally, Math Optimized LLMs perform poorly across
               (ADA)aremorechallengingversions of Table Basic/Com-                all tasks, particularly in tasks requiring code generation.
               putational Operations (TBO/TCO) and Data Analysis (DA).            This may be due to the conflicting optimization goals be-
               Therefore, the scores of ADA are generally lower than other        tween solving mathematical problems or structured data
               tasks due to the complexity of both the tables and the ques-       tasks and handling table specific tasks, leading to subopti-
               tions.                                                             maloutcomesforthis category in table reasoning.
                                                                                  It is important to note that the above analysis represents
                                                                                  preliminary observations based on the experimental settings
                                                                               10
