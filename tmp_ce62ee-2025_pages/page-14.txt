                       TReB:AComprehensiveBenchmarkforEvaluatingTableReasoningCapabilitiesofLargeLanguageModels
               this case, the task involves multiple code-based operations.    information and falling short in assessing a model’s ability
               Under both PoT and TCoT, QwQ-32B produces an incor-             to perform multi-step reasoning, cross-table integration, or
               rect answer, while the model arrives at the correct solution    complex contextual understanding.
               under ICoT. Such phenomenon highlights the distinctive          To overcome the limitations of shallow tasks, subsequent
               advantage of ICoT in handling complex reasoning tasks. In       research has progressively introduced more challenging task
               addition, we carefully examine the inference process, which     dimensions. For instance, ToTTo (Parikh et al., 2020) and
               could reveal the necessity of ICoT. When reasoning by PoT,      FeTaQA (Nan et al., 2022) emphasized multi-source in-
               the model generates erroneous code due to an incorrectly        formation fusion and natural language generation, high-
               defined statistical threshold, leading to mismatch between      lighting the model’s integrative and generative abilities.
               predictions and groundtruth. In contrast, under the TCoT        FinQA(Chenetal., 2021) and AIT-QA (Katsis et al., 2022)
               mode the model attempts to solve the problem through a          focused on numerical reasoning in financial domains, evalu-
               step-by-step textual reasoning process. Besides, TCoT may       ating the model’s capacity for conditional logic and precise
               be inappropriate for computation-intensive tasks. First, the    calculation within structured contexts. HybridQA (Chen
               limited token budget restricts the size of the table that can   et al., 2020) further combined structural perception with
               be processed, rendering TCoT ineffective for large-scale        complex reasoning, establishing a more hierarchical evalua-
               tabular data. Second, complex operations on tables tend to      tion framework. Although these benchmarks significantly
               result in reasoning errors of arithmetic operations, further    broaden the scope of table reasoning evaluation, they still
               limiting the applicability of TCoT to such tasks. Contrastly,   face limitations in reasoning-chain completeness, dataset
               the ICoT method demonstrates robustness through its iter-       diversity, and fidelity to real-world complex tasks.
               ative refinement capability. Although the model initially
               produces incorrect code, with receiving feedback of the in-     With the growing demand for executability and explainabil-
               correct code (i.e., NameError: name ’np’ is not defined),       ity in table reasoning tasks, recent work has explored hybrid
               which enables the model to revise its initial code, fix the     modeling approaches that integrate natural language under-
               error, and ultimately produce the correct answer.               standing with code generation. These methods typically
               Hence, the comparative analysis of this case demonstrates       generate executable programs from questions, which are
               that the ICoT approach offers greater tolerance for interme-    then executed via interpreters or database engines to pro-
               diate errors and provides the model with opportunities for      duce answers (Luo et al., 2024; Wei et al., 2024). One line
               self-reflection and self-correction, which is more similar to   of such approaches focuses on translating queries into SQL,
               the real-world applications of LLMs.                            giving rise to benchmarks like Spider (Yu et al., 2018) and
                                                                               BIRD (Li et al., 2023). Another line of research aims to
               5. Related Work                                                 emulate human-like analytical workflows (Hu et al., 2024),
                                                                               encompassing stages such as structural perception, condi-
               Tables, as a highly structured and compact form of data         tional reasoning, script execution, and answer generation.
               representation, are widely used across domains such as gov-     This paradigm enables the development of transferable and
               ernment, finance, commerce, and scientific research. They       generalizable reasoning pipelines. Representative bench-
               serve as critical carriers for data-driven analysis. With the   marks include iDS-1000 (Lai et al., 2023) and InfiAgent-
               rapid advancement of LLMs, tabular data has been increas-       DABench (Hu et al., 2024), which extended the scope to
               ingly integrated into language understanding and reasoning      diverse data modalities and task complexities.
               frameworks, significantly expanding the model’s capacity to     The growing proficiency of LLMs in understanding natu-
               express and manipulate structured information. Therefore,       ral language queries and interacting with tabular data has
               researchers have proposed benchmarks to assess model’s ta-      spurred extensive research into exploring their potentials
               ble reasoning capabilities in information extraction, logical   for table reasoning tasks. Recent studies have introduced
               reasoning, and structured data integration, in which tasks      morerealistic and comprehensive evaluation settings within
               are often organized as Question-Answer forms.                   benchmark datasets to better reflect practical challenges.
               Early benchmarks primarily focused on measuring struc-          For instance, TableBench (Wu et al., 2025b) integrated six
               tural understanding and explicit fact verification capabili-    sub-datasets covering tasks such as fact verification, numer-
               ties. Representative works such as WTQ (Pasupat & Liang,        ical reasoning, data analysis, and visualization. SUC (Sui
               2015), SQA (Iyyer et al., 2017), and TabFact (Chen et al.,      et al., 2024) proposed seven subtasks to systematically eval-
               2023)weretypicallyconstructedbasedonWikipediaHTML               uate structural understanding. RealTableBench (Su et al.,
               tables and required models to extract answers directly from     2024) constructed test cases from real-world business intel-
               individualcells, thereby evaluating their basic alignment and   ligence (BI) systems, reflecting authentic usage scenarios.
               structural recognition capabilities. However, these bench-      TableQAKit(Leietal.,2023)curatedQApairsfromdiverse
               marksarerelatively shallow, relying mostly on surface-level     sources and provides a unified interface and multi-task eval-
                                                                           14
