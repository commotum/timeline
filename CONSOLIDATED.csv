year,title,url
2020,2WikiMultiHopQA: Comprehensive Evaluation of Reasoning Steps,https://aclanthology.org/2020.coling-main.580/
2021,3DETR: An End-to-End Transformer Model for 3D Object Detection,https://arxiv.org/abs/2109.08141
2023,4D-Former: Multimodal 4D Panoptic Segmentation,https://proceedings.mlr.press/v229/athar23a/athar23a.pdf
2023,4D-Former: Multimodal 4D Panoptic Segmentation,https://arxiv.org/abs/2311.01520
2024,A 2D nGPT Model For ARC Prize,https://arcprize.org/competitions/2024/
2024,A 2D nGPT Model for ARC Prize,https://arxiv.org/html/2412.04604v2
2025,A Benchmark to Evaluate Fundamental Numerical Abilities,https://arxiv.org/pdf/2502.11075
2017,A Distributional Perspective on Reinforcement Learning,https://arxiv.org/abs/1707.06887
2017,A Distributional Perspective on Reinforcement Learning,https://arxiv.org/abs/1707.06887
2006,A Fast Learning Algorithm for Deep Belief Nets,MISSING
1980,A Feature-Integration Theory of Attention,https://pubmed.ncbi.nlm.nih.gov/7351125/
2019,A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex,https://www.frontiersin.org/journals/neural-circuits/articles/10.3389/fncir.2018.00121/full
1999,A Framework for Temporal Abstraction in Reinforcement Learning,https://www.sciencedirect.com/science/article/pii/S0004370299000521
2025,A Fully First-Order Layer for Differentiable Optimization,https://arxiv.org/pdf/2512.02494
2023,A Length-Extrapolatable Transformer,https://aclanthology.org/2023.acl-long.816.pdf
2023,A Length-Extrapolatable Transformer (XPOS / LeX),"https://aclanthology.org/2023.acl-long.816.pdf ""XPOS / LeX (2023) — ACL Anthology"""
1957,A Markovian Decision Process,https://www.jstor.org/stable/24900506
1948,A Mathematical Theory of Communication,https://ia803209.us.archive.org/27/items/bstj27-3-379/bstj27-3-379_text.pdf
1948,A Mathematical Theory of Communication,https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf
2001,A Natural Policy Gradient,https://papers.neurips.cc/paper/2073-a-natural-policy-gradient.pdf
2003,A Neural Probabilistic Language Model,https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
2021,A Neurosymbolic Approach to Abstraction and Reasoning,https://dspace.mit.edu/bitstream/handle/1721.1/139305/Alford-salford-meng-eecs-2021-thesis.pdf
1960,A New Approach to Linear Filtering and Prediction Problems,https://asmedigitalcollection.asme.org/fluidsengineering/article/82/1/35/397706/A-New-Approach-to-Linear-Filtering-and-Prediction
2021,A Simple and Effective Positional Encoding for Transformers,https://aclanthology.org/2021.emnlp-main.236.pdf
2024,A Survey on Evaluation of Large Language Models,https://dl.acm.org/doi/full/10.1145/3641289
1984,A Theory of the Learnable (PAC Learning),https://en.wikipedia.org/wiki/Probably_approximately_correct_learning
2006,A free energy principle for the brain,https://pubmed.ncbi.nlm.nih.gov/17097864/
2022,A-OKVQA: A Benchmark for VQA Using World Knowledge,https://arxiv.org/abs/2206.01718
2023,AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models,https://arxiv.org/pdf/2304.06364
2020,AI Feynman,https://www.science.org/doi/10.1126/sciadv.aay2631
2021,ALBEF: Align Before Fuse,"https://arxiv.org/pdf/2107.07651.pdf ""ALBEF (2021) — arXiv"""
Year unknown,ARC,MISSING
2025,ARC Is a Vision Problem! (Vision ARC / VARC),https://arxiv.org/abs/2511.14761
2025,ARC Is a Vision Problem! + test-time training theme (indexed by ARC Prize 2025),https://arcprize.org/blog/arc-prize-2025-results-analysis
2024,ARC Prize 2024: Technical Report,https://arxiv.org/abs/2412.04604
2024,ARC Prize 2024: Technical Report,MISSING
2025,ARC-AGI Without Pretraining (CompressARC),https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/ARC_AGI_Without_Pretraining.pdf
2025,ARC-AGI Without Pretraining (CompressARC),https://arcprize.org/blog/arc-prize-2025-results-analysis
2025,ARC-AGI is a Vision Problem!,https://arcprize.org/blog/arc-prize-2025-results-analysis
2024,ARC-Heavy / ARC-Potpourri (dataset description embedded in Cornell report),"https://www.cs.cornell.edu/~ellisk/documents/arc_induction_vs_transduction.pdf ""ARC-Heavy / ARC-Potpourri (2024) — Cornell"""
2025,ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus,https://arxiv.org/html/2505.08778v1
2025,ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus,https://arcprize.org/blog/arc-prize-2025-results-analysis
2024,ASGFormer: Point cloud semantic segmentation with adaptive spatial graph transformer,https://www.sciencedirect.com/science/article/pii/S156984322400459X
2021,AST: Audio Spectrogram Transformer,https://arxiv.org/abs/2104.01778
2018,Accelerating Search-Based Program Synthesis using Learned Probabilistic Models (Euphony),https://www.cis.upenn.edu/~alur/PLDI18.pdf
1999,Actor-Critic Algorithms,https://papers.nips.cc/paper/1786-actor-critic-algorithms
2014,Adam: A Method for Stochastic Optimization,https://arxiv.org/abs/1412.6980
2014,Adam: A Method for Stochastic Optimization,https://arxiv.org/pdf/1412.6980
2019,Adaptive Attention Span in Transformers,https://arxiv.org/abs/1905.07799
2016,Adaptive Computation Time,"https://arxiv.org/pdf/1603.08983.pdf ""Adaptive Computation Time (2016) — arXiv"""
2016,Adaptive Computation Time (ACT),https://arxiv.org/abs/1603.08983
2016,Adaptive Computation Time (ACT),https://arxiv.org/abs/1603.08983
2025,Adaptive Patch Selection for ViTs via Reinforcement Learning,"https://doi.org/10.1007/s10489-025-06516-z ""Adaptive Patch Selection for ViTs via RL (2025) — Springer"""
2025,Adaptive Thinking Using Dynamic Computation,https://proceedings.iclr.cc/paper_files/paper/2025/file/955499a8e2860ed746717c1374224c43-Paper-Conference.pdf
1976,"Adaptive pattern classification and universal recoding: II. Feedback, expectation, olfaction, illusions",https://pubmed.ncbi.nlm.nih.gov/963125/
2024,Advancing Process Verification for LLM Reasoning,https://aclanthology.org/2024.emnlp-main.125.pdf
2020,Agent57: Outperforming the Atari Human Benchmark,https://proceedings.mlr.press/v119/badia20a.html
1970,Algoritmin kumulatiivinen pyoristysvirhe yksittaisten pyoristysvirheiden Taylor-kehitelmana (The representation of the cumulative rounding error ...),https://www.idsia.ch/~juergen/linnainmaa1970thesis.pdf
1913,"An Example of Statistical Investigation of the Text ""Eugene Onegin"" Concerning the Connection of Samples in Chains",https://nessie.ilab.sztaki.hu/~kornai/2021/KalmanCL/markov_1913.pdf
2020,An Image Is Worth 16x16 Words (ViT),https://arxiv.org/abs/2010.11929
2020,An Image Is Worth 16x16 Words (ViT),https://arxiv.org/abs/2010.11929
2020,An Image is Worth 16×16 Words: Vision Transformer (ViT),"https://arxiv.org/pdf/2010.11929.pdf ""Vision Transformer (ViT) (2020) — arXiv"""
2001,An Integrative Theory of Prefrontal Cortex Function,https://pubmed.ncbi.nlm.nih.gov/11283309/
2019,Analysing Mathematical Reasoning Abilities of Neural Models,https://arxiv.org/abs/1904.01557
2019,Analysing Mathematical Reasoning Abilities of Neural Models,https://arxiv.org/pdf/1904.01557
2002,Approximately Optimal Approximate Reinforcement Learning,https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf
2025,ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory,https://ar5iv.org/abs/2509.04439
2025,ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory,https://arcprize.org/blog/arc-prize-2025-results-analysis
2025,Arithmetic-Bench: Evaluating Multi-Step Reasoning in LLMs through Basic Arithmetic Operations,https://openreview.net/forum?id=ae6bKeffGZ
2016,Asynchronous Methods for Deep Reinforcement Learning (A3C),https://arxiv.org/abs/1602.01783
2017,Attention Is All You Need,https://arxiv.org/abs/1706.03762
2017,Attention Is All You Need,https://arxiv.org/pdf/1706.03762
2017,Attention Is All You Need,https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf
2017,Attention Is All You Need,https://arxiv.org/pdf/1706.03762
2017,Attention Is All You Need,https://arxiv.org/pdf/1706.03762
2017,Attention Is All You Need,https://arxiv.org/pdf/1706.03762
2017,Attention Is All You Need,"https://arxiv.org/pdf/1706.03762.pdf ""Attention Is All You Need (2017) — arXiv"""
2014,Auto-Encoding Variational Bayes (VAE),MISSING
2011,Automating String Processing in Spreadsheets Using Input-Output Examples (FlashFill),https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/popl11-synthesis.pdf
2011,Automating String Processing in Spreadsheets using Input-Output Examples (FlashFill),https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/synasc12.pdf
2025,"Autoregressive Modeling as Iterative Latent Equilibrium (Equilibrium Transformers, EqT)",https://arxiv.org/html/2511.21882v1
2023,Average-Hard Attention Transformers Are Threshold Circuits,"https://arxiv.org/pdf/2308.03212.pdf ""Average-Hard Attention as Threshold Circuits (2023) — arXiv"""
2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,MISSING
2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805
2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805
2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,"https://arxiv.org/pdf/1810.04805.pdf ""BERT (2018) — arXiv"""
2022,BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers,https://arxiv.org/abs/2203.17270
2022,BEiT: BERT Pre-Training of Image Transformers,https://arxiv.org/abs/2106.08254
2021,BEiT: BERT Pre-Training of Image Transformers,"https://arxiv.org/pdf/2106.08254.pdf ""BEiT (2021) — arXiv"""
2021,BEiT: BERT Pre-Training of Image Transformers,"https://arxiv.org/pdf/2106.08254.pdf ""BEiT (2021) — arXiv"""
2025,BIG-Bench Extra Hard (BBEH),https://aclanthology.org/2025.acl-long.1285/
2022,BIG-Bench Hard (BBH): Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,https://arxiv.org/abs/2210.09261
2023,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Models,"https://arxiv.org/pdf/2301.12597.pdf ""BLIP-2 (2023) — arXiv"""
2022,BLIP: Bootstrapping Language-Image Pre-training,"https://arxiv.org/pdf/2201.12086.pdf ""BLIP (2022) — arXiv"""
2025,BWFormer: Building Wireframe Reconstruction from Airborne LiDAR Point Cloud with Transformer,https://cvpr.thecvf.com/virtual/2025/poster/32868
2024,Base of RoPE Bounds Context Length,"https://arxiv.org/pdf/2405.14591.pdf ""Base of RoPE Bounds Context Length (2024) — arXiv"""
2015,Batch Normalization,https://arxiv.org/abs/1502.03167
2024,Beyond A*: Planning with Transformers,"https://arxiv.org/pdf/2402.14083.pdf ""Beyond A*: Planning with Transformers (2024) — arXiv"""
2025,Beyond Brute Force: A Neuro-Symbolic Architecture for Compositional Reasoning in ARC-AGI-2,https://arcprize.org/blog/arc-prize-2025-results-analysis
2024,Beyond Position: The Emergence of Wavelet-like Properties in Transformers,"https://arxiv.org/pdf/2410.18067.pdf ""Wavelet-like Properties in Transformers (2024) — arXiv"""
2022,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models (BIG-bench),https://arxiv.org/abs/2206.04615
2016,BlinkFill: Semi-supervised Programming By Example for Syntactic String Transformations,https://www.vldb.org/pvldb/vol9/p816-singh.pdf
2016,BlinkFill: Semi-supervised Programming By Example for Syntactic String Transformations,https://www.vldb.org/pvldb/vol9/p816-singh.pdf
2025,Boosting Performance on ARC via Perspective / Augmentations,https://proceedings.mlr.press/v267/franzen25a.html
1972,Broadcast Channels,https://isl.stanford.edu/~cover/papers/transIT/0002cove.pdf
2025,CABLE: Context-aware Biases for Length Extrapolation,https://aclanthology.org/2025.emnlp-main.1545.pdf
2019,CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain,https://arxiv.org/pdf/1911.08962
2017,CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning,https://openaccess.thecvf.com/content_cvpr_2017/papers/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper.pdf
2019,CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text,https://aclanthology.org/D19-1458.pdf
2021,CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows,"https://arxiv.org/pdf/2107.00652.pdf ""CSWin Transformer (2021) — arXiv"""
2021,CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows,"https://arxiv.org/pdf/2107.00652.pdf ""CSWin Transformer (2021) — arXiv"""
2025,CamPoint: Boosting Point Cloud Segmentation with Virtual Camera,https://cvpr.thecvf.com/virtual/2025/poster/34611
1935,Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?,https://link.aps.org/doi/10.1103/PhysRev.47.777
2012,Canonical Microcircuits for Predictive Coding,"https://pubmed.ncbi.nlm.nih.gov/23238495/ ""Canonical Microcircuits for Predictive Coding (2012) — PubMed"""
1990,Cellular and circuit basis of working memory in prefrontal cortex of nonhuman primates,https://pubmed.ncbi.nlm.nih.gov/2094903/
1995,Cellular basis of working memory,https://pubmed.ncbi.nlm.nih.gov/7695894/
1924,Certain Factors Affecting Telegraph Speed,https://monoskop.org/images/9/9f/Nyquist_Harry_1924_Certain_Factors_Affecting_Telegraph_Speed.pdf
2022,ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning,https://arxiv.org/abs/2203.10244
2025,ChartQAPro: A More Diverse and Challenging Benchmark for Real-World Chart QA,https://arxiv.org/html/2504.05506v1
2024,Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference,https://arxiv.org/pdf/2403.04132
2025,Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models,"https://arxiv.org/pdf/2505.16416.pdf ""Circle-RoPE (2025) — arXiv"""
2021,CoAtNet: Marrying Convolution and Attention,https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html
2021,CoAtNet: Marrying Convolution and Attention for All Data Sizes,"https://arxiv.org/pdf/2106.04803.pdf ""CoAtNet (2021) — arXiv"""
2021,CoAtNet: Marrying Convolution and Attention for All Data Sizes,"https://arxiv.org/pdf/2106.04803.pdf ""CoAtNet (2021) — arXiv"""
2022,CoCa: Contrastive Captioners,"https://arxiv.org/pdf/2205.01917.pdf ""CoCa (2022) — arXiv"""
2025,CoPE: A Lightweight Complex Positional Encoding,https://arxiv.org/abs/2508.18308
1948,Cognitive Maps in Rats and Men,https://pubmed.ncbi.nlm.nih.gov/18870876/
2025,ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices,https://arxiv.org/abs/2506.03737
2025,ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices,"https://arxiv.org/pdf/2506.03737.pdf ""ComRoPE (2025) — arXiv"""
2017,Combining Improvements in Deep Reinforcement Learning (Rainbow),https://arxiv.org/abs/1710.02298
2024,Combining Induction and Transduction for Abstract Reasoning,https://arxiv.org/abs/2411.02272
2024,Combining Induction and Transduction for Abstract Reasoning,https://arcprize.org/competitions/2024/
2024,Combining Induction and Transduction for Abstract Reasoning,"https://arxiv.org/pdf/2411.02272.pdf ""Combining Induction and Transduction (2024) — arXiv"""
2019,CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge,https://aclanthology.org/N19-1421/
2010,Component-based Synthesis Applied to Bitvector Programs,https://www.microsoft.com/en-us/research/wp-content/uploads/2010/02/bv.pdf
2021,ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases,"https://arxiv.org/pdf/2103.10697.pdf ""ConViT (2021) — arXiv"""
2023,ConceptARC,"https://arxiv.org/pdf/2305.07141.pdf ""ConceptARC (2023) — arXiv"""
2018,"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset","https://arxiv.org/pdf/1803.10137.pdf ""Conceptual Captions (2018) — arXiv"""
2022,Constitutional AI,MISSING
2017,Constructing Datasets for Multi-hop Reading Comprehension (WikiHop / MedHop; QAngaroo),https://arxiv.org/abs/1710.06481
2025,Context-aware Biases for Length Extrapolation (CABLE),https://aclanthology.org/2025.emnlp-main.1545.pdf
2025,Context-aware Rotary Position Embedding (CARoPE),"https://arxiv.org/pdf/2507.23083.pdf ""CARoPE (2025) — arXiv"""
2015,Continuous Control with Deep Reinforcement Learning (DDPG),https://arxiv.org/abs/1509.02971
2017,Convolutional Sequence to Sequence Learning,https://arxiv.org/pdf/1705.03122
2007,Core Knowledge,"https://www.harvardlds.org/wp-content/uploads/2017/01/SpelkeKinzler07-1.pdf ""Core Knowledge (2007) — Harvard LDS"""
2021,CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification,"https://arxiv.org/pdf/2103.14899.pdf ""CrossViT (2021) — arXiv"""
2021,CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification,"https://arxiv.org/pdf/2103.14899.pdf ""CrossViT (2021) — arXiv"""
2021,CvT: Introducing Convolutions to Vision Transformers,https://arxiv.org/abs/2103.15808
2021,CvT: Introducing Convolutions to Vision Transformers,"https://arxiv.org/pdf/2103.15808.pdf ""CvT (2021) — arXiv"""
2021,CvT: Introducing Convolutions to Vision Transformers,"https://arxiv.org/pdf/2103.15808.pdf ""CvT (2021) — arXiv"""
2024,DAPE: Data-Adaptive Positional Encoding for Length Extrapolation,"https://proceedings.neurips.cc/paper_files/paper/2024/file/2f050fa9f0d898e3f265d515f50ae8f9-Paper-Conference.pdf ""DAPE (2024) — NeurIPS Proceedings"""
2025,DCT: Dynamic Clustering Transformer for LiDAR-Based 3D,https://www.sciencedirect.com/science/article/abs/pii/S0031320325011069
2021,DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries,https://arxiv.org/abs/2110.06922
2019,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,https://arxiv.org/abs/1903.00161
2019,DROP: Discrete Reasoning Over Paragraphs,https://aclanthology.org/N19-1246/
2025,DT-NVS: Diffusion Transformers for Novel View Synthesis,https://arxiv.org/html/2511.08823v1
2017,Dataset for Learning Karel Programs (MSR Karel Dataset),https://msr-redmond.github.io/karel-dataset/
2020,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,https://arxiv.org/abs/2006.03654
2020,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,https://arxiv.org/pdf/2006.03654
2024,DeLiVoTr: Deep and Light-weight Voxel Transformer for 3D Object Detection,https://www.sciencedirect.com/science/article/pii/S2667305324000371
2025,"Decoupling the ""What"" and ""Where"" With Polar Coordinate Positional Embeddings (PoPE)",https://arxiv.org/abs/2509.10534
2025,"Decoupling the ""What"" and ""Where"" With Polar Coordinate Positional Embeddings (PoPE)","https://arxiv.org/pdf/2509.10534.pdf ""PoPE (2025) — arXiv"""
2019,Deep Equilibrium Models,"https://arxiv.org/pdf/1909.01377.pdf ""Deep Equilibrium Models (2019) — arXiv"""
2019,Deep Equilibrium Models (DEQ),https://arxiv.org/abs/1909.01377
2019,Deep Equilibrium Models (DEQ),https://implicit-layers-tutorial.org/deep_equilibrium_models/
2019,Deep Equilibrium Models (DEQ),https://arxiv.org/abs/1909.01377
2016,Deep Reinforcement Learning with Double Q-learning (Double DQN),https://arxiv.org/abs/1509.06461
2015,Deep Residual Learning (ResNet),https://arxiv.org/abs/1512.03385
Year unknown,DeepCoder DSL tasks,https://openreview.net/pdf?id=ByldLrqlx
2016,DeepCoder: Learning to Write Programs,https://arxiv.org/abs/1611.01989
2016,DeepCoder: Learning to Write Programs,https://openreview.net/pdf?id=ByldLrqlx
2018,DeepProbLog: Neural Probabilistic Logic Programming,https://arxiv.org/abs/1805.10872
2025,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/abs/2501.12948
2025,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/abs/2501.12948
2025,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/abs/2501.12948
2021,Deformable DETR,https://arxiv.org/abs/2010.04159
2020,Deformable DETR: Deformable Transformers for End-to-End Object Detection,"https://arxiv.org/pdf/2010.04159.pdf ""Deformable DETR (2020) — arXiv"""
2020,Deformable DETR: Deformable Transformers for End-to-End Object Detection,"https://arxiv.org/pdf/2010.04159.pdf ""Deformable DETR (2020) — arXiv"""
2021,DeiT: Data-efficient Image Transformers,https://proceedings.mlr.press/v139/touvron21a.html
2020,Depth-Adaptive Transformers,https://jiataogu.me/papers/elbayad2020depth.pdf
2020,Depth-adaptive / early-exit Transformers (dynamic depth),MISSING
1963,Deterministic Nonperiodic Flow,https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml
2014,Deterministic Policy Gradient Algorithms,https://proceedings.mlr.press/v32/silver14.html
2024,DiffSAT: Differential MaxSAT Layer for SAT Solving,https://www.cse.cuhk.edu.hk/~byu/papers/C237-ICCAD2024-DiffSAT.pdf
2019,Differentiable Convex Optimization Layers,https://arxiv.org/abs/1910.12430
2019,Differentiable Convex Optimization Layers,https://openreview.net/forum?id=1EuxRTe0WN
2019,Differentiable Convex Optimization Layers (CVXPYLayers / DPP),https://arxiv.org/abs/1910.12430
2016,Differentiable Neural Computer (DNC),https://www.nature.com/articles/nature20101
2016,Differentiable Neural Computer (DNC),MISSING
2023,DiffusionDet: Diffusion Model for Object Detection,https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.pdf
2023,Direct Preference Optimization (DPO),https://arxiv.org/abs/2305.18290
2018,Distributed Prioritized Experience Replay (Ape-X),https://arxiv.org/abs/1803.00933
2021,Do Transformer Modifications Transfer Across Implementations and Applications?,https://arxiv.org/pdf/2102.11972
2021,Do Transformer Modifications Transfer Across Implementations and Applications?,https://arxiv.org/pdf/2102.11972
2025,DoPE: Denoising Rotary Position Embedding,"https://arxiv.org/pdf/2511.09146.pdf ""DoPE (2025) — arXiv"""
2020,DocVQA: A Dataset for VQA on Document Images,https://arxiv.org/abs/2007.00398
2025,Don't throw the baby out with the bathwater: How and why deep learning for ARC,https://arcprize.org/blog/arc-prize-2025-results-analysis
2019,Dream to Control: Learning Behaviors by Latent Imagination (Dreamer),https://openreview.net/forum?id=S1lOTC4tDS
2020,DreamCoder,https://courses.cs.washington.edu/courses/cse599j1/22sp/papers/dreamcoder.pdf
2024,DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving,https://openaccess.thecvf.com/content/CVPR2024/papers/Min_DriveWorld_4D_Pre-trained_Scene_Understanding_via_World_Models_for_Autonomous_CVPR_2024_paper.pdf
2024,DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving,https://openaccess.thecvf.com/content/CVPR2024/papers/Min_DriveWorld_4D_Pre-trained_Scene_Understanding_via_World_Models_for_Autonomous_CVPR_2024_paper.pdf
2014,Dropout: A Simple Way to Prevent Neural Networks from Overfitting,https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf
1991,"Dyna, an Integrated Architecture for Learning, Planning, and Reacting",https://dl.acm.org/doi/abs/10.1145/122344.122377
1957,Dynamic Programming,https://gwern.net/doc/statistics/decision/1957-bellman-dynamicprogramming.pdf
2005,Dynamic dopamine modulation in the basal ganglia,https://pubmed.ncbi.nlm.nih.gov/15701239/
2025,DynamicCity,https://arxiv.org/html/2410.18084v3
2025,EVA02-AT: Egocentric Video-Language with Spatial-Temporal RoPE,"https://arxiv.org/pdf/2506.14356.pdf ""EVA02-AT (2025) — arXiv"""
1908,"Early gradient/variation method note often attributed to Hadamard's ""methode"" discussions",https://people.idsia.ch/~juergen/who-invented-backpropagation-2014.html
2013,Efficient Estimation of Word Representations in Vector Space,https://arxiv.org/pdf/1301.3781
2025,Efficient Evolutionary Program Synthesis,https://arcprize.org/blog/arc-prize-2025-results-analysis
2021,Efficient Large-Scale Language Model Training on GPU Clusters,https://arxiv.org/pdf/2104.04473
2021,Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM,https://arxiv.org/pdf/2104.04473
2025,Efficiently Allocating Test-Time Compute for LLM Agents,https://arxiv.org/html/2509.03581v1
2019,Encoding Word Order in Complex Embeddings,https://arxiv.org/pdf/1912.12333
2015,End-To-End Memory Networks (MemN2N),https://arxiv.org/abs/1503.08895
2017,End-to-End Differentiable Proving,https://arxiv.org/abs/1705.11040
2017,End-to-End Differentiable Proving,https://arxiv.org/abs/1705.11040
2015,End-to-End Memory Networks (MemN2N),MISSING
2020,End-to-End Object Detection with Transformers (DETR),https://arxiv.org/abs/2005.12872
2020,End-to-End Object Detection with Transformers (DETR),https://arxiv.org/abs/2005.12872
2020,End-to-End Object Detection with Transformers (DETR),"https://arxiv.org/pdf/2005.12872.pdf ""DETR (2020) — arXiv"""
2024,Enhanced Enumeration Techniques for Syntax-Guided Synthesis,https://dl.acm.org/doi/10.1145/3632913
2024,Enhanced Scene Understanding on 4D Point Cloud,https://ojs.aaai.org/index.php/AAAI/article/view/28045
2025,Enhancing Modern SAT Solver With Machine Learning,https://dl.acm.org/doi/full/10.1145/3716368.3735251
2021,EntailmentBank: Explaining Answers with Entailment Trees,https://aclanthology.org/2021.emnlp-main.585.pdf
2021,Evaluating Large Language Models Trained on Code (HumanEval),https://arxiv.org/abs/2107.03374
2024,Evolutionary Test-Time Compute (write-up),"https://jeremyberman.substack.com/p/how-i-got-a-record-536-on-arc-agi ""Evolutionary Test-Time Compute (2024) — Substack"""
1972,Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons,https://pubmed.ncbi.nlm.nih.gov/4332108/
2024,Exploring Context Window of LLMs,https://proceedings.neurips.cc/paper_files/paper/2024/file/1403ab1a427050538ec59c7f570aec8b-Paper-Conference.pdf
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/pdf/1910.10683
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5),https://arxiv.org/pdf/1910.10683
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5),https://arxiv.org/pdf/1910.10683
2025,Exploring the combination of search and learn for the ARC25 challenge,https://arcprize.org/blog/arc-prize-2025-results-analysis
2023,Extending Context Window of Large Language Models via Positional Interpolation (PI),https://arxiv.org/abs/2306.15595
2022,FOLIO: Natural Language Reasoning with First-Order Logic,https://arxiv.org/abs/2209.00840
2000,Feedforward and Recurrent Processing in Vision,"https://pubmed.ncbi.nlm.nih.gov/10925037/ ""Feedforward and Recurrent Processing in Vision (2000) — PubMed"""
2024,Fixed Point Diffusion Models,"https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.pdf ""Fixed Point Diffusion Models (2024) — CVF / CVPR"""
2024,Fixed Point Diffusion Models (FPDM),https://arxiv.org/html/2401.08741v1
2022,Flamingo: a Visual Language Model for Few-Shot Learning,https://arxiv.org/abs/2204.14198
2022,Flamingo: a Visual Language Model for Few-Shot Learning,"https://arxiv.org/pdf/2204.14198.pdf ""Flamingo (2022) — arXiv"""
2024,FlashAttention-2 / efficient attention kernels,MISSING
2025,From Parrots to Von Neumanns: How Evolutionary Test-Time Compute Achieved SOTA on ARC-AGI,https://arcprize.org/blog/arc-prize-2025-results-analysis
2023,GAIA: a benchmark for General AI Assistants,https://arxiv.org/abs/2311.12983
2018,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,https://arxiv.org/abs/1804.07461
2023,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,https://arxiv.org/abs/2311.12022
2023,GPT-4 Technical Report,"https://arxiv.org/pdf/2303.08774.pdf ""GPT-4 Technical Report (2023) — arXiv"""
2019,GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering,https://arxiv.org/abs/1902.09506
2021,GSM8K: Training Verifiers to Solve Math Word Problems,https://arxiv.org/abs/2110.14168
2025,"Gated Attention for LLMs: Non-linearity, Sparsity, Sink-Free","https://arxiv.org/pdf/2505.06708.pdf ""Gated Attention for LLMs (2025) — arXiv"""
2024,Gaussian Adaptive Attention Is All You Need,"https://arxiv.org/pdf/2401.11143.pdf ""Gaussian Adaptive Attention (2024) — arXiv"""
2023,Gemini: A Family of Highly Capable Multimodal Models,"https://arxiv.org/pdf/2312.11805.pdf ""Gemini (2023) — arXiv"""
2015,"General Program Synthesis Benchmark Suite (PSB / ""PSB1"")",https://www.cs.hamilton.edu/~thelmuth/Pubs/2015-GECCO-benchmark-suite.pdf
2015,General Program Synthesis Benchmark Suite (PSB / PSB1),https://www.cs.hamilton.edu/~thelmuth/Pubs/2015-GECCO-benchmark-suite.pdf
2024,Generalized Planning for the Abstraction and Reasoning Corpus (GPAR),https://arxiv.org/abs/2401.07426
2019,Generating Long Sequences with Sparse Transformers,https://arxiv.org/abs/1904.10509
2019,Generating Long Sequences with Sparse Transformers,https://arxiv.org/abs/2203.16527
2014,Generative Adversarial Networks (GANs),MISSING
2020,Generative Pretraining from Pixels,"https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf ""Generative Pretraining from Pixels (2020) — OpenAI"""
2020,Generative Pretraining from Pixels,"https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf ""Generative Pretraining from Pixels (2020) — OpenAI"""
2020,Generative Pretraining from Pixels (iGPT),https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf
2020,Generative Pretraining from Pixels (iGPT),https://arxiv.org/abs/2005.12872
2024,Generative Verifiers / GenRM: Reward Modeling as Next-Token Prediction,https://neurips.cc/virtual/2024/104300
2021,Goal-Aware Neural SAT Solver (QuerySAT / goal-aware guidance),https://arxiv.org/pdf/2106.07162
2025,Graph Perceiver IO,https://www.sciencedirect.com/science/article/abs/pii/S0031320325005497
2023,Graph of Thoughts (GoT),https://arxiv.org/abs/2308.09687
2021,Graphormer: Do Transformers Really Perform Bad for Graph Representation?,https://arxiv.org/abs/2106.05234
2023,Grokking Modular Arithmetic,https://arxiv.org/abs/2301.02679
2024,Grokking Modular Polynomials,https://arxiv.org/abs/2406.03495
2022,Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets,https://arxiv.org/pdf/2201.02177
2019,Guiding SAT Solvers with Unsat-Core Predictions (NeuroCore),https://arxiv.org/pdf/1903.04671
2025,H-ARC (Human-ARC): A Comprehensive Behavioral Dataset for the Abstraction and Reasoning Corpus,https://www.nature.com/articles/s41597-025-05687-1
2024,H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark,"https://arxiv.org/pdf/2409.01374.pdf ""H-ARC (2024) — arXiv"""
2025,HARPE: Head-Adaptive Rotary Position Encoding,https://aclanthology.org/2025.coling-main.326/
2024,HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion,https://arxiv.org/abs/2310.14566
2025,Head-Wise Adaptive Rotary Positional Encoding (HARoPE),"https://arxiv.org/pdf/2510.10489.pdf ""HARoPE (2025) — arXiv"""
2019,HellaSwag: Can a Machine Really Finish Your Sentence?,https://arxiv.org/abs/1905.07830
2024,HiRoPE: Length Extrapolation for Code Models Using Hierarchical Rotary Position Embedding,https://aclanthology.org/2024.acl-long.735/
2025,Hierarchical Reasoning Model (HRM),https://arxiv.org/html/2506.21734v1
2025,Hierarchical Reasoning Model (HRM),"https://arxiv.org/pdf/2506.21734.pdf ""Hierarchical Reasoning Model (2025) — arXiv"""
2014,Hierarchy of Intrinsic Timescales in Cortex,"https://www.cns.nyu.edu/wanglab/publications/pdf/murray.nn2014.pdf ""Hierarchy of Intrinsic Timescales in Cortex (2014) — NYU"""
2015,High-Dimensional Continuous Control Using Generalized Advantage Estimation (GAE),https://arxiv.org/abs/1506.02438
2022,Holistic Evaluation of Language Models (HELM),https://arxiv.org/abs/2211.09110
2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://arxiv.org/abs/1809.09600
2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://aclanthology.org/D18-1259/
2020,How Can Self-Attention Networks Recognize Dyck-n Languages?,https://aclanthology.org/2020.findings-emnlp.384.pdf
2020,How Much Position Information Do Convolutional Neural Networks Encode?,https://openreview.net/pdf/2267055f8221e283014aba7ef46092ba93ff450f.pdf
2019,HowTo100M: Learning a Text-Video Embedding by Watching Narrated Videos,"https://arxiv.org/pdf/1906.03327.pdf ""HowTo100M (2019) — arXiv"""
2015,Human-level Control through Deep Reinforcement Learning (DQN),https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf
2015,Human-level Control through Deep Reinforcement Learning (DQN),https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf
2013,ICFP Programming Competition,https://dspace.mit.edu/bitstream/handle/1721.1/137904/1611.07627.pdf?isAllowed=y&sequence=2
2018,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://arxiv.org/abs/1802.01561
2018,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://arxiv.org/abs/1802.01561
2018,Image Transformer,https://arxiv.org/abs/1802.05751
2018,Image Transformer,https://arxiv.org/abs/1802.05751
2022,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,"https://arxiv.org/pdf/2208.10442.pdf ""Image as a Foreign Language / BEiT pretraining (2022) — arXiv"""
2022,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,"https://arxiv.org/pdf/2208.10442.pdf ""Image as a Foreign Language (2022) — arXiv"""
2012,ImageNet Classification with Deep Convolutional Neural Networks (AlexNet),MISSING
2009,ImageNet: A Large-Scale Hierarchical Image Database,https://www.image-net.org/static_files/papers/imagenet_cvpr09.pdf
2024,Implicit Factorized Transformer (IFactFormer),https://www.sciencedirect.com/science/article/pii/S2095034924000382
2018,Improving Language Understanding by Generative Pre-Training,https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
2015,Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,https://arxiv.org/abs/1503.01007
1957,Information Theory and Statistical Mechanics,https://bayes.wustl.edu/etj/articles/theory.1.pdf
2023,Instruction-Following Evaluation for Large Language Models (IFEval),https://arxiv.org/abs/2311.07911
2024,Interactive4D: Interactive 4D LiDAR Segmentation,https://arxiv.org/abs/2410.08206
2021,Is Space-Time Attention All You Need for Video Understanding? (TimeSformer),"https://arxiv.org/pdf/2102.05095.pdf ""TimeSformer (2021) — arXiv"""
2021,Jacobian regularization for equilibrium models,MISSING
2022,Jacobian-Free Backpropagation for Implicit Networks (JFB),https://www.math.emory.edu/site/cmds-reuret/projects/2022-implicit/JFB.pdf
2023,Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,https://arxiv.org/abs/2306.05685
2020,Just-in-Time Learning for Bottom-Up Enumerative Synthesis (PROBE),https://cseweb.ucsd.edu/~hpeleg/probe-oopsla20.pdf
2022,KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,https://arxiv.org/abs/2205.09921
Year unknown,Karel dataset,https://msr-redmond.github.io/karel-dataset/
2023,Kosmos-1: Language Is Not All You Need,"https://arxiv.org/pdf/2302.14045.pdf ""Kosmos-1 (2023) — arXiv"""
2021,LAION-400M: Open Dataset for CLIP Training,"https://arxiv.org/pdf/2111.02114.pdf ""LAION-400M (2021) — arXiv"""
2022,LAION-5B: An Open Large-Scale Dataset for Training Next Generation Image-Text Models,"https://arxiv.org/pdf/2210.08402.pdf ""LAION-5B (2022) — arXiv"""
2022,LAION-5B: An open large-scale dataset for training next generation image-text models,https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/
2022,LIFT: Learning 4D LiDAR Image Fusion Transformer for 3D Object Detection,https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_LIFT_Learning_4D_LiDAR_Image_Fusion_Transformer_for_3D_Object_CVPR_2022_paper.pdf
2022,LIFT: Learning 4D LiDAR Image Fusion Transformer for 3D Object Detection,https://openaccess.thecvf.com/content/CVPR2022/papers/Zeng_LIFT_Learning_4D_LiDAR_Image_Fusion_Transformer_for_3D_Object_CVPR_2022_paper.pdf
2024,LLFormer4D: LiDAR-based lane detection method,https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cvi2.12338
2025,LLaVA-4D: Embedding Spatiotemporal Prompt into LMMs,"https://arxiv.org/pdf/2505.12253.pdf ""LLaVA-4D (2025) — arXiv"""
2023,LLaVA: Large Language-and-Vision Assistant,"https://arxiv.org/pdf/2304.08485.pdf ""LLaVA (2023) — arXiv"""
2025,LOOPE: Learnable Optimal Patch Order in Vision Transformers,"https://arxiv.org/pdf/2504.14386.pdf ""LOOPE (2025) — arXiv"""
2024,LTD-Bench: Evaluating Large Language Models by Letting Them Draw,https://openreview.net/forum?id=TG5rvKyEbu
2025,LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias,https://proceedings.iclr.cc/paper_files/paper/2025/hash/9676c5283df26cabca412ca66b164a7d-Abstract-Conference.html
2019,LXMERT: Learning Cross-Modality Encoder Representations,"https://arxiv.org/pdf/1908.07490.pdf ""LXMERT (2019) — arXiv"""
2023,Language Agent Tree Search (LATS),https://arxiv.org/abs/2310.04406
2020,Language Models are Few-Shot Learners (GPT-3),MISSING
2020,Language Models are Few-Shot Learners (GPT-3),https://arxiv.org/pdf/2005.14165
2020,Language Models are Few-Shot Learners (GPT-3),https://arxiv.org/pdf/2005.14165
2021,Latent Execution for Neural Program Synthesis (LaSynth),https://proceedings.neurips.cc/paper/2021/file/ba3c95c2962d3aab2f6e667932daa3c5-Paper.pdf
2016,Layer Normalization,https://arxiv.org/abs/1607.06450
2023,LeaF: Learning Frames for 4D Point Cloud Sequence Understanding,https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_LeaF_Learning_Frames_for_4D_Point_Cloud_Sequence_Understanding_ICCV_2023_paper.pdf
2022,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering (ScienceQA),https://arxiv.org/abs/2209.09513
2018,Learning Explanatory Rules from Noisy Data / Differentiable ILP (dILP),https://jair.org/index.php/jair/article/view/11172/26376
2024,Learning Iterative Reasoning through Energy Diffusion,"https://arxiv.org/pdf/2406.11179.pdf ""Energy Diffusion Iterative Reasoning (2024) — arXiv"""
2025,Learning Modular Exponentiation with Transformers,https://arxiv.org/html/2506.23679v1
1986,Learning Representations by Back-Propagating Errors,https://www.nature.com/articles/323533a0
2012,Learning Semantic String Transformations from Examples,https://vldb.org/pvldb/vol5/p740_rishabhsingh_vldb2012.pdf
2012,Learning Semantic String Transformations from Examples,MISSING
2021,Learning Transferable Visual Models From Natural Language Supervision (CLIP),https://arxiv.org/pdf/2103.00020
2021,Learning Transferable Visual Models From Natural Language Supervision (CLIP),https://arxiv.org/pdf/2103.00020
2021,Learning Transferable Visual Models From Natural Language Supervision (CLIP),"https://arxiv.org/pdf/2103.00020.pdf ""CLIP (2021) — arXiv"""
2018,Learning a SAT Solver from Single-Bit Supervision (NeuroSAT),https://arxiv.org/abs/1802.03685
2020,Learning to Encode Position for Transformer with Continuous Dynamical Model (FLOATER),https://arxiv.org/pdf/2003.09229
2014,Learning to Execute,https://arxiv.org/abs/1410.4615
1988,Learning to Predict by the Methods of Temporal Differences,https://incompleteideas.net/papers/sutton-88-with-erratum.pdf
2015,Learning to Transduce with Unbounded Memory,https://arxiv.org/abs/1506.02516
2003,Least-Squares Policy Iteration (LSPI),https://www.jmlr.org/papers/v4/lagoudakis03a.html
2024,Length Extrapolation of Causal Transformers without Position Encoding (NoPE),"https://aclanthology.org/2024.findings-acl.834.pdf ""NoPE (2024) — ACL Anthology"""
2024,Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding,"https://aclanthology.org/2024.findings-emnlp.582.pdf ""Length Extrapolation Survey (2024) — ACL Anthology"""
2024,Length Generalization of Causal Transformers without Position Encoding,https://arxiv.org/abs/2404.12224
2024,Length-Controlled AlpacaEval,https://arxiv.org/abs/2404.04475
2025,Less is More: Recursive Reasoning with Tiny Networks,"https://arxiv.org/pdf/2510.04871.pdf ""Less is More: Recursive Reasoning with Tiny Networks (2025) — arXiv"""
2025,Less is More: Recursive Reasoning with Tiny Networks (TRM),https://ar5iv.org/abs/2510.04871
2025,Less is More: Recursive Reasoning with Tiny Networks (TRM),https://arcprize.org/blog/arc-prize-2025-results-analysis
2018,Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,https://arxiv.org/abs/1805.04276
2018,Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,https://arxiv.org/abs/1805.04276
2017,Lie-Access Neural Turing Machines (LANTM),https://openreview.net/pdf?id=Byiy-Pqlx
2024,LieRE: Lie Rotational Positional Encodings,"https://arxiv.org/pdf/2406.10322.pdf ""LieRE (2024) — arXiv"""
2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://arxiv.org/abs/1804.06028
2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://aclanthology.org/N18-4013/
2024,"LiveBench: A Challenging, Contamination-Free LLM Benchmark",https://arxiv.org/abs/2406.19314
2021,LoRA: Low-Rank Adaptation of Large Language Models,https://arxiv.org/abs/2106.09685
2023,LogiQA 2.0 - An Improved Dataset for Logical Reasoning in NLU,https://frcchang.github.io/pub/An%20Improved%20Dataset%20for%20Logical%20Reasoning%20in%20Natural%20Language%20Understanding.pdf
2020,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,https://www.ijcai.org/proceedings/2020/501
2016,Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge,https://arxiv.org/abs/1606.04422
2020,Long Range Arena (LRA): A Benchmark for Efficient Transformers,https://arxiv.org/abs/2011.04006
1997,Long Short-Term Memory (LSTM),https://www.bioinf.jku.at/publications/older/2604.pdf
2023,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",https://arxiv.org/abs/2308.14508
2024,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,https://arxiv.org/pdf/2402.13753
2024,LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate,https://arxiv.org/abs/2405.13985
2021,MATH: Measuring Mathematical Problem Solving,https://arxiv.org/abs/2103.03874
2024,"MHaluBench (from ""Unified Hallucination Detection..."")",https://aclanthology.org/2024.acl-long.178.pdf
2024,MM-Vet v2,https://arxiv.org/abs/2408.00765
2023,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,https://arxiv.org/abs/2308.02490
2023,MMBench: Evaluating Multimodal LLMs,"https://arxiv.org/pdf/2307.06281.pdf ""MMBench (2023) — arXiv"""
2023,MMBench: Is Your Multi-modal Model an All-around Player?,https://arxiv.org/abs/2307.06281
2023,MMBench: Is Your Multi-modal Model an All-around Player?,https://arxiv.org/abs/2307.06281
2023,MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models,https://arxiv.org/abs/2306.13394
2024,MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,https://arxiv.org/abs/2406.01574
2025,MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding and Reasoning Benchmark,https://aclanthology.org/2025.acl-long.736.pdf
2023,MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,https://arxiv.org/abs/2311.16502
2023,MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,https://arxiv.org/abs/2311.16502
2023,MMMU: A Massive Multidiscipline Multimodal Benchmark,"https://arxiv.org/pdf/2311.16502.pdf ""MMMU (2023) — arXiv"""
2024,MT-Bench-101,https://aclanthology.org/2024.acl-long.401.pdf
2024,MVBench: A Comprehensive Multi-modal Video Understanding Benchmark,https://openaccess.thecvf.com/content/CVPR2024/papers/Li_MVBench_A_Comprehensive_Multi-modal_Video_Understanding_Benchmark_CVPR_2024_paper.pdf
2024,Machine Learning for Modular Multiplication,https://arxiv.org/html/2402.19254v1
2006,Making Working Memory Work: A Computational Model of Learning in the Prefrontal Cortex and Basal Ganglia,https://pubmed.ncbi.nlm.nih.gov/16378516/
2022,Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation,https://arxiv.org/abs/2112.01527
2023,Mask4D: End-to-End Mask-Based 4D Panoptic Segmentation for LiDAR Sequences,https://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/marcuzzi2023ral-meem.pdf
2023,Mask4Former: Mask Transformer for 4D Panoptic Segmentation,https://arxiv.org/abs/2309.16133
2023,Mask4Former: Mask Transformer for 4D Panoptic Segmentation,https://arxiv.org/abs/2309.16133
2021,MaskFormer,https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.pdf
2021,MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation,https://arxiv.org/abs/2107.06278
2022,MaskGIT: Masked Generative Image Transformer,https://arxiv.org/abs/2202.04200
2022,Masked Autoencoders Are Scalable Vision Learners (MAE),https://arxiv.org/abs/2111.06377
2021,Masked Autoencoders Are Scalable Vision Learners (MAE),"https://arxiv.org/pdf/2111.06377.pdf ""MAE (2021) — arXiv"""
2022,Masked Autoencoders for Point Cloud Self-supervised Learning (Point-MAE),https://arxiv.org/abs/2203.06604
2019,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero)",https://www.nature.com/articles/s41586-020-03051-4
2020,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero)",https://www.nature.com/articles/s41586-020-03051-4
2017,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (AlphaZero),https://arxiv.org/abs/1712.01815
2025,Mastering diverse control tasks through world models (DreamerV3),https://www.nature.com/articles/s41586-025-08744-2
2016,Mastering the Game of Go with Deep Neural Networks and Tree Search (AlphaGo),https://www.nature.com/articles/nature16961
2025,MathClean: A Benchmark for Synthetic Mathematical Data,https://arxiv.org/html/2502.19058v1
2023,MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts,https://arxiv.org/abs/2310.02255
2023,MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts,https://arxiv.org/abs/2310.02255
2022,MaxViT: Multi-Axis Vision Transformer,"https://arxiv.org/pdf/2204.01697.pdf ""MaxViT (2022) — arXiv"""
2022,MaxViT: Multi-Axis Vision Transformer,"https://arxiv.org/pdf/2204.01697.pdf ""MaxViT (2022) — arXiv"""
2025,Maximizing the Position Embedding for Vision Transformers (MPVG),"# ""MPVG (2025) — Local Copy (no public PDF link provided)"""
2018,Maximum a Posteriori Policy Optimisation (MPO),https://arxiv.org/abs/1806.06920
2020,Measuring Massive Multitask Language Understanding (MMLU),https://arxiv.org/abs/2009.03300
2021,Measuring Mathematical Problem Solving With the MATH Dataset,https://arxiv.org/abs/2103.03874
2019,Megatron-LM / large-scale transformer training,MISSING
2022,Memorizing Transformers,https://arxiv.org/abs/2203.08913
2022,Memorizing Transformers / kNN-augmented attention (inference-time memory),MISSING
2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://arxiv.org/abs/2410.15859
2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,"https://proceedings.neurips.cc/paper_files/paper/2024/file/9446c291a8744a125a0bda5b18f4d5a1-Paper-Conference.pdf ""Mesa-Extrapolation (2024) — NeurIPS Proceedings"""
1847,Methode generale pour la resolution des systemes d'equations simultanees,https://ems.press/content/book-chapter-files/27368?nt=1
2014,Microsoft COCO: Common Objects in Context,https://arxiv.org/abs/1405.0312
2005,Microstructure of a spatial map in the entorhinal cortex,https://www.nature.com/articles/nature03721
2025,MindsAI,https://arcprize.org/competitions/2025/
2024,Mini-ARC: Solving Abstraction and Reasoning Puzzles with Small Transformer Models,https://www.paulfletcherhill.com/mini-arc.pdf
2024,Mini-ARC: Solving Abstraction and Reasoning Puzzles with Small Transformer Models,https://arcprize.org/competitions/2024/
2023,MiniGPT-4,"https://arxiv.org/pdf/2304.10592.pdf ""MiniGPT-4 (2023) — arXiv"""
2022,MuSiQue: Multihop Questions via Single-hop Question Composition,https://arxiv.org/abs/2108.00573
2021,Muesli: Combining Improvements in Policy Optimization,https://proceedings.mlr.press/v139/hessel21a/hessel21a.pdf
2021,Multiscale Vision Transformers (MViT),https://arxiv.org/abs/2104.11227
2018,Music Transformer,https://arxiv.org/pdf/1809.04281
2018,Music Transformer,https://arxiv.org/pdf/1809.04281
2018,NAPS: Natural Program Synthesis Dataset,https://arxiv.org/pdf/1807.03168
2019,NEZHA: Neural Contextualized Representation for Chinese Language Understanding,https://arxiv.org/pdf/1909.00204
2019,NLVR2: A Corpus for Reasoning about Natural Language Grounded in Photographs,https://lil.nlp.cornell.edu/nlvr/
2025,NVARC,https://arcprize.org/competitions/2025/
2025,NVARC solution to ARC-AGI-2 2025,https://arcprize.org/blog/arc-prize-2025-results-analysis
2008,Natural Actor-Critic,https://www.sciencedirect.com/science/article/pii/S0925231208000532
2025,Nested Learning: The Illusion of Deep Learning Architecture,"# ""Nested Learning (2025) — Local Copy (no public PDF link provided)"""
2005,Neural Fitted Q Iteration (NFQ),https://link.springer.com/chapter/10.1007/11564096_32
2015,Neural GPUs Learn Algorithms,https://arxiv.org/abs/1511.08228
2019,Neural Logic Machines (NLM),https://arxiv.org/abs/1904.11694
2016,Neural Module Networks (NMN),https://openaccess.thecvf.com/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html
2018,Neural Ordinary Differential Equations (Neural ODEs),https://arxiv.org/abs/1806.07366
2017,Neural Program Meta-Induction,https://papers.nips.cc/paper/6803-neural-program-meta-induction
2015,Neural Programmer-Interpreters (NPI),https://arxiv.org/abs/1511.06279
2015,Neural Programmer-Interpreters (NPI),MISSING
2015,Neural Programmer: Inducing Latent Programs with Gradient Descent,https://arxiv.org/abs/1511.04834
2017,Neural Theorem Provers / Differentiable Proving,MISSING
2014,Neural Turing Machines,"https://arxiv.org/pdf/1410.5401.pdf ""Neural Turing Machines (2014) — arXiv"""
2014,Neural Turing Machines (NTM),https://arxiv.org/abs/1410.5401
2014,Neural Turing Machines (NTM),https://arxiv.org/abs/1410.5401
1982,Neural networks and physical systems with emergent collective computational abilities,https://www.pnas.org/doi/10.1073/pnas.79.8.2554
1983,Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems,https://incompleteideas.net/papers/barto-sutton-anderson-83.pdf
1961,New Results in Linear Filtering and Prediction Theory,https://asmedigitalcollection.asme.org/fluidsengineering/article/83/1/95/426820/New-Results-in-Linear-Filtering-and-Prediction
2021,Nystromformer: A Nystrom-Based Algorithm for Approximating Self-Attention,https://arxiv.org/pdf/2102.03902
2019,OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge,https://openaccess.thecvf.com/content_CVPR_2019/papers/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.pdf
2025,ONERULER: Benchmarking multilingual long-context language models,https://arxiv.org/abs/2503.01996
2020,OSCAR: Object-Semantics Aligned Pre-training for Vision-Language Tasks,"https://arxiv.org/pdf/2004.06165.pdf ""OSCAR (2020) — arXiv"""
2024,OccSora,https://github.com/wzzheng/OccSora
2024,OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02024.pdf
2024,OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02024.pdf
2025,Olympiad-level formal mathematical reasoning with large language models (AlphaProof),"https://www.nature.com/articles/s41586-025-09833-y.pdf ""AlphaProof (2025) — Nature"""
2024,Omni-ARC,https://arcprize.org/blog/arc-prize-2024-winners-technical-report
2024,Omni-ARC,https://arcprize.org/competitions/2024/
2019,On the Measure of Intelligence,https://arxiv.org/abs/1911.01547
2019,On the Measure of Intelligence,"https://arxiv.org/pdf/1911.01547.pdf ""On the Measure of Intelligence (2019) — arXiv"""
2019,On the Measure of Intelligence (ARC),https://arxiv.org/abs/1911.01547
2019,On the Measure of Intelligence (ARC),https://cavalab.org/srbench/
2019,On the Measure of Intelligence / ARC (Abstraction and Reasoning Corpus),MISSING
1994,On-line Q-learning Using Connectionist Systems,https://www.researchgate.net/profile/Mahesan-Niranjan/publication/2500611_On-Line_Q-Learning_Using_Connectionist_Systems/links/5438d5db0cf204cab1d6db0f/On-Line-Q-Learning-Using-Connectionist-Systems.pdf
2023,"One-Step Diffusion Distillation via Deep Equilibrium Models (Generative Equilibrium Transformer, GET)",https://papers.neurips.cc/paper_files/paper/2023/file/82f05a105c928c10706213952bf0c8b7-Paper-Conference.pdf
2024,OneFormer3D: One Transformer for Unified Point Cloud Segmentation,https://openaccess.thecvf.com/content/CVPR2024/papers/Kolodiazhnyi_OneFormer3D_One_Transformer_for_Unified_Point_Cloud_Segmentation_CVPR_2024_paper.pdf
2024,Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance,https://openaccess.thecvf.com/content/CVPR2024/papers/Nguyen_Open3DIS_Open-Vocabulary_3D_Instance_Segmentation_with_2D_Mask_Guidance_CVPR_2024_paper.pdf
2018,OpenBookQA: Can a Suit of Armor Conduct Electricity?,https://arxiv.org/abs/1809.02789
2017,OptNet: Differentiable Optimization as a Layer in Neural Networks,https://arxiv.org/abs/1703.00443
2002,Optimal feedback control as a theory of motor coordination,https://pubmed.ncbi.nlm.nih.gov/12404008/
2004,Optimality principles in sensorimotor control,https://pubmed.ncbi.nlm.nih.gov/15332089/
2020,PCT: Point Cloud Transformer,https://arxiv.org/abs/2012.09688
2021,PCT: Point Cloud Transformer,https://link.springer.com/article/10.1007/s41095-021-0229-5
2025,PMFormer: Point mask transformer for outdoor point cloud semantic segmentation,https://www.sciopen.com/article/10.26599/CVM.2025.9450388
2023,"POPE (Polling-based Object Probing Evaluation) / ""Evaluating Object Hallucination in Large Vision-Language Models""",https://arxiv.org/abs/2305.10355
2024,POS-BERT: Point cloud one-stage BERT pre-training,https://www.sciencedirect.com/science/article/abs/pii/S0957417423030658
2021,PROGRES: A large-scale benchmark for few-shot program induction and synthesis,https://proceedings.mlr.press/v139/alet21a.html
Year unknown,PSB / PSB2,https://www.cs.hamilton.edu/~thelmuth/Pubs/2015-GECCO-benchmark-suite.pdf
2021,PSB2: The Second Program Synthesis Benchmark Suite,https://arxiv.org/pdf/2106.06086
2021,PSB2: The Second Program Synthesis Benchmark Suite,https://arxiv.org/pdf/2106.06086
2025,PV-DT3D: Point-voxel dual transformer for LiDAR 3D object detection,https://link.springer.com/article/10.1007/s11801-025-3134-9
2023,PaLM-E: An Embodied Multimodal Language Model,"https://arxiv.org/pdf/2303.03378.pdf ""PaLM-E (2023) — arXiv"""
2025,PanSt3R: Multi-view Consistent Panoptic Segmentation,https://openaccess.thecvf.com/content/ICCV2025/papers/Zust_PanSt3R_Multi-view_Consistent_Panoptic_Segmentation_ICCV_2025_paper.pdf
2021,Perceiver / Perceiver IO,https://arxiv.org/abs/2103.03206
2023,Plan-and-Solve Prompting,https://arxiv.org/abs/2305.04091
2021,Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos (P4Transformer),https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_Point_4D_Transformer_Networks_for_Spatio-Temporal_Modeling_in_Point_Cloud_CVPR_2021_paper.pdf
2021,Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos (P4Transformer),https://openaccess.thecvf.com/content/CVPR2021/papers/Fan_Point_4D_Transformer_Networks_for_Spatio-Temporal_Modeling_in_Point_Cloud_CVPR_2021_paper.pdf
2022,Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding,https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136890018.pdf
2020,Point Transformer,https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf
2021,Point Transformer,https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_Point_Transformer_ICCV_2021_paper.pdf
2024,"Point Transformer V3: Simpler, Faster, Stronger",https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Point_Transformer_V3_Simpler_Faster_Stronger_CVPR_2024_paper.pdf
2024,"Point Transformer V3: Simpler, Faster, Stronger",https://openaccess.thecvf.com/content/CVPR2024/papers/Wu_Point_Transformer_V3_Simpler_Faster_Stronger_CVPR_2024_paper.pdf
2021,Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling,https://arxiv.org/abs/2111.14819
2022,Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling,https://arxiv.org/abs/2111.14819
2024,PointRegion: Transformer based 3D tooth segmentation via point cloud processing,https://pubmed.ncbi.nlm.nih.gov/39557955/
2000,Policy Gradient Methods for Reinforcement Learning with Function Approximation,https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation
2021,PonderNet: Learning to Ponder,https://arxiv.org/abs/2107.05407
2021,PonderNet: Learning to Ponder,https://arxiv.org/abs/2107.05407
1999,Predictive coding in the visual cortex,https://pmc.ncbi.nlm.nih.gov/articles/PMC4311762/
1999,Predictive coding in the visual cortex: a functional interpretation,https://pubmed.ncbi.nlm.nih.gov/10195184/
2015,Prioritized Experience Replay,https://arxiv.org/abs/1511.05952
2015,Prioritized Experience Replay,https://arxiv.org/abs/1511.05952
2023,Process Reward Models (PRM) introduced/standardized for reasoning traces,MISSING
2025,Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective,https://arxiv.org/abs/2505.07859
2025,Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective,https://arcprize.org/blog/arc-prize-2025-results-analysis
2025,"Productive ""refinement loop"" systems from ARC Prize writeups (2025 winners list)",https://arcprize.org/blog/arc-prize-2025-results-analysis
2021,"ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",https://aclanthology.org/2021.findings-acl.317.pdf
2017,Proximal Policy Optimization (PPO),https://arxiv.org/abs/1707.06347
2017,Proximal Policy Optimization (PPO),https://arxiv.org/abs/1707.06347
2021,Pyramid Vision Transformer (PVT),https://arxiv.org/abs/2102.12122
2021,Pyramid Vision Transformer (PVT),https://arxiv.org/abs/2102.12122
2021,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,"https://arxiv.org/pdf/2102.12122.pdf ""Pyramid Vision Transformer (2021) — arXiv"""
2021,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,"https://arxiv.org/pdf/2102.12122.pdf ""PVT (2021) — arXiv"""
2023,QLoRA,MISSING
2025,R-PRM: Reasoning-Driven Process Reward Modeling,https://aclanthology.org/2025.emnlp-main.679.pdf
2020,REALM: Retrieval-Augmented Language Model Pre-Training,https://arxiv.org/abs/2002.08909
2020,REALM: Retrieval-Augmented Language Model Pre-Training,MISSING
2024,RETRO-style retrieval-augmented pretraining and variants,https://arxiv.org/abs/2112.04426
2024,RULER: What's the Real Context Size of Your Long-Context Language Models?,https://arxiv.org/abs/2404.06654
2018,Rainbow: Combining Improvements in Deep Reinforcement Learning,https://arxiv.org/abs/1710.02298
2022,ReAct: Synergizing Reasoning and Acting in Language Models,https://arxiv.org/abs/2210.03629
2022,ReAct: Synergizing Reasoning and Acting in Language Models,https://arxiv.org/abs/2210.03629
2022,ReAct: Synergizing Reasoning and Acting in Language Models,https://arxiv.org/abs/2210.03629
2020,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning,https://arxiv.org/abs/2002.04326
2020,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,https://arxiv.org/abs/2009.11462
2023,Reasoning with Language Model is Planning with World Model (RAP),https://arxiv.org/abs/2305.14992
1962,"Receptive Fields, Binocular Interaction, and Functional Architecture in the Cat's Visual Cortex",https://www.gatsby.ucl.ac.uk/~lmatthey/teaching/tn1/additional/systems/JPhysiol-1962-Hubel-106-54.pdf
1959,Receptive fields of single neurones in the cat's striate cortex,https://pmc.ncbi.nlm.nih.gov/articles/PMC1363130/
2019,Recurrent Experience Replay in Distributed Reinforcement Learning (R2D2),https://openreview.net/forum?id=r1lyTjAqYX
2018,Recurrent Relational Networks,"https://proceedings.neurips.cc/paper_files/paper/2018/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf ""Recurrent Relational Networks (2018) — NeurIPS"""
2018,Recurrent Relational Networks (RRN),https://papers.neurips.cc/paper/7597-recurrent-relational-networks.pdf
2018,Recurrent Relational Networks (RRN),https://arxiv.org/abs/1711.08028
2025,Reflection System for the Abstraction and Reasoning Corpus,https://openreview.net/forum?id=kRFwzuv0ze
2023,Reflexion / Self-Refine family (test-time improvement loops),MISSING
2023,Reflexion: Language Agents with Verbal Reinforcement Learning,https://arxiv.org/abs/2303.11366
2025,Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation,https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Relation3D__Enhancing_Relation_Modeling_for_Point_Cloud_Instance_Segmentation_CVPR_2025_paper.html
2010,Relative Entropy Policy Search (REPS),https://ojs.aaai.org/index.php/AAAI/article/view/7727
2024,Repeated Examples Help Learn Arithmetic,https://openreview.net/pdf?id=qoUHqnE6A0
2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://arxiv.org/abs/2403.00071
2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://aclanthology.org/2024.findings-acl.32.pdf
2025,RetentiveBEV: BEV transformer for visual 3D object detection,https://journals.sagepub.com/doi/10.1177/01423312241308367
2020,Rethinking Attention with Performers,https://arxiv.org/pdf/2009.14794
2020,Rethinking Attention with Performers,https://arxiv.org/pdf/2009.14794
2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://openreview.net/pdf?id=09-528y2Fgf
2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://arxiv.org/pdf/2006.15595
2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://arxiv.org/pdf/2006.15595
2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://arxiv.org/pdf/2006.15595
2025,Rethinking Visual Intelligence: Insights from Video Pretraining,https://arcprize.org/blog/arc-prize-2025-results-analysis
2021,Rethinking and Improving Relative Position Encoding for Vision Transformer (iRPE),https://arxiv.org/abs/2107.14222
2025,Revisiting the Test-Time Scaling of o1-like Models,https://aclanthology.org/2025.acl-long.232.pdf
2024,"Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning (PAV / ""progress rewards"")",https://arxiv.org/abs/2410.08146
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding,https://arxiv.org/pdf/2104.09864
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding,https://arxiv.org/pdf/2104.09864
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding,https://arxiv.org/pdf/2104.09864
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE),https://arxiv.org/abs/2104.09864
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE),"https://arxiv.org/pdf/2104.09864.pdf ""RoFormer (2021) — arXiv"""
2024,RoPE scaling analysis + new scaling,https://aclanthology.org/2024.emnlp-main.414.pdf
2024,RoTHP: Rotary Position Embedding-based Transformer Hawkes Process,"https://arxiv.org/pdf/2405.06985.pdf ""RoTHP (2024) — arXiv"""
2017,RobustFill: Neural Program Learning under Noisy I/O,https://arxiv.org/abs/1703.07469
2017,RobustFill: Neural Program Learning under Noisy I/O,https://github.com/thelmuth/program-synthesis-benchmark-datasets
2025,Rotary Masked Autoencoders Are Versatile Learners,"https://arxiv.org/pdf/2505.20535.pdf ""Rotary Masked Autoencoders (2025) — arXiv"""
2024,Rotary Position Embedding for Vision Transformer,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01584.pdf
2024,Rotary Position Embedding for Vision Transformer,https://arxiv.org/abs/2403.13298
2024,Rotary Position Embedding for Vision Transformer (RoPE-Mixed / 2D RoPE study),"https://arxiv.org/pdf/2403.13298.pdf ""RoPE for ViT / RoPE-Mixed 2D study (2024) — arXiv"""
2019,SATNet: Bridging Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver,https://arxiv.org/pdf/1905.12149
2019,SATNet: Differentiable Satisfiability Solver,https://arxiv.org/abs/1905.12149
2020,SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks,https://proceedings.neurips.cc/paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html
2020,SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks,https://proceedings.neurips.cc/paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html
2024,SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension,https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SEED-Bench_Benchmarking_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf
2021,SETR: Rethinking Semantic Segmentation as Seq2Seq with Transformers,https://arxiv.org/abs/2012.15840
2020,SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,https://arxiv.org/abs/2012.15840
2016,"SQuAD: 100,000+ Questions for Machine Comprehension of Text",https://arxiv.org/abs/1606.05250
2022,SRBench,https://github.com/cavalab/srbench
2021,SRBench (Symbolic Regression Benchmarks),https://cavalab.github.io/symbolic-regression/
2025,"SRBench update (""next generation"" SRBench)",https://arxiv.org/html/2505.03977v1
2024,SRBench++: principled benchmarking of symbolic regression,https://pmc.ncbi.nlm.nih.gov/articles/PMC12321164/
2024,SRBench++: principled benchmarking of symbolic regression,https://pubmed.ncbi.nlm.nih.gov/40761553/
2022,STaR: Self-Taught Reasoner (Bootstrapping Reasoning With Reasoning),https://arxiv.org/abs/2203.14465
2021,SVAMP: Simple Variations on Arithmetic Math Word Problems,https://aclanthology.org/2021.naacl-main.168/
2024,SWE-bench,https://proceedings.iclr.cc/paper_files/paper/2024/file/edac78c3e300629acfe6cbe9ca88fb84-Paper-Conference.pdf
2023,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,https://arxiv.org/abs/2310.06770
2016,Safe and Efficient Off-Policy Reinforcement Learning (Retrace(lambda)),https://arxiv.org/abs/1606.02647
2022,Scalable Diffusion Models with Transformers (DiT),https://arxiv.org/abs/2212.09748
2024,Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters,"https://arxiv.org/pdf/2408.03314.pdf ""Scaling LLM Test-Time Compute Optimally (2024) — arXiv"""
2020,Scaling Laws for Neural Language Models,https://arxiv.org/abs/2001.08361
2025,Scaling Transformer-Based Novel View Synthesis with Models Token Disentanglement and Synthetic Data,https://openaccess.thecvf.com/content/ICCV2025/papers/Nair_Scaling_Transformer-Based_Novel_View_Synthesis_with_Models_Token_Disentanglement_and_ICCV_2025_paper.pdf
2021,Scaling Up Vision-Language Learning With Noisy Text Supervision (ALIGN),"https://arxiv.org/pdf/2102.05918.pdf ""ALIGN (2021) — arXiv"""
2025,Scaling up Test-Time Compute with Latent Reasoning,https://neurips.cc/virtual/2025/poster/117966
2024,ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention,MISSING
2022,"ScienceQA (""Learn to Explain..."")",https://arxiv.org/abs/2209.09513
2022,ScienceQA: Benchmark for Multimodal Reasoning,"https://arxiv.org/pdf/2209.09513.pdf ""ScienceQA (2022) — arXiv"""
2018,Search-based Program Synthesis,https://www.cis.upenn.edu/~alur/CACM18.pdf
2024,Searching Latent Program Spaces,https://arcprize.org/competitions/2024/
2024,Searching Latent Program Spaces,"https://arxiv.org/pdf/2411.08706.pdf ""Searching Latent Program Spaces (2024) — arXiv"""
2021,SegFormer,https://arxiv.org/abs/2105.15203
2021,SegFormer,https://arxiv.org/abs/2105.15203
2021,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,"https://arxiv.org/pdf/2105.15203.pdf ""SegFormer (2021) — arXiv"""
2021,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,"https://arxiv.org/pdf/2105.15203.pdf ""SegFormer (2021) — arXiv"""
2024,SegPoint: Segment Any Point Cloud via Large Language,MISSING
2023,Segment Anything,"https://arxiv.org/pdf/2304.02643.pdf ""Segment Anything (2023) — arXiv"""
2023,Segment Anything,"https://arxiv.org/pdf/2304.02643.pdf ""Segment Anything (2023) — arXiv"""
2023,Segment Anything (SAM),https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf
2025,Selective Rotary Position Embedding,"https://arxiv.org/pdf/2511.17388.pdf ""Selective Rotary Position Embedding (2025) — arXiv"""
2018,Self-Attention with Relative Position Representations,https://arxiv.org/abs/1803.02155
2018,Self-Attention with Relative Position Representations,https://arxiv.org/pdf/1803.02155
2018,Self-Attention with Relative Position Representations,https://arxiv.org/pdf/1803.02155
2018,Self-Attention with Relative Position Representations,https://arxiv.org/pdf/1803.02155
2022,Self-Consistency Improves Chain-of-Thought Reasoning,https://arxiv.org/abs/2203.11171
2022,Self-Consistency for Chain-of-Thought,MISSING
2025,Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI (SOAR),https://openreview.net/pdf?id=z4IG090qt2
2025,Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI (SOAR),https://arcprize.org/blog/arc-prize-2025-results-analysis
2023,Self-Refine: Iterative Refinement with Self-Feedback,https://arxiv.org/abs/2303.17651
1992,Separate visual pathways for perception and action,https://pubmed.ncbi.nlm.nih.gov/1374953/
2018,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,https://arxiv.org/abs/1810.00825
2018,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,MISSING
2020,Sharpness-Aware Minimization (SAM),MISSING
2020,Shortformer: Better Language Modeling using Shorter Inputs,https://arxiv.org/pdf/2012.15832
2020,Shortformer: Better Language Modeling using Shorter Inputs,https://arxiv.org/pdf/2012.15832
1992,Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning (REINFORCE),https://link.springer.com/article/10.1007/BF00992696
2024,Simultaneous Instance Pooling & Bag Selection for MIL using ViTs,"https://doi.org/10.1007/s00521-024-09417-3 ""MIL with ViTs (2024) — Springer"""
2025,SmolVLM: Redefining small and efficient multimodal models,"https://arxiv.org/pdf/2504.05299.pdf ""SmolVLM (2025) — arXiv"""
2018,Soft Actor-Critic (SAC),https://arxiv.org/abs/1812.05905
2018,Soft Actor-Critic (SAC),https://arxiv.org/abs/1801.01290
2020,Solver-in-the-Loop: Learning from Differentiable Physics to Interact with Iterative PDE Solvers,https://proceedings.neurips.cc/paper_files/paper/2020/hash/43e4e6a6f341e00671e123714de019a8-Abstract.html
2018,Solving Programming Tasks from Description and Examples,https://arxiv.org/pdf/1802.04335
2024,Solving olympiad geometry without human demonstrations (AlphaGeometry),"https://www.nature.com/articles/s41586-023-06747-5.pdf ""AlphaGeometry (2024) — Nature"""
2016,Sparse Differentiable Neural Computer (SDNC),https://arxiv.org/pdf/1610.09027
2022,Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion,https://arxiv.org/abs/2211.10581
2022,Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion,https://arxiv.org/abs/2211.10581
2025,SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection,https://arxiv.org/html/2503.08092v1
2025,SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection,https://arxiv.org/html/2503.08092v1
2024,Spherical Mask: Coarse-to-Fine 3D Point Cloud Instance Segmentation with Spherical Representation,https://openaccess.thecvf.com/content/CVPR2024/papers/Shin_Spherical_Mask_Coarse-to-Fine_3D_Point_Cloud_Instance_Segmentation_with_Spherical_CVPR_2024_paper.pdf
2023,Spherical Position Encoding for Transformers,"https://arxiv.org/pdf/2310.04454.pdf ""Spherical Position Encoding (2023) — arXiv"""
2021,Stabilizing Equilibrium Models by Jacobian Regularization,https://arxiv.org/abs/2106.14342
2015,Stack/Queue/Deque-augmented RNNs,MISSING
2020,Stand-Alone Axial-Attention (Axial-DeepLab),https://arxiv.org/abs/2105.15203
2020,Stand-Alone Axial-Attention for Panoptic Segmentation (Axial-DeepLab),https://www.cs.jhu.edu/~alanlab/Pubs20/wang2020axial.pdf
2021,StrategyQA: A Benchmark with Implicit Reasoning Strategies,https://aclanthology.org/2021.tacl-1.21/
2025,Streaming 4D Panoptic Segmentation via Dual Threads,https://arxiv.org/html/2510.17664v1
2019,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,https://arxiv.org/abs/1905.00537
1995,Support-Vector Networks,https://link.springer.com/article/10.1007/BF00994018
2021,Swin Transformer,https://arxiv.org/pdf/2103.14030
2021,Swin Transformer,https://arxiv.org/abs/2103.14030
2021,Swin Transformer,"https://arxiv.org/pdf/2103.14030.pdf ""Swin Transformer (2021) — arXiv"""
2021,Swin Transformer V2: Scaling Up Capacity and Resolution,"https://arxiv.org/pdf/2111.09883.pdf ""Swin Transformer V2 (2021) — arXiv"""
2021,Swin Transformer V2: Scaling Up Capacity and Resolution,"https://arxiv.org/pdf/2111.09883.pdf ""Swin Transformer V2 (2021) — arXiv"""
2021,Swin Transformer: Hierarchical Vision Transformer using Shifted Windows,https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf
Year unknown,SyGuS PBE-BV / PBE-Strings,https://sygus-org.github.io/comp/2019/
2019,"SyGuS-Comp ""PBE tracks"" mature (e.g., PBE-Strings, PBE-BV)",https://sygus-org.github.io/comp/2019/
2016,SyGuS-Comp 2016 Results/Benchmarks (PBE track),https://arxiv.org/pdf/1611.07627
2016,SyGuS-Comp 2016: Results and Analysis,https://dspace.mit.edu/bitstream/handle/1721.1/137904/1611.07627.pdf?isAllowed=y&sequence=2
2017,SyGuS-Comp 2017: Results and Analysis,https://arxiv.org/pdf/1711.11438
2017,SyGuS-Comp track definitions (PBE-BV),https://sygus.org/comp/2017/
2015,SyGuS-Comp'15 Results/Analysis,https://rishabhmit.bitbucket.io/papers/synt15.pdf
Year unknown,"Symbolic regression benchmarks (AI Feynman, SRBench)",https://www.science.org/doi/10.1126/sciadv.aay2631
1977,Synergetics: An Introduction,https://books.google.com/books/about/Synergetics.html?id=KHn1CAAAQBAJ
2013,Syntax-Guided Synthesis (SyGuS),https://www.cis.upenn.edu/~alur/SyGuS13.pdf
2013,Syntax-Guided Synthesis (SyGuS),https://www.cis.upenn.edu/~alur/SyGuS13.pdf
2014,Syntax-Guided Synthesis (SyGuS) + SyGuS-Comp 2014,https://sygus.org/comp/2014/
2025,TAPA: Positional Encoding via Token-Aware Phase Attention,https://arxiv.org/abs/2509.12635
1992,"TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play",https://cdn.aaai.org/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf
2025,TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution,https://arxiv.org/html/2506.18421v1
2022,Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the ViTARC Architecture,https://openreview.net/forum?id=0gOQeSHNX1
2020,Taming Transformers for High-Resolution Image Synthesis,"https://arxiv.org/pdf/2012.09841.pdf ""Taming Transformers (2020) — arXiv"""
2020,Taming Transformers for High-Resolution Image Synthesis,"https://arxiv.org/pdf/2012.09841.pdf ""Taming Transformers (2020) — arXiv"""
2021,Taming Transformers for High-Resolution Image Synthesis (VQGAN + Transformer),https://arxiv.org/abs/2012.09841
2024,Teaching Transformers Modular Arithmetic at Scale,https://www.arxiv.org/abs/2410.03569v1
1992,Technical Note: Q-learning,https://link.springer.com/article/10.1023/A%3A1022676722315
2025,Test-Time Learning for Large Language Models (TLM / TTL),https://openreview.net/forum?id=iCYbIaGKSR&noteId=ScPdA3KZCL
2024,Test-Time Training on Nearest Neighbors for Large Language Models,https://arxiv.org/abs/2305.18466
2025,Test-time Adaptation of Tiny Recursive Models,https://arcprize.org/blog/arc-prize-2025-results-analysis
2019,TextVQA: Towards VQA Models That Can Read,https://arxiv.org/abs/1904.08920
2019,The Abstraction and Reasoning Corpus (ARC),https://arxiv.org/abs/2412.04604
2025,The Art of Scaling Test-Time Compute for LLMs,https://arxiv.org/html/2512.02008v1
2007,The Hidden Logic of Sudoku (Second Edition),"# ""The Hidden Logic of Sudoku (2007) — Book (local copy)"""
2024,The LLM ARChitect: Solving ARC-AGI Is a Matter of Perspective,https://arcprize.org/competitions/2024/
2025,The Lessons of Developing Process Reward Models...,https://arxiv.org/pdf/2501.07301
2019,The Lottery Ticket Hypothesis,MISSING
1956,"The Magical Number Seven, Plus or Minus Two",https://pubmed.ncbi.nlm.nih.gov/13310704/
1949,The Organization of Behavior,https://www.dengfanxin.cn/wp-content/uploads/2016/03/1949Hebb.pdf
1957,The Perceptron,MISSING
2021,The Pile: An 800GB Dataset of Diverse Text for Language Modeling,https://arxiv.org/pdf/2101.00027
2021,The Pile: An 800GB Dataset of Diverse Text for Language Modeling,https://arxiv.org/pdf/2101.00027
2025,The Rotary Position Embedding May Cause Dimension Inefficiency,"https://arxiv.org/pdf/2502.11276.pdf ""The Rotary Position Embedding May Cause Dimension Inefficiency (2025) — arXiv"""
2024,The Surprising Effectiveness of Test-Time Training for Abstract Reasoning,https://arcprize.org/competitions/2024/
2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://arxiv.org/abs/2411.07279
2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,"https://ekinakyurek.github.io/papers/ttt.pdf ""Test-Time Training for Few-Shot Learning (2024) — Project Page"""
1954,The Theory of Dynamic Programming,https://www.rand.org/content/dam/rand/pubs/papers/2008/P550.pdf
2010,The free-energy principle: a unified brain theory?,https://www.uab.edu/medicine/cinl/images/KFriston_FreeEnergy_BrainTheory.pdf
1971,The hippocampus as a spatial map,https://pubmed.ncbi.nlm.nih.gov/5124915/
2018,"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",https://arxiv.org/abs/1803.05457
2018,"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",https://arxiv.org/abs/1803.05457
2021,TimeSformer: Is Space-Time Attention All You Need for Video Understanding?,https://arxiv.org/abs/2102.05095
2021,Tokens-to-Token ViT (T2T-ViT),https://arxiv.org/abs/2101.11986
2023,Toolformer: Language Models Can Teach Themselves to Use Tools,https://arxiv.org/abs/2302.04761
2023,Toolformer: Language Models Can Teach Themselves to Use Tools,https://arxiv.org/abs/2302.04761
2007,Toward an executive without a homunculus: computational models of the prefrontal cortex/basal ganglia system,https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2007.2055
2015,Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks (bAbI),https://arxiv.org/abs/1502.05698
2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arxiv.org/abs/2411.17708
2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arcprize.org/competitions/2024/
2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,"https://arxiv.org/pdf/2411.17708.pdf ""Neurally-Guided Program Induction for ARC-AGI (2024) — arXiv"""
2025,Towards the Next Generation of Symbolic Regression Benchmarks,https://arxiv.org/html/2505.03977v1
2021,"Train Short, Test Long: Attention with Linear Biases (ALiBi)",https://arxiv.org/abs/2108.12409
2021,"Train Short, Test Long: Attention with Linear Biases (ALiBi)","https://arxiv.org/pdf/2108.12409.pdf ""ALiBi (2021) — arXiv"""
2022,Training Compute-Optimal Large Language Models (Chinchilla),https://arxiv.org/abs/2203.15556
2022,Training Iterative Refinement Algorithms with Implicit Differentiation,https://proceedings.neurips.cc/paper_files/paper/2022/file/d301e2878a7ebadf1a95029e904fc7d0-Paper-Conference.pdf
2021,Training Verifiers to Solve Math Word Problems (GSM8K),https://arxiv.org/abs/2110.14168
2020,Training data-efficient image transformers & distillation through attention,"https://arxiv.org/pdf/2012.12877.pdf ""DeiT: Data-efficient Image Transformers (2020) — arXiv"""
2020,Training data-efficient image transformers & distillation through attention,"https://arxiv.org/pdf/2012.12877.pdf ""DeiT (2020) — arXiv"""
2022,Training language models to follow instructions with human feedback (InstructGPT / RLHF pipeline),https://arxiv.org/abs/2203.02155
2022,TransFusion: Robust LiDAR-Camera Fusion for 3D Object Detection with Transformers,https://openaccess.thecvf.com/content/CVPR2022/papers/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.pdf
2022,TransNeRF: Generalizable Neural Radiance Fields for Novel View Synthesis with Transformer,https://arxiv.org/abs/2206.05375
2025,TransXSSM: Hybrid Transformer–SSM with Unified RoPE,"https://arxiv.org/pdf/2506.09507.pdf ""TransXSSM (2025) — arXiv"""
2022,Transformer Language Models without Positional Encodings Still Learn Positional Information,"https://arxiv.org/pdf/2203.16634.pdf ""Transformers without PEs still learn position (2022) — arXiv"""
2021,Transformer in Transformer,"https://arxiv.org/pdf/2103.00112.pdf ""Transformer in Transformer (2021) — arXiv"""
2021,Transformer in Transformer,"https://arxiv.org/pdf/2103.00112.pdf ""Transformer in Transformer (2021) — arXiv"""
2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://arxiv.org/abs/1901.02860
2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://aclanthology.org/P19-1285.pdf
2023,Transformer-based 3D point cloud generation networks,https://dl.acm.org/doi/10.1145/3581783.3612226
2024,Transformers Can Do Arithmetic with the Right Embeddings,https://proceedings.neurips.cc/paper_files/paper/2024/file/c35986bc1ee29b31c1011481b77fe540-Paper-Conference.pdf
2024,Transformers Can Do Arithmetic with the Right Embeddings,https://proceedings.neurips.cc/paper_files/paper/2024/hash/c35986bc1ee29b31c1011481b77fe540-Abstract-Conference.html
1949,Translation (Weaver Memorandum),https://www.mt-archive.net/50/Weaver-1949.pdf
1928,Transmission of Information,https://monoskop.org/images/a/a6/Hartley_Ralph_VL_1928_Transmission_of_Information.pdf
2023,Tree of Thoughts (ToT),https://arxiv.org/abs/2305.10601
2023,Tree of Thoughts (ToT),https://arxiv.org/abs/2305.10601
2023,Tree of Thoughts (ToT): Deliberate Problem Solving with Large Language Models,https://arxiv.org/abs/2305.10601
2005,Tree-Based Batch Mode Reinforcement Learning,https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf
2015,Trust Region Policy Optimization (TRPO),https://arxiv.org/abs/1502.05477
2015,Trust Region Policy Optimization (TRPO),https://arxiv.org/abs/1502.05477
2021,TruthfulQA: Measuring How Models Mimic Human Falsehoods,https://arxiv.org/abs/2109.07958
2018,Twin Delayed DDPG (TD3),https://arxiv.org/pdf/1802.09477
2021,Twins: Revisiting the Design of Spatial Attention in Vision Transformers,"https://arxiv.org/pdf/2104.13840.pdf ""Twins (2021) — arXiv"""
2021,Twins: Revisiting the Design of Spatial Attention in Vision Transformers,"https://arxiv.org/pdf/2104.13840.pdf ""Twins (2021) — arXiv"""
1982,Two cortical visual systems,https://www.cns.nyu.edu/~tony/vns/readings/ungerleider-mishkin-1982.pdf
2020,UNITER: Universal Image-Text Representation Learning,"https://arxiv.org/pdf/1909.11740.pdf ""UNITER (2020) — arXiv"""
2025,Understanding the RoPE Extensions of Long-Context LLMs,https://aclanthology.org/2025.coling-main.600.pdf
2023,Uni3DL: Unified Model for 3D and Language Understanding,"https://arxiv.org/pdf/2312.03026.pdf ""Uni3DL (2023) — arXiv"""
1990,Unified Theories of Cognition,https://www.hup.harvard.edu/books/9780674921016
2018,Universal Transformer (UT),MISSING
2018,Universal Transformers,"https://arxiv.org/pdf/1807.03819.pdf ""Universal Transformers (2018) — arXiv"""
2018,Universal Transformers (UT),https://arxiv.org/abs/1807.03819
2021,VATT: Transformers for Multimodal Self-Supervised Learning,"https://arxiv.org/pdf/2104.11178.pdf ""VATT (2021) — arXiv"""
2019,"VCR: Visual Commonsense Reasoning (""From Recognition to Cognition..."")",https://arxiv.org/abs/1811.10830
2024,VG4D: Vision-Language Model Goes 4D Video Recognition,"https://arxiv.org/pdf/2404.11605.pdf ""VG4D (2024) — arXiv"""
2017,"VQA v2.0 (""Making the V in VQA Matter"")",https://arxiv.org/abs/1612.00837
2015,VQA: Visual Question Answering,https://arxiv.org/abs/1505.00468
2025,VRoPE: Rotary Position Embedding for Video Large Language Models,"https://arxiv.org/pdf/2502.11664.pdf ""VRoPE (2025) — arXiv"""
2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://arxiv.org/abs/2511.08747
2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://arcprize.org/blog/arc-prize-2025-results-analysis
2019,ViLBERT: Pretraining Task-Agnostic Vision-and-Language Representations,"https://arxiv.org/pdf/1908.02265.pdf ""ViLBERT (2019) — arXiv"""
2021,ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision,"https://arxiv.org/pdf/2102.03334.pdf ""ViLT (2021) — arXiv"""
2022,ViTDet: Exploring Plain ViT Backbones for Object Detection,https://arxiv.org/abs/2203.16527
2021,ViViT: A Video Vision Transformer,https://openaccess.thecvf.com/content/ICCV2021/papers/Arnab_ViViT_A_Video_Vision_Transformer_ICCV_2021_paper.pdf
2021,Video Swin Transformer,https://arxiv.org/abs/2106.13230
2024,Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis,https://arxiv.org/abs/2405.21075
2019,VideoBERT: A Joint Model for Video and Language Representation Learning,"https://arxiv.org/pdf/1904.01766.pdf ""VideoBERT (2019) — arXiv"""
2021,VideoCLIP: Contrastive Pretraining for Zero-Shot Video-Text Understanding,"https://arxiv.org/pdf/2109.14084.pdf ""VideoCLIP (2021) — arXiv"""
2022,ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers,https://arxiv.org/abs/2203.10157
2025,VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs,https://arxiv.org/abs/2506.06727
1982,Vision: A Computational Investigation into the Human Representation and Processing of Visual Information,https://direct.mit.edu/books/monograph/3299/VisionA-Computational-Investigation-into-the-Human
2019,VisualBERT: A Simple and Performant Baseline for Vision and Language,"https://arxiv.org/pdf/1908.03557.pdf ""VisualBERT (2019) — arXiv"""
2018,VizWiz Grand Challenge: Answering Visual Questions from Blind People,https://arxiv.org/abs/1802.08218
2020,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",https://arxiv.org/pdf/2010.06775
2020,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",https://arxiv.org/pdf/2010.06775
2023,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,https://arxiv.org/abs/2302.12251
2021,Voxel Transformer for 3D Object Detection (VoTr),https://arxiv.org/abs/2109.02497
2024,Voxel self-attention and center-point for 3D object detector,https://www.cell.com/iscience/fulltext/S2589-0042%2824%2901984-9
2025,WALRUS: A Cross-Domain Foundation Model for Continuum Dynamics,"https://arxiv.org/pdf/2511.15684.pdf ""WALRUS (2025) — arXiv"""
2025,Wavelet-based Positional Representation for Long Context,https://openreview.net/forum?id=OhauMUNW8T
2024,What matters when building vision-language models? (Idefics2),"https://arxiv.org/pdf/2405.02246.pdf ""Idefics2 study (2024) — arXiv"""
2022,Winoground: Probing Vision-Language Models for Compositionality,"https://arxiv.org/pdf/2204.03162.pdf ""Winoground (2022) — arXiv"""
1974,Working Memory,https://www.sciencedirect.com/science/article/pii/S0079742108604521
2012,"Working Memory: Theories, Models, and Controversies",https://pubmed.ncbi.nlm.nih.gov/21961947/
2019,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/pdf/1906.08237
2022,XPos / Length-Extrapolatable Transformer,https://arxiv.org/pdf/2212.10554
2023,YaRN: Efficient Context Window Extension of Large Language Models,https://arxiv.org/abs/2309.00071
2023,YaRN: Efficient Context Window Extension of Large Language Models,"https://arxiv.org/pdf/2309.00071.pdf ""YaRN (2023) — arXiv"""
2020,ZeRO: Memory Optimizations Toward Training Trillion Parameter Models,https://arxiv.org/abs/1910.02054
2025,Zero-Shot 4D LiDAR Panoptic Segmentation (SAL-4D),https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Zero-Shot_4D_Lidar_Panoptic_Segmentation_CVPR_2025_paper.pdf
2025,Zero-Shot 4D LiDAR Panoptic Segmentation (SAL-4D),https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Zero-Shot_4D_Lidar_Panoptic_Segmentation_CVPR_2025_paper.pdf
2021,Zero-Shot Text-to-Image Generation,"https://arxiv.org/pdf/2102.12092.pdf ""Zero-Shot Text-to-Image Generation (2021) — arXiv"""
2021,Zero-Shot Text-to-Image Generation,"https://arxiv.org/pdf/2102.12092.pdf ""Zero-Shot Text-to-Image Generation (2021) — arXiv"""
2021,Zero-Shot Text-to-Image Generation (DALL-E),https://arxiv.org/abs/2102.12092
2021,diff-SAT - Sampling and Probabilistic Reasoning for SAT and Answer Set Programming,https://arxiv.org/abs/2101.00589
2021,miniF2F: a cross-system benchmark for formal Olympiad-level mathematics,https://arxiv.org/abs/2109.00110
2025,the ARChitects,https://arcprize.org/competitions/2025/
2013,word2vec (Skip-gram / CBOW),MISSING
