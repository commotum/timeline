year,title,url



2014,Deterministic Policy Gradient Algorithms,https://proceedings.mlr.press/v32/silver14.pdf
2019,Differentiable Convex Optimization Layers,https://arxiv.org/pdf/1910.12430.pdf
2019,Differentiable Convex Optimization Layers (CVXPYLayers / DPP),https://arxiv.org/pdf/1910.12430.pdf
2016,Differentiable Neural Computer (DNC),https://www.nature.com/articles/nature20101
2023,Direct Preference Optimization (DPO),https://arxiv.org/pdf/2305.18290.pdf
2018,Distributed Prioritized Experience Replay (Ape-X),https://arxiv.org/pdf/1803.00933.pdf
2021,Do Transformer Modifications Transfer Across Implementations and Applications?,https://arxiv.org/pdf/2102.11972.pdf
2025,DoPE: Denoising Rotary Position Embedding,https://arxiv.org/pdf/2511.09146.pdf
2020,DocVQA: A Dataset for VQA on Document Images,https://arxiv.org/pdf/2007.00398.pdf
2025,Don't throw the baby out with the bathwater: How and why deep learning for ARC,https://arcprize.org/blog/arc-prize-2025-results-analysis
2019,Dream to Control: Learning Behaviors by Latent Imagination (Dreamer),https://openreview.net/forum?id=S1lOTC4tDS
1991,"Dyna, an Integrated Architecture for Learning, Planning, and Reacting",https://dl.acm.org/doi/abs/10.1145/122344.122377
2005,Dynamic dopamine modulation in the basal ganglia,https://pubmed.ncbi.nlm.nih.gov/15701239/
2025,DynamicCity,https://arxiv.org/pdf/2410.18084v3.pdf
2025,EVA02-AT: Egocentric Video-Language with Spatial-Temporal RoPE,https://arxiv.org/pdf/2506.14356.pdf
1908,"Early gradient/variation method note often attributed to Hadamard's ""methode"" discussions",https://people.idsia.ch/~juergen/who-invented-backpropagation-2014.html
2025,Efficient Evolutionary Program Synthesis,https://arcprize.org/blog/arc-prize-2025-results-analysis
2021,Efficient Large-Scale Language Model Training on GPU Clusters,https://arxiv.org/pdf/2104.04473.pdf
2021,Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM,https://arxiv.org/pdf/2104.04473.pdf
2025,Efficiently Allocating Test-Time Compute for LLM Agents,https://arxiv.org/pdf/2509.03581v1.pdf
2019,Encoding Word Order in Complex Embeddings,https://arxiv.org/pdf/1912.12333.pdf
2015,End-to-End Memory Networks (MemN2N),MISSING
2020,End-to-End Object Detection with Transformers (DETR),https://arxiv.org/pdf/2005.12872.pdf
2024,Enhanced Scene Understanding on 4D Point Cloud,https://ojs.aaai.org/index.php/AAAI/article/view/28045
2025,Enhancing Modern SAT Solver With Machine Learning,https://dl.acm.org/doi/full/10.1145/3716368.3735251
2021,Evaluating Large Language Models Trained on Code (HumanEval),https://arxiv.org/pdf/2107.03374.pdf
2024,Evolutionary Test-Time Compute (write-up),https://jeremyberman.substack.com/p/how-i-got-a-record-536-on-arc-agi
1972,Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons,https://pubmed.ncbi.nlm.nih.gov/4332108/
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,https://arxiv.org/pdf/1910.10683.pdf
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5),https://arxiv.org/pdf/1910.10683.pdf
2025,Exploring the combination of search and learn for the ARC25 challenge,https://arcprize.org/blog/arc-prize-2025-results-analysis
2023,Extending Context Window of Large Language Models via Positional Interpolation (PI),https://arxiv.org/pdf/2306.15595.pdf
2022,FOLIO: Natural Language Reasoning with First-Order Logic,https://arxiv.org/pdf/2209.00840.pdf
2000,Feedforward and Recurrent Processing in Vision,https://pubmed.ncbi.nlm.nih.gov/10925037/
2024,Fixed Point Diffusion Models,https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.pdf
2024,Fixed Point Diffusion Models (FPDM),https://arxiv.org/pdf/2401.08741v1.pdf
2022,Flamingo: a Visual Language Model for Few-Shot Learning,https://arxiv.org/pdf/2204.14198.pdf
2024,FlashAttention-2 / efficient attention kernels,MISSING
2025,From Parrots to Von Neumanns: How Evolutionary Test-Time Compute Achieved SOTA on ARC-AGI,https://arcprize.org/blog/arc-prize-2025-results-analysis
2023,GAIA: a benchmark for General AI Assistants,https://arxiv.org/pdf/2311.12983.pdf
2018,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,https://arxiv.org/pdf/1804.07461.pdf
2023,GPQA: A Graduate-Level Google-Proof Q&A Benchmark,https://arxiv.org/pdf/2311.12022.pdf
2023,GPT-4 Technical Report,https://arxiv.org/pdf/2303.08774.pdf
2019,GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering,https://arxiv.org/pdf/1902.09506.pdf
2021,GSM8K: Training Verifiers to Solve Math Word Problems,https://arxiv.org/pdf/2110.14168.pdf
2025,"Gated Attention for LLMs: Non-linearity, Sparsity, Sink-Free",https://arxiv.org/pdf/2505.06708.pdf
2024,Gaussian Adaptive Attention Is All You Need,https://arxiv.org/pdf/2401.11143.pdf
2023,Gemini: A Family of Highly Capable Multimodal Models,https://arxiv.org/pdf/2312.11805.pdf
2024,Generalized Planning for the Abstraction and Reasoning Corpus (GPAR),https://arxiv.org/pdf/2401.07426.pdf
2019,Generating Long Sequences with Sparse Transformers,https://arxiv.org/pdf/1904.10509.pdf
2014,Generative Adversarial Networks (GANs),MISSING
2020,Generative Pretraining from Pixels (iGPT),https://arxiv.org/pdf/2005.12872.pdf
2024,Generative Verifiers / GenRM: Reward Modeling as Next-Token Prediction,https://neurips.cc/virtual/2024/104300
2021,Goal-Aware Neural SAT Solver (QuerySAT / goal-aware guidance),https://arxiv.org/pdf/2106.07162.pdf
2025,Graph Perceiver IO,https://www.sciencedirect.com/science/article/abs/pii/S0031320325005497
2023,Graph of Thoughts (GoT),https://arxiv.org/pdf/2308.09687.pdf
2021,Graphormer: Do Transformers Really Perform Bad for Graph Representation?,https://arxiv.org/pdf/2106.05234.pdf
2023,Grokking Modular Arithmetic,https://arxiv.org/pdf/2301.02679.pdf
2024,Grokking Modular Polynomials,https://arxiv.org/pdf/2406.03495.pdf
2022,Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets,https://arxiv.org/pdf/2201.02177.pdf
2019,Guiding SAT Solvers with Unsat-Core Predictions (NeuroCore),https://arxiv.org/pdf/1903.04671.pdf
2025,H-ARC (Human-ARC): A Comprehensive Behavioral Dataset for the Abstraction and Reasoning Corpus,https://www.nature.com/articles/s41597-025-05687-1
2024,H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark,https://arxiv.org/pdf/2409.01374.pdf
2025,HARPE: Head-Adaptive Rotary Position Encoding,https://aclanthology.org/2025.coling-main.326/
2024,HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion,https://arxiv.org/pdf/2310.14566.pdf
2025,Head-Wise Adaptive Rotary Positional Encoding (HARoPE),https://arxiv.org/pdf/2510.10489.pdf
2019,HellaSwag: Can a Machine Really Finish Your Sentence?,https://arxiv.org/pdf/1905.07830.pdf
2024,HiRoPE: Length Extrapolation for Code Models Using Hierarchical Rotary Position Embedding,https://aclanthology.org/2024.acl-long.735/
2025,Hierarchical Reasoning Model (HRM),https://arxiv.org/pdf/2506.21734v1.pdf
2015,High-Dimensional Continuous Control Using Generalized Advantage Estimation (GAE),https://arxiv.org/pdf/1506.02438.pdf
2022,Holistic Evaluation of Language Models (HELM),https://arxiv.org/pdf/2211.09110.pdf
2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://arxiv.org/pdf/1809.09600.pdf
2019,HowTo100M: Learning a Text-Video Embedding by Watching Narrated Videos,https://arxiv.org/pdf/1906.03327.pdf
2018,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,https://arxiv.org/pdf/1802.01561.pdf
2018,Image Transformer,https://arxiv.org/pdf/1802.05751.pdf
2022,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,https://arxiv.org/pdf/2208.10442.pdf
2012,ImageNet Classification with Deep Convolutional Neural Networks (AlexNet),MISSING
2024,Implicit Factorized Transformer (IFactFormer),https://www.sciencedirect.com/science/article/pii/S2095034924000382
2015,Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,https://arxiv.org/pdf/1503.01007.pdf
2023,Instruction-Following Evaluation for Large Language Models (IFEval),https://arxiv.org/pdf/2311.07911.pdf
2024,Interactive4D: Interactive 4D LiDAR Segmentation,https://arxiv.org/pdf/2410.08206.pdf
2021,Is Space-Time Attention All You Need for Video Understanding? (TimeSformer),https://arxiv.org/pdf/2102.05095.pdf
2021,Jacobian regularization for equilibrium models,MISSING
2023,Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,https://arxiv.org/pdf/2306.05685.pdf
2022,KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation,https://arxiv.org/pdf/2205.09921.pdf
Year unknown,Karel dataset,https://msr-redmond.github.io/karel-dataset/
2023,Kosmos-1: Language Is Not All You Need,https://arxiv.org/pdf/2302.14045.pdf
2021,LAION-400M: Open Dataset for CLIP Training,https://arxiv.org/pdf/2111.02114.pdf
2022,LAION-5B: An Open Large-Scale Dataset for Training Next Generation Image-Text Models,https://arxiv.org/pdf/2210.08402.pdf
2022,LAION-5B: An open large-scale dataset for training next generation image-text models,https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/
2024,LLFormer4D: LiDAR-based lane detection method,https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/cvi2.12338
2025,LLaVA-4D: Embedding Spatiotemporal Prompt into LMMs,https://arxiv.org/pdf/2505.12253.pdf
2023,LLaVA: Large Language-and-Vision Assistant,https://arxiv.org/pdf/2304.08485.pdf
2025,LOOPE: Learnable Optimal Patch Order in Vision Transformers,https://arxiv.org/pdf/2504.14386.pdf
2024,LTD-Bench: Evaluating Large Language Models by Letting Them Draw,https://openreview.net/forum?id=TG5rvKyEbu
2025,LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias,https://proceedings.iclr.cc/paper_files/paper/2025/hash/9676c5283df26cabca412ca66b164a7d-Abstract-Conference.html
2019,LXMERT: Learning Cross-Modality Encoder Representations,https://arxiv.org/pdf/1908.07490.pdf
2023,Language Agent Tree Search (LATS),https://arxiv.org/pdf/2310.04406.pdf
2016,Layer Normalization,https://arxiv.org/pdf/1607.06450.pdf
2022,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering (ScienceQA),https://arxiv.org/pdf/2209.09513.pdf
2024,Learning Iterative Reasoning through Energy Diffusion,https://arxiv.org/pdf/2406.11179.pdf
2025,Learning Modular Exponentiation with Transformers,https://arxiv.org/pdf/2506.23679v1.pdf
1986,Learning Representations by Back-Propagating Errors,https://www.nature.com/articles/323533a0
2012,Learning Semantic String Transformations from Examples,MISSING
2021,Learning Transferable Visual Models From Natural Language Supervision (CLIP),https://arxiv.org/pdf/2103.00020.pdf
2018,Learning a SAT Solver from Single-Bit Supervision (NeuroSAT),https://arxiv.org/pdf/1802.03685.pdf
2020,Learning to Encode Position for Transformer with Continuous Dynamical Model (FLOATER),https://arxiv.org/pdf/2003.09229.pdf
2014,Learning to Execute,https://arxiv.org/pdf/1410.4615.pdf
2024,Length Generalization of Causal Transformers without Position Encoding,https://arxiv.org/pdf/2404.12224.pdf
2024,Length-Controlled AlpacaEval,https://arxiv.org/pdf/2404.04475.pdf
2025,Less is More: Recursive Reasoning with Tiny Networks (TRM),https://ar5iv.org/abs/2510.04871
2025,Less is More: Recursive Reasoning with Tiny Networks (TRM),https://arcprize.org/blog/arc-prize-2025-results-analysis
2018,Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis,https://arxiv.org/pdf/1805.04276.pdf
2024,LieRE: Lie Rotational Positional Encodings,https://arxiv.org/pdf/2406.10322.pdf
2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://arxiv.org/pdf/1804.06028.pdf
2024,"LiveBench: A Challenging, Contamination-Free LLM Benchmark",https://arxiv.org/pdf/2406.19314.pdf
2021,LoRA: Low-Rank Adaptation of Large Language Models,https://arxiv.org/pdf/2106.09685.pdf
2020,LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning,https://www.ijcai.org/proceedings/2020/501
2016,Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge,https://arxiv.org/pdf/1606.04422.pdf
2020,Long Range Arena (LRA): A Benchmark for Efficient Transformers,https://arxiv.org/pdf/2011.04006.pdf
2023,"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",https://arxiv.org/pdf/2308.14508.pdf
2024,LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens,https://arxiv.org/pdf/2402.13753.pdf
2024,LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate,https://arxiv.org/pdf/2405.13985.pdf
2021,MATH: Measuring Mathematical Problem Solving,https://arxiv.org/pdf/2103.03874.pdf
2024,MM-Vet v2,https://arxiv.org/pdf/2408.00765.pdf
2023,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,https://arxiv.org/pdf/2308.02490.pdf
2023,MMBench: Evaluating Multimodal LLMs,https://arxiv.org/pdf/2307.06281.pdf
2023,MMBench: Is Your Multi-modal Model an All-around Player?,https://arxiv.org/pdf/2307.06281.pdf
2023,MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models,https://arxiv.org/pdf/2306.13394.pdf
2024,MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,https://arxiv.org/pdf/2406.01574.pdf
2023,MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,https://arxiv.org/pdf/2311.16502.pdf
2023,MMMU: A Massive Multidiscipline Multimodal Benchmark,https://arxiv.org/pdf/2311.16502.pdf
2024,Machine Learning for Modular Multiplication,https://arxiv.org/pdf/2402.19254v1.pdf
2006,Making Working Memory Work: A Computational Model of Learning in the Prefrontal Cortex and Basal Ganglia,https://pubmed.ncbi.nlm.nih.gov/16378516/
2022,Mask2Former: Masked-attention Mask Transformer for Universal Image Segmentation,https://arxiv.org/pdf/2112.01527.pdf
2023,Mask4Former: Mask Transformer for 4D Panoptic Segmentation,https://arxiv.org/pdf/2309.16133.pdf
2021,MaskFormer: Per-Pixel Classification is Not All You Need for Semantic Segmentation,https://arxiv.org/pdf/2107.06278.pdf
2022,MaskGIT: Masked Generative Image Transformer,https://arxiv.org/pdf/2202.04200.pdf
2022,Masked Autoencoders Are Scalable Vision Learners (MAE),https://arxiv.org/pdf/2111.06377.pdf
2021,Masked Autoencoders Are Scalable Vision Learners (MAE),https://arxiv.org/pdf/2111.06377.pdf
2022,Masked Autoencoders for Point Cloud Self-supervised Learning (Point-MAE),https://arxiv.org/pdf/2203.06604.pdf
2019,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero)",https://www.nature.com/articles/s41586-020-03051-4
2020,"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model (MuZero)",https://www.nature.com/articles/s41586-020-03051-4
2017,Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (AlphaZero),https://arxiv.org/pdf/1712.01815.pdf
2025,Mastering diverse control tasks through world models (DreamerV3),https://www.nature.com/articles/s41586-025-08744-2
2016,Mastering the Game of Go with Deep Neural Networks and Tree Search (AlphaGo),https://www.nature.com/articles/nature16961
2025,MathClean: A Benchmark for Synthetic Mathematical Data,https://arxiv.org/pdf/2502.19058v1.pdf
2023,MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts,https://arxiv.org/pdf/2310.02255.pdf
2022,MaxViT: Multi-Axis Vision Transformer,https://arxiv.org/pdf/2204.01697.pdf
2025,Maximizing the Position Embedding for Vision Transformers (MPVG),"# ""MPVG (2025) — Local Copy (no public PDF link provided)"""
2018,Maximum a Posteriori Policy Optimisation (MPO),https://arxiv.org/pdf/1806.06920.pdf
2020,Measuring Massive Multitask Language Understanding (MMLU),https://arxiv.org/pdf/2009.03300.pdf
2021,Measuring Mathematical Problem Solving With the MATH Dataset,https://arxiv.org/pdf/2103.03874.pdf
2019,Megatron-LM / large-scale transformer training,MISSING
2022,Memorizing Transformers / kNN-augmented attention (inference-time memory),MISSING
2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://arxiv.org/pdf/2410.15859.pdf
2014,Microsoft COCO: Common Objects in Context,https://arxiv.org/pdf/1405.0312.pdf
2005,Microstructure of a spatial map in the entorhinal cortex,https://www.nature.com/articles/nature03721
2025,MindsAI,https://arcprize.org/competitions/2025/
2024,Mini-ARC: Solving Abstraction and Reasoning Puzzles with Small Transformer Models,https://arcprize.org/competitions/2024/
2023,MiniGPT-4,https://arxiv.org/pdf/2304.10592.pdf
2022,MuSiQue: Multihop Questions via Single-hop Question Composition,https://arxiv.org/pdf/2108.00573.pdf
2021,Multiscale Vision Transformers (MViT),https://arxiv.org/pdf/2104.11227.pdf
2018,Music Transformer,https://arxiv.org/pdf/1809.04281.pdf
2018,NAPS: Natural Program Synthesis Dataset,https://arxiv.org/pdf/1807.03168.pdf
2019,NEZHA: Neural Contextualized Representation for Chinese Language Understanding,https://arxiv.org/pdf/1909.00204.pdf
2019,NLVR2: A Corpus for Reasoning about Natural Language Grounded in Photographs,https://lil.nlp.cornell.edu/nlvr/
2025,NVARC,https://arcprize.org/competitions/2025/
2025,NVARC solution to ARC-AGI-2 2025,https://arcprize.org/blog/arc-prize-2025-results-analysis
2008,Natural Actor-Critic,https://www.sciencedirect.com/science/article/pii/S0925231208000532
2025,Nested Learning: The Illusion of Deep Learning Architecture,"# ""Nested Learning (2025) — Local Copy (no public PDF link provided)"""
2005,Neural Fitted Q Iteration (NFQ),https://link.springer.com/chapter/10.1007/11564096_32
2015,Neural GPUs Learn Algorithms,https://arxiv.org/pdf/1511.08228.pdf
2019,Neural Logic Machines (NLM),https://arxiv.org/pdf/1904.11694.pdf
2016,Neural Module Networks (NMN),https://openaccess.thecvf.com/content_cvpr_2016/html/Andreas_Neural_Module_Networks_CVPR_2016_paper.html
2018,Neural Ordinary Differential Equations (Neural ODEs),https://arxiv.org/pdf/1806.07366.pdf
2017,Neural Program Meta-Induction,https://papers.nips.cc/paper/6803-neural-program-meta-induction
2015,Neural Programmer: Inducing Latent Programs with Gradient Descent,https://arxiv.org/pdf/1511.04834.pdf
2017,Neural Theorem Provers / Differentiable Proving,MISSING
2014,Neural Turing Machines,https://arxiv.org/pdf/1410.5401.pdf
2014,Neural Turing Machines (NTM),https://arxiv.org/pdf/1410.5401.pdf
1961,New Results in Linear Filtering and Prediction Theory,https://asmedigitalcollection.asme.org/fluidsengineering/article/83/1/95/426820/New-Results-in-Linear-Filtering-and-Prediction
2021,Nystromformer: A Nystrom-Based Algorithm for Approximating Self-Attention,https://arxiv.org/pdf/2102.03902.pdf
2025,ONERULER: Benchmarking multilingual long-context language models,https://arxiv.org/pdf/2503.01996.pdf
2020,OSCAR: Object-Semantics Aligned Pre-training for Vision-Language Tasks,https://arxiv.org/pdf/2004.06165.pdf
2024,OccSora,https://github.com/wzzheng/OccSora
2024,Omni-ARC,https://arcprize.org/blog/arc-prize-2024-winners-technical-report
2024,Omni-ARC,https://arcprize.org/competitions/2024/
2019,On the Measure of Intelligence / ARC (Abstraction and Reasoning Corpus),MISSING
2018,OpenBookQA: Can a Suit of Armor Conduct Electricity?,https://arxiv.org/pdf/1809.02789.pdf
2017,OptNet: Differentiable Optimization as a Layer in Neural Networks,https://arxiv.org/pdf/1703.00443.pdf
2002,Optimal feedback control as a theory of motor coordination,https://pubmed.ncbi.nlm.nih.gov/12404008/
2004,Optimality principles in sensorimotor control,https://pubmed.ncbi.nlm.nih.gov/15332089/
2020,PCT: Point Cloud Transformer,https://arxiv.org/pdf/2012.09688.pdf
2021,PCT: Point Cloud Transformer,https://link.springer.com/article/10.1007/s41095-021-0229-5
2025,PMFormer: Point mask transformer for outdoor point cloud semantic segmentation,https://www.sciopen.com/article/10.26599/CVM.2025.9450388
2023,"POPE (Polling-based Object Probing Evaluation) / ""Evaluating Object Hallucination in Large Vision-Language Models""",https://arxiv.org/pdf/2305.10355.pdf
2024,POS-BERT: Point cloud one-stage BERT pre-training,https://www.sciencedirect.com/science/article/abs/pii/S0957417423030658
2021,PROGRES: A large-scale benchmark for few-shot program induction and synthesis,https://proceedings.mlr.press/v139/alet21a.html
2025,PV-DT3D: Point-voxel dual transformer for LiDAR 3D object detection,https://link.springer.com/article/10.1007/s11801-025-3134-9
2023,PaLM-E: An Embodied Multimodal Language Model,https://arxiv.org/pdf/2303.03378.pdf
2021,Perceiver / Perceiver IO,https://arxiv.org/pdf/2103.03206.pdf
2023,Plan-and-Solve Prompting,https://arxiv.org/pdf/2305.04091.pdf
2021,Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling,https://arxiv.org/pdf/2111.14819.pdf
2022,Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling,https://arxiv.org/pdf/2111.14819.pdf
2024,PointRegion: Transformer based 3D tooth segmentation via point cloud processing,https://pubmed.ncbi.nlm.nih.gov/39557955/
2000,Policy Gradient Methods for Reinforcement Learning with Function Approximation,https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation
2021,PonderNet: Learning to Ponder,https://arxiv.org/pdf/2107.05407.pdf
1999,Predictive coding in the visual cortex: a functional interpretation,https://pubmed.ncbi.nlm.nih.gov/10195184/
2015,Prioritized Experience Replay,https://arxiv.org/pdf/1511.05952.pdf
2023,Process Reward Models (PRM) introduced/standardized for reasoning traces,MISSING
2025,"Productive ""refinement loop"" systems from ARC Prize writeups (2025 winners list)",https://arcprize.org/blog/arc-prize-2025-results-analysis
2017,Proximal Policy Optimization (PPO),https://arxiv.org/pdf/1707.06347.pdf
2021,Pyramid Vision Transformer (PVT),https://arxiv.org/pdf/2102.12122.pdf
2021,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,https://arxiv.org/pdf/2102.12122.pdf
2023,QLoRA,MISSING
2024,RETRO-style retrieval-augmented pretraining and variants,https://arxiv.org/pdf/2112.04426.pdf
2024,RULER: What's the Real Context Size of Your Long-Context Language Models?,https://arxiv.org/pdf/2404.06654.pdf
2018,Rainbow: Combining Improvements in Deep Reinforcement Learning,https://arxiv.org/pdf/1710.02298.pdf
2022,ReAct: Synergizing Reasoning and Acting in Language Models,https://arxiv.org/pdf/2210.03629.pdf
2020,ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning,https://arxiv.org/pdf/2002.04326.pdf
2020,RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models,https://arxiv.org/pdf/2009.11462.pdf
2023,Reasoning with Language Model is Planning with World Model (RAP),https://arxiv.org/pdf/2305.14992.pdf
2019,Recurrent Experience Replay in Distributed Reinforcement Learning (R2D2),https://openreview.net/forum?id=r1lyTjAqYX
2018,Recurrent Relational Networks (RRN),https://arxiv.org/pdf/1711.08028.pdf
2025,Reflection System for the Abstraction and Reasoning Corpus,https://openreview.net/forum?id=kRFwzuv0ze
2023,Reflexion / Self-Refine family (test-time improvement loops),MISSING
2023,Reflexion: Language Agents with Verbal Reinforcement Learning,https://arxiv.org/pdf/2303.11366.pdf
2025,Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation,https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Relation3D__Enhancing_Relation_Modeling_for_Point_Cloud_Instance_Segmentation_CVPR_2025_paper.html
2010,Relative Entropy Policy Search (REPS),https://ojs.aaai.org/index.php/AAAI/article/view/7727
2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://arxiv.org/pdf/2403.00071.pdf
2025,RetentiveBEV: BEV transformer for visual 3D object detection,https://journals.sagepub.com/doi/10.1177/01423312241308367
2020,Rethinking Attention with Performers,https://arxiv.org/pdf/2009.14794.pdf
2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://arxiv.org/pdf/2006.15595.pdf
2025,Rethinking Visual Intelligence: Insights from Video Pretraining,https://arcprize.org/blog/arc-prize-2025-results-analysis
2021,Rethinking and Improving Relative Position Encoding for Vision Transformer (iRPE),https://arxiv.org/pdf/2107.14222.pdf
2024,"Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning (PAV / ""progress rewards"")",https://arxiv.org/pdf/2410.08146.pdf
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding,https://arxiv.org/pdf/2104.09864.pdf
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE),https://arxiv.org/pdf/2104.09864.pdf
2024,RoTHP: Rotary Position Embedding-based Transformer Hawkes Process,https://arxiv.org/pdf/2405.06985.pdf
2017,RobustFill: Neural Program Learning under Noisy I/O,https://arxiv.org/pdf/1703.07469.pdf
2025,Rotary Masked Autoencoders Are Versatile Learners,https://arxiv.org/pdf/2505.20535.pdf
2024,Rotary Position Embedding for Vision Transformer,https://arxiv.org/pdf/2403.13298.pdf
2024,Rotary Position Embedding for Vision Transformer (RoPE-Mixed / 2D RoPE study),https://arxiv.org/pdf/2403.13298.pdf
2019,SATNet: Bridging Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver,https://arxiv.org/pdf/1905.12149.pdf
2019,SATNet: Differentiable Satisfiability Solver,https://arxiv.org/pdf/1905.12149.pdf
2020,SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks,https://proceedings.neurips.cc/paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html
2021,SETR: Rethinking Semantic Segmentation as Seq2Seq with Transformers,https://arxiv.org/pdf/2012.15840.pdf
2020,SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,https://arxiv.org/pdf/2012.15840.pdf
2016,"SQuAD: 100,000+ Questions for Machine Comprehension of Text",https://arxiv.org/pdf/1606.05250.pdf
2022,SRBench,https://github.com/cavalab/srbench
2021,SRBench (Symbolic Regression Benchmarks),https://cavalab.github.io/symbolic-regression/
2025,"SRBench update (""next generation"" SRBench)",https://arxiv.org/pdf/2505.03977v1.pdf
2024,SRBench++: principled benchmarking of symbolic regression,https://pmc.ncbi.nlm.nih.gov/articles/PMC12321164/
2024,SRBench++: principled benchmarking of symbolic regression,https://pubmed.ncbi.nlm.nih.gov/40761553/
2022,STaR: Self-Taught Reasoner (Bootstrapping Reasoning With Reasoning),https://arxiv.org/pdf/2203.14465.pdf
2021,SVAMP: Simple Variations on Arithmetic Math Word Problems,https://aclanthology.org/2021.naacl-main.168/
2023,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,https://arxiv.org/pdf/2310.06770.pdf
2016,Safe and Efficient Off-Policy Reinforcement Learning (Retrace(lambda)),https://arxiv.org/pdf/1606.02647.pdf
2022,Scalable Diffusion Models with Transformers (DiT),https://arxiv.org/pdf/2212.09748.pdf
2024,Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters,https://arxiv.org/pdf/2408.03314.pdf
2020,Scaling Laws for Neural Language Models,https://arxiv.org/pdf/2001.08361.pdf
2021,Scaling Up Vision-Language Learning With Noisy Text Supervision (ALIGN),https://arxiv.org/pdf/2102.05918.pdf
2025,Scaling up Test-Time Compute with Latent Reasoning,https://neurips.cc/virtual/2025/poster/117966
2024,ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention,MISSING
2022,"ScienceQA (""Learn to Explain..."")",https://arxiv.org/pdf/2209.09513.pdf
2022,ScienceQA: Benchmark for Multimodal Reasoning,https://arxiv.org/pdf/2209.09513.pdf
2024,Searching Latent Program Spaces,https://arxiv.org/pdf/2411.08706.pdf
2021,SegFormer,https://arxiv.org/pdf/2105.15203.pdf
2021,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,https://arxiv.org/pdf/2105.15203.pdf
2024,SegPoint: Segment Any Point Cloud via Large Language,MISSING
2023,Segment Anything,https://arxiv.org/pdf/2304.02643.pdf
2025,Selective Rotary Position Embedding,https://arxiv.org/pdf/2511.17388.pdf
2018,Self-Attention with Relative Position Representations,https://arxiv.org/pdf/1803.02155.pdf
2022,Self-Consistency for Chain-of-Thought,MISSING
2025,Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI (SOAR),https://arcprize.org/blog/arc-prize-2025-results-analysis
2023,Self-Refine: Iterative Refinement with Self-Feedback,https://arxiv.org/pdf/2303.17651.pdf
1992,Separate visual pathways for perception and action,https://pubmed.ncbi.nlm.nih.gov/1374953/
2020,Sharpness-Aware Minimization (SAM),MISSING
2020,Shortformer: Better Language Modeling using Shorter Inputs,https://arxiv.org/pdf/2012.15832.pdf
1992,Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning (REINFORCE),https://link.springer.com/article/10.1007/BF00992696
2024,Simultaneous Instance Pooling & Bag Selection for MIL using ViTs,https://doi.org/10.1007/s00521-024-09417-3
2025,SmolVLM: Redefining small and efficient multimodal models,https://arxiv.org/pdf/2504.05299.pdf
2018,Soft Actor-Critic (SAC),https://arxiv.org/pdf/1812.05905.pdf
2018,Solving Programming Tasks from Description and Examples,https://arxiv.org/pdf/1802.04335.pdf
2016,Sparse Differentiable Neural Computer (SDNC),https://arxiv.org/pdf/1610.09027.pdf
2022,Sparse4D: Multi-view 3D Object Detection with Sparse Spatial-Temporal Fusion,https://arxiv.org/pdf/2211.10581.pdf
2025,SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection,https://arxiv.org/pdf/2503.08092v1.pdf
2023,Spherical Position Encoding for Transformers,https://arxiv.org/pdf/2310.04454.pdf
2015,Stack/Queue/Deque-augmented RNNs,MISSING
2020,Stand-Alone Axial-Attention (Axial-DeepLab),https://arxiv.org/pdf/2105.15203.pdf
2021,StrategyQA: A Benchmark with Implicit Reasoning Strategies,https://aclanthology.org/2021.tacl-1.21/
2025,Streaming 4D Panoptic Segmentation via Dual Threads,https://arxiv.org/pdf/2510.17664v1.pdf
2019,SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems,https://arxiv.org/pdf/1905.00537.pdf
1995,Support-Vector Networks,https://link.springer.com/article/10.1007/BF00994018
2021,Swin Transformer,https://arxiv.org/pdf/2103.14030.pdf
2021,Swin Transformer V2: Scaling Up Capacity and Resolution,https://arxiv.org/pdf/2111.09883.pdf
Year unknown,SyGuS PBE-BV / PBE-Strings,https://sygus-org.github.io/comp/2019/
2019,"SyGuS-Comp ""PBE tracks"" mature (e.g., PBE-Strings, PBE-BV)",https://sygus-org.github.io/comp/2019/
2016,SyGuS-Comp 2016 Results/Benchmarks (PBE track),https://arxiv.org/pdf/1611.07627.pdf
2017,SyGuS-Comp 2017: Results and Analysis,https://arxiv.org/pdf/1711.11438.pdf
2017,SyGuS-Comp track definitions (PBE-BV),https://sygus.org/comp/2017/
Year unknown,"Symbolic regression benchmarks (AI Feynman, SRBench)",https://www.science.org/doi/10.1126/sciadv.aay2631
1977,Synergetics: An Introduction,https://books.google.com/books/about/Synergetics.html?id=KHn1CAAAQBAJ
2014,Syntax-Guided Synthesis (SyGuS) + SyGuS-Comp 2014,https://sygus.org/comp/2014/
2025,TAPA: Positional Encoding via Token-Aware Phase Attention,https://arxiv.org/pdf/2509.12635.pdf
2025,TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution,https://arxiv.org/pdf/2506.18421v1.pdf
2022,Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the ViTARC Architecture,https://openreview.net/forum?id=0gOQeSHNX1
2020,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2012.09841.pdf
2021,Taming Transformers for High-Resolution Image Synthesis (VQGAN + Transformer),https://arxiv.org/pdf/2012.09841.pdf
2024,Teaching Transformers Modular Arithmetic at Scale,https://arxiv.org/pdf/2410.03569.pdf
1992,Technical Note: Q-learning,https://link.springer.com/article/10.1023/A%3A1022676722315
2025,Test-Time Learning for Large Language Models (TLM / TTL),https://openreview.net/forum?id=iCYbIaGKSR&noteId=ScPdA3KZCL
2024,Test-Time Training on Nearest Neighbors for Large Language Models,https://arxiv.org/pdf/2305.18466.pdf
2025,Test-time Adaptation of Tiny Recursive Models,https://arcprize.org/blog/arc-prize-2025-results-analysis
2019,TextVQA: Towards VQA Models That Can Read,https://arxiv.org/pdf/1904.08920.pdf
2025,The Art of Scaling Test-Time Compute for LLMs,https://arxiv.org/pdf/2512.02008v1.pdf
2007,The Hidden Logic of Sudoku (Second Edition),"# ""The Hidden Logic of Sudoku (2007) — Book (local copy)"""
2024,The LLM ARChitect: Solving ARC-AGI Is a Matter of Perspective,https://arcprize.org/competitions/2024/
2025,The Lessons of Developing Process Reward Models...,https://arxiv.org/pdf/2501.07301.pdf
2019,The Lottery Ticket Hypothesis,MISSING
1956,"The Magical Number Seven, Plus or Minus Two",https://pubmed.ncbi.nlm.nih.gov/13310704/
1957,The Perceptron,MISSING
2021,The Pile: An 800GB Dataset of Diverse Text for Language Modeling,https://arxiv.org/pdf/2101.00027.pdf
2025,The Rotary Position Embedding May Cause Dimension Inefficiency,https://arxiv.org/pdf/2502.11276.pdf
2024,The Surprising Effectiveness of Test-Time Training for Abstract Reasoning,https://arcprize.org/competitions/2024/
1971,The hippocampus as a spatial map,https://pubmed.ncbi.nlm.nih.gov/5124915/
2018,"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",https://arxiv.org/pdf/1803.05457.pdf
2021,TimeSformer: Is Space-Time Attention All You Need for Video Understanding?,https://arxiv.org/pdf/2102.05095.pdf
2021,Tokens-to-Token ViT (T2T-ViT),https://arxiv.org/pdf/2101.11986.pdf
2023,Toolformer: Language Models Can Teach Themselves to Use Tools,https://arxiv.org/pdf/2302.04761.pdf
2015,Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks (bAbI),https://arxiv.org/pdf/1502.05698.pdf
2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arxiv.org/pdf/2411.17708.pdf
2025,Towards the Next Generation of Symbolic Regression Benchmarks,https://arxiv.org/pdf/2505.03977v1.pdf
2021,"Train Short, Test Long: Attention with Linear Biases (ALiBi)",https://arxiv.org/pdf/2108.12409.pdf
2022,Training Compute-Optimal Large Language Models (Chinchilla),https://arxiv.org/pdf/2203.15556.pdf
2021,Training Verifiers to Solve Math Word Problems (GSM8K),https://arxiv.org/pdf/2110.14168.pdf
2020,Training data-efficient image transformers & distillation through attention,https://arxiv.org/pdf/2012.12877.pdf
2022,Training language models to follow instructions with human feedback (InstructGPT / RLHF pipeline),https://arxiv.org/pdf/2203.02155.pdf
2022,TransNeRF: Generalizable Neural Radiance Fields for Novel View Synthesis with Transformer,https://arxiv.org/pdf/2206.05375.pdf
2025,TransXSSM: Hybrid Transformer–SSM with Unified RoPE,https://arxiv.org/pdf/2506.09507.pdf
2022,Transformer Language Models without Positional Encodings Still Learn Positional Information,https://arxiv.org/pdf/2203.16634.pdf
2021,Transformer in Transformer,https://arxiv.org/pdf/2103.00112.pdf
2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://arxiv.org/pdf/1901.02860.pdf
2023,Transformer-based 3D point cloud generation networks,https://dl.acm.org/doi/10.1145/3581783.3612226
2024,Transformers Can Do Arithmetic with the Right Embeddings,https://proceedings.neurips.cc/paper_files/paper/2024/hash/c35986bc1ee29b31c1011481b77fe540-Abstract-Conference.html
2023,Tree of Thoughts (ToT),https://arxiv.org/pdf/2305.10601.pdf
2023,Tree of Thoughts (ToT): Deliberate Problem Solving with Large Language Models,https://arxiv.org/pdf/2305.10601.pdf
2015,Trust Region Policy Optimization (TRPO),https://arxiv.org/pdf/1502.05477.pdf
2021,TruthfulQA: Measuring How Models Mimic Human Falsehoods,https://arxiv.org/pdf/2109.07958.pdf
2018,Twin Delayed DDPG (TD3),https://arxiv.org/pdf/1802.09477.pdf
2021,Twins: Revisiting the Design of Spatial Attention in Vision Transformers,https://arxiv.org/pdf/2104.13840.pdf
2020,UNITER: Universal Image-Text Representation Learning,https://arxiv.org/pdf/1909.11740.pdf
2023,Uni3DL: Unified Model for 3D and Language Understanding,https://arxiv.org/pdf/2312.03026.pdf
2018,Universal Transformer (UT),MISSING
2021,VATT: Transformers for Multimodal Self-Supervised Learning,https://arxiv.org/pdf/2104.11178.pdf
2019,"VCR: Visual Commonsense Reasoning (""From Recognition to Cognition..."")",https://arxiv.org/pdf/1811.10830.pdf
2024,VG4D: Vision-Language Model Goes 4D Video Recognition,https://arxiv.org/pdf/2404.11605.pdf
2017,"VQA v2.0 (""Making the V in VQA Matter"")",https://arxiv.org/pdf/1612.00837.pdf
2015,VQA: Visual Question Answering,https://arxiv.org/pdf/1505.00468.pdf
2025,VRoPE: Rotary Position Embedding for Video Large Language Models,https://arxiv.org/pdf/2502.11664.pdf
2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2511.08747.pdf
2019,ViLBERT: Pretraining Task-Agnostic Vision-and-Language Representations,https://arxiv.org/pdf/1908.02265.pdf
2021,ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision,https://arxiv.org/pdf/2102.03334.pdf
2022,ViTDet: Exploring Plain ViT Backbones for Object Detection,https://arxiv.org/pdf/2203.16527.pdf
2021,Video Swin Transformer,https://arxiv.org/pdf/2106.13230.pdf
2024,Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis,https://arxiv.org/pdf/2405.21075.pdf
2019,VideoBERT: A Joint Model for Video and Language Representation Learning,https://arxiv.org/pdf/1904.01766.pdf
2021,VideoCLIP: Contrastive Pretraining for Zero-Shot Video-Text Understanding,https://arxiv.org/pdf/2109.14084.pdf
2022,ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers,https://arxiv.org/pdf/2203.10157.pdf
2025,VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs,https://arxiv.org/pdf/2506.06727.pdf
2019,VisualBERT: A Simple and Performant Baseline for Vision and Language,https://arxiv.org/pdf/1908.03557.pdf
2018,VizWiz Grand Challenge: Answering Visual Questions from Blind People,https://arxiv.org/pdf/1802.08218.pdf
2020,"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision",https://arxiv.org/pdf/2010.06775.pdf
2023,VoxFormer: Sparse Voxel Transformer for Camera-based 3D Semantic Scene Completion,https://arxiv.org/pdf/2302.12251.pdf
2021,Voxel Transformer for 3D Object Detection (VoTr),https://arxiv.org/pdf/2109.02497.pdf
2025,WALRUS: A Cross-Domain Foundation Model for Continuum Dynamics,https://arxiv.org/pdf/2511.15684.pdf
2025,Wavelet-based Positional Representation for Long Context,https://openreview.net/forum?id=OhauMUNW8T
2024,What matters when building vision-language models? (Idefics2),https://arxiv.org/pdf/2405.02246.pdf
2022,Winoground: Probing Vision-Language Models for Compositionality,https://arxiv.org/pdf/2204.03162.pdf
1974,Working Memory,https://www.sciencedirect.com/science/article/pii/S0079742108604521
2012,"Working Memory: Theories, Models, and Controversies",https://pubmed.ncbi.nlm.nih.gov/21961947/
2019,XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/pdf/1906.08237.pdf
2022,XPos / Length-Extrapolatable Transformer,https://arxiv.org/pdf/2212.10554.pdf
2023,YaRN: Efficient Context Window Extension of Large Language Models,https://arxiv.org/pdf/2309.00071.pdf
2020,ZeRO: Memory Optimizations Toward Training Trillion Parameter Models,https://arxiv.org/pdf/1910.02054.pdf
2021,Zero-Shot Text-to-Image Generation,https://arxiv.org/pdf/2102.12092.pdf
2021,Zero-Shot Text-to-Image Generation (DALL-E),https://arxiv.org/pdf/2102.12092.pdf
2021,diff-SAT - Sampling and Probabilistic Reasoning for SAT and Answer Set Programming,https://arxiv.org/pdf/2101.00589.pdf
2021,miniF2F: a cross-system benchmark for formal Olympiad-level mathematics,https://arxiv.org/pdf/2109.00110.pdf
