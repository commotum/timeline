                 1.  COMPLEXITY AND FOUNDATIONS OF PROBABILITY
       460
       as many times as you want, and this will never give you more than 1% compression. 
        (Such a compression is possible for less than a 2“10000-fraction of all sequences.) 
       This statement deserves to be called a law of nature:  it can be checked experimen­
       tally in the same way as other laws.  So the question is,  Does this law of nature 
       follow from dynamical laws we know?
          To see where the problem is, it is convenient to simplify the situation.  Imagine 
       for a while that we have discrete time, phase space is [0,1), and the dynamical law 
       is
                  X i—^ T(x) =  if 2a; < 1  then 2x else 2x — 1.
       So  we  get  a sequence of states  xq,x\  =  T(xo),X2  = T(xi),...;  at  each step we 
       observe where the current state is—writing 0 if xn  is in  [0,1/ 2)  and  1  if xn  is in 
        (1/ 2. 1).
          This transformation T has the mixing property we spoke about:  If for some 
       large t we look at the set of points that after t iterations are in the left half of the 
       interval, we see that it is just the set of reals where tth bit of the binary representa­
       tion is zero, and these reals occupy about a half in every (not too short) interval.  In 
       other words, we see that a sequence of bits obtained is just the binary representa­
       tion of the initial condition.  So our process just reveals the initial condition bit by 
       bit, and any statement about the resulting bit sequence (e.g., its incompressibility) 
       is just a statement about the initial condition.
          So what?  Do we need to add to the dynamical laws just one more metaphys­
       ical law saying that the world was created at a random (incompressible) state? 
       Indeed, algorithmic transformations (including dynamical laws) cannot increase sig­
       nificantly the Kolmogorov complexity of the state, so if objects of high complexity 
       exist in the  (otherwise deterministic, as we assume for now) real world now, they 
       should  be there  at  the  very  beginning.  (Note  that  it  is  difficult  to  explain  the 
       randomness observed saying that we just observe the world at random time or in 
       a random place.  The number of bits needed to encode the time and place in the 
       world is not enough to explain an incompressible string of length,  say  106,  if we 
       use standard estimates for the size and age of the world.  The logarithms of the 
       ratios of the maximal and minimal lengths (or time intervals) that exist in nature 
       are  negligible  compared  to  106,  and  therefore  the  position  in  space-time  cannot 
       determine a string of this complexity.)
          Should we conclude then that instead of playing dice (as Einstein could put it), 
       God provided “concentrated randomness”  (a state of high Kolmogorov complexity) 
       while creating the world?
         Randomness as ignorance:  Blum-Micali-Yao pseudo-randomness
          This discussion becomes too philosophical to continue it seriously.  However, 
       there are important  mathematical results that could influence the opinion of the 
       philosophers  discussing  the  notions  of probability  and  randomness  if they  knew 
       these results.  In this book we did not touch complexity with bounded resources 
       (an important but not well-studied topic) and instead stayed in the realm of general 
       computability theory, but we cannot avoid this topic when discussing the philosoph­
       ical aspects of the notion of probability.
          This result  is the existence of pseudo-random number generators  (as defined 
       by Blum, Micali and Yao; they are standard tools in computational cryptography; 
       see,  e.g.,  the  Goldreich  textbook  [61]).  Their  existence  has  been  proven  using
