                       12.6.  INFORMATION  DISTANCE AND  SIMULTANEOUS  ENCODING        377
                It  would  be interesting to compare this argument with the proof of Slcpian- 
            Wolf theorem and find some  “common denominator”,  a combinatorial fact  that 
            implies both Muchnik and Slepian-Wolf theorems.  It would be also interesting to 
            find another proof of Theorem 231  (e.g., a direct probabilistic argument, or, even 
            better, some explicit construction of the graph).
                One can also  prove  Muchnik’s  theorem  using  another  famous  combinatorial 
            tool,  randomness extractors.  This idea was suggested  (in a different context and 
            before Muchnik’s paper) in [23], and it was applied to Muchnik’s theorem in [145]. 
            The advantage of this  approach  is that  one  can  use  known  explicit  randomness 
            extractors  and  other  known  techniques  (e.g.,  pseudo-randomness  generators,  as 
            suggested by Romashchenko) to prove the space-bounded version of Muchnik’s the­
            orem [143,  144].  (Let us mention that resource-bounded Kolmogorov complexity 
            and its relations with computational complexity theory is an important topic that 
            is outside the scope of our book.)
                     12.6.  Information distance and simultaneous encoding
               Now we consider a request  (Figure 41) where the capacity of the dotted line 
            is bounded by k, and the k-bit string transmitted along this channel must contain 
            enough information to transform A to В and vice versa.
                Obviously, it is possible only if C{A\ В) < к and C(B | A) ^ к (with logarithmic 
            precision, as in all our considerations).  Indeed, the left output node receives A (or 
            some string derived from A) and X (or some string derived from X ) and produces 
            B, so C(B\ A) ^ k.  A symmetric argument shows that C(A\B) < k.
               We get a necessary condition for the feasibility of this request:
                                     max(C(A\B),C(B\A))  <   A;
            (as  usual,  logarithmic  terms  are  omitted).  It  was  shown  by  Bennett,  Gacs,  Li, 
            Vitånyi, and Zurek [9] that this necessary condition is at the same time sufficient. 
            Here is the exact statement of their result:
               Theorem 232.  Let A, В be strings such that C{A\B) < к and C(B\A) < k. 
            Then there exists a string X of length к such that
              C{A\B,X) = 0{logk),  C{B\A,X) = O(logfc),  and  C(X\A, B) — 0(\ogk).
               Proof.  Consider  all  pairs  (A,B)  of  strings  such  that  C(A\B)  <  к  and 
            C(B I A) < к at the same time.  We get an enumerable binary relation on strings; 
            all its vertical  (A is fixed)  and horizontal  (В is fixed) sections contain at most 2k
                   Figure 41.  Bennett-Gacs-Li-Vitanyi-Zurek information request
