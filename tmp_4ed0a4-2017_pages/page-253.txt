              240                                8.  SOME  APPLICATIONS
              has maximum at p = 1/2, and the second derivative at this point is non-zero (equals 
               —4/In 2).  Therefore, the Taylor expansion gives us
                                            h{ 1/2 + <5) = 1 -          + o(62)
              as Ô —> 0, and for 6n = pn — 1/2 we have
                                                     Si = 0(logn/n),
              So we get at least something, though the bound we need is much stronger.  (Let us 
              mention that in the probability theory the final bound was obtained in many steps. 
              First Hausdorff (1913)  proved the bound 0{n£ / л/п)\  then Hardy and Littlewood 
              (1914)  improved it  and replaced n£  by  ^/logn;  then Steinhaus  (1922)  came with 
              the bound (1 + e) y/(2 In n)/n, and only later Khinchin (1924) got the final result. 
              So we are now on the level of Hardy and Littlewood in this respect—not that bad.)
                   Let  us think about  possible improvements for the upper bound that we had 
              for KM((oj)n).  This upper bound was obtained by comparing KM((oj)n) and the 
              negative logarithm of the probability of the prefix (cu)n with respect to the Bernoulli 
              measure with parameter pn.  This  logarithm  is  exactly  nh(pn, 1 — pn),  but  the 
              Bernoulli measure used for comparison depends on n, so the construction used in 
              the  proof of Theorem 89  needs an additional term that  is K(pn)  (we start  with 
              a self-delimiting encoding of pn).  Here K(pn) does not exceed (2 + e) log n, since 
              both numerator and denominator of the fraction pn do not exceed n.  Altogether 
              we get the bound 
                                 2
                               — (pn -  1/2)2 « 1 -  h(pn, 1 — Pn) < (2 + e) logn/n,
              which is still not good enough.
                   What else can we do?  Note that we already know that pn is rather close to 1/2: 
              with denominator n the numerator differs from n/2 by л/п or a bit more.  So (when 
              the denominator n is known) we can use fewer bits to describe the numerator, and 
              this allows us to replace 2 by 1.5 in the right-hand side.  But this is still not enough 
              for us.
                   The crucial idea is to use approximations for pn instead of the exact values.  Let 
              us assume that pn = 1/2 + Sn > 1/2 (the case when pn < 1/2 is similar).  Instead of 
              pn we use (while constructing the Bernoulli measure used to get the upper bound for 
              complexity) its approximation 1/2 + <5^ where 6'n is an approximation to Sn from 
              below with a small  (fixed)  relative error.  For example,  let  us take  S'n  such  that 
              0,9<5n < S'n  < Sn.  Such a 6'n can be founded among the geometric sequence (0,9)k, 
              and its complexity is about log A:, i.e., about log(— log Sn/log 0,9) = log(— log<5n)+c. 
              Note that if 6n < lf^/n, then we have nothing to prove, so the complexity of 6'n can 
              be upper bounded by (1+e) log log n (for every e this bound holds for all sufficiently 
              large n).
                   This is good news; the bad news is that we have a more complicated bound for 
              the complexity of (ш)п.  Now instead of h(pn, 1 — pn) we have
              (*)                       Pn[~ bgp'n] + (1 - Pn)[~ log(l - p'n)\,
