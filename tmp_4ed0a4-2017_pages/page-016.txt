                                           BASIC  NOTIONS  AND  NOTATION                              XVII
              identify such sequences with probability distributions on the set Nj_, which consists 
              of natural numbers and of the special symbol _L (undefined value).
                  Among all semimeasures  (on the tree or on natural numbers)  we distinguish 
              lower semicornputable ones.  Both the class of lower seinicomputable serniineasures 
              on the tree and the class of lower semicornputable semimeasures on natural numbers 
              have a maximal semimeasure (up to a multiplicative constant).  Any maximal lower 
              semicornputable semimeasure is called an  a priori probability  (on  the  tree or  on 
              natural numbers).  The a priori probability of a natural number n is denoted by 
              m(n); the a priori probability of a node x in the binary tree (that is, of the string x) 
              is denoted by a(x).  We use also the notation m{x) for a binary string x, which means 
              an a priori probability of the number of x with respect to some fixed computable 
              one-to-one correspondence between strings and natural numbers.
                  The plain Kolmogorov complexity is denoted by C(x), the prefix Kolmogorov 
              complexity is denoted by K(x) (and by K'{x) when we want to stress that we are us­
              ing prefix-free description modes).  The same letters are used to denote complexities 
              of pairs,  triples,  etc.,  and to denote conditional complexity.  For instance,  C(x\y) 
              stands for the plain conditional complexity of x when у is known, and m(x,y\z) 
              denotes  the  a priori  probability  of the  pair  (x,y)  (that  is,  of the  corresponding 
              number) when z is known.  The monotone Kolmogorov complexity is denoted by 
              AM, and the a priori complexity (negative logarithm of the a priori probability on 
              the tree)  is denoted by KA.  (In the literature monotone complexity is sometimes 
              denoted by Km and Km and the a priori complexity is denoted by KM.) Finally, 
              the decision complexity is denoted by KR.
                  В В (n) denotes the maximal halting time of the optimal decompressor on inputs 
              of length at most n (if the optimal prefix decompressor is meant, then we use the 
              notation  BP(n)).  The  function  BB{n)  is  closely  related  to  the  function  B(n) 
              defined as the maximal natural number of Kolmogorov complexity at most n.
                  We  use  also  several  topological  notions.  The  space  Nj_  consists  of natural 
              numbers  and  of a special  element  _L  (undefined  value);  the  family  of open  sets 
              consists of the whole space and of all sets that do not contain _L.  This topological 
              space, as well as the space £  (where the family of open sets consists of all unions 
              of sets of the form £x),  is used for the general classification of complexities.  For 
              the spaces 0. and £ and for the space of real numbers, we call a set effectively open 
              if it is a union of a computably enumerable family of intervals (sets of the form £x 
              for the second space and intervals with rational endpoints for the space of reals).
                  Most notions of computability theory (including Kolmogorov complexity) can 
              be relativized, which means that all involved algorithms are supplied by an external 
              procedure, called an oracle.  That procedure can be asked whether any given number 
              belongs to a set A.  That set is also called an oracle.  Thus we get the notions of 
              “decidability relative to  an oracle A”,  “computability relative to A”,  etc.  In the 
              corresponding notation we use the superscript A, for example, CA(x).
                  In the chapter on classical information theory, we use the notion of Shannon en­
              tropy of a random variable £.  If the variable has к possible outcomes and pi,... ,Pk 
              are their probabilities, then its Shannon entropy H(£) is defined as — YlkPk l°gVk- 
              This definition makes sense also for pairs of jointly distributed random variables. 
              For such a pair the conditional entropy of a random variable £ when i] is known is 
              defined as H{£, rf) —H(r)).  The difference if(£) + H(rj) — H(£, 77) is called the mutual
