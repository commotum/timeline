           12               INTRODUCTION.  WHAT IS THIS BOOK  ABOUT?
           has measure zero.  This implies that the set of sequences that do not satisfy SLLN 
           is included in a set of measure 0 and hence has measure 0 itself.
               The notion of a random sequence is philosophically interesting in its own right. 
           In the beginning of twentieth century Richard von Mises suggested using this no­
           tion  (he called it in  German  Kollektiv)  as a basis for probability theory  (at  that 
           time the measure theory approach had not yet been developed).  He considered the 
           so-called  “frequency stability”  as a main property of random sequences.  We will 
           consider von Mises’ approach to the definition of a random sequence (and subse­
           quent developments) in Chapter 9.
               Lower  bounds  for  computational  complexity.  Kolmogorov complexity 
           turned out to be a useful technical tool when proving lower bounds for computa­
           tional complexity.  Let us explain the idea using the following model example.
               Consider the following problem.  Initially,  a string x of length n is located in 
           the n leftmost cells of the tape of a Turing machine.  The machine has to copy x, 
           that is, to get xx on the tape (the string x is intact and its copy is appended), and 
           halt.
               Since the middle of the 1960s it has been well known that a (one-tape) Turing 
           machine needs time proportional to n2 to perform this task.  More specifically, one 
           can show that  for every Turing machine M that  can copy every string x,  there 
           exists some e > 0 such that for all n there is a string x of length n whose copying 
           requires at least en2 steps.
               Consider the following intuitive argument supporting this claim.  The number 
           of internal states of a Turing machine is a constant  (depending on the machine). 
           That  is,  the  machine can keep in its  memory only a finite number of bits.  The 
           speed of the head movement is also limited:  one cell per step.  Hence the rate of 
           information transfer (measured in bit■ cell/step) is bounded by a constant depending 
           on the number of internal states.  To copy a string x of length n, we need to move n 
           bits by n cells to the right;  therefore, the number of steps should be proportional 
           to n2  (or more).
              Using Kolmogorov complexity, we can make this argument rigorous.  A string 
           is hard to copy if it contains maximal amount of information, i.e., its complexity is 
           close to n.  We consider this example in detail in Section 8.2 (p. 233).
              A  combinatorial  interpretation  of Kolmogorov  complexity.  We con­
           sider here one example of this kind (see Chapter 10, p. 313, for more detail).  One 
           can  prove  the  following  inequality  for  the  complexity  of three  strings  and  their 
           combinations:
                           2C(xyz) ^ C(xy) + C{xz) + C(yz) + O(logn)
           for all strings x, y, z of length at most n.
              It turns out that this inequality has natural interpretations that are not related 
           to  complexity at all.  In particular,  it implies  (see  [65])  the following geometrical 
           fact:
              Consider a body В in a three-dimensional Euclidean space with coordinate axes 
           OX, OY, and OZ. Let V be R’s volume.  Consider B's orthogonal projections onto 
           coordinate planes OXY, OXZ, and OYZ.  Let Sxy, Sxz, and Syz be the areas of 
           these projections.  Then
                                      V2 ^ Sxy • Sxz • Syz.
