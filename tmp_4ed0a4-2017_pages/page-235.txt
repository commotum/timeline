                      222                       7.  SHANNON ENTROPY AND KOLMOGOROV COMPLEXITY
                      I f  t h i s   m in im u m   c o in c id e s  w ith   th e   m in im u m   ta k e n   o v er  a ll  m a tric e s   qjj  (th e   la tte r  
                      is  a c h ie v e d   fo r  qij — pij),  th e n   w e  h av e
                                                                                     Pij = Pi*  ' P*j i
                      a n d   v a ria b le s   f   a n d   77  a r e   in d e p e n d e n t.                                                                              □
                               221 Provide an another  (though sim ilar)  proof using T heorem   142.
                               222 Prove  th at  three  random   variables  a, ß, 7  are  independent  (this  m eans 
                      t h a t   t h e   p r o b a b ility   o f  th e   e v e n t  (a = cti,ß  =   ßj, 7   =   7*,)  e q u a ls  th e   p ro d u c t  o f 
                      t h r e e   p r o b a b ilitie s   fo r  e a c h   o f th e   v a ria b le s )  if  a n d   o n ly   if
                                                                H((a, /3,7)) =  Ща) + H(ß) + H (7 ).
                              T h e o re m s  141  a n d   144  show   th a t  th e   d ifference  H(£) + H(rj) — H((£,r)))  is 
                      alw ays  non-negative and equals zero if and only if f  and  77  are independent.  So we 
                      can  tak e  th is  difference  for  a   q u a n tita tiv e   m e asu re  o f  d e p e n d e n c e   b e tw e e n   f   a n d  
                      77.   T h is  d ifference  is  d e n o te d   b y   /( £   :  77)  a n d   is  c a lle d   th e   mutual  information  of 
                      tw o  ra n d o m   v a ria b le s  £  a n d   77.  T h e o re m   142  allo w s  u s  to   re w rite   th e   d e fin itio n   for 
                      / ( £   :  77)  in   t h e   fo llo w in g   w ay :
                                                          Hi ■ v) = H(v) - я(г,Ю = як) - як|„)
                      ( m u tu a l  in fo rm a tio n   sh o w s  how   m u ch   th e   k n o w led g e  o f  o n e   ra n d o m   v a ria b le   d e ­
                      creases th e entropy of th e o th er one).
                              To see all these notions in action,  let us retu rn  to th e M cM illan inequality.  Now 
                      we  change  th e  order  and  prove  first  th a t  a   u n iq u ely   d eco d ab le  code  for  a  ran d o m  
                      variable f   h as  th e   av erag e  le n g th   o f th e   co d ew o rd   a t  le a st  H(£).
                              F irs t  n o te  th a t   fo r  a n   in je c tiv e  c o d e  w h e re   a ll  c o d e w o rd s  h a v e   le n g th   less  th a n   c 
                      t h e   a v e ra g e   le n g th   is  a t  le a s t  H(£) —  lo g e .              I n d e e d ,  if  щ  a re   th e   le n g th s  o f  th e  
                      codewords, the sum  of 2 ~ni  does not exceed c  (for every fixed length th e sum  does 
                      n o t  e x c e e d   1 ).     Therefore,  th e  inequality  of  T heorem   138  is  violated  at  m ost  by 
                      lo g e .
                              T h is  is  n o t  e n o u g h ,  a n d   to   g e t  a   tig h t  b o u n d   w e  c o n s id e r  N  in d e p e n d e n t  id e n ­
                      tic a lly   d is tr ib u te d   c o p ie s  o f th e   ra n d o m   v a ria b le   £.  W e  g e t  a   ra n d o m   v a ria b le   t h a t 
                      could  be  denoted  by  ÇN.  Its  entropy  is  NH(£).  L et  us  use  our  code  for  each  of 
                      N coordinates and then concatenate all the strings.  T he unique decoding property 
                      g u a r a n te e s   t h a t   th is   is  a n   in je c tiv e   c o d e .  I ts   a v e ra g e   le n g th   is  N  tim e s   g r e a te r  th a n  
                      t h e   a v e r a g e   le n g th   o f in itia l  c o d e   fo r  f   (lin e a rity   o f e x p e c ta tio n ) .  A n d   th e   m a x im a l 
                      l e n g th   d o e s   n o t  e x c e e d   cN  w h e re   c  is  a n   u p p e r   b o u n d   fo r  th e   le n g th   o f  th e   c o d e ­
                      words of the uniquely decodable code we started w ith.  So  the previous paragraph 
                      gives  us
                               N •  (average length of th e  uniquely decodable code)  ^   NH(£) — \og(cN).
                      Now we divide over  N and  take  N —>  00.  Since  \og(cN)/N —>  0  as  N —>  00,  this 
                      gives us th e bound  H(£)  for th e  average  length of a uniquely decodable code.
                             Now  the  M cM illan  inequality  is  easy.  A ssum e  th a t  th e  uniquely  decodable 
                      code  has  code  lengths  П1,...,пд,  and                                    2 ~Ui  >  1.          We  sta rt  w ith   p ro b ab ilities 
                      Pi  =   2 ~n‘  a n d   th e n   p ro p o rtio n a lly   d e c re a se   a ll  o f  th e m   m a k in g   th e ir  su m   e q u a l 
                      t o   1.   Consider  th e  random   variable  th a t  has  th e   d istrib u tio n   Pi  (o b tain ed   in  th is 
                      way)  and its coding by m eans of our uniquely decodable code.  T he average length 
                      is  YlPini  w hich  is  less  th a n   H = Y^Pi(~^°ëPi)  (recall  th a t  щ <  — lo g pi  since  we 
                      have decreased the values 77).
