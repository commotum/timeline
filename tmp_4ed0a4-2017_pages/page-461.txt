              452                           14.  ALGORITHMIC  STATISTICS
              segments correspond to the hypotheses of described type (since for such A the grey 
              area where В resides has only one common point with Px).
                  Notice that  the information that  is  contained in these  hypotheses,  does not 
              really depend on x:  the hypothesis В contains the same information as the (I — s)- 
              bit  prefix of the string JV).  As we have seen in Problem 355  (p. 437),  this prefix 
              can be replaced by Ni-S,  which has the same information as the first  I — s bits 
              of Chaitin’s    number.  Thus the larger the complexity of our model is, the more 
              information about       it  has.  This  is  discouraging,  since  the  number     does not 
              depend on x.
                  It  might  be that  other parameters  (than complexity and cardinality)  help to 
              distinguish  models  of the same size and complexity,  as explanations for x.  The 
              paper  [199]  suggests  one such parameter,  namely the total complexity A condi­
              tional  to  x.  In  all  our  examples  intuitively  right  models  for  x  have  small  total 
              complexity conditional to x.  On the other hand, one can show that models from 
              the universal family from Theorem 261 have large total complexity conditional to 
              some  of their  members.  We omit  the  proof of this  claim,  which  may  be  found 
              in  [199].
                  Note also that this observation (saying that different hypotheses contain almost 
              the same information) is applicable only to hypotheses of our special type and not 
              to  arbitrary hypotheses on the boundary of Px,  as the following example shows. 
              Let x be a random n-bit string.  Consider two hypotheses:  the set of n-bit strings 
              у  that  have the same first  half as x and the set  of n-bit strings у that  have the 
              same second half as x.  Both hypotheses have small optimality deficiency, but the 
              information contained in them is completely different.  (This does not contradict 
              our results above, since the set of all n-bit strings as В has better parameters than 
              both.)
                  Historical remarks.  Cutting the list of all strings of complexity at most к into 
              portions  according to the binary expansion of JV*  was  introduced in  [60],  where 
              it  was noticed that for к = C(x) we obtain in this way a model for x with small 
             optimality deficiency.  Later in  [203]  models of this type were considered also for 
              к > C(x), and Theorems 261 and 262 were proven.
                                           14.7.  A bit of philosophy
                  There are several philosophical questions related to the task of finding a good 
             two-part description for a given string x.  For instance, we can let x be the sequence 
             of all observations about the world made by mankind (encoded in binary) and then 
             consider scientific theories as models A for x.  Among those theories we want do 
             identify the right  ones.  Our criteria are the simplicity of the theory in question 
              (measured  by  the  Kolmogorov  complexity  of A—the  less  the  complexity  is  the 
             better), and the  “concreteness”  or the  “explanatory capability”  (measured by the 
             size of A—the less the size is, the more concrete the model is, hence the better). 
              One can also recall the ancient philosopher Occam and his razor  ( “entities must not 
             be multiplied beyond necessity”), which advises choosing the simplest explanation. 
              Or we can look for a scientific theory A such that the randomness deficiency of the 
             data x with respect to A is small (“a good theory should explain all the regularities 
             in the data” ).
                  There  are  also  more  practical  issues  related  to  algorithmic  statistics.  Kol­
             mogorov complexity can be considered as a theory of “ultimate compression” :  the
