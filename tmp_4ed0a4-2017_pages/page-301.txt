            288          9.  FREQUENCY  AND  GAME  APPROACHES  TO  RANDOMNESS
            for some c and for some large enough n.  We want to show that u> is not Mises- 
            Church-Daley random, i.e., construct a rule that selects an unbalanced sequence.
                Let us first consider the case when c <  1.  The set of all strings of complexity 
            less than clogn is an enumerable set of at most nc elements, and for large n the 
            number or elements in this set (we denote it by Cn) is bounded, say, by n/10.  Fix 
            some of those large values of n.
                Reading the n-bit prefix from left to right, we try to the predict the next bit 
            (after reading all the previous ones).  Let us show that we can guarantee at least 
            90% success.  Enumerating Cn, we find a first element in this enumeration, call it 
            the  “current  candidate”,  and predict  the bits that  are there until  our prediction 
            turns out to be incorrect.  As soon as this happens, we continue the enumeration of 
            Cn until we find a new element that is consistent with all already discovered bits. 
            Then it becomes the current candidate, and it is used for predictions until one of 
            the predictions turns out to be incorrect, etc.  Since we know that the actual prefix 
            is in Cn, we will never run out of candidates, and the number of changes (=number 
            of errors) is bounded by the cardinality of Cn, i.e., by n/10.  So at least 90% of the 
            predictions are correct.
                This can be done for every large enough n.  To deal with an infinite sequence, 
            we consider  a fast  growing computable sequence no  <  n\  <  П2 ■ • ■  where no  is 
            large enough (so our prediction method works for all щ).  Using CUi for predictions 
            between Щ-\  and щ, we make at most 0.1щ errors, and in total we get at most 
            0.2n errors  (even if all  previous predictions  are false,  which is  not  the  case,  but 
            we do not need a better estimate).  So our prediction method will be successful 
            infinitely often.
                It remains to note (as was done in Theorem 169) that the prediction algorithm 
            corresponds to two selection rules:  one selects terms when we predict ones, and the 
            other selects terms when we predict  zeros.  If predictions are successful,  at  least 
            one of these selection rules will select a highly unbalanced sequence.  This ends the 
            proof for c < 1.
                This trick does not work for c > 1.  For example, if c = 1.5 we have n1-5 candi­
            dates,  and all our predictions could be false (leading to the change in the current 
            candidate without any contradiction).  But we can use a more clever argument.
                Let us split the string u>o • ■ -cun_ i  into two halves and get a pair  (u,v)  where 
            и and v are (n/2)-bit strings.  The complexity of this pair is at most  1.5 log n (we 
            still  consider  our  example with c =  1.5).  On the other hand,  the complexity of 
            the pair is equal to C(u) + C(v\u) up to 0(\ogC(u, v)), so either C(u) < 0.8logn 
            or C(v\u)  < 0.8logn.  In both cases we can apply the trick used for c <  1, since 
            n0-8 is much less than nf 2.  Note also that, while predicting the bits in the second 
            half, we already know the bits in the first half, so the condition и in the inequality 
            C(v |u) < 0.8 log n is not an obstacle.
                So at least one of the two prediction algorithms is successful (on its half).  Then 
            one of the two selection rules corresponding to this algorithm will select a highly 
            unbalanced sequence.  (The selection rule does not select any terms from the other 
            half.)
                There is a problem, however.  All this can be done for every n, but how do we 
            combine the selection rules for different n?  Imagine, for example, that we tried to 
            predict bits in the left half assuming that C(u) < 0.8n while in fact that is not the 
            case.  Then our algorithm can make many errors (this is not a big problem)  and
