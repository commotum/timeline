        58              3.  MARTIN-LOF RANDOMNESS
        called L, is the uniform distribution:  all strings have probability 2~n.  The second 
       one,  called  S,  is  biased  (ones  are  more  likely  than  zeros)  and  corresponds  to  n 
        independent coin tosses where one appears with probability p = 1/2 + e.  In other 
       words, S(x) = qupv for a string x that has и zeros and v ones (here q = 1/2 — £ is 
       the probability of zero outcome).  The ratio S{x)/L{x) increases when the number 
       of ones  in  x  increases,  and  for  all  bad  strings  this  ratio  is  at  least  2n/2H^p,q^n. 
       Therefore,  the total  L-measure  of all  bad strings  does  not  exceed  their  total  S- 
       measure divided by this lower bound.  Recalling that the total S'-measure of all bad 
       strings does not exceed 1, we conclude that the total L-measure (i.e., the fraction) 
       of all bad strings does not exceed 2H(j>,q'>n/2n. So we get another proof of our bound, 
       which is less technical (though more difficult to find).  This proof works not only for 
       the uniform Bernoulli measure (p = 1/2), but also for arbitrary p (after appropriate 
       changes).
          67  Prove the Strong Law of Large Numbers for arbitrary p.
          (Hint:  Let po  and  qo  be  fixed  positive  reals  such  that  po + qo  =  1.  Then 
       the expression — po logp — go log q, where p, q are arbitrary positive reals such that 
       p + q — 1, is minimal when p — po, q = go-  See also Section 9.6, p. 275.)
          People often say that  “the Strong Law of Large Numbers guarantees that in 
       every random sequence (with respect to uniform Bernoulli measure) the frequency 
       of ones  tends  to  1/2.”  (The case of non-uniform  Bernoulli measures  is similar.) 
       However, in this sentence the word  “random”  should not be understood literally: 
       the phrase  “every random sequence satisfies a”  (for some condition a)  is  an id­
       iomatic expression that means that the set of all sequences that do not satisfy a is 
       a null set.
          A natural question arises:  Can we define the notion of a random sequence in 
       such a way that this idiomatic expression can be understood literally?  Let us fix 
       some distribution on Q, say, the uniform Bernoulli distribution.  We would like to 
       find some subset of Q and call its elements  “random sequences”.  Our goal would 
       be achieved if for any condition a the following two statements were equivalent:
           •  all random sequences satisfy the condition a\
           •  the set of all sequences that does not satisfy a is a null set.
          In other words, the sets of measure 1 should be exactly those sets that contain 
       all random sequences (and, maybe, some non-random ones).
          One more reformulation:  the set of all random sequences should be the smallest 
       (with respect to inclusion) set of measure 1, and the set of non-random sequences 
       should be the largest  (with respect to inclusion) null set.  Now it easy to see that 
       our goal cannot be achieved.  Indeed, any singleton in Q is a null set.  However, the 
       union of all these singletons is the entire space Q.
          In 1965 Per Martin-Löf (a Swedish mathematician who was Kolmogorov’s stu­
       dent  at  that  time)  found  that  we  can  save  the  game  if we  restrict  ourselves  to 
       “effectively null sets”.  There exists a largest  (with respect to inclusion) effectively 
       null  set,  and  therefore we can define  the notion of a random sequence  is such a 
       way that any condition a is satisfied for all random sequences if and only if the 
       set of all sequences that do not satisfy a is an effectively null set.  The Martin-Löf 
       construction is explained in Section 3.3.
