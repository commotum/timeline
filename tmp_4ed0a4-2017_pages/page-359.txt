                                    10.13.  NON-SHANNON  INEQUALITIES                      347
                The latter inequality is in fact the sum of eight basic inequalities:
                                           I({a,ß):e |7 ,£) ^ 0,
                                               I(a:ß |e,7) ^ 0,
                                               I(a:ß\e,6) ^ 0,
                                                 I(T.ô\e) ^ 0,
                                                 I(T-e\a) > 0,
                                                 I{l'-£\ß) > о,
                                                 I(S:e I a)  ^  0,
                                                 I(6:e\ß) ^ 0.
            There is no problem to check this:  we just need to express all the mutual information 
            in terms of entropies of tuples, and we get the same inequalities after canceling the 
            opposite terms.  Still it remains unclear why this happens and how one can invent 
            such a trick.                                                                  □
                One may say that our non-Shannon inequality, while not being a positive linear 
            combination of basic inequalities, still follows from them in a more general sense. 
            In fact, we have discovered a general deduction rule for entropy inequalities:  if we 
            manage to split the variables in some inequality into three groups in such a way 
            that the second and third group never meet, then this inequality can be derived 
            from a  (generally)  weaker  inequality where the mutual  information  between the 
            variables of the second and third group (conditional to all the variables of the first 
            group) is added.
                This rule can be used to prove many other non-Shannon inequalities for en­
            tropies  (it  is  known that  one can get  in this way infinitely many inequalities for 
            four variables, and each of them is not a positive linear combination of others).
                Deleting the unique information.  There is one more tool that can be used 
            to  derive  new  inequalities  for entropies.  It  is  based  on  the  following  Ahlswede- 
            Körner theorem [2] that we state here without proof.  (See [113, Lemma 5] for the 
            proof.)
                Theorem 220.  Let a,ß,e be random variables  with some joint  distribution. 
            Consider n  independent  copies  of this  triple,  and  denote  them by ai,ßi,£{  (i  = 
            1,..., n). Let
                           A = (ax,...,an),B = {ßi,... ,ßn),E = (el5...,en).
            Then there exist a random variable E',  defined on the same space as A,B,E such 
            that all seven entropies on the diagram for the triple А, В, E'  are the same  (up to 
            o(n))  as for the triple А, В , E,  except for one, H(E' | A, В), that is now o(n) instead 
            of H(E\A,B).
                Informally speaking, E' is like E but does not contain any information that is 
            missing in A, B.  A similar statement is true for an arbitrary number of random 
            variables  ai,...,otk  (with  some joint  distribution).  Let  us  take  n  independent 
            samples from this distribution  and denote the resulting variables by  A\,..., Ak ■ 
            One can “delete” the information that is unique for Ak (is missing in A\,..., Ak-1) 
            and replace  Ak  by  some  A'k  in such  a way  that  all  the  regions on  the  diagram
