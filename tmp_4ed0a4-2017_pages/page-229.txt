             216           7.  SHANNON  ENTROPY AND KOLMOGOROV  COMPLEXITY
                 This proof is a kind of a “relaxation argument” :  if we forget that code-lengths 
             should be integers and allow any щ such that        2~Ui  <  1, the optimal choice is 
             Ш — — log pi (convexity of the logarithm function); making щ integers, we lose less 
             than 1.
                 Theorem 139.  The entropy of the distribution pi,... ,pn  (with n possible val­
             ues)  does not exceed logn.  It equals logn  only if all pi  are equal.
                 Proof.  If n is a power of 2, the inequality H ^ log n follows from Theorem 138 
             (consider a prefix code where n codewords all have length logn.  In the general case 
             we  use  Gibbs  inequality  for  g*  =  1/n  (for  all  г)  and  recall  that  this  inequality 
             becomes an equality only if pi = g*.                                               □
                 7.1.3.      Huffman code.  We have shown that the average length of an optimal 
             prefix code (for given Pi,... ,Pk) is somewhere between H and H + 1.  But how can 
            we find this optimal code?
                 Let n\,..., nk be the lengths of codewords for an optimal code (for given fre­
            quencies pi,... ,pk).  Rearranging the letters, we may assume that
                                             Pi  ^ P2  < • • •  < Pk-
            in this case we may assume that
                                             ni ^ n2 ^      > nk.
            Indeed, if one letter has a longer code than another letter that is less frequent, the 
            exchange of codewords (between these two letters) decreases the average length of 
            the code.
                 One can note also that щ — n2 for an optimal code (the two less frequent letters 
            have the same code length).  Indeed, if щ  > n2, then n\  is greater than all n*.  So 
            the first  term in the sum      2~Ui  is  smaller than  all other terms,  the inequality 
            Xa 2“ni < 1 cannot be an equality (all terms except the first one are multiples of the 
            second term), and the difference between its two sides is at least 2~ni.  Therefore, 
            we can decrease щ  by  1  and still not violate the  inequality      2~ni  <  1.  This 
            means that the code is not optimal (contrary to our assumption).
                 We can look for an optimal code among codes that have n\ = n2; this optimal 
            code minimizes the sum
                     Pini + p2n2 + p3n3 H-----+ pknk = (pi + p2)n + p3n3 + ■ • • + pknk
             (here n is the common value of щ  and n2).  In the last expression the minimum 
            should be taken over all sequences n, n3,... ,nk such that
                                    2~n + 2~n + 2~Пз + • ■ • + 2~Пк  ^ 1.
            This inequality can be rewritten as
                                      2-(n-l) +  2-n3 +  . . . +  2~nk  ^
            and the expression that is minimized can be rewritten as
                              (pi +P2) + (pi +p2)(n- 1) +p3n3 + • • • +pknk.
            The term  (pi + p2)  is  a constant  that  does  not  influence  the  minimal  point,  so 
            the problem reduces to finding an optimal prefix code for к — 1 letters that have 
            probabilities Pi + Р2,Рз, ■ • • ,Pk-
