                                       3.  MARTIN-LÖF RANDOMNESS
            56
            to p, i.e.,
                                                  +   •  •  •  +  OJn-1
                                      lim  -------------------------- = p.
                                     n—>oo          n
                Theorem 27.  The set Ap has measure 1 with respect to Bernoulli distribution 
            with parameters p and q.
                In other words, the complement of Ap, i.e., the set of all sequences that either 
            do not have limit frequency at all or have a limit frequency different from p, is a 
            null set (according to this distribution).
                Proof.  We prove this theorem for the uniform case (i.e., for p — q — 1/2) by 
            an explicit calculation.  The general case is left as an exercise (see also Section 9.6).
                Let  us  consider  first  a  finite  number  of coin  tossings  and  fix  some  n.  All 
            binary strings of length n have the same probability.  We claim that most of them 
            have approximately nj2 ones.  Assume that some threshold e is fixed.  How many 
            sequences have more than  (1/2 + e)n ones?  The answer can be found using the 
            Pascal triangle:  we have to sum up all the terms in the nth row starting from some 
            point that is slightly on the right of the midpoint.  In this part we have a decreasing 
            sequence of less than n terms, so the sum in question is bounded by the first term 
            multiplied by n.  (We do not need to be very accurate in our bounds and ignore 
            factors that are polynomial in n.  So we can omit the factor n in our bound.)
                The first term of the sum is the binomial coefficient
                                                   n!
                                               k\(n — k)V
            where к is the smallest integer not less than (1/2 + e)n.  We use Stirling’s approxi­
            mation
                                     m! = \/(2tt + o(l))m     ^   ,
            where e is the base of natural logarithms.  Ignoring polynomial (in n) factors and 
            using the notation и = к/n, v = (n — k)/n, we get
                n!    ^         (n/e)n                nn
            k\(n — k)\   {k/e)k(Jji — k)/e)n~k   kk{n — k)n~k
                                                      _ ____^                1   _ 2H(u,v)n
                                                         (un)UTl(vn)vn    yUTlyvn           ’
            where
                                      H(u, v) = — и log и — v log V.
            The value H(u, v) is called the Shannon entropy of a random variable that has two 
            values whose probabilities are и and v.  (We study Shannon entropy in Chapter 7.) 
            Figure 8 shows the corresponding graph (note that v = 1 — u).  It is easy to check 
            that H(u, 1 — u) achieves its maximal value (equal to 1) only at и = 1/2.
                Now we see that the number of binary strings of length n that have frequency of 
            ones greater than (1/2 + e) does not exceed ро1у(п)2я (1/2+е,1/2~е)п and therefore 
            is  bounded by  2cn+°^n\   where  c  is some constant  less  than  1  (depending on e). 
            Therefore, the fraction of these strings (among all strings of length n) exponentially 
            decreases as n increases.  The same is true for the strings that have frequency of 
            ones less than(l/2 — e).
