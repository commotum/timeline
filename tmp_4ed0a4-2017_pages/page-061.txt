            46          2.  COMPLEXITY  OF  PAIRS  AND  CONDITIONAL  COMPLEXITY
                       Figure 3.  Mutual information and conditional complexity
                                                  x,y
                       Figure 4.  Common information in overlapping substrings
            bits  (matching  the  complexity  of x).  Similarly,  the  central  part  together  with 
            C(y\x) (the right part) give C(y).  Finally, all three parts together give us
                 C(x I y) + I(x : y) + C{y I x) = C(x) + C(y \ x) = C(x \ y) + C(tj) = C(x, y)
            bits (all equalities are true up to О (log n) for strings x and у of length at most n).
                In  some  cases  this  picture  can  be  understood  quite  literally.  Consider,  for 
            instance,  an incompressible string r  =  rq • • -rn  of length  n,  so  C{r\ ■ ■ - rn)  ^  n. 
            Then any substring и of x has complexity l(u) up to O(logn) terms.  Indeed, since 
            и is a substring of r, we have r = tuv for some strings t, v.  Then
                       l(r) = C(r) ^ C(t) + C(u) + C(v) ^ l(t) + l(u) + l(v) — l(r)
            (up to a logarithmic error),  and therefore all the inequalities are equalities  (with 
            the same logarithmic precision).
                Now take two overlapping substrings x  and у  (Figure 4).  Then C(x)  is the 
            length of x and C(y) is the length of у (up to O(logn)).
                The complexity C(x, y) is equal to the length of the union of segments (since 
            the pair  (x,y) is equivalent to this union plus information about lengths requiring 
            0(log?r) bits).
                Therefore,  conditional complexities C(x\y), C(y \x)  and the mutual informa­
            tion I(x:y) are equal to the lengths of the corresponding segments (up to O(logn)).
                However,  the mutual information cannot  always be extracted in the form of 
            some string (like it happened in our example, where this common information is 
            just the intersection of strings x and y).  As we will see in Chapter 11, there exist 
            two strings x and у that have large mutual information I{x:y) but there is no string 
            г that represents (materializes) this information in the following sense:  C(z \ x) ~ 0, 
            C(z I y) « 0 (all information that is present in г is also present both in x and in y), 
            and C(z) ~ I(x:y)  (all mutual information is extracted).  In our last example we 
            can take the intersection substring for г, but in general this is not possible.
                 61  Prove that for any string x of length at most n the expected value of the
            mutual information I(x:y) in x and the random string у of length n is O(logn)
