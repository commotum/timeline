         232      7.  SHANNON  ENTROPY AND KOLMOGOROV COMPLEXITY
         NH(£) — cVN + O(logiV)  (the latter term corresponds to the complexity of pa­
         rameters and can be omitted if we increase c).  As we know (Theorem 149, p. 230), 
         the probability of this event is at most e.            □
            240 As before, we assume that probabilities pi are known exactly, and if Pi 
         are not computable, we get some problems.  Correct the argument replacing pi by 
         their approximations with sufficient precision.
            241 Give a statement and proof for a similar result about conditional coding
         and conditional entropy.
            (Hint:  Assume that two dependent random variables £ and rj are given.  We 
         make n independent trials, the value of rjN  is known both to the sender and the 
         receiver, and the sender wants to send m bits in such a way that the receiver could 
         reconstruct the value of ÇN.  How large should m be?)
