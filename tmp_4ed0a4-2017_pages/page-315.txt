               302             9.  FREQUENCY  AND  GAME APPROACHES  TO RANDOMNESS
                    Now assume that the trials are independent, but the success probabilities are 
               different  for  different  trials  (pi  for  the  îth  trial).    One would  expect  that  if pi 
               converges to some p, then again for almost all (with respect to the product measure) 
               sequences,  the  limit  frequency  is p.  The  intuitive  explanation  follows:  Consider 
               some e > 0.  All pi except for a finite number of them (which could be ignored) are 
               smaller than p+ef 2.  We know that if we replace all pi by p+ej2, then almost surely 
               the frequency will be less than p + e starting from some moment.  By monotonicity 
               reasons, this should be true also for the original pi.
                    With some effort, this argument can be made precise, but we prefer to prove 
               a more general statement that is useful in many cases.  It deals with an arbitrary 
               measure ß on Q, let p(x) = ß(flx) be the corresponding function on strings.  For 
               every binary string x = xq ■ • • xn_i  consider the number of ones in x, and also the 
               sum of conditional probabilities pi of the appearance of 1 in the ith position of x 
               (after the corresponding prefix),
                                             Pi =p{x0 • ■ -Xi-il)/p(x0 ■ ■ -Xi_i).
               Both quantities (m and the sum of pi) depend on x; the bound below guarantees 
               that with high ß-probability for a given n these quantities are close to each other. 
               Here is the exact statement (we have explained what m and Pi are; the probability 
               is taken with respect to ß):
                    Theorem 198.
                                       Pr[|m -  (po + ----bPn—\)\ > d] ^ 2e~d2/4n.
                    This theorem is essentially finite,  and this inequality is true for every proba­
               bility distribution on n-bit strings.  This is a weak form of a classical inequality in 
               probability theory called the Azuma-Hoeffding inequality.  We will give a simple 
               argument that uses the same technique that we used for the proof of the SLLN, 
               though it does not give the best possible result  (in fact, the constant  1/4 can be 
               replaced by 2, so one can get the bound with right-hand side 2e~2d ln  [66, Theo­
               rem 2]).
                    P r o o f.  We  consider  separately  the  probabilities  of m  being  too  big  or  too 
               small.  Both cases are similar,  so we need to consider one of them and prove,  for 
               example, that
                                         Pr[m -  (po H------ bpn-i) > d] ^ e~d2/4n.
               We use a standard trick and construct some measure ß' for which the ratio ßl jß is 
               big for all sequences that belong to the event in the left-hand side.  (The ratio ß'/ß 
               is a martingale that reaches ed !4n if this event happens.)
                    Since we want to increase the measure for the sequence with many ones, it is 
               natural to increase the conditional probability of 1 in ß' (compared to ß).  Namely, 
               if for the original sequence the conditional probabilities of 1 and 0 after some x are 
               p and q, in the new measure we let these probabilities be
                                                p' =p + epq,  q' = q~ epq,
               where e is some positive real.  The value of e will be chosen later; now we say only 
               that £ does not exceed 1/2.  It is easy to check the p' and q' are still in [0,1].
                    So we get another probability distribution on n-bit strings.  Let us compute the 
               ratio of these two distributions on some string x of length n.  Each bit of x adds a 
               factor p'/p (if this symbol is 1) or q'/q, where p, q,p', q' are conditional probabilities
