             226             7.  SHANNON  ENTROPY  AND  KOLMOGOROV  COMPLEXITY
                                        7.3.  Complexity and entropy
                  As you surely have  noticed,  the  properties  of Shannon  entropy  (defined  for 
             random variables)  resemble the properties of Kolmogorov complexity (defined for 
             strings;  see Chapter 2).  Is it possible to formalize this similarity by converting it 
             into exact statements?
                  This question has two interpretations.  First,  one can prove that Kolmogorov 
             complexity and Shannon entropy have similar properties (in particular,  the same 
             linear inequalities are true for them; see Section 10.6, p. 326).  On the other hand, 
             one may compare the numeric values for complexity and entropy, and this is what 
             we do in this section.
                  The problem here is that Kolmogorov complexity is defined for strings while 
             Shannon entropy is defined for random variables, so how could one compare them? 
             However, sometimes this comparison is possible, as we shall see.  Let us start with 
             a very vague and philosophical description of the results below:  Shannon entropy 
             takes into account only frequency regularities while Kolmogorov complexity takes 
             into account all algorithmic regularities, so in general the latter is smaller.  On the 
             other hand, if an object is generated by a random process in such a way that it has 
             only frequency regularities, entropy is close to complexity with high probability.
                  Let us give now some specific results that illustrate this general statement.
                  7.3.1.       Complexity and entropy of frequencies.  Consider an arbitrary fi­
             nite alphabet A which may contain more than two letters.  Kolmogorov complexity 
             for А-strings can be defined in a natural way.  (Note that we have never used that 
             finite  objects  whose complexity is defined are binary strings.  However,  it  is  im­
             portant that binary strings are used as descriptions:  complexity measured in bytes 
             would be eight times less than complexity measured in bits!)
                  Let  X  be  an  А-string  of length  A/",  and  let  p\,..., pk  be  the  frequencies  of 
             letters  in  x.  All these frequencies are fractions with denominator N and integer 
             numerators.  The sum of frequencies equals  1.  Let  h(p\,... ,pk)  be the Shannon 
             entropy of corresponding distribution.
                  Theorem  146.         C(x)                      O(logiV)
                                         N < h{pi, ■ ■,Pk) +          N
                  Here the constant in О (log N)  does not depend on N, x and the frequencies 
             pi,... ,pk-  However, this constant may depend on к (we consider an alphabet of a 
             fixed size).
                  PROOF.  In fact this is a purely combinatorial statement.  Indeed, the complex­
             ity C(x\N,p\,... ,pk) does not exceed logC(iV,pi,... ,p*.) + 0(1), where
                                                                   N\
                                  C{N,pu ...  pк)       (pt jV)!(p2iV)! • • • (pkN)\
             is  the  number  of А-strings  of length  N  that  have  frequencies pi,...,pk-  (Each 
             string with given frequencies can be determined by its ordinal number in this set if 
             the parameters N,p\,... ,pt are known, and this number has log C(A/’,pi,... ,pk) 
             bits.)
                  The number C(iV,pi,... ,Pk) can be estimated using Stirling’s approximation. 
             Ignoring factors bounded by a polynomial in N (that appear due to the term \fbik 
             in Stirling’s approximation formula k\ ~ V2Îтк(к/е)к), we get exactly 2Nh^Pl'---'pA.
