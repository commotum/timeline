                                         7.3.  COMPLEXITY AND  ENTROPY                            231
                  7.3.5.  Shannon coding theorem.  The theorem of the last section is a natu­
             ral translation of classical Shannon results into complexity language.  These results 
             deal with the length of a code that allows us to transmit iV-letter blocks correctly 
             with high probability (according to the given distribution).
                  Let f be (again) a random variable with к values (letters of A) and some fixed 
             distribution.  Let N be a positive integer.  By     we denote a random variable with 
             range AN that is formed by N independent copies of £.  We want to encode values 
             of £n by m-bit strings (see Figure 22).
                               : N                                               -N
                               Figure 22.  Using m bits for transmission of
                  Here  an  encoder  is  any  mapping  of type  AN  —>  Bm,  and  a  decoder  is  any 
             mapping of type B'm  —y  AN.  A given value of ÇN  causes  an  error  if the  input 
             and  output  А-strings  (of length N)  differ.  The  probability of error  is  measured 
             according to the distribution of ÇN.  The question is:  What conditions on m and N 
             guarantee the existence of an encoder/decoder pair that has small error probability? 
             First, let us make the following evident remark:
                  Theorem  150.  For given N,m  and e > 0,  the code with error probability at 
             most £ exists if and only if the 2m  most probable values of£N  have total probability 
             at least 1 — e.
                  P r o o f.  Indeed, when m bits are used for encoding, one may transmit (without 
             errors) at most 2m values.  To minimize the error probability, we should choose 2m 
             most probable values.                                                                 □
                 In the next theorem the alphabet A and the random variable f are fixed.
                 Theorem 151.  For each e > 0 there exists a constant c such that:
                  (a)  The values of£N  can be encoded/decoded with NH(^) + c\/N bits with error 
             probability at most e;
                  (b)  Any code for ÇN  of length at most NH(£) — cy/N has error probability at 
             least 1 — e (i.eth e probability of correct decoding is at most e).
                 PROOF,  (a) As we know, for a suitable c the value of random variable fN  has 
             complexity less than m — NH(£) + cy/~N with probability at least 1 — e.  So for these 
             values one can use shortest descriptions (see the definition of plain complexity) as 
             codes.  (Formally speaking, we get strings not of length m, but of length less than m, 
             but there are at most 2m of them, and they can be replaced by strings of length m.)
                 Note that coding is not performed by an algorithm, but the theorem (as stated) 
             does not say anything about that, it claims the existence of a code mapping.
                  (b)  Here we  need  to  use  some  trick.  If there  exists  a code  of given  length, 
             then such a code can be constructed algorithmically using the previous theorem 
             (or just by an exhaustive search).  Then the decoding function for this code can be 
             considered as a conditional decompressor (where conditions are pi and N).  There­
             fore,  all  values  of f N  that  are  decoded  without  error,  have  complexity  at  most
