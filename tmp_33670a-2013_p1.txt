                                        Auto-Encoding Variational Bayes
                                      Diederik P. Kingma                       MaxWelling
                                    Machine Learning Group                Machine Learning Group
                                   Universiteit van Amsterdam            Universiteit van Amsterdam
                                    dpkingma@gmail.com                  welling.max@gmail.com
                                                              Abstract
                                How can we perform efﬁcient inference and learning in directed probabilistic
                                models, in the presence of continuous latent variables with intractable posterior
                                distributions, and large datasets? We introduce a stochastic variational inference
                                and learning algorithm that scales to large datasets and, under some mild differ-
                                entiability conditions, even works in the intractable case. Our contributions are
                                two-fold. First, we show that a reparameterization of the variational lower bound
                                yields a lower bound estimator that can be straightforwardly optimized using stan-
                                dard stochastic gradient methods. Second, we show that for i.i.d. datasets with
                                continuous latent variables per datapoint, posterior inference can be made espe-
                                cially efﬁcient by ﬁtting an approximate inference model (also called a recogni-
                                tion model) to the intractable posterior using the proposed lower bound estimator.
                                Theoretical advantages are reﬂected in experimental results.
                        1   Introduction
                        Howcanweperformefﬁcientapproximateinferenceandlearningwithdirectedprobabilisticmodels
                        whose continuous latent variables and/or parameters have intractable posterior distributions? The
                        variational Bayesian (VB) approach involves the optimization of an approximation to the intractable
                        posterior. Unfortunately, the common mean-ﬁeld approach requires analytical solutions of expecta-
                        tions w.r.t. the approximate posterior, which are also intractable in the general case. We show how a
                        reparameterization of the variational lower bound yields a simple differentiable unbiased estimator
                        of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for ef-
                        ﬁcient approximate posterior inference in almost any model with continuous latent variables and/or
                        parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.
                        For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-
       arXiv:1312.6114v11  [stat.ML]  10 Dec 2022EncodingVB(AEVB)algorithm. IntheAEVBalgorithmwemakeinferenceandlearningespecially
                        efﬁcientbyusingtheSGVBestimatortooptimizearecognitionmodelthatallowsustoperformvery
                        efﬁcient approximate posterior inference using simple ancestral sampling, which in turn allows us
                        to efﬁciently learn the model parameters, without the need of expensive iterative inference schemes
                        (suchasMCMC)perdatapoint. Thelearnedapproximateposteriorinferencemodelcanalsobeused
                        for a host of tasks such as recognition, denoising, representation and visualization purposes. When
                        a neural network is used for the recognition model, we arrive at the variational auto-encoder.
                        2   Method
                        The strategy in this section can be used to derive a lower bound estimator (a stochastic objective
                        function) for a variety of directed graphical models with continuous latent variables. We will restrict
                        ourselvesheretothecommoncasewherewehaveani.i.d. datasetwithlatentvariablesperdatapoint,
                        andwhereweliketoperformmaximumlikelihood(ML)ormaximumaposteriori(MAP)inference
                        on the (global) parameters, and variational inference on the latent variables. It is, for example,
                                                                  1
