                                                                                S                  C                 F                     ScatterFormer                D 5
                                                           V            B       c          B                 B                                             V            e
                                                                                a               I  r                 e                                     o            t
                                                           o            a     A t          a    n  o         a     N e                                     x     B      e
                                                                                t                  s
                                                  Input    x            t     t e          t    t  s         t       d         D                           e     E      c
                                                           e    C       c     t r          c    e            c     e -                                     l     V      t
                                                           l            h     e e          h    r  W         h     t F         o                           -            i
                                                                                                a                  w                                       t            o
                                                  Point    i    P             n                                      o         w                           o      
                                                           z
                                                           a    E       N     t d          N    c  i         N     o r                 ScatterFormer       -     C      n
                                                                        o     i            o    t  n         o       w         n                                 N       
                                                           t                  o L                                                                          P            H
                                                 Clouds i                                       i  d               r
                                                                                                o
                                                           o            r     n i          r                 r     k                     Block ×3          i     N
                                                                        m       n          m    n  o         m       a                                     l            e
                                                           n                    e                                    r                                     l            a
                                                                                a                  w                 d                                     a            d
                                                                                r                                                                          r            s
                                                                                 
                                                                                                 ScatterFormer Block ×3
                                                 Fig.2: The macro design of ScatterFormer. The backbone comprises a Conditional
                                                 Position Encoding (CPE) and six transformer blocks. Each block is composed by a
                                                 Scattered Linear Attention (SLA) module, a Cross-Window Interaction (CWI) module,
                                                 and a Feed-Forward Network (FFN).
                                                 Transformer framework. To achieve context-rich representations, several stud-
                                                 ies [8,14,23,24,47] have integrated the attention module into point- or voxel-
                                                 basedencoders.Forinstance, VoTr[24] utilizes dilated attention for expanded re-
                                                 ceptive fields; VoxSet [14] applies set attention for extracting point-based features
                                                 in set-to-set translation; SST [8] employs local attention with shifted windows;
                                                 and OcTr [59] adopts an Octree-based attention for efÏcient hierarchical context
                                                 learning. Nevertheless, how to efÏciently leverage global context from attention
                                                 remains a challenge due to the intrinsic sparsity of point clouds. DSVT [47] and
                                                 FlatFormer [23] group the voxels within each window into a series of fixed-length
                                                 voxel sets, thus extracting the features in a fully parallel manner. Recently, a
                                                 group-free state-space model [54] was proposed to directly process voxels as a
                                                 sequence. However, these approaches more or less lose the spatial proximity and
                                                 incur extensive computational overhead in grouping and sorting the voxels.
                                                 3      Method
                                                 The overall architecture of our proposed ScatterFormer is depicted in Figure 2.
                                                 It begins with the input point clouds, which are voxelized and transformed into
                                                 high-dimensional embeddings using a VFE layer [60]. These embeddings are then
                                                 processed through Conditional Positional Encoding (CPE) using a shallow con-
                                                 volutional network [6]. The encoded features enter the ScatterFormer backbone,
                                                 consisting of six ScatterFormer blocks. Each block includes a Scattered Lin-
                                                 ear Attention (SLA) module, a Cross-Window Interaction (CWI) module, and
                                                 a Feed-Forward Network (FFN), interspersed with Batch Normalization layers
                                                 and skip connections. After three ScatterFormer blocks, the voxel features are
                                                 downsampled via a sparse convolutional layer. The downsampled features are
                                                 then converted into pillar features [47], generating compact BEV features for
                                                 bounding-box prediction. ScatterFormer ScatterFormer stands out by not re-
                                                 quiring voxel features to be organized into fixed-length sets [8,23,47], enabling
                                                 flexible attention computation across windows. Additionally, the CWI module
                                                 obviates the need for window shifting. These innovations allow ScatterFormer
                                                 to avoid unnecessary memory allocation and permutation operations, achieving
                                                 high efÏciency comparable to CNN-based models.
