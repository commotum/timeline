                              CoAtNet: MarryingConvolutionandAttention
                                                      for All Data Sizes
                                            ZihangDai,HanxiaoLiu,QuocV.Le,MingxingTan
                                                       Google Research, Brain Team
                                          {zihangd,hanxiaol,qvl,tanmingxing}@google.com
                                                                Abstract
                                Transformers have attracted increasing interests in computer vision, but they still
                                fall behind state-of-the-art convolutional networks. In this work, we show that
                                while Transformers tend to have larger model capacity, their generalization can be
                                worse than convolutional networks due to the lack of the right inductive bias. To
                                effectively combine the strengths from both architectures, we present CoAtNets
                                (pronounced “coat” nets), a family of hybrid models built from two key insights:
                                (1) depthwise Convolution and self-Attention can be naturally uniﬁed via simple
                                relative attention; (2) vertically stacking convolution layers and attention layers in
                                a principled way is surprisingly effective in improving generalization, capacity and
                                efﬁciency. Experiments show that our CoAtNets achieve state-of-the-art perfor-
                                manceunderdifferent resource constraints across various datasets: Without extra
                                data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with
                                13MimagesfromImageNet-21K,ourCoAtNetachieves88.56%top-1accuracy,
                                matching ViT-huge pre-trained with 300M images from JFT-300M while using
                                23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves
                                90.88%top-1 accuracy on ImageNet, establishing a new state-of-the-art result.
                        1   Introduction
                        Since the breakthrough of AlexNet [1], Convolutional Neural Networks (ConvNets) have been the
                        dominating model architecture for computer vision [2, 3, 4, 5]. Meanwhile, with the success of
                        self-attention models like Transformers [6] in natural language processing [7, 8], many previous
                        works have attempted to bring in the power of attention into computer vision [9, 10, 11, 12]. More
                                                                                   1
                        recently, Vision Transformer (ViT) [13] has shown that with almost only vanilla Transformer layers,
                        one could obtain reasonable performance on ImageNet-1K [14] alone. More importantly, when
                        pre-trained on large-scale weakly labeled JFT-300M dataset [15], ViT achieves comparable results
                        to state-of-the-art (SOTA) ConvNets, indicating that Transformer models potentially have higher
                        capacity at scale than ConvNets.
                        While ViT has shown impressive results with enormous JFT 300M training images, its performance
                        still falls behind ConvNets in the low data regime. For example, without extra JFT-300M pre-training,
                        the ImageNet accuracy of ViT is still signiﬁcantly lower than ConvNets with comparable model
                        size [5] (see Table 13). Subsequent works use special regularization and stronger data augmentation
                        to improve the vanilla ViT [16, 17, 18], yet none of these ViT variants could outperform the SOTA
                        convolution-only models on ImageNet classiﬁcation given the same amount of data and computa-
                        tion [19, 20]. This suggests that vanilla Transformer layers may lack certain desirable inductive biases
                        possessed by ConvNets, and thus require signiﬁcant amount of data and computational resource
                        to compensate. Not surprisingly, many recent works have been trying to incorporate the induc-
                        tive biases of ConvNets into Transformer models, by imposing local receptive ﬁelds for attention
                           1The initial projection stage can be seen as an aggressive down-sampling convolutional stem.
                        35th Conference on Neural Information Processing Systems (NeurIPS 2021).
