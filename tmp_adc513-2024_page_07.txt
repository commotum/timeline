                                                                                                         ¯
              Table 2. Evaluation results of various MLLMs in different capability levels of SEED-Bench. T denotes the averaged accuracy across
              corresponding dimensions, and R¯ denotes the rank based on the the averaged accuracy. The evaluation dimensions of part-2, together
                                             T
              with L , encompass L , while the evaluation dimensions of part-3, together with L , encompass L .
                    1              2                                                       2              3
                                                                       L (Part-1)       Part-2          L            Part-3          L
                            Model                 Language Model         1                                2                            3
                                                                         ¯             ¯              ¯             ¯              ¯
                                                                         T     R¯      T     R¯      T      R¯      T     R¯      T     R¯
                                                                                 T             T              T             T             T
                         BLIP-2 [18]                 Flan-T5-XL        41.0     8     35.3    9     40.5     7      -      -       -      -
                       InstructBLIP [8]              Flan-T5-XL        42.2     6     35.7    5     41.7     6      -      -       -      -
                   InstructBLIP Vicuna [8]           Vicuna-7B         41.4     7     29.7    18    40.5     8      -      -       -      -
                         LLaVA[24]                   LLaMA-7B          38.7    11     30.2    17    38.0    12      -      -       -      -
                       MiniGPT-4[47]                 Vicuna-7B         39.4     9     34.1    12    39.0     9      -      -       -      -
                       VPGTrans[44]                  LLaMA-7B          36.2    19     23.9    20    35.2    18      -      -       -      -
                    MultiModal-GPT[15]               Vicuna-7B         37.4    14     34.9    11    37.1    13      -      -       -      -
                          Otter [17]                 LLaMA-7B          36.4    17     36.6    4     36.4    16      -      -       -      -
                     OpenFlamingo [29]               LLaMA-7B          37.3    15     35.5    8     37.1    14      -      -       -      -
                  LLaMA-AdapterV2[12]                LLaMA-7B          37.5    13      -       -      -      -      -      -       -      -
                          GVT[38]                    Vicuna-7B         34.4    21     38.6    3     34.8    19      -      -       -      -
                      mPLUG-Owl[41]                  LLaMA-7B          39.4    10     28.9    19    38.5    10      -      -       -      -
                        Kosmos-2[32]             Decoder only 1.3B     46.3     3     23.3    21    44.4     3      -      -       -      -
                      Qwen-VL-Chat[2]                 Qwen-7B          43.1     4     35.5    7     42.5     4      -      -       -      -
                       LLaVA-1.5[23]                 Vicuna-7B         47.3     2     30.8    16    46.0     2      -      -       -      -
                  IDEFICS-9B-Instruct [16]           LLaMA-7B          38.0    12     40.3    2     38.2    11      -      -       -      -
                InternLM-Xcomposer-VL [45]          InternLM-7B        59.2     1     32.1    14    56.9     1      -      -       -      -
                       VideoChat [19]                Vicuna-7B         37.0    16     35.3    9     36.8    15      -      -       -      -
                     Video-ChatGPT [28]              LLaMA-7B          36.4    18     31.0    15    35.9    17      -      -       -      -
                          Valley [27]               LLaMA-13B          34.5    20     32.2    13    34.3    20      -      -       -      -
                          Emu[35]                   LLaMA-13B          42.5     5     41.1    1     42.4     5    41.4     1     42.3     1
                       NExt-GPT[39]                  Vicuna-7B         30.7    22     35.6    6     31.1    21    33.9     2     31.4     2
              nal prediction of the given multiple-choice question.              plementations. For each model, we first determine its ca-
                                                                                 pability level and then evaluate the corresponding dimen-
              Evaluationoftextandimageoutput.           Forquestionswith         sions. Note that we have confirmed with the authors that the
              text and image answers, we first employ an answer rank-            LLaMA-AdapterV2’scapabilitylevelisL1. SomeMLLMs
              ing strategy to select the most likely text prediction. If it      can reach the capability level L3, but they are not available
              matches the ground truth, we evaluate the image output us-         as open-source.
              ing the CLIP similarity score [33] between the generated           4.2. Main Results
              image and each candidate. The model is deemed correct
              only if both text and image predictions match the ground           Theevaluation results of various MLLMs in different capa-
              truth.                                                             bility levels of SEED-Bench are listed in Tab. 2. The de-
              4. Evaluation Results                                              tailed leaderboard of each evaluation dimension is provided
                                                                                 in the supplemental materials. InternLM-Xcomposer-VL
              4.1. Models                                                        outperforms a large number of MLLMs, achieving the best
                                                                                 performance based on the averaged accuracy in capability
              We evaluate a total of 22 open-source MLLMs includ-                level L and L , and Emu ranks top-1 in capability level L
                                                                                        1       2                                            3
              ing BLIP-2 [18], InstructBLIP [8], InstructBLIP Vi-                with only one competitor. Because InternLM-Xcomposer-
              cuna [8], LLaVA [24], MiniGPT-4 [47], VPGTrans [44],               VL retrieves images from the available image pool rather
              MultiModal-GPT [15], Otter [17], OpenFlamingo [29],                than generate images, it does not reach the capability level
              LLaMA-Adapter V2 [12], GVT [38], mPLUG-Owl [41],                   L . To better showcase the capabilities of models across
                                                                                   3
              Kosmos-2 [32], Qwen-VL-Chat [2], LLaVA1.5 [23],                    different evaluation dimensions, we further visualize the
              IDEFICS-9B-Instruct [16], InternLM-Xcomposer-VL [45],              ranking of each model within each evaluation dimension
              VideoChat [19],     Video-ChatGPT [28],        Valley   [27],      in Fig. 5, where darker colors represent higher ranks and
              Emu [35], and NExt-GPT [39] based on their official im-            grey color indicates that the model has not yet reached the
                                                                             13305
