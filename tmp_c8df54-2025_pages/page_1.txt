         000
         001
         002                                        Submission and Formatting Instructions for
         003                        International Conference on Machine Learning (ICML 2025)
         004
         005
         006
         007                                                                                            1
         008                                                                 AnonymousAuthors
         009
         010                                    Abstract                                         Corpus (ARC) (Chollet, 2019). ARC consists of 1000 vi-
         011                                                                                     sual tasks, that capture essential aspects of abstraction and
         012            The Abstraction and Reasoning Corpus (ARC)                               analogy. The ARC tasks are split into 400 for training, 400
         013            benchmarks broad generalization in artificial in-                        for evaluation and 200 hidden tasks for testing. A Program
         014            telligence, and presents a significant challenge to                      Synthesis approach from 2020 solved 40% of the complete
         015            existing machine learning models and program                             evaluation set (Icecuber, 2023), and a voting ensemble from
         016            synthesis solvers. In this work, we introduce a                          2024achieved 40.25% (Bober-Irizar & Banerjee, 2024).
         017            Reflection System for ARC. It combines Large
         018            Language Models (LLMs) and a program synthe-
         019            sis solver based on a Domain Specific Language
         020            (DSL). We analyse the base accuracy of publicly
         021            available LLMs on ARC and demonstrate unsat-
         022            isfactory results. We create AugARC, an aug-
         023            mented ARCbenchmark,whichconsistently im-                                 Input 1   15x15    Input 2  15x15    Input 3  15x15    Test 1    15x15
         024            proves the performance of LLMs compared to the
         025            normal ARCbenchmark. Using augmented ARC
         026            data, we fine-tune LLMs and observe a significant
         027            gain in ARC accuracy after training. By utilizing                                                                                      ?
         028            reflection, we combine LLMs and a previous DSL
         029            solver into our Reflection System for abstraction                         Output 1 15x15     Output 2 15x15    Output 3 15x15
         030            and reasoning. The proposed Reflection System                                                                                         Task b7fb29bc
         031            motivates research to advance previous ARC at-
         032            tempts by using reflection to combine the advan-                         Figure 1. Visualisation of an ARC task. The test-taker is provided
         033            tages of LLMs and program synthesis solvers.                             with some input-output pairs as examples. The objective is to
         034                                                                                     recognise the transformation used in the given input-output pairs
         035                                                                                     and apply it to the test input grid to obtain the test output grid.
         036      1. Introduction
         037                                                                                     Many systems that attempt to solve the ARC test set use
         038      Incorporating abstract reasoning into machines has been an                     heuristic search. Such models are heavily handcrafted and
         039      active research topic since the 1955 Dartmouth AI workshop                     designed entirely with the goal of solving ARC. Recent
         040      (McCarthy et al., 2006). Despite the significant progress                      attempts have tried solving ARC with Graph Abstractions
         041      in machine learning, today’s AI systems still lack human-                      (Xu et al., 2023a) and Generalized Planning (Lei et al.,
         042      level abstract reasoning (Korteling et al., 2021). Studies                     2024). However, these two approaches have only been
         043      have shown that digital systems are significantly inferior to                  tested on a subset of ARC evaluation data. Some attempts
         044      humansinterms of abstract cognitive abilities (Boden et al.,                   have been made to use Large Language Models (LLMs) to
         045      2017; Shneiderman, 2020).                                                      solve ARC (Xu et al., 2023b; Min, 2023; Mitchell et al.,
         046      Toaddress the gap between human intelligence and AI mod-                       2023), with some of the previous publications testing LLMs
         047      els, Franc¸ois Chollet created the Abstraction and Reasoning                   on the ARC evaluation set (Bober-Irizar & Banerjee, 2024;
         048                                                                                     Opiełka et al., 2024; Gendron et al., 2023; Lee et al., 2024b).
         049         1AnonymousInstitution,AnonymousCity,AnonymousRegion,                        Nevertheless, previous studies test LLMs only on subsets of
         050      Anonymous Country. Correspondence to: Anonymous Author                         the ARCevaluation data and do not attempt to build more
         051      <anon.email@domain.com>.                                                       advanced systems with reflection based on several LLMs.
         052      Preliminary work. Under review by the International Conference                 Hence, we aim to fully explore the abilities of base LLMs
         053      onMachineLearning(ICML). Donotdistribute.                                      on ARCandhowthosecanbecombinedinsystems with
         054
                                                                                             1
