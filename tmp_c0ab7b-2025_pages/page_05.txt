                                                                FineWeb-Edu-10B
                          Sequence Length CABLE CABLENW ALiBi         Fire   T5-bias Kerple RoPE Learnable Sinusoidal
                          512               17.00    17.22    17.30   17.60   17.79   17.22  17.39    17.99     17.98
                          1024              16.52    16.73    16.79   17.11   17.26   16.70  16.89    17.47     17.48
                          2048              15.97    16.24    16.56   19.60   38.32   16.28  38.95     —       219.80
                          4096              15.34    15.79    16.67 101.98 243.69     16.78 146.72     —       1058.84
                          8192              15.41    15.97    17.23 383.08 799.53     20.32 361.26     —       2485.84
                          15360             15.41    16.03    17.46 835.92 1450.83 26.13 691.90        —       3355.86
                                                                  WikiText-103
                          Sequence Length CABLE CABLENW ALiBi         Fire   T5-bias Kerple RoPE Learnable Sinusoidal
                          512              23.70     24.32    24.09   24.34   25.06   23.95  23.66    24.94     25.18
                          1024             22.32     23.01    22.74   22.90   23.60   22.56  22.26    23.53     23.73
                          2048              21.48    22.19    22.05   22.68   27.64   21.72  41.40     —       172.33
                          4096              20.94    21.70    21.73   29.57   73.99   21.32 114.77     —       607.48
                          8192              20.65    21.46    21.58   54.89  198.64   21.33 220.56     —       1348.23
                          15360             20.33    21.13    21.30 104.79 411.09     21.58 375.62     —       2017.42
                  Table 1: Perplexity comparison on the FineWeb-Edu-10B and WikiText-103 evaluation sets. The models in the
                  upper table are GPT-2 Medium variants trained on the FineWeb-Edu-10B training set for 19k steps with a sequence
                  length of 1024. The models in the lower table are GPT-2 Tiny variants trained on the WikiText-103 training set for
                  9ksteps, also with a sequence length of 1024.
                  distance. It can be seen as a generalization of AL-      4 ExperimentSetup
                  iBi. while ALiBi applies fixed linear biases, CA-        4.1   Datasets
                  BLE learns context-aware biases for each token.
                  Notably, if we set each token’s bias and weight          For training, we use the FineWeb dataset (Penedo
                  to -1 and 1/2h respectively, CABLE reduces to            et al., 2024), a large-scale dataset (15 trillion to-
                  ALiBi, with relative biases simply reflecting token      kens) for LLM pretraining, derived from 96 Com-
                  distances. However, CABLE’s key advantage is its         monCrawlsnapshots. FineWeb has been shown to
                  ability to adapt these biases based on token context,    produce better-performing LLMs than other open
                  enabling more expressive and flexible positional         pretraining datasets (Penedo et al., 2024). More
                  encoding.                                                specifically, we use a 10B sample of the FineWeb-
                                                                           Edu dataset, which consists of 1.3T tokens from
                    As with most RPE methods, CABLE adds po-               educational web pages filtered from the FineWeb
                  sitional information only to the queries and keys        dataset. We allocate 9.9B tokens for training and
                  (not the values), a practice shown to enhance length     0.1B for evaluation. Furthermore, we also train the
                  extrapolation in methods like ALiBi, T5-bias, and        models on WikiText-103 (Merity et al., 2016), a
                  RoPE.                                                    small dataset containing a preprocessed version
                                                                           of Wikipedia, widely used in many NLP tasks.
                    CABLEissimple,lightweight, and easily inte-            For evaluation, we use the test sets of FineWeb-
                  grates into standard attention mechanisms. It re-        Edu, WikiText-103, and a 1B-token sample of the
                  quires only two additional linear layers, minimal        FineWebdataset.
                  parameters, and can be implemented in a few lines        4.2   Settings
                  of code. The design involves two unfolding opera-
                  tions, a cumulative summation, and bias addition to      Forall next-token prediction tasks, we use the GPT-
                  the attention logits. Despite its simplicity, CABLE      2variants (Brown et al., 2020). For the FineWeb-
                  significantly improves extrapolation performance         Edu-10Bdataset, we use its small version (12 lay-
                  with negligible time and memory overhead com-            ers, 10 heads, and a hidden dimension of 768)
                  pared to the vanilla transformer. Furthermore, it       with 124M parameters, and its medium version
                  offers training time and memory usage on par with        (24 layers, 16 heads, and a hidden dimension of
                  existing RPE methods, while maintaining low in-         1024) with 334M parameters. We also incorporate
                  ference overhead and demonstrating notable gains         a tiny version of GPT-2 (44M parameters) with 6
                  in extrapolation, as shown in Section 5.                 layers, 8 heads, and a hidden dimension of 512
                                                                      30367
