                                                                                               25            Extrapolation of CABLE and K-CABLE on Wikitext-103
                                  MLDR–nDCG@10vs.SequenceLength                                                                                      Pos Method
                                                                                                                                                        CABLE
                       Seq. Len.    CABLE     ALiBi     RoPE    Learnable    Sinusoidal                                                                 K-CABLE
                                                                                               24
                       512           14.96     14.02    15.16     10.42        13.57
                       1024          15.15     12.88    14.41       —          12.80
                       2048          16.77     14.30    10.26       —           1.03           xity23
                       4096          21.36     18.71     1.17       —           0.00
                       8192          24.59     22.86     0.12       —           0.00           erple
                       16384         25.10     23.44     0.12       —           0.00           P22
                      Table 3: Retrieval performance (nDCG@10) on the                          21
                      MLDRtest set for BERT models with different posi-
                      tional encodings, trained at sequence length 512 and                     20  512        1024       2048        4096       8192      15360
                      evaluated on longer inputs.                                                                          Context Length
                                                                                             Figure 5: Extrapolation performance of CABLE vs. its
                      formance compared to existing encoder-only mod-                        kernelized variant (K-CABLE). Both models are GPT-2
                      els. However, standardized long-context bench-                         Tiny architectures trained on the WikiText-103 training
                      marks for encoder-only architectures remain lim-                       set with a sequence length of 1024.
                      ited. Following Warner et al. (2024), we evaluate
                      long-context performance using the English subset                      In this setting, we use −log(b2 + 1) as a kernel,
                      of MLDR (Chen et al., 2024), a retrieval bench-                        whichisappliedtotherelativebiasesbeforeadding
                      markconsisting of over 200,000 long documents.                         them to the attention scores. Figure 5 shows the
                         To adapt BERT models for this task, we fine-                        extrapolation comparison between CABLE and K-
                      tune them on MS-MARCO(Nguyenetal.,2016)                                CABLE.Ascanbeseen,K-CABLEachievesbetter
                      using mined hard negatives (Xuan et al., 2020),                        PPLwhentrainedonsequencelengthof1024,com-
                      with 1.25M samples, a batch size of 128, and a 5%                      pared to original CABLE, demonstrating improved
                      learning rate warmup over one epoch, leveraging                        extrapolation. This improvement is related to the
                      the sentence-transformers framework (Reimers and                       sliding window nature of additive RPEs, where,
                      Gurevych, 2019). We then evaluate the fine-tuned                       with a low number of layers, they struggle to prop-
                      models on the MLDR test set using nDCG@10                              agate information across longer sequences. In con-
                      as the evaluation metric. Table 3 presents the re-                     trast, K-CABLE has a slower slope for biases and
                      sults for several competitive positional encoding
                                  2                                                          behaves less like a sliding window, making it more
                      methods.                                                               suitable for networks with fewer layers. More gen-
                         At shorter sequence lengths (512 tokens), RoPE                      erally, different types of kernels can be applied to
                      performs slightly better than other methods. How-                      CABLEbasedonthenetworkarchitecture, allow-
                      ever, as sequence length increases, CABLE consis-                      ing it to achieve optimal performance.
                      tently outperforms all baselines, showing notable
                      gainsbeyond1024tokens. ALiBimaintainsreason-
                      able performance but still trails behind CABLE. In                     6 Conclusion
                      contrast, RoPE’s effectiveness drops sharply after
                      1024 tokens, becoming nearly unusable at longer
                      lengths. Learnable absolute encodings perform not                      Weintroduced CABLE, a novel additive relative
                      competitively even at 512 tokens and cannot gen-                       positional encoding method that learns context-
                      eralize to longer sequences. Sinusoidal encoding                       aware biases for each token by injecting them
                      is effective at short lengths but fails completely                     into the attention matrix at every decoder layer.
                      beyond 1024 tokens. Overall, CABLE exhibits                            Unlike constant linear biases in ALiBi, CABLE
                      the strongest scalability to long sequences, while                     adapts to tokens’ roles within the sequence. Exper-
                      others either degrade significantly or become unus-                    iments show that CABLE lowers perplexity, signif-
                      able.                                                                  icantly improves length extrapolation, and consis-
                      5.6    Ablation Study: Kernelized CABLE                                tently outperforms baselines on edu-fineweb10B
                                                                                             and wikitext-103. Moreover, CABLE improves the
                      For models with a low number of layers, we tested                      long-context retrieval performance of encoder-only
                      a kernelized version of our method (K-CABLE).                          models. These gains come with only minimal train-
                          2Wereport only ALiBi among additive RPEs for BERT, as              ing overhead but faster inference compared to the
                      it outperformed other variants in our experiments.                     existing RPEs.
                                                                                       30371
