                                       0            CABLE, Head 4                               0            CABLE, Head 8                               0           CABLE, Head 12                               0            ALiBi, Head 12
                                                                                      0                                                        0                                                        0                                                        0.00
                                                                                                                                                 200                                                                                                              0.25
                                     100                                                2     100                                                      100                                                      100
                                                                                                                                                                                                         1                                                        0.50
                                                                                                                                                 400
                                     200                                                4     200                                                      200                                                      200                                               0.75
                                                                                                                                                 600                                                     2
                                                                                        6                                                                                                                                                                         1.00
                                     300                                                      300                                                800   300                                                      300
                                                                                                                                                                                                         3                                                        1.25
                                                                                        8                                                        1000
                                     400                                                      400                                                      400                                                      400                                               1.50
                                                                                        10                                                       1200                                                    4                                                        1.75
                                     500 0                                                    500 0                                              1400  500 0                                                    500 0
                                                  100    200     300     400     500                       100    200     300     400     500                      100     200     300     400     500                      100     200     300     400     500
                                    Figure 4: Comparison of CABLE and ALiBi positional biases in GPT-2 Small. CABLE biases (left three panels)
                                    vary with context and attention head, while ALiBi biases (right panel) are fixed across all layers and tokens. This
                                    figure illustrates how CABLE can modulate attention selectively based on token context, capturing richer structural
                                    information compared to the uniform ALiBi biases.
                                      Seq. Len.          CABLE Kerple DAPEv2+Cable DAPEv2+Kerple                                                         tention to them even at long distances. In contrast,
                                           512             21.17          21.41                 20.41                         20.56                      less informative tokens may be more heavily pe-
                                          1024             20.72          21.12                 19.91                         20.07                      nalized. Such adaptive behavior could explain why
                                          2048             20.23          22.58                 19.30                         19.51
                                          4096             19.60          28.04                 18.55                         18.79                      CABLEprovidesstronger representational flexibil-
                                          8192             19.87          39.38                 18.63                         18.92                      ity and, in practice, has been observed to improve
                                    Table 2: Extrapolation results for CABLE and Kerple                                                                  extrapolation and performance on tasks requiring
                                    whenaugmentedwithDAPEv2.                                                                                             nuancedhandlingofcontextlength. Takentogether,
                                                                                                                                                         these findings suggest that while ALiBi provides a
                                    alone. Similar slowdowns are observed for Kerple                                                                     simple and efficient positional prior, its uniformity
                                    with DAPEv2.                                                                                                         across layers and tokens constrains its ability to
                                         Overall, these findings suggest that CABLE of-                                                                  model diverse contextual dependencies. By con-
                                    fers the best trade-off between accuracy and ef-                                                                     trast, CABLE introduces a more expressive bias
                                    ficiency, achieving state-of-the-art extrapolation                                                                   structure that adapts to both token content and at-
                                    quality while preserving high training and infer-                                                                    tention head, potentially offering a more faithful
                                    ence throughput.                                                                                                     integration of positional and semantic information.
                                                                                                                                                         5.5          Bidirectional Models
                                    5.4         Analysis of Positional Biases                                                                            In this major experiment, we evaluate the effec-
                                    In this section, we analyze the positional biases                                                                    tiveness of our proposed additive RPE method in
                                    added by CABLEandALiBitotheoriginalatten-                                                                            learning contextual representations. To do this,
                                    tion scores, with a focus on how contextual infor-                                                                   wereplace the original fixed learnable positional
                                    mation influences these biases. Figure 4 illustrates                                                                 encodings in BERT (Devlin et al., 2019) with CA-
                                    the relative biases for three different heads in the                                                                 BLEduringpre-training. We use the 10B-sample
                                    last layer, alongside the ALiBi biases. Unlike AL-                                                                   FineWeb-Edu dataset and train the models using
                                    iBi, which applies a fixed bias uniformly across all                                                                 only the masked language modeling (MLM) objec-
                                    layers and tokens, CABLE biases are both head-                                                                       tive, following Liu (2019), who showed that remov-
                                    and context-dependent. Appendix A.3 presents                                                                         ing next sentence prediction (NSP) can improve
                                    the complete set of visualizations for all attention                                                                 performance. Our BERT models are based on the
                                    heads across all examined methods.                                                                                   bert-base-uncased architecture and are trained on
                                         Thebiases presented in Figure 4 were computed                                                                   four H100 GPUs with a batch size of 32 and a
                                    using a GPT-2 Small model trained and evaluated                                                                      maximum sequence length of 512 for 14k steps
                                    with a sequence length of 512 tokens. The contex-                                                                    (~1 epoch on FineWeb-Edu-10B). We use Adam
                                    tual dependence of CABLE can be interpreted as a                                                                     (KingmaandBa,2014)withalearningrateof1e-4.
                                    mechanism that allows the model to dynamically                                                                       AsshowninFigureA,CABLEachievesfastercon-
                                    calibrate positional information based on the struc-                                                                 vergenceinMLMlosscomparedtootherpositional
                                    ture of the input sequence. For example, tokens                                                                      encoding baselines.
                                    that serve as anchors (e.g., punctuation, repeated                                                                         Since RPE methods do not impose strict limita-
                                    entities, or syntactic markers) may receive weaker                                                                   tions on context length, BERT models trained with
                                    biases, enabling the model to maintain stronger at-                                                                  these encodings show improved long-context per-
                                                                                                                                               30370
