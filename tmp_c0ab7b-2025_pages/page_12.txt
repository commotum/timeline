                  Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,          MingyuXu,XinMen,BingningWang,QingyuZhang,
                     Wen Bo, and Yunfeng Liu. 2024. Roformer: En-               HongyuLin,XianpeiHan,etal. 2024. Base of rope
                     hanced transformer with rotary position embedding.         bounds context length. In The Thirty-eighth Annual
                     Neurocomputing, 568:127063.                                Conference on Neural Information Processing Sys-
                                                                                tems.
                  Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tian-
                     jun Zhang, Kurt Keutzer, Joseph E Gonzalez, and         HongXuan,AbbyStylianou,XiaotongLiu, and Robert
                     Raluca Ada Popa. 2024. Lloco: Learning long con-           Pless. 2020. Hard negative examples are hard, but
                     texts offline. arXiv preprint arXiv:2404.07979.            useful. In Computer Vision–ECCV 2020: 16th Euro-
                                                                                pean Conference, Glasgow, UK, August 23–28, 2020,
                  Gemini Team, Rohan Anil, Sebastian Borgeaud,                  Proceedings, Part XIV 16, pages 126–142. Springer.
                     Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,           Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh
                     Radu Soricut, Johan Schalkwyk, Andrew M Dai,               Rawat, Sashank J Reddi, and Sanjiv Kumar.
                     Anja Hauth, et al. 2023.     Gemini: a family of           2019. Are transformers universal approximators of
                     highly capable multimodal models. arXiv preprint           sequence-to-sequence functions?      arXiv preprint
                     arXiv:2312.11805.                                          arXiv:1912.10077.
                  Gemma Team, Thomas Mesnard, Cassidy Hardin,                Chuanyang Zheng, Yihang Gao, Han Shi, Minbin
                     Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,          Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren,
                     Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale,         Michael Ng, Xin Jiang, Zhenguo Li, et al. 2024a.
                     Juliette Love, et al. 2024a. Gemma: Open models            Dape: Data-adaptive positional encoding for length
                     based on gemini research and technology. arXiv             extrapolation. Advances in Neural Information Pro-
                     preprint arXiv:2403.08295.                                 cessing Systems, 37:26659–26700.
                  Gemma Team, Morgane Riviere, Shreya Pathak,                Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong,
                     Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-        Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe
                     raju, Léonard Hussenot, Thomas Mesnard, Bobak              Ren, Michael Ng, Xin Jiang, et al. 2024b. Dape
                     Shahriari, Alexandre Ramé, et al. 2024b. Gemma 2:          v2: Process attention score as feature map for length
                     Improving open language models at a practical size.        extrapolation. arXiv preprint arXiv:2410.04798.
                     arXiv preprint arXiv:2408.00118.
                                                                             Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin,
                  HugoTouvron,Thibaut Lavril, Gautier Izacard, Xavier           OmidSaremi,JoshSusskind,SamyBengio,andPree-
                     Martinet, Marie-Anne Lachaux, Timothée Lacroix,            tumNakkiran. 2023. What algorithms can transform-
                     Baptiste Rozière, Naman Goyal, Eric Hambro,                ers learn? a study in length generalization. arXiv
                     Faisal Azhar, et al. 2023. Llama: Open and effi-           preprint arXiv:2310.16028.
                     cient foundation language models. arXiv preprint
                     arXiv:2302.13971.                                       DaweiZhu,NanYang,LiangWang,YifanSong,Wen-
                                                                                hao Wu, Furu Wei, and Sujian Li. 2023. Pose: Effi-
                  Szymon Tworkowski, Konrad Staniszewski, Mikołaj               cient context window extension of llms via positional
                     Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr            skip-wise training. arXiv preprint arXiv:2309.10400.
                          ´
                     Miłos. 2024. Focused transformer: Contrastive train-    Jiajun Zhu, Peihao Wang, Ruisi Cai, Jason D Lee,
                     ing for context scaling. Advances in Neural Informa-       Pan Li, and Zhangyang Wang. 2025.          Rethink-
                     tion Processing Systems, 36.                               ing addressing in language models via contexual-
                  AVaswani.2017. Attention is all you need. Advances            ized equivariant positional encoding. arXiv preprint
                     in Neural Information Processing Systems.                  arXiv:2501.00712.
                  Jie Wang, Tao Ji, Yuanbin Wu, Hang Yan, Tao Gui,
                     Qi Zhang, Xuanjing Huang, and Xiaoling Wang.
                     2024. Length generalization of causal transform-
                     ers without position encoding.       arXiv preprint
                     arXiv:2404.12224.
                  Benjamin Warner, Antoine Chaffin, Benjamin Clavié,
                     Orion Weller, Oskar Hallström, Said Taghadouini,
                     Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom
                     Aarsen, et al. 2024. Smarter, better, faster, longer:
                     A modern bidirectional encoder for fast, memory
                     efficient, and long context finetuning and inference.
                     arXiv preprint arXiv:2412.13663.
                  Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and
                     Christian Szegedy. 2022. Memorizing transformers.
                     arXiv preprint arXiv:2203.08913.
                                                                        30374
