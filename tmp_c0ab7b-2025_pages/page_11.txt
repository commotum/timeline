                  Jonas Gehring, Michael Auli, David Grangier, Denis          Yinhan Liu. 2019.         Roberta:   A robustly opti-
                     Yarats, and Yann N Dauphin. 2017. Convolutional se-         mized bert pretraining approach.     arXiv preprint
                     quence to sequence learning. In International confer-       arXiv:1907.11692, 364.
                     ence on machine learning, pages 1243–1252. PMLR.
                                                                              Stephen Merity, Caiming Xiong, James Bradbury, and
                  Olga Golovneva, Tianlu Wang, Jason Weston, and Sain-           Richard Socher. 2016. Pointer sentinel mixture mod-
                     bayar Sukhbaatar. 2024.     Contextual position en-         els. arXiv preprint arXiv:1609.07843.
                     coding: Learning to count what’s important. arXiv        Jesse Mu, Xiang Li, and Noah Goodman. 2024. Learn-
                     preprint arXiv:2405.18719.                                  ing to compress prompts with gist tokens. Advances
                  AdiHaviv, Ori Ram, Ofir Press, Peter Izsak, and Omer           in Neural Information Processing Systems, 36.
                     Levy. 2022. Transformer language models without          Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,
                     positional encodingsstill learn positional information.     Saurabh Tiwary, Rangan Majumder, and Li Deng.
                     arXiv preprint arXiv:2203.16634.                            2016. Ms marco: A human-generated machine read-
                  Arthur Jacot, Franck Gabriel, and Clément Hongler.             ing comprehension dataset.
                     2018. Neural tangent kernel: Convergence and gen-        MyleOtt, Sergey Edunov, David Grangier, and Michael
                     eralization in neural networks. Advances in neural          Auli. 2018.    Scaling neural machine translation.
                     information processing systems, 31.                         arXiv preprint arXiv:1806.00187.
                  Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan                                              ˇ
                     Natesan Ramamurthy, Payel Das, and Siva Reddy.           Guilherme Penedo, Hynek Kydlícek, Anton Lozhkov,
                     2024. The impact of positional encoding on length           Margaret Mitchell, Colin Raffel, Leandro Von Werra,
                     generalization in transformers. Advances in Neural          Thomas Wolf, et al. 2024. The fineweb datasets:
                     Information Processing Systems, 36.                         Decanting the web for the finest text data at scale.
                                                                                 arXiv preprint arXiv:2406.17557.
                  Diederik P Kingma and Jimmy Ba. 2014. Adam: A               BowenPeng, Jeffrey Quesnelle, Honglu Fan, and En-
                     method for stochastic optimization. arXiv preprint          rico Shippole. 2023. Yarn: Efficient context window
                     arXiv:1412.6980.                                            extension of large language models. arXiv preprint
                  Shun Kiyono, Sosuke Kobayashi, Jun Suzuki, and                 arXiv:2309.00071.
                     Kentaro Inui. 2021. Shape: Shifted absolute po-          Ofir Press, Noah A Smith, and Mike Lewis. 2021.
                     sition embedding for transformers. arXiv preprint           Train short, test long: Attention with linear biases
                     arXiv:2109.05644.                                           enables input length extrapolation. arXiv preprint
                  Teven Le Scao, Angela Fan, Christopher Akiki, El-              arXiv:2108.12409.
                                             ´
                     lie Pavlick, Suzana Ilic, Daniel Hesslow, Roman          Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
                     Castagné, Alexandra Sasha Luccioni, François Yvon,          Dario Amodei, Ilya Sutskever, et al. 2019. Language
                     Matthias Gallé, et al. 2023.      Bloom: A 176b-            models are unsupervised multitask learners. OpenAI
                     parameter open-access multilingual language model.          blog, 1(8):9.
                  Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman             Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
                     Goyal, and Luke Zettlemoyer. 2021. Base layers:             Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
                     Simplifying training of large, sparse models. In In-        Wei Li, and Peter J Liu. 2020. Exploring the lim-
                     ternational Conference on Machine Learning, pages           its of transfer learning with a unified text-to-text
                     6265–6274. PMLR.                                            transformer. Journal of machine learning research,
                  Shanda Li, Chong You, Guru Guruganesh, Joshua                  21(140):1–67.
                     Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit          Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
                     Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh             Sentence embeddings using siamese bert-networks.
                     Bhojanapalli. 2023. Functional interpolation for rel-       arXiv preprint arXiv:1908.10084.
                     ative positions improves long context transformers.
                     arXiv preprint arXiv:2310.04418.                         Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.
                  Tatiana Likhomanenko, Qiantong Xu, Gabriel Syn-                Self-attention with relative position representations.
                     naeve, Ronan Collobert, and Alex Rogozhnikov.               arXiv preprint arXiv:1803.02155.
                     2021. Cape: Encoding relative positions with contin-     Ruoqi Shen, Sébastien Bubeck, Ronen Eldan, Yin Tat
                     uous augmented positional embeddings. Advances in           Lee, Yuanzhi Li, and Yi Zhang. 2023. Positional de-
                     Neural Information Processing Systems, 34:16079–            scription matters for transformers arithmetic. arXiv
                     16092.                                                      preprint arXiv:2311.14737.
                  Zhixuan Lin, Evgenii Nikishin, Xu He, and Aaron             Alex Sherstinsky. 2020. Fundamentals of recurrent
                     Courville. 2025. Forgetting transformer: Softmax            neural network (rnn) and long short-term memory
                     attention with a forget gate. In The Thirteenth Inter-      (lstm) network. Physica D: Nonlinear Phenomena,
                     national Conference on Learning Representations.            404:132306.
                                                                         30373
