year,title,url
2025,Rotary Masked Autoencoders Are Versatile Learners,"https://arxiv.org/pdf/2505.20535.pdf ""Rotary Masked Autoencoders (2025) — arXiv"""
2024,Rotary Position Embedding for Vision Transformer,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01584.pdf
2024,Rotary Position Embedding for Vision Transformer,https://arxiv.org/pdf/2403.13298.pdf
2024,Rotary Position Embedding for Vision Transformer (RoPE-Mixed / 2D RoPE study),"https://arxiv.org/pdf/2403.13298.pdf ""RoPE for ViT / RoPE-Mixed 2D study (2024) — arXiv"""
2019,SATNet: Bridging Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver,https://arxiv.org/pdf/1905.12149
2019,SATNet: Differentiable Satisfiability Solver,https://arxiv.org/pdf/1905.12149.pdf
2020,SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks,https://proceedings.neurips.cc/paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html
2020,SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks,https://proceedings.neurips.cc/paper/2020/hash/15231a7ce4ba789d13b722cc5c955834-Abstract.html
2024,SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension,https://openaccess.thecvf.com/content/CVPR2024/papers/Li_SEED-Bench_Benchmarking_Multimodal_Large_Language_Models_CVPR_2024_paper.pdf
2021,SETR: Rethinking Semantic Segmentation as Seq2Seq with Transformers,https://arxiv.org/pdf/2012.15840.pdf
2020,SETR: Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,https://arxiv.org/pdf/2012.15840.pdf
2016,"SQuAD: 100,000+ Questions for Machine Comprehension of Text",https://arxiv.org/pdf/1606.05250.pdf
2022,SRBench,https://github.com/cavalab/srbench
2021,SRBench (Symbolic Regression Benchmarks),https://cavalab.github.io/symbolic-regression/
2025,"SRBench update (""next generation"" SRBench)",https://arxiv.org/html/2505.03977v1
2024,SRBench++: principled benchmarking of symbolic regression,https://pmc.ncbi.nlm.nih.gov/articles/PMC12321164/
2024,SRBench++: principled benchmarking of symbolic regression,https://pubmed.ncbi.nlm.nih.gov/40761553/
2022,STaR: Self-Taught Reasoner (Bootstrapping Reasoning With Reasoning),https://arxiv.org/pdf/2203.14465.pdf
2021,SVAMP: Simple Variations on Arithmetic Math Word Problems,https://aclanthology.org/2021.naacl-main.168/
2024,SWE-bench,https://proceedings.iclr.cc/paper_files/paper/2024/file/edac78c3e300629acfe6cbe9ca88fb84-Paper-Conference.pdf
2023,SWE-bench: Can Language Models Resolve Real-World GitHub Issues?,https://arxiv.org/pdf/2310.06770.pdf
2016,Safe and Efficient Off-Policy Reinforcement Learning (Retrace(lambda)),https://arxiv.org/pdf/1606.02647.pdf
2022,Scalable Diffusion Models with Transformers (DiT),https://arxiv.org/pdf/2212.09748.pdf
2024,Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters,"https://arxiv.org/pdf/2408.03314.pdf ""Scaling LLM Test-Time Compute Optimally (2024) — arXiv"""
2020,Scaling Laws for Neural Language Models,https://arxiv.org/pdf/2001.08361.pdf
2025,Scaling Transformer-Based Novel View Synthesis with Models Token Disentanglement and Synthetic Data,https://openaccess.thecvf.com/content/ICCV2025/papers/Nair_Scaling_Transformer-Based_Novel_View_Synthesis_with_Models_Token_Disentanglement_and_ICCV_2025_paper.pdf
2021,Scaling Up Vision-Language Learning With Noisy Text Supervision (ALIGN),"https://arxiv.org/pdf/2102.05918.pdf ""ALIGN (2021) — arXiv"""
2025,Scaling up Test-Time Compute with Latent Reasoning,https://neurips.cc/virtual/2025/poster/117966
2024,ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention,MISSING
