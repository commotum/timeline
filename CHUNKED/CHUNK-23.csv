year,title,url
2016,SyGuS-Comp 2016 Results/Benchmarks (PBE track),https://arxiv.org/pdf/1611.07627.pdf
2016,SyGuS-Comp 2016: Results and Analysis,https://dspace.mit.edu/bitstream/handle/1721.1/137904/1611.07627.pdf?isAllowed=y&sequence=2
2017,SyGuS-Comp 2017: Results and Analysis,https://arxiv.org/pdf/1711.11438.pdf
2017,SyGuS-Comp track definitions (PBE-BV),https://sygus.org/comp/2017/
2015,SyGuS-Comp'15 Results/Analysis,https://rishabhmit.bitbucket.io/papers/synt15.pdf
Year unknown,"Symbolic regression benchmarks (AI Feynman, SRBench)",https://www.science.org/doi/10.1126/sciadv.aay2631
1977,Synergetics: An Introduction,https://books.google.com/books/about/Synergetics.html?id=KHn1CAAAQBAJ
2013,Syntax-Guided Synthesis (SyGuS),https://www.cis.upenn.edu/~alur/SyGuS13.pdf
2013,Syntax-Guided Synthesis (SyGuS),https://www.cis.upenn.edu/~alur/SyGuS13.pdf
2014,Syntax-Guided Synthesis (SyGuS) + SyGuS-Comp 2014,https://sygus.org/comp/2014/
2025,TAPA: Positional Encoding via Token-Aware Phase Attention,https://arxiv.org/pdf/2509.12635.pdf
1992,"TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play",https://cdn.aaai.org/Symposia/Fall/1993/FS-93-02/FS93-02-003.pdf
2025,TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution,https://arxiv.org/html/2506.18421v1
2022,Tackling the Abstraction and Reasoning Corpus with Vision Transformers: the ViTARC Architecture,https://openreview.net/forum?id=0gOQeSHNX1
2020,Taming Transformers for High-Resolution Image Synthesis,"https://arxiv.org/pdf/2012.09841.pdf ""Taming Transformers (2020) — arXiv"".pdf"
2020,Taming Transformers for High-Resolution Image Synthesis,"https://arxiv.org/pdf/2012.09841.pdf ""Taming Transformers (2020) — arXiv"".pdf"
2021,Taming Transformers for High-Resolution Image Synthesis (VQGAN + Transformer),https://arxiv.org/pdf/2012.09841.pdf
2024,Teaching Transformers Modular Arithmetic at Scale,https://www.arxiv.org/pdf/2410.03569v1.pdf
1992,Technical Note: Q-learning,https://link.springer.com/article/10.1023/A%3A1022676722315
2025,Test-Time Learning for Large Language Models (TLM / TTL),https://openreview.net/forum?id=iCYbIaGKSR&noteId=ScPdA3KZCL
2024,Test-Time Training on Nearest Neighbors for Large Language Models,https://arxiv.org/pdf/2305.18466.pdf
2025,Test-time Adaptation of Tiny Recursive Models,https://arcprize.org/blog/arc-prize-2025-results-analysis
2019,TextVQA: Towards VQA Models That Can Read,https://arxiv.org/pdf/1904.08920.pdf
2019,The Abstraction and Reasoning Corpus (ARC),https://arxiv.org/pdf/2412.04604.pdf
2025,The Art of Scaling Test-Time Compute for LLMs,https://arxiv.org/html/2512.02008v1
2007,The Hidden Logic of Sudoku (Second Edition),"# ""The Hidden Logic of Sudoku (2007) — Book (local copy)"""
2024,The LLM ARChitect: Solving ARC-AGI Is a Matter of Perspective,https://arcprize.org/competitions/2024/
2025,The Lessons of Developing Process Reward Models...,https://arxiv.org/pdf/2501.07301.pdf
2019,The Lottery Ticket Hypothesis,MISSING
1956,"The Magical Number Seven, Plus or Minus Two",https://pubmed.ncbi.nlm.nih.gov/13310704/
1949,The Organization of Behavior,https://www.dengfanxin.cn/wp-content/uploads/2016/03/1949Hebb.pdf
