year,title,url
1957,The Perceptron,MISSING
2021,The Pile: An 800GB Dataset of Diverse Text for Language Modeling,https://arxiv.org/pdf/2101.00027
2021,The Pile: An 800GB Dataset of Diverse Text for Language Modeling,https://arxiv.org/pdf/2101.00027
2025,The Rotary Position Embedding May Cause Dimension Inefficiency,"https://arxiv.org/pdf/2502.11276.pdf ""The Rotary Position Embedding May Cause Dimension Inefficiency (2025) — arXiv"""
2024,The Surprising Effectiveness of Test-Time Training for Abstract Reasoning,https://arcprize.org/competitions/2024/
2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://arxiv.org/abs/2411.07279
2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,"https://ekinakyurek.github.io/papers/ttt.pdf ""Test-Time Training for Few-Shot Learning (2024) — Project Page"""
1954,The Theory of Dynamic Programming,https://www.rand.org/content/dam/rand/pubs/papers/2008/P550.pdf
2010,The free-energy principle: a unified brain theory?,https://www.uab.edu/medicine/cinl/images/KFriston_FreeEnergy_BrainTheory.pdf
1971,The hippocampus as a spatial map,https://pubmed.ncbi.nlm.nih.gov/5124915/
2018,"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",https://arxiv.org/abs/1803.05457
2018,"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge",https://arxiv.org/abs/1803.05457
2021,TimeSformer: Is Space-Time Attention All You Need for Video Understanding?,https://arxiv.org/abs/2102.05095
2021,Tokens-to-Token ViT (T2T-ViT),https://arxiv.org/abs/2101.11986
2023,Toolformer: Language Models Can Teach Themselves to Use Tools,https://arxiv.org/abs/2302.04761
2023,Toolformer: Language Models Can Teach Themselves to Use Tools,https://arxiv.org/abs/2302.04761
2007,Toward an executive without a homunculus: computational models of the prefrontal cortex/basal ganglia system,https://royalsocietypublishing.org/doi/abs/10.1098/rstb.2007.2055
2015,Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks (bAbI),https://arxiv.org/abs/1502.05698
2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arxiv.org/abs/2411.17708
2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arcprize.org/competitions/2024/
2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,"https://arxiv.org/pdf/2411.17708.pdf ""Neurally-Guided Program Induction for ARC-AGI (2024) — arXiv"""
2025,Towards the Next Generation of Symbolic Regression Benchmarks,https://arxiv.org/html/2505.03977v1
2021,"Train Short, Test Long: Attention with Linear Biases (ALiBi)",https://arxiv.org/abs/2108.12409
2021,"Train Short, Test Long: Attention with Linear Biases (ALiBi)","https://arxiv.org/pdf/2108.12409.pdf ""ALiBi (2021) — arXiv"""
2022,Training Compute-Optimal Large Language Models (Chinchilla),https://arxiv.org/abs/2203.15556
2022,Training Iterative Refinement Algorithms with Implicit Differentiation,https://proceedings.neurips.cc/paper_files/paper/2022/file/d301e2878a7ebadf1a95029e904fc7d0-Paper-Conference.pdf
2021,Training Verifiers to Solve Math Word Problems (GSM8K),https://arxiv.org/abs/2110.14168
2020,Training data-efficient image transformers & distillation through attention,"https://arxiv.org/pdf/2012.12877.pdf ""DeiT: Data-efficient Image Transformers (2020) — arXiv"""
2020,Training data-efficient image transformers & distillation through attention,"https://arxiv.org/pdf/2012.12877.pdf ""DeiT (2020) — arXiv"""
2022,Training language models to follow instructions with human feedback (InstructGPT / RLHF pipeline),https://arxiv.org/abs/2203.02155
