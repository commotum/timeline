                                                                 Hiddenlayersize
                                             Model              256           512
                                                                     5            6
                                             1-layer LSTM    3.3 ×10       1.2 ×10
                                                                     5            6
                                             2-layer LSTM    9.1 ×10       3.4 ×10
                                                                     6            6
                                             4-layer LSTM    2.1 ×10       7.8 ×10
                                                                     6            7
                                             8-layer LSTM    4.5 ×10       1.7 ×10
                                                                     5            6
                                             Stack-LSTM      6.7 ×10       1.9 ×10
                                                                     5            6
                                             Queue-LSTM      6.7 ×10       1.9 ×10
                                                                     6            6
                                             DeQue-LSTM      1.0 ×10       2.5 ×10
                                           Table 2: Number of trainable parameters per model
                       E FullResults
                       WeshowinTable3thefull results for each task of the best performing models. The procedure for
                       selecting the best performing model is described in Section 5.
                                                                       Training       Testing
                                   Experiment         Model         Coarse  Fine   Coarse  Fine
                                                      1-layer LSTM  0.62    0.87   0.00    0.38
                                                      2-layer LSTM  0.80    0.95   0.00    0.47
                                                      4-layer LSTM  0.98    0.98   0.01    0.50
                                   SequenceCopying    8-layer LSTM  0.57    0.83   0.00    0.31
                                                      Stack-LSTM    0.89    0.94   0.00    0.22
                                                      Queue-LSTM    1.00    1.00   1.00    1.00
                                                      DeQue-LSTM    1.00    1.00   1.00    1.00
                                                      1-layer LSTM  0.78    0.87   0.01    0.09
                                                      2-layer LSTM  0.91    0.94   0.02    0.06
                                                      4-layer LSTM  0.93    0.96   0.03    0.15
                                   SequenceReversal   8-layer LSTM  0.95    0.98   0.04    0.13
                                                      Stack-LSTM    1.00    1.00   1.00    1.00
                                                      Queue-LSTM    0.44    0.61   0.00    0.07
                                                      DeQue-LSTM    1.00    1.00   1.00    1.00
                                                      1-layer LSTM  0.53    0.93   0.01    0.53
                                                      2-layer LSTM  0.54    0.93   0.02    0.52
                                                      4-layer LSTM  0.52    0.93   0.01    0.56
                                   BigramFlipping     8-layer LSTM  0.52    0.93   0.01    0.53
                                                      Stack-LSTM    0.44    0.90   0.00    0.48
                                                      Queue-LSTM    0.55    0.94   0.55    0.98
                                                      DeQue-LSTM    0.55    0.94   0.53    0.98
                                                      1-layer LSTM  0.96    0.98   0.96    0.99
                                                      2-layer LSTM  0.97    0.99   0.96    0.99
                                                      4-layer LSTM  0.97    0.99   0.97    0.99
                                   SVOtoSOV           8-layer LSTM  0.98    0.99   0.98    0.99
                                                      Stack-LSTM    1.00    1.00   1.00    1.00
                                                      Queue-LSTM    1.00    1.00   1.00    1.00
                                                      DeQue-LSTM    1.00    1.00   1.00    1.00
                                                      1-layer LSTM  0.97    0.99   0.97    0.99
                                                      2-layer LSTM  0.98    0.99   0.98    0.99
                                                      4-layer LSTM  0.98    0.99   0.98    0.99
                                   GenderConjugation  8-layer LSTM  0.98    0.99   0.99    0.99
                                                      Stack-LSTM    0.93    0.97   0.93    0.97
                                                      Queue-LSTM    1.00    1.00   1.00    1.00
                                                      DeQue-LSTM    1.00    1.00   1.00    1.00
                                           Table 3: Summary of Results for Transduction Tasks
                                                                14
