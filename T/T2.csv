year,title,url
2000,Feedforward and Recurrent Processing in Vision,https://pubmed.ncbi.nlm.nih.gov/10925037/
2007,The Hidden Logic of Sudoku (Second Edition),
2007,Core Knowledge,https://www.harvardlds.org/wp-content/uploads/2017/01/SpelkeKinzler07-1.pdf
2012,Canonical Microcircuits for Predictive Coding,https://pubmed.ncbi.nlm.nih.gov/23238495/
2014,Hierarchy of Intrinsic Timescales in Cortex,https://www.cns.nyu.edu/wanglab/publications/pdf/murray.nn2014.pdf
2014,Neural Turing Machines,https://arxiv.org/pdf/1410.5401.pdf
2016,Adaptive Computation Time,https://arxiv.org/pdf/1603.08983.pdf
2017,Attention Is All You Need,https://arxiv.org/pdf/1706.03762.pdf
2018,Recurrent Relational Networks,https://proceedings.neurips.cc/paper_files/paper/2018/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf
2018,Universal Transformers,https://arxiv.org/pdf/1807.03819.pdf
2018,"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset",https://arxiv.org/pdf/1803.10137.pdf
2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805.pdf
2019,LXMERT: Learning Cross-Modality Encoder Representations,https://arxiv.org/pdf/1908.07490.pdf
2019,ViLBERT: Pretraining Task-Agnostic V-L Representations,https://arxiv.org/pdf/1908.02265.pdf
2019,VisualBERT: A Simple and Performant Baseline for Vision and Language,https://arxiv.org/pdf/1908.03557.pdf
2019,Deep Equilibrium Models,https://arxiv.org/pdf/1909.01377.pdf
2019,VideoBERT: A Joint Model for Video and Language Representation Learning,https://arxiv.org/pdf/1904.01766.pdf
2019,HowTo100M: Learning a Text-Video Embedding by Watching Narrated Videos,https://arxiv.org/pdf/1906.03327.pdf
2019,On the Measure of Intelligence,https://arxiv.org/pdf/1911.01547.pdf
2020,UNITER: Universal Image-Text Representation Learning,https://arxiv.org/pdf/1909.11740.pdf
2020,OSCAR: Object-Semantics Aligned Pre-training for Vision-Language Tasks,https://arxiv.org/pdf/2004.06165.pdf
2020,End-to-End Object Detection with Transformers (DETR),https://arxiv.org/pdf/2005.12872.pdf
2020,An Image is Worth 16×16 Words: Vision Transformer (ViT),https://arxiv.org/pdf/2010.11929.pdf
2020,Deformable DETR: Deformable Transformers for End-to-End Object Detection,https://arxiv.org/pdf/2010.04159.pdf
2020,Training data-efficient image transformers & distillation through attention,https://arxiv.org/pdf/2012.12877.pdf
2020,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2012.09841.pdf
2020,Generative Pretraining from Pixels,https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf
2021,ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases,https://arxiv.org/pdf/2103.10697.pdf
2021,Is Space-Time Attention All You Need for Video Understanding? (TimeSformer),https://arxiv.org/pdf/2102.05095.pdf
2021,Scaling Up Vision-Language Learning With Noisy Text Supervision (ALIGN),https://arxiv.org/pdf/2102.05918.pdf
2021,Learning Transferable Visual Models From Natural Language Supervision (CLIP),https://arxiv.org/pdf/2103.00020.pdf
2021,Zero-Shot Text-to-Image Generation,https://arxiv.org/pdf/2102.12092.pdf
2021,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,https://arxiv.org/pdf/2102.12122.pdf
2021,Swin Transformer,https://arxiv.org/pdf/2103.14030.pdf
2021,CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification,https://arxiv.org/pdf/2103.14899.pdf
2021,CvT: Introducing Convolutions to Vision Transformers,https://arxiv.org/pdf/2103.15808.pdf
2021,Transformer in Transformer,https://arxiv.org/pdf/2103.00112.pdf
2021,RoFormer: Enhanced Transformer with Rotary Position Embedding (RoPE),https://arxiv.org/pdf/2104.09864.pdf
2021,Twins: Revisiting the Design of Spatial Attention in Vision Transformers,https://arxiv.org/pdf/2104.13840.pdf
2021,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,https://arxiv.org/pdf/2105.15203.pdf
2021,VATT: Transformers for Multimodal Self-Supervised Learning,https://arxiv.org/pdf/2104.11178.pdf
2021,BEiT: BERT Pre-Training of Image Transformers,https://arxiv.org/pdf/2106.08254.pdf
2021,CoAtNet: Marrying Convolution and Attention for All Data Sizes,https://arxiv.org/pdf/2106.04803.pdf
2021,ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision,https://arxiv.org/pdf/2102.03334.pdf
2021,VideoCLIP: Contrastive Pretraining for Zero-Shot Video-Text Understanding,https://arxiv.org/pdf/2109.14084.pdf
2021,ALBEF: Align Before Fuse,https://arxiv.org/pdf/2107.07651.pdf
2021,CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows,https://arxiv.org/pdf/2107.00652.pdf
2021,"Train Short, Test Long: Attention with Linear Biases (ALiBi)",https://arxiv.org/pdf/2108.12409.pdf
2021,LAION-400M: Open Dataset for CLIP Training,https://arxiv.org/pdf/2111.02114.pdf
2021,Masked Autoencoders Are Scalable Vision Learners (MAE),https://arxiv.org/pdf/2111.06377.pdf
2021,Swin Transformer V2: Scaling Up Capacity and Resolution,https://arxiv.org/pdf/2111.09883.pdf
2022,BLIP: Bootstrapping Language-Image Pre-training,https://arxiv.org/pdf/2201.12086.pdf
2022,Transformer Language Models without Positional Encodings Still Learn Positional Information,https://arxiv.org/pdf/2203.16634.pdf
2022,CoCa: Contrastive Captioners,https://arxiv.org/pdf/2205.01917.pdf
2022,Flamingo: a Visual Language Model for Few-Shot Learning,https://arxiv.org/pdf/2204.14198.pdf
2022,MaxViT: Multi-Axis Vision Transformer,https://arxiv.org/pdf/2204.01697.pdf
2022,Winoground: Probing Vision-Language Models for Compositionality,https://arxiv.org/pdf/2204.03162.pdf
2022,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,https://arxiv.org/pdf/2208.10442.pdf
2022,LAION-5B: An Open Large-Scale Dataset for Training Next Generation Image-Text Models,https://arxiv.org/pdf/2210.08402.pdf
2022,ScienceQA: Benchmark for Multimodal Reasoning,https://arxiv.org/pdf/2209.09513.pdf
2023,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Models,https://arxiv.org/pdf/2301.12597.pdf
2023,GPT-4 Technical Report,https://arxiv.org/pdf/2303.08774.pdf
2023,Kosmos-1: Language Is Not All You Need,https://arxiv.org/pdf/2302.14045.pdf
2023,PaLM-E: An Embodied Multimodal Language Model,https://arxiv.org/pdf/2303.03378.pdf
2023,LLaVA: Large Language-and-Vision Assistant,https://arxiv.org/pdf/2304.08485.pdf
2023,MiniGPT-4,https://arxiv.org/pdf/2304.10592.pdf
2023,Segment Anything,https://arxiv.org/pdf/2304.02643.pdf
2023,ConceptARC,https://arxiv.org/pdf/2305.07141.pdf
2023,MMBench: Evaluating Multimodal LLMs,https://arxiv.org/pdf/2307.06281.pdf
2023,A Length-Extrapolatable Transformer (XPOS / LeX),https://aclanthology.org/2023.acl-long.816.pdf
2023,Average-Hard Attention Transformers Are Threshold Circuits,https://arxiv.org/pdf/2308.03212.pdf
2023,YaRN: Efficient Context Window Extension of Large Language Models,https://arxiv.org/pdf/2309.00071.pdf
2023,Spherical Position Encoding for Transformers,https://arxiv.org/pdf/2310.04454.pdf
2023,MMMU: A Massive Multidiscipline Multimodal Benchmark,https://arxiv.org/pdf/2311.16502.pdf
2023,Gemini: A Family of Highly Capable Multimodal Models,https://arxiv.org/pdf/2312.11805.pdf
2023,Uni3DL: Unified Model for 3D and Language Understanding,https://arxiv.org/pdf/2312.03026.pdf
2024,Evolutionary Test-Time Compute (write-up),https://jeremyberman.substack.com/p/how-i-got-a-record-536-on-arc-agi
2024,Fixed Point Diffusion Models,https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_Fixed_Point_Diffusion_Models_CVPR_2024_paper.pdf
2024,Solving olympiad geometry without human demonstrations (AlphaGeometry),https://www.nature.com/articles/s41586-023-06747-5.pdf
2024,Gaussian Adaptive Attention Is All You Need,https://arxiv.org/pdf/2401.11143.pdf
2024,Beyond A*: Planning with Transformers,https://arxiv.org/pdf/2402.14083.pdf
2024,Rotary Position Embedding for Vision Transformer (RoPE-Mixed / 2D RoPE study),https://arxiv.org/pdf/2403.13298.pdf
2024,VG4D: Vision-Language Model Goes 4D Video Recognition,https://arxiv.org/pdf/2404.11605.pdf
2024,DAPE: Data-Adaptive Positional Encoding for Length Extrapolation,https://proceedings.neurips.cc/paper_files/paper/2024/file/2f050fa9f0d898e3f265d515f50ae8f9-Paper-Conference.pdf
2024,What matters when building vision-language models? (Idefics2),https://arxiv.org/pdf/2405.02246.pdf
2024,Base of RoPE Bounds Context Length,https://arxiv.org/pdf/2405.14591.pdf
2024,RoTHP: Rotary Position Embedding-based Transformer Hawkes Process,https://arxiv.org/pdf/2405.06985.pdf
2024,LieRE: Lie Rotational Positional Encodings,https://arxiv.org/pdf/2406.10322.pdf
2024,Learning Iterative Reasoning through Energy Diffusion,https://arxiv.org/pdf/2406.11179.pdf
2024,Simultaneous Instance Pooling & Bag Selection for MIL using ViTs,https://doi.org/10.1007/s00521-024-09417-3
2024,Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters,https://arxiv.org/pdf/2408.03314.pdf
2024,Length Extrapolation of Causal Transformers without Position Encoding (NoPE),https://aclanthology.org/2024.findings-acl.834.pdf
2024,H-ARC: A Robust Estimate of Human Performance on the Abstraction and Reasoning Corpus Benchmark,https://arxiv.org/pdf/2409.01374.pdf
2024,Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding,https://aclanthology.org/2024.findings-emnlp.582.pdf
2024,Beyond Position: The Emergence of Wavelet-like Properties in Transformers,https://arxiv.org/pdf/2410.18067.pdf
2024,ARC-Heavy / ARC-Potpourri (dataset description embedded in Cornell report),https://www.cs.cornell.edu/~ellisk/documents/arc_induction_vs_transduction.pdf
2024,Combining Induction and Transduction for Abstract Reasoning,https://arxiv.org/pdf/2411.02272.pdf
2024,Searching Latent Program Spaces,https://arxiv.org/pdf/2411.08706.pdf
2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://ekinakyurek.github.io/papers/ttt.pdf
2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arxiv.org/pdf/2411.17708.pdf
2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://proceedings.neurips.cc/paper_files/paper/2024/file/9446c291a8744a125a0bda5b18f4d5a1-Paper-Conference.pdf
2025,Adaptive Patch Selection for ViTs via Reinforcement Learning,https://doi.org/10.1007/s10489-025-06516-z
2025,Olympiad-level formal mathematical reasoning with large language models (AlphaProof),https://www.nature.com/articles/s41586-025-09833-y.pdf
2025,The Rotary Position Embedding May Cause Dimension Inefficiency,https://arxiv.org/pdf/2502.11276.pdf
2025,VRoPE: Rotary Position Embedding for Video Large Language Models,https://arxiv.org/pdf/2502.11664.pdf
2025,Maximizing the Position Embedding for Vision Transformers (MPVG),
2025,SmolVLM: Redefining small and efficient multimodal models,https://arxiv.org/pdf/2504.05299.pdf
2025,LOOPE: Learnable Optimal Patch Order in Vision Transformers,https://arxiv.org/pdf/2504.14386.pdf
2025,"Gated Attention for LLMs: Non-linearity, Sparsity, Sink-Free",https://arxiv.org/pdf/2505.06708.pdf
2025,Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models,https://arxiv.org/pdf/2505.16416.pdf
2025,Rotary Masked Autoencoders Are Versatile Learners,https://arxiv.org/pdf/2505.20535.pdf
2025,LLaVA-4D: Embedding Spatiotemporal Prompt into LMMs,https://arxiv.org/pdf/2505.12253.pdf
2025,ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices,https://arxiv.org/pdf/2506.03737.pdf
2025,Hierarchical Reasoning Model (HRM),https://arxiv.org/pdf/2506.21734.pdf
2025,EVA02-AT: Egocentric Video-Language with Spatial-Temporal RoPE,https://arxiv.org/pdf/2506.14356.pdf
2025,TransXSSM: Hybrid Transformer–SSM with Unified RoPE,https://arxiv.org/pdf/2506.09507.pdf
2025,Context-aware Rotary Position Embedding (CARoPE),https://arxiv.org/pdf/2507.23083.pdf
2025,"Decoupling the ""What"" and ""Where"" With Polar Coordinate Positional Embeddings (PoPE)",https://arxiv.org/pdf/2509.10534.pdf
2025,Less is More: Recursive Reasoning with Tiny Networks,https://arxiv.org/pdf/2510.04871.pdf
2025,Head-Wise Adaptive Rotary Positional Encoding (HARoPE),https://arxiv.org/pdf/2510.10489.pdf
2025,Nested Learning: The Illusion of Deep Learning Architecture,
2025,DoPE: Denoising Rotary Position Embedding,https://arxiv.org/pdf/2511.09146.pdf
2025,WALRUS: A Cross-Domain Foundation Model for Continuum Dynamics,https://arxiv.org/pdf/2511.15684.pdf
2025,Selective Rotary Position Embedding,https://arxiv.org/pdf/2511.17388.pdf
