             Article
                                                                            Symbolic deduction
              abc
                             Sample                                                                                                   Synthetic
                        random premises                                       and traceback                                     problems and proofs
                                                                                                                                         …
                                                                                                                                  AA
                                                                                             …
                                                                                                                                          DD
                                                                   cyclic(E,A,D,H)
                                                                                                                           EE
                                                                                                                                                 DD
                     AA
                                                                 …               ∠EAH = ∠EDH                                      EE
                                    DD                                                                                                                         CC
                                                                    ∠EDH = ∠ECB                                                  BB
                                                                                                                                             AAA
                         GG
                EE                                           cyclic(E,B,C,D)
                                                                                                                                                    DDD
                         HH                                                                        HA ⊥ BC                               EEE
                                                                       EC⊥ EA
                                                                                                                                                HH
               BB      FF                               CC
                                                                                            …                                         BB
                                                                                                                                                                    CC
             Fig. 3 | AlphaGeometry synthetic-data-generation process. a, We first sample    for the rightmost node ‘HA ⊥ BC’, traceback returns the green subgraph.  
             a large set of random theorem premises. b, We use the symbolic deduction        c, The minimal premise and the corresponding subgraph constitute a synthetic 
             engine to obtain a deduction closure. This returns a directed acyclic graph     problem and its solution. In the bottom example, points E and D took part in the 
             of statements. For each node in the graph, we perform traceback to find its     proof despite being irrelevant to the construction of HA and BC; therefore, they 
             minimal set of necessary premise and dependency deductions. For example,        are learned by the language model as auxiliary constructions.
             can be found in ref. 10. To widen the scope of the generated synthetic           
             theorems and proofs, we also introduce another component to the                 Training a language model on synthetic data
                                                                                                                18
             symbolic engine that can deduce new statements through algebraic                The transformer  language model is a powerful deep neural network 
             rules (AR), as described in Methods. AR is necessary to perform angle,          that learns to generate text sequences through next-token predic-
             ratio and distance chasing, as often required in many olympiad-                 tion, powering substantial advances in generative AI technology. We 
             level proofs. We included concrete examples of AR in Extended Data              serialize (P, N, G(N)) into a text string with the structure ‘<premises>
             Table 2. The combination DD + AR, which includes both their for-                <conclusion><proof>’. By training on such sequences of symbols, a 
             ward deduction and traceback algorithms, is a new contribution in               language model effectively learns to generate the proof, conditioning 
             our work and represents a new state of the art in symbolic reasoning            on theorem premises and conclusion.
             in geometry.
                                                                                             Combining language modelling and symbolic engines
             Generating proofs beyond symbolic deduction                                     On a high level, proof search is a loop in which the language model and 
             So far, the generated proofs consist purely of deduction steps that are         the symbolic deduction engine take turns to run, as shown in Fig. 1b,c. 
             already reachable by the highly efficient symbolic deduction engine             Proof search terminates whenever the theorem conclusion is found or 
             DD + AR. To solve olympiad-level problems, however, the key missing             when the loop reaches a maximum number of iterations. The language 
             piece is generating new proof terms. In the above algorithm, it can be          model is seeded with the problem statement string and generates one 
             seen that such terms form the subset of P that N is independent of. In          extra sentence at each turn, conditioning on the problem statement 
             other words, these terms are the dependency difference between the              and past constructions, describing one new auxiliary construction 
             conclusion statement and the conclusion objects. We move this dif-              such as “construct point X so that ABCX is a parallelogram”. Each time 
             ference from P to the proof so that a generative model that learns to           the language model generates one such construction, the symbolic 
             generate the proof can learn to construct them, as illustrated in Fig. 3c.      engine is provided with new inputs to work with and, therefore, its 
             Such proof steps perform auxiliary constructions that symbolic deduc-           deduction closure expands, potentially reaching the conclusion. We 
             tion engines are not designed to do. In the general theorem-proving             use beam search to explore the top k constructions generated by the 
             context, auxiliary construction is an instance of exogenous term gen-           language model and describe the parallelization of this proof-search 
             eration, a notable challenge to all proof-search algorithms because it          algorithm in Methods.
             introduces infinite branching points to the search tree. In geometry 
             theorem proving, auxiliary constructions are the longest-standing               Empirical evaluation
             subject of study since inception of the field in 1959 (refs. 6,7). Previ-
             ous methods to generate them are based on hand-crafted templates                An olympiad-level benchmark for geometry
             and domain-specific heuristics8–12, and are, therefore, limited by a            Existing benchmarks of olympiad mathematics do not cover geometry 
             subset of human experiences expressible in hard-coded rules. Any                because of a focus on formal mathematics in general-purpose lan-
                                                                                                     1,9
             neural solver trained on our synthetic data, on the other hand, learns          guages , whose formulation poses great challenges to representing 
             to perform auxiliary constructions from scratch without human                   geometry. Solving these challenges requires deep expertise and large 
             demonstrations.                                                                 research investment that are outside the scope of our work, which 
             478 | Nature | Vol 625 | 18 January 2024
