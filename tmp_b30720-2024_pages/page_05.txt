               Article
               Table 1 | Main results on our IMO-AG-30 test benchmark                                              solved 18 problems, making use of the algebraic reasoning engine devel-
                                                                                                                                                                                                            17
                                                                                                                   oped in this work and the human heuristics designed by Chou et al. . To 
               Method                                                                  Problems solved             match the test time compute of AlphaGeometry, this strongest baseline 
                                                                                       (out of 30)                 makes use of 250 parallel workers running for 1.5 h, each attempting 
                                                          21
               Computer algebra           Wu’s method  (previous state of              10                          different sets of auxiliary constructions suggested by human-designed 
                                          the art)                                                                 heuristics in parallel, until success or timeout. Other baselines such 
                                                           20
                                          Gröbner basis                                4                           as Wu’s method or the full-angle method are not affected by parallel 
               Search (human-like)        GPT-4 (ref. 25)                              0                           compute resources as they carry out fixed, step-by-step algorithms 
                                                                30                                                 until termination.
                                          Full-angle method                            2
                                                                        10             7                              Measuring the improvements made on top of the base symbolic 
                                          Deductive database (DD)
                                                                                17                                 deduction engine (DD), we found that incorporating algebraic deduc-
                                          DD + human-designed heuristics               9                           tion added seven solved problems to a total of 14 (DD + AR), whereas the 
                                          DD + AR (ours)                               14                          language model’s auxiliary construction remarkably added another 11 
                                          DD + AR + GPT-4 auxiliary                    15                          solved problems, resulting in a total of 25. As reported in Extended Data 
                                          constructions                                                            Fig. 6, we find that, using only 20% of the training data, AlphaGeometry 
                                          DD + AR + human-designed heuristics 18                                   still achieves state-of-the-art results with 21 problems solved. Similarly, 
                                          AlphaGeometry                                25                          using less than 2% of the search budget (beam size of 8 versus 512) dur-
                                          • Without pretraining                        21                          ing test time, AlphaGeometry can still solve 21 problems. On a larger 
                                          • Without fine-tuning                        23                          and more diverse test set of 231 geometry problems, which covers 
               We compare AlphaGeometry to other state-of-the-art methods (computer algebra and search             textbook exercises, regional olympiads and famous theorems, we find 
               approaches), most notably Wu’s method. We also show the results of DD + AR (our contribution)       that baselines in Table 1 remain at the same performance rankings, with 
               and its variants, resulting in the strongest baseline DD + AR + human-designed heuristics.          AlphaGeometry solving almost all problems (98.7%), whereas Wu’s 
               Finally, we include ablation settings for AlphaGeometry without pretraining and fine-tuning.        method solved 75% and DD + AR + human-designed heuristics solved 
                                                                                                                   92.2%, as reported in Extended Data Fig. 6b.
                                                                                                                      Notably, AlphaGeometry solved both geometry problems of the 
               premises. This can be attributed to the use of composite actions                                    same year in 2000 and 2015, a threshold widely considered difficult 
               described in Extended Data Table 1, such as ‘taking centroid’ or ‘tak-                              to the average human contestant at the IMO. Further, the traceback 
               ing excentre’, which—by chance—sampled a superset of well-known                                     process of AlphaGeometry found an unused premise in the translated 
               theorem premises, under our large-scale exploration setting described                               IMO 2004 P1, as shown in Fig. 5, therefore discovering a more general 
               in Methods. To study the complexity of synthetic proofs, Fig. 4 shows                               version of the translated IMO theorem itself. We included AlphaGeo-
               a histogram of synthetic proof lengths juxtaposed with proof lengths                                metry solutions to all problems in IMO-AG-30 in the Supplementary 
               found on the test set of olympiad problems. Although the synthetic                                  Information and manually analysed some notable AlphaGeometry 
               proof lengths are skewed towards shorter proofs, a small number of                                  solutions and failures in Extended Data Figs. 2–5. Overall, we find that 
               them still have lengths up to 30% longer than the hardest problem in the                            AlphaGeometry operates with a much lower-level toolkit for proving 
               IMO test set. We find that synthetic theorems found by this process are                             than humans do, limiting the coverage of the synthetic data, test-time 
               not constrained by human aesthetic biases such as being symmetrical,                                performance and proof readability.
               therefore covering a wider set of scenarios known to Euclidean geom-
               etry. We performed deduplication as described in Methods, resulting                                 Human expert evaluation of AlphaGeometry outputs
               in more than 100 millions unique theorems and proofs, and did not find                              Because AlphaGeometry outputs highly interpretable proofs, we 
               any IMO-AG-30 theorems, showing that the space of possible geometry                                 used a simple template to automatically translate its solutions to 
               theorems is still much larger than our discovered set.                                              natural language. To obtain an expert evaluation in 2000 and 2015, 
                                                                                                                   during which AlphaGeometry solves all geometry problems and 
               Language model pretraining and fine-tuning                                                          potentially passes the medal threshold, we submit these solutions 
               We first pretrained the language model on all 100 million synthetically                             to the USA IMO team coach, who is experienced in grading mathe-
               generated proofs, including ones of pure symbolic deduction. We then                                matical olympiads and has authored books for olympiad geometry 
               fine-tuned the language model on the subset of proofs that requires                                 training. AlphaGeometry solutions are recommended to receive 
               auxiliary constructions, accounting for roughly 9% of the total pre-                                full scores, thus passing the medal threshold of 14/42 in the corre-
               training data, that is, 9 million proofs, to better focus on its assigned                           sponding years. We note that IMO tests also evaluate humans under 
               task during proof search.                                                                           three other mathematical domains besides geometry and under 
                                                                                                                   human-centric constraints, such as no calculator use or 4.5-h time 
               Proving results on IMO-AG-30                                                                        limits. We study time-constrained settings with 4.5-h and 1.5-h limits 
                                                                                                                   for AlphaGeometry in Methods and report the results in Extended  
               The performance of ten different solvers on the IMO-AG-30 benchmark                                 Data Fig. 1.
               is reported in Table 1, of which eight, including AlphaGeometry, are 
               search-based methods. Besides prompting GPT-4 to produce full proofs                                Learning to predict the symbolic engine’s output improves the 
               in natural language with several rounds of reflections and revisions, we                            language model’s auxiliary construction
               also combine GPT-4 with DD + AR as another baseline to enhance its                                  In principle, auxiliary construction strategies must depend on the 
               deduction accuracy. To achieve this, we use detailed instructions and                               details of the specific deduction engine they work with during proof 
               few-shot examples in the prompt to help GPT-4 successfully interface                                search. We find that a language model without pretraining only 
               with DD + AR, providing auxiliary constructions in the correct gram-                                solves 21 problems. This suggests that pretraining on pure deduction 
               mar. Prompting details of baselines involving GPT-4 is included in the                              proofs generated by the symbolic engine DD + AR improves the suc-
               Supplementary Information.                                                                          cess rate of auxiliary constructions. On the other hand, a language 
                  AlphaGeometry achieves the best result, with 25 problems solved in                               model without fine-tuning also degrades the performance but not as 
               total. The previous state of the art (Wu’s method) solved ten problems,                             severely, with 23 problems solved compared with AlphaGeometry’s full  
               whereas the strongest baseline (DD + AR + human-designed heuristics)                                setting at 25.
               480 | Nature | Vol 625 | 18 January 2024
