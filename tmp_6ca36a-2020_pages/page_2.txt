                 is a semi-structured natural language form that is used to explain the answers. R4C is created based on
                 HotpotQAandhas4,588questions. However,thesmallsizeofthedatasetimpliesthatthedatasetcannot
                 be used as a multi-hop dataset with a comprehensive explanation for training end-to-end systems.
                                                                                                        2
                   In this study, we create a large and high quality multi-hop dataset 2WikiMultiHopQA with a com-
                 prehensive explanation by combining structured and unstructured data. To enhance the explanation and
                 evaluation process when answering a multi-hop question on Wikipedia articles, we introduce new infor-
                 mationineachsample,namelyevidencethatcontainscomprehensiveandconciseinformationtoexplain
                 the predictions. Evidence information in our dataset is a set of triples, where each triple is a structured
                 data (subject entity, property, object entity) obtained from the Wikidata (see Figure 1 for an example).
                 Figure 1: Example of an inference question in our dataset. The difference between our dataset and
                 HotpotQAistheevidence information that explains the reasoning path.
                   Ourdatasethasfourtypesofquestions: comparison,inference,compositional,andbridgecomparison.
                 All questions in our dataset are created by using a set of predeﬁned templates. Min et al. (2019) clas-
                 siﬁed the comparison questions in HotpotQA in three types: multi-hop, context-dependent multi-hop,
                 and single-hop. Based on this classiﬁcation, we removed all templates in our list that make questions
                 becomesingle-hop or context-dependent multi-hop to ensure that our comparison questions and bridge-
                 comparisonquestionsaremulti-hop. Wecarefullydesignedapipelinetoutilizetheintersectioninforma-
                 tion between the summary3 ofWikipediaarticlesandWikidataandhaveaspecialtreatmentforeachtype
                 of question that guarantees multi-hop steps and the quality of the questions. Further, by utilizing the logi-
                 cal rule information in the knowledge graph, such as father(a,b)∧father(b,c) ⇒ grandfather(a,c),
                 wecancreate more natural questions that still require multi-hop reasoning.
                   We conducted two different evaluations on our dataset: difﬁculty and multi-hop reasoning of the
                 dataset. To evaluate the difﬁculty, we used a multi-hop model to compare the performance of HotpotQA
                 and our dataset. Overall, the results from our dataset are lower than those observed in HotpotQA, while
                 human scores are comparable on both datasets. This suggests that the number of difﬁcult questions in
                 our dataset is greater than that in HotpotQA. Similar to Min et al. (2019), we used a single-hop BERT
                 model to test the multi-hop reasoning in our dataset. The result of our dataset is lower than the result
                 of HotpotQA by 8.7 F1, indicating that a lot of examples in our dataset require multi-hop reasoning to
                 be solved. Through experiments, we conﬁrmed that although our dataset is generated by hand-crafted
                 templates and the set of predeﬁned logical rules, it is challenging for multi-hop models and requires
                 multi-hop reasoning.
                   22Wiki is a combination of Wikipedia and Wikidata.
                   3Another name is “short description”; The short description at the top of an article that summarizes the content. See also
                 https://en.wikipedia.org/wiki/Wikipedia:Short_description
                                                                 6610
