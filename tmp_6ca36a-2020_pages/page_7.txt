             Answer Types We preserved all information when generating the data; hence, we used the answer
             information (both string and Wikidata id) to classify the types of answers. Based on the value of the
             property instance of in Wikidata, we obtained 708 unique types of answers. The top-5 types of answers
             in our dataset are: yes/no (31.2%), date (16.9%; e.g., July 10, 2010), ﬁlm (13.5%; e.g., La La Land),
             human(11.7%;e.g.,GeorgeWashington),andbigcity(4.7%;e.g.,Chicago). Fortheremainingtypesof
             answers (22.0%), they are various types of entities in Wikidata.
             5  Experiments
             5.1 Evaluate the Dataset Quality
             Weconductedtwodifferent evaluations on our dataset: evaluate the difﬁculty and the multi-hop reason-
             ing. To evaluate the difﬁculty, we used the multi-hop model as described in Yang et al. (2018) to obtain
             the results on HotpotQA (distractor setting) and our dataset. Table 4 presents the results. For the SFs
             prediction task, the scores on our dataset are higher than those on HotpotQA. However, for the answer
             prediction task, the scores on our dataset are lower than those on HotpotQA. Overall, on the joint met-
             rics, the scores on our dataset are lower than those on HotpotQA. This indicates that given the human
             performance on both datasets is comparable (see Section 5.3), the number of difﬁcult questions in our
             dataset is greater than that in HotpotQA.
                         Dataset            Answer      SpFact       Joint
                                           EM     F1   EM     F1   EM     F1
                         HotpotQA        44.48  58.54 20.68 65.66 10.97 40.52
                         OurDataset      34.14  40.95 26.47 66.94  9.22 26.76
             Table4: Results (%) of the multi-hop model on HotpotQA (Yang et al., 2018) and our dataset. “Sp Fact”
             is the abbreviation for the sentence-level supporting facts prediction task.
              Similar to Min et al. (2019), we used a single-hop BERT model (Devlin et al., 2019) to test the
             multi-hop reasoning in our dataset. The F1 score on HotpotQA is 64.6 (67.0 F1 in Min et al. (2019));
             meanwhile, the F1 score on our dataset is 55.9. The result of our dataset is lower than the result of
             HotpotQA by 8.7 F1. It indicates that a large number of examples in our dataset require multi-hop
             reasoning to be solved. Moreover, it is veriﬁed that our data generation and our templates guarantee
             multi-hopreasoning. Insummary,theseresultsshowthatourdatasetischallengingformulti-hopmodels
             and requires multi-hop reasoning to be solved.
              Figure 3: Our baseline model. The right part is the baseline model of HotpotQA (Yang et al., 2018).
                                                 6615
