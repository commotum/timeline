                   Insummary,ourmaincontributionsareasfollows: (1)WeuseWikipediaandWikidatatocreatealarge
                andhighquality multi-hop dataset that has comprehensive explanations from question to answer. (2) We
                provide new information in each sample—evidence information useful for interpreting the predictions
                and testing the reasoning and inference skills of the model. (3) We use logical rules to generate a simple
                natural question but still require the model to undertake multi-hop reasoning when answering a question.
                The full dataset, baseline model, and all information that we used when constructing the dataset are
                available at https://github.com/Alab-NII/2wikimultihop.
                2   TaskOverview
                2.1   TaskFormalization and Metrics
                We formulated (1) answer prediction, (2) sentence-level SFs prediction, and (3) evidence generation
                tasks as follows:
                   • Input: a question Q and a set of documents D.
                   • Output: (1) ﬁnd an answer A (a textual span in D) for Q, (2) ﬁnd a set of sentence-level SFs
                     (sentences) in D that a model used to answer Q, and (3) generate a set of evidence E which consists
                     of triples that describes the reasoning path from Q to A.
                   Weevaluate the three tasks by using two evaluation metrics: exact match (EM) and F1 score. Fol-
                lowing previous work (Yang et al., 2018), to assess the entire capacity of the model, we introduced joint
                metrics that combine the evaluation of answer spans, sentence-level SFs, and evidence as follows:
                                                                  2PjointRjoint
                                                    JointF1= Pjoint +Rjoint                                      (1)
                   where Pjoint = PansPsupPevi and Rjoint = RansRsupRevi. (Pans, Rans), (Psup,Rsup), and
                (Pevi,Revi) denote the precision and recall of the answer spans, sentence-level SFs, and evidence, re-
                spectively. Joint EM is 1 only when all the three tasks obtain an exact match or otherwise 0.
                2.2   Question Types
                In our dataset, we have the following four types of questions: (1) comparison, (2) inference, (3) compo-
                sitional, and (4) bridge comparison. The inference and compositional questions are the two subtypes of
                the bridge question which comprises a bridge entity that connects the two paragraphs (Yang et al., 2018).
                  1. Comparisonquestionisatypeofquestionthatcomparestwoormoreentitiesfromthesamegroup
                     in some aspects of the entity (Yang et al., 2018). For instance, a comparison question compares two
                     or more people with the date of birth or date of death (e.g., Who was born ﬁrst, Albert Einstein or
                     AbrahamLincoln?).
                  2. Inference question is created from the two triples (e,r ,e ) and (e ,r ,e ) in the KB. We utilized
                                                                          1  1        1  2  2
                     the logical rule to acquire the new triple (e,r,e ), where r is the inference relation obtained from
                                                                   2
                     the two relations r and r . A question–answer pair is created by using the new triple (e,r,e ),
                                       1       2                                                                  2
                     its question is created from (e,r) and its answer is e . For instance, using two triples (Abraham
                                                                         2
                     Lincoln, mother, Nancy HanksLincoln)and(NancyHanksLincoln,father,JamesHanks),weobtain
                     a new triple (Abraham Lincoln, maternal grandfather, James Hanks). A question is: Who is the
                     maternal grandfather of Abraham Lincoln? An answer is James Hanks (Section 3.2).
                  3. Compositional question is created from the two triples (e,r ,e ) and (e ,r ,e ) in the KB. Com-
                                                                               1   1       1  2   2
                     pared with inference question, the difference is that no inference relation r exists from the two
                     relations r and r . For instance, there are two triples (La La Land, distributor, Summit Entertain-
                               1      2
                     ment) and (Summit Entertainment, founded by, Bernd Eichinger). There is no inference relation r
                     fromthetworelations distributor and founded-by. In this case, a question is created from the entity
                     e and the two relations r and r : Who is the founder of the company that distributed La La Land
                                             1      2
                     ﬁlm? Anansweristheentity e2 of the second triple: Bernd Eichinger (Section 3.2).
                                                                6611
