                5.2   Baseline Results
                We modiﬁed the baseline model in Yang et al. (2018) and added a new component (the orange block
                in Figure 3) to perform the evidence generation task. We re-used several techniques of the previous
                baseline, such as bi-attention, to predict the evidence. Our evidence information is a set of triples, with
                each triple including subject entity, relation, and object entity. First, we used the question to predict the
                relations and then used the predicted relations and the context (after predicting sentence-level SFs) to
                obtain the subject and object entities.
                   Table 5 presents the results of our baseline model. We used the evaluation metrics as described in
                Section 2.1. As shown in the table, the scores of the sentence-level SFs prediction task are quite high.
                This is a binary classiﬁcation task that classiﬁes whether each sentence is a SF. As discussed, this task
                is incapable of evaluating the reasoning and inference skills of the model. The scores of the evidence
                generation task are quite low which indicates this task is difﬁcult. Our error analysis shows that the
                model can predict one correct triple in the set of the triples. However, accurately obtaining the set of
                triples is extremely challenging. This is the reason why the EM score is very low. We believe that adding
                the evidence generation task is appropriate to test the reasoning and inference skills.
                           Split/Task              Answer          SpFact         Evidence        Joint
                                                  EM       F1     EM       F1    EM       F1    EM     F1
                           Dev                  35.30   42.45   23.85   64.31   1.08   14.77   0.37   5.03
                           Test                 36.53   43.93   24.99   65.26   1.07   14.94   0.35   5.41
                                             Table 5: Results (%) of the baseline model.
                   Toinvestigate the difﬁculty of each type of question, we categorized the performance for each type of
                question (on the test split). Table 6 shows the results. For the answer prediction task, the model obtained
                high scores on inference and compositional questions. Meanwhile, for the sentence-level SFs prediction
                task, the model obtained high scores on comparison and bridge-comparison questions. Overall, the joint
                metric score of the inference question is the lowest. This indicates that this type of question is more
                challenging for the model. The evidence generation task has the lowest score for all types of questions
                whencompared with the other two tasks. This suggests that the evidence generation task is challenging
                for all types of questions.
                         TypeofQuestion          Answer          SpFact          Evidence          Joint
                                                EM       F1     EM       F1     EM       F1     EM       F1
                         Comparison           26.49   27.86   26.76   65.02     0.00  12.40     0.00    2.45
                         Inference            41.10   62.60   10.77   49.45     0.00    2.85    0.00    1.40
                         Compositional        50.40   59.94   18.28   57.44     2.57  17.65     0.84    9.19
                         Bridge-Comparison    18.47   20.45   43.74   89.16     0.00  19.17     0.00    3.60
                               Table 6: Results (%) of the baseline model on different types of questions.
                5.3   HumanPerformance
                Weobtained a human performance on 100 samples that are randomly chosen from the test split. Each
                sample was annotated by three workers (graduate students). We provided the question, context, and a
                set of predeﬁned relations (for the evidence generation task) and asked a worker to provide an answer,
                a set of sentence-level SFs, and a set of evidence. Similar to the previous work (Yang et al., 2018),
                wecomputed the upper bound for human performance by acquiring the maximum EM and F1 for each
                sample. All the results are presented in Table 7.
                   Theworkersachievedhigherperformance than that of the model. The human performance for the an-
                swerpredictiontaskis91.0EMand91.8F1. Therestillseemstoberoomforimprovement,whichmight
                be because the mismatch information between Wikipedia and Wikidata makes questions unanswerable
                                                                6616
