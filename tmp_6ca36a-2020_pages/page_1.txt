                      Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of
                                                                     Reasoning Steps
                                          ♥♣                                        ♦                         ♣                      ♥♣
                             XanhHo ,Anh-KhoaDuongNguyen ,SakuSugawara ,AkikoAizawa
                                       ♥ The Graduate University for Advanced Studies, Kanagawa, Japan
                                                   ♣ National Institute of Informatics, Tokyo, Japan
                             ♦ National Institute of Advanced Industrial Science and Technology, Tokyo, Japan
                                                      {xanh, saku, aizawa}@nii.ac.jp
                                                               khoa.duong@aist.go.jp
                                                                             Abstract
                         A multi-hop question answering (QA) dataset aims to test reasoning and inference skills by
                         requiring a model to read multiple paragraphs to answer a given question. However, current
                         datasets do not provide a complete explanation for the reasoning process from the question to
                         the answer. Further, previous studies revealed that many examples in existing multi-hop datasets
                         do not require multi-hop reasoning to answer a question. In this study, we present a new multi-
                         hop QA dataset, called 2WikiMultiHopQA, which uses structured and unstructured data. In
                         our dataset, we introduce the evidence information containing a reasoning path for multi-hop
                         questions. The evidence information has two beneﬁts: (i) providing a comprehensive explanation
                         for predictions and (ii) evaluating the reasoning skills of a model. We carefully design a pipeline
                         and a set of templates when generating a question–answer pair that guarantees the multi-hop
                         steps and the quality of the questions. We also exploit the structured format in Wikidata and use
                         logical rules to create questions that are natural but still require multi-hop reasoning. Through
                         experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures
                         that multi-hop reasoning is required.
                    1    Introduction
                    Machine reading comprehension (MRC) aims at teaching machines to read and understand given text.
                    Many current models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have defeated humans
                    on the performance of SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), as shown on its leader-
                           1
                    board . However, such performances do not indicate that these models can completely understand the
                    text. Speciﬁcally, using an adversarial method, Jia and Liang (2017) demonstrated that the current mod-
                    els do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that
                    many datasets contain a considerable number of easy instances that can be answered based on the ﬁrst
                    few words of the questions.
                       Multi-hop MRCdatasetsrequireamodeltoreadandperformmulti-hopreasoningovermultiplepara-
                    graphs to answer a question. Currently, there are four multi-hop datasets over textual data: ComplexWe-
                    bQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018),
                    and R4C (Inoue et al., 2020). The ﬁrst two datasets were created by incorporating the documents (from
                    WeborWikipedia)withaknowledgebase(KB).Owingtotheirbuildingprocedures,thesedatasetshave
                    noinformation to explain the predicted answers. Meanwhile, the other two datasets were created mainly
                    based on crowdsourcing. In HotpotQA, the authors introduced the sentence-level supporting facts (SFs)
                    information that are used to explain the predicted answers. However, as discussed in Inoue et al. (2020),
                    the task of classifying sentence-level SFs is a binary classiﬁcation task that is incapable of evaluating the
                    reasoning and inference skills of the model. Further, data analyses (Chen and Durrett, 2019; Min et al.,
                    2019) revealed that many examples in HotpotQA do not require multi-hop reasoning to solve.
                       Recently, to evaluate the internal reasoning of the reading comprehension system, Inoue et al. (2020)
                                                    4
                    proposed a new dataset R C that requires systems to provide an answer and derivations. A derivation
                     This work is licensed under a Creative Commons Attribution 4.0 International License.           License details:  http://
                    creativecommons.org/licenses/by/4.0/.
                        1https://rajpurkar.github.io/SQuAD-explorer/
                                                                                6609
                                  Proceedings of the 28th International Conference on Computational Linguistics, pages 6609–6625
                                                          Barcelona, Spain (Online), December 8-13, 2020
