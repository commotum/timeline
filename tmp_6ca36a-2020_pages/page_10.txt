                 HybridQAdataset—amulti-hop question answering over both tabular and textual data. The dataset was
                 created by crowdsourcing based on Wikipedia tables and Wikipedia articles.
                 Multi-hop questions in KB domain        Question answering over the knowledge graph has been investi-
                 gated for decades. However, most current datasets (Berant et al., 2013; Bordes et al., 2015; Yih et al.,
                 2015; Diefenbach et al., 2017) consist of simple questions (single-hop). Zhang et al. (2018b) introduced
                 the METAQA dataset that contains both single-hop and multi-hop questions. Abujabal et al. (2017) in-
                 troduced the ComplexQuestions dataset comprising 150 compositional questions. All of these datasets
                 are solved by using the KB only. Our dataset is constructed based on the intersection between Wikipedia
                 and Wikidata. Therefore, it can be solved by using structured or unstructured data.
                 Compositional Knowledge Base Inference         Extracting Horn rules from the KB has been studied ex-
                 tensively in the Inductive Logic Programming literature (Quinlan, 1990; Muggleton, 1995). From the
                 KB, there are several approaches that mine association rules (Agrawal et al., 1993) and several mine
                                                                ´
                 logical rules (Schoenmackers et al., 2010; Galarraga et al., 2013). We observed that these rules can be
                 used to test the reasoning skill of the model. Therefore, in this study, we utilized the logical rules in the
                 form: r (a,b)∧r (b,c) ⇒ r(a,c). ComplexWebQuestions and QAngaroo datasets are also utilized KB
                        1          2
                 whenconstructing the dataset, but they do not utilize the logical rules as we did.
                 RCdatasets with explanations       Table 8 presents several existing datasets that provide explanations.
                                  4
                 HotpotQA and R C are the most similar works to ours. HotpotQA provides a justiﬁcation explanation
                                                                                                             4
                 (collections of evidence to support the decision) in the form of a set of sentence-level SFs. R C provides
                 both justiﬁcation and introspective explanations (how a decision is made). Our study also provides both
                 justiﬁcation and introspective explanations. The difference is that the explanation in our dataset is a set
                 of triples, where each triple is a structured data obtained from Wikidata. Meanwhile, the explanation in
                 R4Cisaset of semi-structured data. R4C is created based on HotpotQA and has 4,588 questions. The
                 small size of the dataset implies that it cannot be used for training end-to-end neural network models
                 involving the multi-hop reasoning with comprehensive explanation.
                             Task/Dataset                                    Explanations             Size
                                                                      Justiﬁcation   Introspective
                             Ourwork                                                                 192,606
                             R4C(Inoueetal., 2020)                                                     4,588
                             CoS-E(Rajani et al., 2019)                                               19,522
                             HotpotQA(Yangetal., 2018)                                               112,779
                             Science Exam QA (Jansen et al., 2016)                                       363
                                       Table 8: Comparison with other datasets with explanations.
                 7   Conclusion
                 In this study, we presented 2WikiMultiHopQA—a large and high quality multi-hop dataset that provides
                 comprehensive explanations for predictions. We utilized logical rules in the KB to create more natural
                 questions that still require multi-hop reasoning. Through experiments, we demonstrated that our dataset
                 ensures multi-hop reasoning while being challenging for the multi-hop models. We also demonstrated
                 that bootstrapping the multi-hop MRC dataset is beneﬁcial by utilizing large-scale available data on
                 Wikipedia and Wikidata.
                 Acknowledgments
                 We would like to thank An Tuan Dao, Johannes Mario Meissner Blanco, Kazutoshi Shinoda, Napat
                 Thumwanit, Taichi Iki, Thanakrit Julavanich, and Vitou Phy for their valuable support in the procedure
                 of constructing the dataset. We thank the anonymous reviewers for suggestions on how to improve the
                 dataset and the paper. This work was supported by JSPS KAKENHI Grant Number 18H03297.
                                                                  6618
