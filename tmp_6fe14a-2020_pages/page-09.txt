                  been used as the standard method applied broadly         effective solution that shows stronger empirical per-
                  to various QA tasks (e.g., Chen et al., 2017; Yang       formance, without relying on additional pretraining
                  et al., 2019a,b; Nie et al., 2019; Min et al., 2019a;    or complex joint training schemes.
                  Wolfson et al., 2020). Augmenting text-based re-            DPRhas also been used as an important mod-
                  trieval with external structured information, such       ule in very recent work. For instance, extending
                  as knowledge graph and Wikipedia hyperlinks, has         the idea of leveraging hard negatives, Xiong et al.
                  also been explored recently (Min et al., 2019b; Asai     (2020a) use the retrieval model trained in the pre-
                  et al., 2020).                                           vious iteration to discover new negatives and con-
                     The use of dense vector representations for re-       struct a different set of examples in each training
                  trieval has a long history since Latent Semantic         iteration. Starting from our trained DPR model,
                  Analysis (Deerwester et al., 1990). Using labeled        they show that the retrieval performance can be
                  pairs of queries and documents, discriminatively         further improved. Recent work (Izacard and Grave,
                  trained dense encoders have become popular re-           2020; Lewis et al., 2020b) have also shown that
                  cently (Yih et al., 2011; Huang et al., 2013; Gillick    DPR can be combined with generation models
                  et al., 2019), with applications to cross-lingual        such as BART (Lewis et al., 2020a) and T5 (Raf-
                  document retrieval, ad relevance prediction, Web         fel et al., 2019), achieving good performance on
                  search and entity retrieval. Such approaches com-        open-domain QA and other knowledge-intensive
                  plement the sparse vector methods as they can po-        tasks.
                  tentially give high similarity scores to semantically    8   Conclusion
                  relevant text pairs, even without exact token match-
                  ing. The dense representation alone, however, is         In this work, we demonstrated that dense retrieval
                  typically inferior to the sparse one. While not the      can outperform and potentially replace the tradi-
                  focus of this work, dense representations from pre-      tional sparse retrieval component in open-domain
                  trained models, along with cross-attention mecha-        question answering. While a simple dual-encoder
                  nisms, have also been shown effective in passage         approach can be made to work surprisingly well,
                  or dialogue re-ranking tasks (Nogueira and Cho,          weshowedthatthereare some critical ingredients
                  2019; Humeauet al., 2020). Finally, a concurrent         to training a dense retriever successfully. Moreover,
                  work (Khattab and Zaharia, 2020) demonstrates            our empirical analysis and ablation studies indicate
                  the feasibility of full dense retrieval in IR tasks.     that more complex model frameworks or similarity
                  Instead of employing the dual-encoder framework,         functions do not necessarily provide additional val-
                  they introduced a late-interaction operator on top       ues. As a result of improved retrieval performance,
                  of the BERT encoders.                                    weobtainednewstate-of-the-artresults on multiple
                     Dense retrieval for open-domain QA has been           open-domain question answering benchmarks.
                  explored by Das et al. (2019), who propose to re-        Acknowledgments
                  trieve relevant passages iteratively using reformu-      Wethanktheanonymousreviewersfortheirhelpful
                  lated question vectors. As an alternative approach       commentsandsuggestions.
                  that skips passage retrieval, Seo et al. (2019) pro-
                  posetoencodecandidateanswerphrasesasvectors
                  and directly retrieve the answers to the input ques-     References
                  tions efﬁciently. Using additional pretraining with
                  the objective that matches surrogates of questions       Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,
                  andrelevantpassages, Leeetal.(2019)jointlytrain             Richard Socher, and Caiming Xiong. 2020. Learn-
                  the question encoder and reader. Their approach             ing to retrieve reasoning paths over Wikipedia graph
                                                                              for question answering. In International Conference
                  outperforms the BM25 plus reader paradigm on                onLearning Representations (ICLR).
                  multiple open-domain QA datasets in QA accuracy,                    ˇ         ˇ     `
                  and is further extended by REALM (Guu et al.,            Petr Baudis and Jan Sedivy. 2015. Modeling of the
                  2020), which includes tuning the passage encoder            question answering task in the yodaqa system. In In-
                                                                              ternational Conference of the Cross-Language Eval-
                  asynchronously by re-indexing the passages dur-             uation Forum for European Languages, pages 222–
                  ing training. The pretraining objective has also            228. Springer.
                  recently been improved by Xiong et al. (2020b).          JonathanBerant,AndrewChou,RoyFrostig,andPercy
                  In contrast, our model provides a simple and yet            Liang. 2013. Semantic parsing on Freebase from
