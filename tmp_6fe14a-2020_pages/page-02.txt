                 QAdatasets, it also suffers from two weaknesses.        the extractive QA setting, in which the answer is
                 First, ICT pretraining is computationally intensive     restricted to a span appearing in one or more pas-
                 and it is not completely clear that regular sentences   sages in the corpus. Assume that our collection
                 are good surrogates of questions in the objective       contains D documents, d ,d ,··· ,d . We ﬁrst
                                                                                                    1  2        D
                 function. Second, because the context encoder is        split each of the documents into text passages of
                                                                                                                3
                 not ﬁne-tuned using pairs of questions and answers,     equallengthsasthebasicretrievalunits andgetM
                 the corresponding representations could be subop-       total passages in our corpus C = {p ,p ,...,p     },
                                                                                                             1  2       M
                 timal.                                                  whereeachpassagep canbeviewedasasequence
                                                                                              i
                    In this paper, we address the question: can we       of tokens w(i),w(i),··· ,w(i) . Given a question q,
                                                                                     1    2         |p |
                                                                                                      i
                 train a better dense embedding model using only         the task is to ﬁnd a span w(i),w(i) ,··· ,w(i) from
                 pairs of questions and passages (or answers), with-                               s     s+1        e
                 out additional pretraining? By leveraging the now       one of the passages pi that can answer the question.
                 standard BERT pretrained model (Devlin et al.,          Notice that to cover a wide variety of domains, the
                 2019) and a dual-encoder architecture (Bromley          corpus size can easily range from millions of docu-
                 et al., 1994), we focus on developing the right         ments (e.g., Wikipedia) to billions (e.g., the Web).
                 training scheme using a relatively small number         Asaresult, any open-domain QA system needs to
                 of question and passage pairs. Through a series         include an efﬁcient retriever component that can se-
                 of careful ablation studies, our ﬁnal solution is       lect a small set of relevant texts, before applying the
                                                                                                                           4
                 surprisingly simple: the embedding is optimized         reader to extract the answer (Chen et al., 2017).
                 for maximizing inner products of the question and       Formally speaking, a retriever R : (q,C) → CF
                 relevant passage vectors, with an objective compar-     is a function that takes as input a question q and a
                 ing all pairs of questions and passages in a batch.     corpus C and returns a much smaller ﬁlter set of
                                                                         texts C  ⊂C,where|C | = k  |C|. For a ﬁxed
                 OurDensePassageRetriever (DPR) is exception-                   F                F
                 ally strong. It not only outperforms BM25 by a          k, a retriever can be evaluated in isolation on top-k
                 large margin (65.2% vs. 42.9% in Top-5 accuracy),       retrieval accuracy, which is the fraction of ques-
                 but also results in a substantial improvement on        tions for which CF contains a span that answers the
                 the end-to-end QA accuracy compared to ORQA             question.
                 (41.5% vs. 33.3%) in the open Natural Questions         3   DensePassageRetriever (DPR)
                 setting (Lee et al., 2019; Kwiatkowski et al., 2019).
                    Ourcontributions are twofold. First, we demon-       We focus our research in this work on improv-
                 strate that with the proper training setup, sim-        ing the retrieval component in open-domain QA.
                 ply ﬁne-tuning the question and passage encoders        Given a collection of M text passages, the goal of
                 on existing question-passage pairs is sufﬁcient to      our dense passage retriever (DPR) is to index all
                 greatly outperform BM25. Our empirical results          the passages in a low-dimensional and continuous
                 also suggest that additional pretraining may not be     space, such that it can retrieve efﬁciently the top
                 needed. Second, we verify that, in the context of       k passages relevant to the input question for the
                 open-domainquestionanswering,ahigherretrieval           reader at run-time. Note that M can be very large
                 precision indeed translates to a higher end-to-end      (e.g., 21 million passages in our experiments, de-
                 QAaccuracy. By applying a modern reader model           scribed in Section 4.1) and k is usually small, such
                 to the top retrieved passages, we achieve compara-      as 20–100.
                 ble or better results on multiple QA datasets in the    3.1   Overview
                 open-retrieval setting, compared to several, much
                 complicated systems.                                    Our dense passage retriever (DPR) uses a dense
                                                                         encoder EP(·) which maps any text passage to a d-
                 2    Background                                         dimensionalreal-valuedvectorsandbuildsanindex
                                                                         for all the M passages that we will use for retrieval.
                 The problem of open-domain QA studied in this              3The ideal size and boundary of a text passage are func-
                 paper can be described as follows. Given a factoid      tions of both the retriever and reader. We also experimented
                 question, such as “Who ﬁrst voiced Meg on Family        withnaturalparagraphsinourpreliminarytrialsandfoundthat
                 Guy?”or“Wherewasthe8thDalaiLamaborn?”,a                 using ﬁxed-length passages performs better in both retrieval
                                                                         and ﬁnal QA accuracy, as observed by Wang et al. (2019).
                 system is required to answer it using a large corpus       4Exceptions include (Seo et al., 2019) and (Roberts et al.,
                 of diversiﬁed topics. More speciﬁcally, we assume       2020),whichretrievesandgeneratestheanswers,respectively.
