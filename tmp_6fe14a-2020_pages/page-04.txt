                   negative passages for each question.                            Dataset                  Train          Dev       Test
                      Thetrick of in-batch negatives has been used in              Natural Questions   79,168   58,880    8,757     3,610
                   the full batch setting (Yih et al., 2011) and more              TriviaQA            78,785   60,413    8,837   11,313
                   recently for mini-batch (Henderson et al., 2017;                WebQuestions         3,417     2,474     361     2,032
                   Gillick et al., 2019). It has been shown to be an               CuratedTREC          1,353     1,125     133      694
                                                                                   SQuAD               78,713   70,096    8,886   10,570
                   effective strategy for learning a dual-encoder model
                   that boosts the number of training examples.                  Table 1: Number of questions in each QA dataset. The
                                                                                 two columns of Train denote the original training ex-
                   4    Experimental Setup                                       amples in the dataset and the actual questions used for
                                                                                 training DPR after ﬁltering. See text for more details.
                   In this section, we describe the data we used for
                   experiments and the basic setup.                              as well as various Web sources and is intended for
                   4.1    Wikipedia Data Pre-processing                          open-domain QA from unstructured corpora.
                   Following (Lee et al., 2019), we use the English              SQuAD v1.1 (Rajpurkar et al., 2016) is a popu-
                   Wikipedia dump from Dec. 20, 2018 as the source               lar benchmark dataset for reading comprehension.
                   documents for answering questions. We ﬁrst apply              Annotators were presented with a Wikipedia para-
                   the pre-processing code released in DrQA (Chen                graph, and asked to write questions that could be
                   et al., 2017) to extract the clean, text-portion of           answered from the given text. Although SQuAD
                   articles from the Wikipedia dump. This step re-               has been used previously for open-domain QA re-
                   moves semi-structured data, such as tables, info-             search, it is not ideal because many questions lack
                   boxes, lists, as well as the disambiguation pages.            context in absence of the provided paragraph. We
                   Wethenspliteacharticleintomultiple, disjoint text             still include it in our experiments for providing
                   blocks of 100 words as passages, serving as our               a fair comparison to previous work and we will
                   basic retrieval units, following (Wang et al., 2019),         discuss more in Section 5.1.
                   which results in 21,015,324 passages in the end.5             Selection of positive passages           Because only
                   Each passage is also prepended with the title of the          pairs of questions and answers are provided in
                   Wikipedia article where the passage is from, along            TREC,WebQuestionsandTriviaQA6,weusethe
                   with an [SEP] token.                                          highest-ranked passage from BM25 that contains
                   4.2    Question Answering Datasets                            the answer as the positive passage. If none of the
                                                                                 top 100 retrieved passages has the answer, the ques-
                   We use the same ﬁve QA datasets and train-                    tion will be discarded. For SQuAD and Natural
                   ing/dev/testing splitting method as in previous               Questions, since the original passages have been
                   work(Leeetal., 2019). Below we brieﬂy describe                split and processed differently than our pool of
                   each dataset and refer readers to their paper for the         candidate passages, we match and replace each
                   details of data preparation.                                  gold passage with the corresponding passage in the
                   Natural Questions (NQ) (Kwiatkowski et al.,                   candidate pool.7 We discard the questions when
                   2019) was designed for end-to-end question an-                the matching is failed due to different Wikipedia
                   swering. The questions were mined from real                   versions or pre-processing. Table 1 shows the num-
                   Google search queries and the answers were spans              ber of questions in training/dev/test sets for all the
                   in Wikipedia articles identiﬁed by annotators.                datasets and the actual questions used for training
                   TriviaQA(Joshietal.,2017)containsasetoftrivia                 the retriever.
                   questionswithanswersthatwereoriginallyscraped
                   from the Web.                                                 5    Experiments: Passage Retrieval
                   WebQuestions(WQ)(Berantetal.,2013)consists                    In this section, we evaluate the retrieval perfor-
                   of questions selected using Google Suggest API,               mance of our Dense Passage Retriever (DPR),
                   where the answers are entities in Freebase.                   along with analysis on how its output differs from
                                                           ˇ         ˇ     `
                   CuratedTREC (TREC) (Baudis and Sedivy,
                   2015) sources questions from TREC QA tracks                      6Weusetheunﬁltered TriviaQA version and discard the
                                                                                 noisy evidence documents mined from Bing.
                       5However, Wang et al. (2019) also propose splitting docu-    7The improvement of using gold contexts over passages
                   ments into overlapping passages, which we do not ﬁnd advan-   that contain answers is small.  See Section 5.2 and Ap-
                   tageous compared to the non-overlapping version.              pendix A.
