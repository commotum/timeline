                                            REALM:Retrieval-AugmentedLanguageModelPre-Training
              Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,    Rajpurkar, P., Jia, R., and Liang, P. Know what you don’t
                Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.        know: Unanswerablequestionsforsquad.arXivpreprint
                Roberta: Arobustlyoptimizedbertpretrainingapproach.           arXiv:1806.03822,2018.
                arXiv preprint arXiv:1907.11692,2019.
                                                                           Ram,P.andGray,A.G.Maximuminner-productsearchus-
              Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efﬁcient       ingconetrees. InProceedingsofthe18thACMSIGKDD
                estimationofwordrepresentationsinvectorspace. arXiv           international conference on Knowledge discovery and
                preprint arXiv:1301.3781,2013a.                               data mining, pp. 931–939,2012.
              Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S.,        Roberts, A., Raffel, C., and Shazeer, N. How much knowl-
                and Dean, J. Distributed representations of words and         edge can you pack into the parameters of a language
                phrases and their compositionality.    In Advances in         model? arXiv preprint arXiv:TBD, 2020.
                neural information processing systems, pp. 3111–3119,
                2013b.                                                     Robertson, S., Zaragoza, H., et al. The probabilistic rele-
                                                                              vance framework: Bm25 and beyond. Foundations and
              Miller, A., Fisch, A., Dodge, J., Karimi, A.-H., Bordes, A.,    Trends in Information Retrieval, 3(4):333–389,2009.
                and Weston, J. Key-value memory networks for directly
                reading documents. arXiv preprint arXiv:1606.03126,        Sang,E.T.K.andDeMeulder,F. Introductiontotheconll-
                2016.                                                         2003 shared task: Language-independent named entity
                                                                              recognition. In Proceedings of the Seventh Conference
              Min,S.,Chen,D.,Hajishirzi,H.,andZettlemoyer,L. Adis-            onNaturalLanguageLearningatHLT-NAACL2003,pp.
                crete hard em approach for weakly supervised question         142–147,2003.
                answering. arXiv preprint arXiv:1909.04849,2019a.
                                                                           Seo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H.
              Min, S., Chen, D., Zettlemoyer, L., and Hajishirzi,             Bidirectional attention ﬂow for machine comprehension.
                H.     Knowledge guided text retrieval and reading            In International Conference on Learning Representa-
                for open domain question answering.      arXiv preprint       tions, 2016.
                arXiv:1911.03868,2019b.
                                                                           Shen, F., Liu, W., Zhang, S., Yang, Y., and Tao Shen,
              Peters, M. E., Neumann,M., Iyyer,M., Gardner,M., Clark,         H. Learning binary codes for maximum inner product
                C., Lee, K., and Zettlemoyer, L. Deep contextualized          search. In Proceedings of the IEEE International Con-
                wordrepresentations. In Proc. of NAACL, 2018.                 ference on Computer Vision, pp. 4148–4156,2015.
              Peters, M. E., Neumann, M., IV, R. L. L., Schwartz, R.,      Shrivastava, A. and Li, P. Asymmetric lsh (alsh) for sub-
                Joshi, V., Singh, S., and Smith, N. A. Knowledge en-          linear time maximum inner product search (mips). In
                hancedcontextualword representations, 2019.                   Advancesin Neural InformationProcessing Systems, pp.
              Petroni, F., Rockta¨schel, T., Lewis, P., Bakhtin, A., Wu, Y.,  2321–2329,2014.
                Miller, A. H., and Riedel, S. Language modelsas knowl-     Sukhbaatar, S., Weston, J., Fergus, R., et al. End-to-end
                edge bases? arXiv preprint arXiv:1909.01066,2019.             memory networks. In Advances in neural information
              Radford, A., Narasimhan, K., Salimans, T., and Sutskever,       processing systems, 2015.
                I. Improvinglanguageunderstandingwith unsupervised         Weston, J., Chopra, S., and Bordes, A. Memory networks.
                learning. Technical report, OpenAI, 2018.                     arXiv preprint arXiv:1410.3916,2014.
              Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
                Sutskever, I. Language models are unsupervised multi-
                task learners. OpenAI Blog, 2019.
              Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
                Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring
                the limits of transfer learning with a uniﬁed text-to-text
                transformer. arXiv preprint arXiv:1910.10683,2019.
              Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:
                100,000+ questions for machine comprehension of text.
                In Proceedings of the 2016 Conference on Empirical
                Methods in Natural Language Processing, pp. 2383–
                2392, 2016.
