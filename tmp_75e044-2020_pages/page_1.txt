                              REALM:Retrieval-AugmentedLanguageModelPre-Training
                                 Kelvin Guu*1 KentonLee*1 ZoraTung1 PanupongPasupat1 Ming-WeiChang1
                                          Abstract
                      Language model pre-training has been shown to
                      capture a surprising amount of world knowledge,
                      crucial for NLP tasks such as question answer-
                      ing. However, this knowledge is stored implic-
                      itly in the parameters of a neural network, requir-
                      ing ever-larger networks to cover more facts. To
                      capture knowledge in a more modular and inter-
                      pretable way, we augment language model pre-
                      training with a latent knowledge retriever, which
                      allows the model to retrieve and attend over doc-
                      uments from a large corpus such as Wikipedia,
                      used during pre-training, ﬁne-tuning and infer-
                      ence. For the ﬁrst time, we show how to pre-
                      train such a knowledge retriever in an unsuper-
                     vised manner, using masked language model-
                      ing as the learning signal and backpropagating
                      through a retrieval step that considers millions
                      of documents.      We demonstrate the effective-
                      ness of Retrieval-Augmented Language Model                      Figure 1.   REALMaugments language model pre-training with
                      pre-training(REALM)byﬁne-tuningonthechal-                       a neural knowledge retriever that retrieves knowledge from a
                      lenging task of Open-domain Question Answer-                    textual knowledge corpus, Z (e.g., all of Wikipedia). Signal
                      ing(Open-QA).Wecompareagainststate-of-the-                      fromthelanguage modeling objective backpropagates all the way
                      art models for both explicit and implicit knowl-                through the retriever, which must consider millions of documents
                      edge storage on three popular Open-QA bench-                    in Z—asigniﬁcant computational challenge that we address.
                      marks, and ﬁnd that we outperform all previous
                      methodsbyasigniﬁcantmargin(4-16%absolute
                      accuracy), while also providing qualitative bene-               correctly predict the missing word in the following sen-
                      ﬁts such as interpretability and modularity.                    tence:   “The         is the currency of the United
                                                                                      Kingdom”(answer: “pound”).
         arXiv:2002.08909v1  [cs.CL]  10 Feb 2020
                1. Introduction                                                       In these language models, the learned world knowledge is
                                                                                      stored implicitly in the parameters of the underlying neural
                Recent advances in language model pre-training have                   network. This makes it difﬁcult to determine what knowl-
                shown that models such as BERT (Devlinet al., 2018),                  edgeis stored in the network and where. Furthermore,stor-
                RoBERTa (Liuetal., 2019) and T5 (Raffel et al., 2019)                 age space is limited by the size of the network—to cap-
                store a surprising amount of world knowledge, ac-                     ture more world knowledge, one must train ever-larger net-
                quired from the massive text corpora they are trained                 works, which can be prohibitively slow or expensive.
                on (Petroni et al., 2019).    For example, BERT is able to
                                                                                      To capture knowledge in a more interpretable and modular
                   *Equal contribution    1Google Research.      Correspondence       way, we propose a novel framework, Retrieval-Augmented
                to:  Kelvin Guu <kguu@google.com>, Kenton Lee <ken-                   Language Model (REALM) pre-training, which augments
                tonl@google.com>, Zora Tung <gatoatigrado@google.com>,                language model pre-training algorithms with a learned tex-
                Panupong Pasupat <ppasupat@google.com>, Ming-Wei Chang                tual knowledge retriever. In contrast to models that store
                <mingweichang@google.com>.
                                                                                      knowledge in their parameters, this approach explicitly ex-
                                                                                      poses the role of world knowledge by asking the model to
