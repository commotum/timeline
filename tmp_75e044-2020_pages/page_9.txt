                                              REALM:Retrieval-AugmentedLanguageModelPre-Training
              text is generated with latent selection of relevant tokens.     Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
              This results in a set of model-centric unsupervised align-        Pre-training of deep bidirectional transformers for lan-
              ments between target and source tokens.       Analogously,        guage understanding. arXiv preprint arXiv:1810.04805,
              REALM also generates text with latent selection of                2018.
              relevant documents. A by-product of our method is that
              we offer a set of model-centric unsupervised alignments         Graves, A., Wayne, G., and Danihelka, I. Neural turing
              between text in the pre-training corpus X and knowledge           machines. ArXiv, abs/1410.5401,2014.
              corpus Z.                                                       Guu, K., Hashimoto, T. B., Oren, Y., and Liang, P. Gen-
                                                                                erating sentences by editing prototypes. Transactions
              6. Future Work                                                    of the Association for ComputationalLinguistics, 6:437–
                                                                                450, 2018.
              The work presented here is the minimal instantiation of a
              family of REALM-like approaches where a representation          Hashimoto, T. B., Guu, K., Oren, Y., and Liang, P. S.
              is pre-trained to perform reasoning over a large corpus of        Aretrieve-and-edit framework for predicting structured
              knowledgeon-the-ﬂyduringinference. We are particularly            outputs. In Advances in Neural Information Processing
              optimistic about generalizations of this work to (1) struc-       Systems, pp. 10052–10062,2018.
              tured knowledge,which would result in a generalization of       Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer,
              Peters et al. (2019) where we would also learn the decision       L., and Levy, O.    SpanBERT: Improving pre-training
              of which entities are informative, (2) the multi-lingual set-     by representing and predicting spans.     arXiv preprint
              ting, e.g., retrievingknowledgeina high-resourcelanguage          arXiv:1907.10529,2019.
              to better represent text in a low-resource language, and (3)
              the multi-modal setting, e.g., retrieving images or videos      Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer,
              that can provide knowledge rarely observed in text.               L., and Lewis, M.        Generalization through memo-
                                                                                rization:  Nearest neighbor language models.       ArXiv,
              References                                                        abs/1911.00172,2019.
              Asai, A., Hashimoto, K., Hajishirzi, H., Socher, R., and        Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urta-
                 Xiong, C.    Learning to retrieve reasoning paths over         sun, R., Torralba, A., and Fidler, S. Skip-thoughtvectors.
                 wikipedia graph for question answering. arXiv preprint         In Advances in neural information processing systems,
                 arXiv:1911.10470,2019.                                         pp. 3294–3302,2015.
              Bahdanau, D., Cho, K., and Bengio, Y. Neural machine            Kwiatkowski, T., Palomaki, J., Rhinehart, O., Collins, M.,
                 translation by jointly learning to align and translate.        Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel-
                 arXiv preprint arXiv:1409.0473,2014.                           cey, M., Devlin, J., et al. Natural questions: a benchmark
                                                                                for question answering research. Transactions of the As-
              Berant, J., Chou, A., Frostig, R., and Liang, P. Semantic         sociation for Computational Linguistics, 2019.
                 parsing on freebase from question-answer pairs. In Pro-
                 ceedings of the 2013 Conference on Empirical Methods         Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L.,
                 in Natural LanguageProcessing, pp. 1533–1544,2013.             and Je´gou, H. Large memory layers with product keys.
                                                                                In Advances in Neural Information Processing Systems,
              Brill, E., Dumais, S., and Banko, M. An analysis of the           pp. 8546–8557,2019.
                 askmsr question-answering system. In Empirical Meth-
                 ods in Natural Language Processing, 2002.                    Lee, K., Salant, S., Kwiatkowski, T., Parikh, A., Das,
                                                                                D., and Berant, J. Learning recurrent span representa-
              Chen, D., Fisch, A., Weston, J., and Bordes, A.       Read-       tions for extractive question answering. arXiv preprint
                 ing wikipedia to answer open-domainquestions. In Pro-          arXiv:1611.01436,2016.
                 ceedings of the 55th Annual Meeting of the Association
                 for ComputationalLinguistics (Volume 1: Long Papers),        Lee, K., Chang, M.-W., and Toutanova, K.         Latent re-
                 volume1,pp.1870–1879,2017.                                     trieval for weakly supervised open domain question an-
                                                                                swering. In Proceedings of the Conference of Associa-
              Clark, C. and Gardner, M.      Simple and effective multi-        tion for Computational Linguistics, 2019.
                 paragraph reading comprehension. In Annual Meeting
                 of the Association for Computational Linguistics, 2017.      Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
                                                                                hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.
              Dai, A. M. and Le, Q. V. Semi-supervised sequence learn-          Bart: Denoising sequence-to-sequence pre-training for
                 ing. In Advances in neural information processing sys-         natural languagegeneration,translation, and comprehen-
                 tems, pp. 3079–3087,2015.                                      sion. ArXiv, abs/1910.13461,2019.
