=== METADATA ===
Title: 
Author: 
Subject: 
Keywords: 
=== END METADATA ===

FindingsoftheAssociationforComputationalLinguistics:ACL2024
,pages14024Â–14040
August11-16,2024Â©2024AssociationforComputationalLinguistics
14024
LengthGeneralizationofCausalTransformerswithoutPositionEncoding
JieWang
1
*
,TaoJi
2
*
,YuanbinWu
1
,
HangYan
5
,TaoGui
3
,QiZhang
2
,XuanjingHuang
2,4
,XiaolingWang
1
1
SchoolofComputerScience,EastChinaNormalUniversity,Shanghai,China
2
SchoolofComputerScience,FudanUniversity,Shanghai,China
3
InstituteofModernLanguagesandLinguistics,FudanUniversity,Shanghai,China
4
InternationalHumanPhenomeInstitutes,Shanghai,China
5
ShanghaiAILab
jiewang.cs@stu.ecnu.edu.cn,taoji@fudan.edu.cn,ybwu@cs.ecnu.edu.cn
Abstract
Generalizingtolongersentencesisimportant
forrecentTransformer-basedlanguagemod-
els.Besidesalgorithmsmanipulatingexplicit
positionfeatures,thesuccessofTransform-
erswithoutpositionencodings(NoPE)pro-
videsanewwaytoovercomethechallenge.
Inthispaper,westudythelengthgeneraliza-
tionpropertyofNoPE.Wendthatalthough
NoPEcanextendtolongersequencesthan
thecommonlyusedexplicitpositionencod-
ings,itstillhasalimitedcontextlength.We
identifyaconnectionbetweenthefailureof
NoPE'sgeneralizationandthedistractionofat-
tentiondistributions.Weproposeaparameter-
efcienttuningforsearchingattentionheads'
besttemperaturehyper-parameters,whichsub-
stantiallyexpandsNoPE'scontextsize.Experi-
mentsonlongsequencelanguagemodeling,the
syntheticpasskeyretrievaltaskandreal-world
longcontexttasksshowthatNoPEcanachieve
competitiveperformanceswithstate-of-the-art
lengthgeneralizationalgorithms.Thesource
codeispubliclyaccessible
1
.
1Introduction
CausalTransformerhasbeenwidelyappliedin
modernlanguagemodels.Tohelpmodelsrecog-
nizethecorrectorderingofwords,itiscommonto
congureTransformerswith
explicit
positionen-
codings(e.g.,thesinusoidalembeddingsintheorig-
inaldevelopmentofTransformer(
Vaswanietal.
,
2017
),therelativepositionencodinginT5(
Raffel
etal.
,
2020
),andtherotarypositionencodingin
GPTseries(
Suetal.
,
2021
)).Thesetupofposi-
tionfeaturesprovidesexibilitytoincludeprior
knowledgestructureondescribingdistance,butit
alsobringstheproblemof
lengthgeneralization
:
languagemodelstrainedwithin-domainposition
featurescannothandlelongersentences(i.e.,those
without-of-domainpositionfeatures)intesting
*
Equalcontribution.
1
https://github.com/AntNLP/nope_head_scale
Figure1:Lengthgeneralizationfrom
2
Kto
4
K.For
differenttestinglengths(or,positionsofsequences),
dashedlinesdrawthelog-perplexityofmodels(mea-
suredonvalidationsetofthepre-trainingdataset),and
solidlinesrepresenttheentropyofattentionheads(av-
eragedonallheads).
time.Generalizingtounseensentencelengthiscru-
cialinmanylanguagemodelapplicationslikere-
trievalaugmentedlanguagemodels(
Izacardetal.
,
2023
),personalizedlanguagemodels(
Wangetal.
,
2023
),language-model-basedagents(
Parketal.
,
2023
).
Departingfromthestandardwaysofencoding
positions,onemayask(followingtheprinciple
ofparsimony)thataretheexplicitpositionfea-
turesnecessary?Theanswerisno.Bothempir-
ically(
Havivetal.
,
2022
)andtheoretically(
Chi
etal.
,
2023
;
Kazemnejadetal.
,
2023
),thecasu-
allymaskedTransformersareshowntobeable
tosuccessfullymodellanguageswithoutanyprior
positionencoding(
NoPE
).Thendingcallsfora
deeperunderstandingof
implicit
positioninforma-
tioninTransformer-basedlanguagemodels,and
alsoinspiresanewdirectionforlengthgeneraliza-
tion:
withoutexplicitpositionfeatures,canNoPE
generalize?
Inthispaper,westudythelengthgeneralization
propertyofNoPE.Ourmainndingsare,
Â•
Whenextendingtounseensentencelength,NoPE
haslessperformanceloss.However,beyonda
certainrange,NoPEalsofailstoextend,with
1
Ã¡Â‰MÂ¿Â›Â†Â¦RÃ¥Â»Ã¾ÃÂ¢Â£Â§Â˜Ã´ÂªTÃ‘2ÂºÃ _Ã§ÃÃ—
YI-ÂÂŒl`Â®ÃµÃ®;`Ã•Ã§Ã¦AÃH:Â´Â®Â Ã¡YHb1Â“ÂµÃ¢Â»HÃ‡{Â¸wÂƒÃ‡R^YÃ‘Âš,
Â $FÂ¨Ã”Â–HDÃ™Ã²V?pÃ²yÃ‘Â—Â¹PÃ%F=Â»Â‚Ã‚Ã‡Ã†BÂ‡Ã¶Ã™Ã¶Â¡GKÃ˜PhÂ³~NÂ´[Â¸IÃŠ_ÃªÂˆÂ€CÃ°TzÃÃ’f Ã§ <Ã“Â–Â”ÂŒÃ¼Â`4|mÂƒE1Ã–J'Â¼Ã½Ã¡Ã$`}R Â‚ÃšÂ¨Â€Ã¢N2kfÂ¢aÃ³ouJ73ÂÃªÂ´Ã²vÃ¡ÃªÂ¿Ã´KÃ›W{ÂœMÃ“Ã3?Ã—ÂÂÂ©MÃ²]Ã¹:W;vÂÂ…Â˜XÃ¢;Â„BÂ’ Â‰Â§XÂ.PÃ¡I;?ÂŸ!TÂ¤Âˆ .Â·sÃ¬Ã¶U]Ã¤Â®ÂƒÂ’=Ã‘XÂŠÃ¾Ã½Â’5Â´Ã€ÂŠÂ€rÂ€Mt>y=Â¯Â†*Ã£ÂbCÂ¾~^Â½tÃ…ArÂ±Â¡l'Â¶Ã¸Ã…Â¦Â•Ã¼Ã³Ã´Ã·Ã˜ÂŸÃ‰LÂÃ«ÂƒÂ¡PÂ“&7ttÃ¶uGz;Kg4ÃšÃ³Ã‘Â;Â·u
Ã©g]GÂÃ†ÂŠ/eÃ†Ã¨Ã˜TÃÃ”_Sf@ÃŒeÂÂ´EA_Â¨ZÃ¶Â½Ãš*qÂ Ã—ÂšÂƒÃ¢Ã¼Ã™"ÃšÂ›<Ã®Ã´Â¯ÂÂ’Ã„`ÃoÃ“l
14025
Figure2:
UniformScale
modiesthetemperaturehyper-parameterofthe
SoftMax
operatorinself-attentionlayers
(Left,NoPE;Right,RoPE).NoPEcangeneralizetolongercontextbymerelyscalingthesoftmaxscores.However,
thisexacttechniquedoesnotdirectlyapplytoRoPEmodels.
nosubstantialdifferenceobservedwhencom-
paredtoexplicitpositionencodings.Forexam-
ple,NoPEcaneffectivelyextendthetraining
lengthby
20%
(from
2
Kto
2
:
4
K,Figure
1
)with-
outasignicantincreaseinperplexity.Incon-
trast,therotarypositionencoding(RoPE)isonly
capableofextendingby
10%
.
Â•
WeanalyzethefailurecasesofNoPE'sgeneral-
izationandndthattheyalwaysco-occurwith
thedistractionofattentiondistributions:theat-
tentionheadsbegintoallocatetheirweightsto
tokensevenlywhenNoPE'sextensionperfor-
mancebeginstocollapse.Theconnectionbe-
tweenNoPE'sgeneralizationandconcentration
ofattentionheadssuggestscontrollingthebehav-
iorsofattentionheadsduringlengthextension.
Â•
Weshowthatbysimplysearchingonetempera-
turehyper-parameter,NoPE'slengthgeneraliza-
tioncanbesignicantlyimproved.Forexample,
byscalingtheattentionscorebyafactorof
1
:
2
,
NoPEcanimmediatelygeneralizetoover
4
K
tokens(Figure
1
).
Â•
Moreover,wedevelopedanadvancedversionof
thisstrategybysearchingtemperatureparame-
tersforeachhead,inthelightthatdifferentlayers
andheadsexhibitvariedbehaviors.Theproce-
dureresemblesaparameter-efcientne-tuning,
withanextremelysmallnumberoftunablepa-
rameters(
704
deltaparametersover
1
Bmodel
parameters).Weshowthattheproposedmethod
canhelpNoPEtogeneralizefurther(Figure
4
).
Weconductlengthgeneralizationexperiments
onlongsequencelanguagemodeling,synthetic
tasks(passkeyretrieval),andLongBench.There-
sultsshowthatNoPEenjoysacompetitiveexten-
sionperformancestostate-of-the-artlengthgener-
alizationmethodsforexplicitpositionencodings
(e.g.,PI(
Chenetal.
,
2023
),YaRN(
Pengetal.
,
2024
)).
2LengthGeneralizationofNoPE
2.1LanguageModelingwithNoPE
Beforedivingintothelengthgeneralizationprob-
lem,werstbrieydescribetheNoPEmodelsused
inthispaper.
2
OurdefaultNoPEhas
1
:
1
Bparame-
ters.ItistrainedfromtheTinyLlama(
Zhangetal.
,
2024b
)codebase
3
,withtrainingsequencelength
L
=2048
and
50
Ksteps(

100
Btokens).More
detailscanbefoundinSection
4.1
.
WealsoincludetheoriginalTinyLlamamodel
whichusesrotarypositionencoding(RoPE)for
comparison.Bydefault,bothmodelsaretrained
withidenticalsettings.
2.2LengthGeneralization
Givenalanguagemodel(LM)withpre-trained
maximalsequencelength
L
,thegoaloflength
generalizationistoexpandittolength
L
0
>L
.
Lengthgeneralizationcanbetestedinazero-shot
manner(Â“trainshort,testlongÂ”)orwithsomene-
tuning.
Figure
1
depictslanguagemodelingperfor-
mancesofNoPE(andRoPE).Wecanobservethat,
withinthepre-traininglength(
L
=2048
),NoPE
hasasimilarperformanceasRoPE,whichagrees
withexistingworks:casualmaskingcanimplicitly
encodethepositionsofasequence(
Havivetal.
,
2022
;
Chietal.
,
2023
).
Whenthetestingsequencelengthexceedsthe
traininglength,weseethat1)NoPE'slengthgen-
2
Forsimplicity,wereferNoPEtoboththeimplicitwayof
encodingpositionsandthelanguagemodeltrainedwithout
positionencoding.
3
https://github.com/jzhang38/TinyLlama
2
Â¼[!<VHÂ¹Âª]JÂŒÂ¶ÂÂ™Ã¡,Â¸Ã Â»"Ã³MC}|
~Â­ÂˆpÃ½Ã—Â‡3ÂÂ»a
14026
eralizationerror(lightbluedashedline,measured
withlog-perplexity)islowerthanRoPE(lightred
dashedline).2)vanillaNoPEstillhasanincreased
perplexitythanin-domaintests.Therefore,though
itisnotaperfectsolution,removingexplicitpo-
sitionencodingcaneffectivelyreducethelength
generalizationerror.Next,wewilltrytondthe
reasonforthefailureofNoPE'slengthgeneraliza-
tion,andalsodevelopalgorithmsforimproving
it.
2.3Extension?Attention!
ToanalyzeNoPE'sgeneralizationfailure,werst
seethatsinceexplicitpositionencodingshavebeen
dropped,thecasualTransformerblockisonlyleft
withthreecoremodules,theembeddinglayer,feed-
forwardlayers,andself-attentionlayers.Theout-
putsoftheformertwomodulesareindependentof
theirinputs'positioninsequence(i.e.,nomatter
whichposition,theyalwayshavethesameoutput).
Therefore,multi-headattentionlayersbecomeour
maintarget.
WevisualizetheattentionpatternofNoPEat
differentlengths.Specically,givenavalidation
setwithasize
n
andatargetposition
i
,wedene
theaverageattentionentropy
H
i
atposition
i
,as
H
i
=
1
n

m
X
x;h
H
(
h
)
i
(
x
)
(1)
H
(
h
)
i
(
x
)=
 
i
X
j
=1

(
h
)
ij
(
x
)

log

(
h
)
ij
(
x
)
(2)
where
x
isasample,

(
h
)
ij
(
x
)
istheattentionprob-
abilityoftoken
i
focusingontoken
j
inthe
h
-th
attentionhead(
h
2f
1
;
2
;:::;m
g
),
H
(
h
)
i
(
x
)
isthe
entropyoftheattentiondistribution

(
h
)
ij
(
x
)
evalu-
atedatposition
i
.
ThelightsolidlinesinFigure
1
showtheaverage
entropyforNoPE(lightblue)andRoPE(lightred).
Wecanobservethat,
theinectionpointof
H
i
is
highlyconsistentwiththeinectionpointofper-
plexity
.Itimpliesthatfailedlengthgeneralization
ofNoPE(andRoPE)mightbeconnectedtothe
distractionofattention:attentionheadsbeginto
allocateattentiontomoretokens.Tofurtherverify
theconnection,wealsodrawasuccessfulextension
algorithmforRoPE(RoPE-NTK(
bloc97
,
2023b
)
whichinterpolatesout-of-domainencodingstoin-
domainencodings).Itslengthgeneralizationloss
curveisat,whileitsentropycurvealsohasno
steeplyincreasingpoint.
Unlikeexplicitpositionencodings,NoPEhasno
cleartargetobjectstomanipulate,thusitisquite
challengingtoperformlengthgeneralizationwith-
outne-tuningonlongersequences.However,the
strongcorrelationbetweenlengthextensionand
attentionpatterntransitionsuggestssuchanobject,
theentropyofattentionheads.
2.4UniformAttentionScale
Wewritethegeneralscaleddot-productattention
as

(
h
)
ij
=
e

q
(
h
)
i

k
(
h
)
j
P
k
e

q
(
h
)
i

k
(
h
)
k
(3)
wherethescalingfactor

isthetemperaturehyper-
parameterofthe
SoftMax
operator.Theprevalent
settingis

=
1
p
d
.
BasedonobservationsinSection
2.3
,weknow
thatNoPE'sfailureoflengthgeneralizationmight
becorrelatedwithdistractedattention,hencewe
cantrytograduallyincreasethescalefactor

to
reconcentrateattention,andseewhetherthegener-
alizationerrorcanbereduced.Figure
2
visualizes
theaverageentropyunderdifferentscalevalues
andthecorrespondingperplexitycurves.
Werstndthatwhenincreasingthescalefactor
duringlengthgeneralizationevaluation(e.g.,the
pre-trainingscale

=
1
p
d
isincreasedto

=
1
:
2
p
d
),
theinectionpointsofentropycurvesareshifted
tolongerlengths,atthesametime,NoPEallgen-
eralizetofurtherpositions(
L
=2k
!
L
0
=4k
).That
is,withallNoPE'sparametersfrozenandonly
uni-
formly
increasingthesoftmax'stemperature,NoPE
cansuccessfullygeneralizetounseenlengths.
Thesameconclusiondoesn'tholdforRoPE(Fig-
ure
2
Right):nomatterwhatvaluethescaletakes
(from

=0.8to

=1.4),theinectionpointsofen-
tropycurvesremainalmostunchanged,meaning
thatitfailstogeneralizetolongerlengths.On
theotherside,successfulRoPEextensionalgo-
rithms(e.g.,RoPE-NTKinFigure
1
)cancontrol
thedistractionofentropybyexplicitlymanipulate
positionencodings.Therefore,thoughattention
scalinghasbeenusedforRoPE(
Su
,
2021
;
Chiang
andCholak
,
2022
),itmaycontributemarginallyto
RoPE'sgeneration.
WealsondthatextendingNoPEtomoredis-
tantpositionsgenerallyrequiresalargerscale(i.e.,
amoreconcentratedattentiondistribution).Asthe
positionbecomesfurther,thenumberoftokensin-
volvedintheattentioncalculationincreases,the
attentionismoreeasilyscattered,andtherefore,a
3
14027
Figure3:Theattentionentropyacrossallheadsforthe
originalNoPE,head-basedscaledNoPEanduniform-
scaledNoPE,witheachmodelrepresentedinaseparate
row.Theattentionheadsexhibitdivergentpatterns.
largerscalingfactorisneededtoconcentratethe
attention.Inparticular,forourNoPEmodel,gen-
eralizingtotwicethepre-traininglengthrequires
about1.2timesthescale,fourtimesthelengthre-
quiresabout1.5timesthescale,andeighttimes
thelengthrequiresabout1.8timesthescale.Ap-
pendix
B
reportsthettedfunctionofthescaling
factorwithrespecttothegeneralizationlength
L
0
.
Finally,wenoteremarkthattheattentionscaling
factorinthissectiontakesthe
same
valueforall
positions,includingthepre-traininglength(
uni-
form
scaling).Weexperimentedwithapiecewise
functionwhichusetheoriginalscalewithinthe
pre-trainingpositions,andamoreconcentratedat-
tentionscalefortheextrapolatedpositions.Wealso
tryposition-dependentfunctions,wherethescale
increaseswithposition.However,noneofthese
methodscouldfurtherimprovegeneralization.We
speculatethatiftheattentionatearlierpositionsis
nothighlyconcentrated,thelearnedtokenrepresen-
tationsmayhindertheconcentrationofattentionat
latterpositions.Weleaveadeeperdiscussionand
analysisofthisobservationinfuturework.
3Head-basedAttentionScale
Afterverifyingthattheattentionscalingcanhelp
NoPEgeneralizing,wedelveddeeperintothe
multi-headattentionmechanismandposedanew
question,Â“
Doeseachattentionheadrequirea
uniquescalingfactor?
Â”
Inthissection,werstvisualizetheaverageen-
tropycurvesforeachheadandndthattheyhave
differentattentionpatterns.Henceweproposeto
replacetheuniformscalingwithhead-basedscal-
ing(fromonefactorto
22

32=704
factors).To
addresstheissueofanexplodingsearchspace,we
efcientlydeterminethevaluesofscalingfactors
Figure4:Comparinguniformandhead-basedscale
(denotedas

(
h
)
).UniformScalefailseventuallyasthe
perplexityincreaseswithlongersequences.HeadScale
iscapableofhandlingmuchlongercontextbyassigning
differentscalefactorstoeachattentionhead.
throughautomatedhyperparametersearch,consid-
eringbothparameterefciencyanddataefciency.
Asaresult,head-basedscalinggeneralizesbetter
thanuniformscaling.Moreover,correlationanal-
ysisshowsthatwithineachlayer,thesmallerthe
convergedentropy(i.e.,themoreconcentratedat-
tention),thelargertherequiredscalingfactorto
maintainthatconcentration.
3.1VisualAnalysis
Theentropyvaluesspanabroadspectrum,with
eachattentionheaddemonstratingadistinctatten-
tionpattern.InFigure
3
,certainattentionheads
showahighlyconcentratedpattern,withentropy
valuesconvergingto

1
,whileothersexhibita
highlydispersedpattern,withentropyvaluescon-
vergingto

10
.Thefullheadvisualizationof
Figure
3
islocatedinAppendix
D
.
Thisphenomenoncastsdoubtonuniformscal-
ingÂ—howcanasinglescalingfactorcatertodi-
verseattentionheads?Inspiredbythis,wefurther
proposeahead-basedscalemethod.
3.2Head-basedScale
Wereformulatetheuniformattentionscaleashead-
baseattentionscales

(
h
)
ij
=
e

(
h
)
q
(
h
)
i

k
(
h
)
j
P
k
e

(
h
)
q
(
h
)
i

k
(
h
)
k
(4)
where

(
h
)
isauniqueattentionscalingfactorfor
eachhead,totaling704.Comparedtoauniform
attentionscale,704head-basedscalesmakeitdif-
culttodeterminetheoptimalvaluesbygridsearch.
SimilartoAutoML(
Heetal.
,
2021
),wemodel
thescales'optimalsearchasaparameter-efcient
ne-tuningtask.GivenaNoPEmodel
M
andaset
4
Â±Ã´Ã¶Â¥PY
ÂÂ–I|^ÃIÃ¹ÃŸÂ¹AK#ÃÂ˜Ã‚ÃÃ²Ã¿^Â–Â©Ã„ÃµÂ»"Ã¨ÃÃ™Ã¸3Ã-i[Â“Ã›_EÂ˜Ãµmm5SEÂ¹Â”Ã—T|TQÃ–+?59tÂ‘RÃ Â©Â™Ã€'Â©Â¤
K|[ÃœÂ†Ã¼Q]Â¦Â¶ÃÂ§ Â²ÃºÃ”SÃ€ÂÂ•@2O-OHQ%DÂuÂ€eÃŠ4Ãµ3Â‰"IÃ²Ã–'UÃ©ÃÃ›Â¥C1yÂ¯Â½Â¥Â©Ã»Â†dÂ’BEcFM1Ã­Ã‹Ã²RLÃÂ­xÂ§en<Ã‘ÂŠIÃ­sÂ§Â·PLÂ†@ÃÂŠÃ‰ÂZ1Â©MÂ¼nÃ‰Ã¹ Ã‰$98#DÂ†LÂ’Â¤@0Â©Â—Â«6Â“Ã†-Â˜dj>Â“ÂºÃ§Â½PrÃ™ÃÃˆ$Â©Âª}Â¤ÂÃ´1Â²ÃªM#ÂÃŸÃ´V|&Â³GÂÃ»CÃ¡Ã®Â>2Ã‰ÂÃ™Ã¬Â‹,ÃÂ—fÂ¶}Ã¦Â¸;Â½:rÃœÂ‘ÂŸÃÃ·!J }Â„Â‘ÂœIÃ®CÃ°ÂIÃ®#Â“0Â“Ãœ1Ã·ÂX2YT2Â¦6Â•Â„Ã–ÂœTÂ’IÃ¸Â Â’ÂÃ¡dÃª;Ã’Ã–Â™Ã£Ã®j ÃŒqYÃ¿Â‘Ã‹>Â©ÃŒeÃ†lÃ¦Â²k"sÃ™Â‡Â¥BÃÂ»ÂÂ *ÂˆLZVfÂ­Â“ÂœÂ‘aopFZ*Ã ÂŒÂ´Â"k9Ã®Ã†ÂÂ—Â€?ÂŠÃÃŒÃŸÃ«Â¥Ã‡Â¤Â¡Ã¾HÂ‘Ã¸#M&Ã°GÂ…ÃUÂOÂ¢Ãº{Ã˜ Ã²5Ã¼ HÂ†Ã 
"
14028
Figure5:Correlationanalysisforhead-basedscale
whenextendedto8Kcontext.Theanalysiswascon-
ductedontheconvergedentropyvaluesat8Kposition,
inrelationtothescalesearched.Eachdatapointrepre-
sentsauniqueattentionhead.
ofhead-basedscales
f

(1)
;
(2)
;:::;
(
m
)
g
,wex
themodel
M
anddenethehead-basedscalesas
trainableparameters

=
f

(1)
;
(2)
;:::;
(
m
)
g
.
Weaimtondanoptimalsetofvalues


=
f


(1)
;

(2)
;:::;

(
m
)
g
,thatallowsthemodel
M
(


)
tosuccessfullyextendtothetargetlength
L
0
.Tothisend,weoptimizethelanguagemodel-
inglossfunction
L
LM
onthepre-trainingdataset
D
withlength
L
0
andsize
n
0
;n
0

n
.


=
minimize
x
2
D
L
LM
(
M
(
;x
))
(5)
Thesearchprocessishighlyefcient.(1)Thenum-
beroftunableparametersisextremelysmall,only
704deltaparametersover1Bmodelparameters;
2)Theamountoftrainingtokensforne-tuningis
extremelysmalltoo,only0.03%ofthepre-training
data.
Inaddition,toensurethattheattentionisrecon-
centratedinsteadofdistractedbythescalingfac-
tors,weapplyafocusconstraintduringtheopti-
mizationofEquation
5

(

)

1
p
d
(6)
InitializingHeadScale
Inpractice,wefound
thattheinitialvalueofhead-basedscaleshasa
signicantimpactonthesearchof


.Anobvious
approachistousethedefaultvalue

(

)
=
1
p
d
from
thepre-trainingphase.However,itslengthgeneral-
izationresultsarequiteunstable,withmostbeing
subpar,astheoptimalscaleoftendeviatessigni-
cantlyfromthedefaultvalue.Weproposeanother
approachtoutilizethebestuniformscalefromthe
gridsearchastheinitialvalue.Theablationstudy
fortheinitializationapproachisinSection
4.5
.
Figure
4
comparesthetwogeneralizationmeth-
odsofNoPE,uniformscaleversushead-based
scales.Head-basedscaleexhibitsbettergeneral-
izationthantheuniformscale,achievingalower
log-PPLby0.2at4Kpositions(
2

L
)andby0.8
at8Kpositions(
4

L
).Theaverageentropy
H
i
ofthehead-basedscaleishigherthanthatofthe
uniformscale,suggestingthattheuniformscale
methodover-concentratesattention,particularlyfor
someheadsthatinherentlyhavemoredistracted
patterns.
Figure
5
showsthecorrelationbetweenthecon-
vergedentropyandthesearchedscale.Tosave
space,weuniformlysampled7layersandalltheir
respectiveheads.Weobservedthatthecorrelation
islayer-dependent,withineachlayer,headswith
moreconcentratedattention(i.e.,lowerentropy)
searchedforlargerscales,whileheadswithmore
dispersedattention(i.e.,higherentropy)searched
forsmallerscales.Theresultisasexpected,the
moreconcentratedtheattentionpattern,thelarger
thescalingfactorneededtomaintainitsfocus.Fur-
thermore,weobservedthatattentionheadsinlower
layersaregenerallymoredispersed,whereasheads
inhigherlayersaregenerallymoreconcentrated
(notethatthisisnotstrictlyobserved).
4Experiment
WetrainaNoPEbasemodelfromscratchand
investigateitscapabilityinlengthgeneralization.
Weconductlengthgeneralizationexperimentson
longsequencelanguagemodeling,synthetictasks
(passkeyretrieval),andreal-worldlongcontext
tasks(LongBench).Detailedexperimentsetupcan
befoundinAppendix
A
.
4.1NoPEpre-trainedmodel
ForafaircomparisonwithRoPE,wetrainaNoPE
modelwith
1
:
1
BparametersfromtheTinyLlama
(
Zhangetal.
,
2024b
)codebase
4
.TheNoPEmodel
has
22
layersofTransformerblocks,
32
attention
headsperlayer,
2048
embeddingsize.Themodel
istrainedonSlimpajama(
Sobolevaetal.
,
2023
)
4
https://github.com/jzhang38/TinyLlama
5
Âœ^VXÃ½ÂµÃ‹4ÂˆÂ±*ÂŒÂÃÂ¼ Ã…Ã›Ã¢Ã«_Ã’ uÂÂ»C-Â¸Ã’Ã–KÂ®ÃÂ‘Ã¨fÂŠOWafÂ´v]Ã«Â¾Â¦[Ã¹;ÂIÃ…Ã–T6Ã–Â„Ã¥kÂ®1ÃÃ–Ã›Ã¶Ã“mÃ–Ã³DÃ±ÂU2rÃzÂš8Â—ÃµÂ’Ã§Â„eÂ‘Â™QY^f=Â¹LJÃŠ_Ã²t5 Ã¸Ã…:RÃ«ÂÂ›ÃºahÃŒÂŠ JÂ°Â¦Â¡xÂ¸X6Ã™Â‹Â˜bÂµ{ÂŒÃ½ 56qÂŒÂ²Â6rÂ±Ã¢Â´Â’_Ã¡Â¸I Ã±Â€W6Â¢ÃœÃ¢"o0fÂ¯Â©Â®h/Ã¤Ã¦BÃ‚Â¬ÃµÃ¹26ÂªCyCÃ®=-uÃ¶Â–Â± Â¤KÃ´Ã‘Â£Â‹Â¼Ã±Â¦Â‹AÂuÂ˜ÂÃšÃ€Â½Â‹Â—Â¡ÂXÃ–ÃÂ›ÂÃ»Ã£Ã“ÂÃ¾ÂƒÂ´ÃˆÃÃQ Ã£ÃÂºaÃªÂÂƒÂ£ÂˆÃºEÃ‡Ã‹9GÂÂ¿Â¶@Â­ÂÃQ`mÂ¯s=QÃ ,Â™ÃªÃ­ÂœÃ¥Â¹<Ã¢8&G!Â—Ã¦Â‚Â’;&GyÃ±eYÃ¨:ÃÃ‰QBÃ¦u
14029
ModelAvg.arc_challengearc_easyboolqhellaswagopenbookqapiqawinogrande
RoPE46.1
24.344.959.743.5
29.867.3
53.3
NoPE
46.2
24.0
44.9
58.143.4
31.868.4
52.9
Table1:Commonsensereasoningabilityofthepre-trainedbasemodels.
Model
FTPG19Proof-pile
L
0
Tokens2K4K8K16K2K4K8K16K
OriginalLMs
RoPE--14.5491.4488.5599.53.5303.0432.1759.5
NoPE--14.6326.9
>
10
3
>
10
3
3.5117.4
>
10
3
>
10
3
BLOOM--27.7158.0264.6403.46.974.1176.2334.5
MPT--10.6103.6361.6345.12.870.1
>
10
3
>
10
3
GeneralizationforRoPE
NTK
zero
--14.514.922.880.43.53.34.113.3
YaRN
zero
--14.514.515.017.13.53.33.23.6
PI
fair
4K6M16.015.9551.9
>
10
3
3.83.4307.9633.8
8K13M17.417.117.1752.84.03.63.4406.3
16K30M18.718.418.318.24.33.93.63.6
YaRN
fair
4K6M15.515.4545.2
>
10
3
3.73.4351.5698.2
8K13M15.715.415.5794.63.83.43.2492.8
16K30M15.915.615.415.53.83.53.23.2
PI
raw
4K33M15.215.0623.8951.73.63.3334.4595.5
8K66M15.415.115.0909.63.63.33.0463.0
16K131M15.615.315.014.93.73.33.03.0
YaRN
raw
4K33M15.115.0573.3951.43.63.3358.8656.8
8K66M15.114.814.8816.03.63.33.1501.5
16K131M15.014.814.514.53.63.33.03.0
GeneralizationforNoPE

=
1
:
2
p
d
--15.016.0513.7
>
10
3
3.63.3175.3
>
10
3

=
1
:
5
p
d
--19.020.245.3224.13.93.74.999.2

=
1
:
8
p
d
--30.442.469.1198.85.15.68.538.2

(
h
)
4K6M14.815.3404.5
>
10
3
3.53.2153.4
>
10
3
8K13M15.715.321.1721.73.63.33.2318.5
18K30M18.319.018.830.44.03.73.34.1
Table2:SlidingwindowperplexityofdifferentcontextwindowextensionmethodstestedonPG19andProofPile.
TheÂ“fairÂ”andÂ“rawÂ”versionsofPIandYaRNdifferfromthetrainingdata,asdetailedinAppendix
A
.Thenotation

=

denotesuniformattentionscalebythegivennumber,and

(
h
)
representshead-basedscale.
jointwithStarcoderdata(
Lietal.
,
2023
)by
50
K
steps(

100
Btokens)withsequencelength
L
=
2048
.
AllsettingsarekeptidenticaltothoseofTinyL-
lama,includingthemodelarchitecture,training
data,trainingprocedure,andhyper-parameters,ex-
ceptthattherotarypositionembedding(RoPE)in
TinyLlamaisremoved,makingitaNoPEmodel,
andthelearningrateissetto
3
:
5

10
 
4
.
FollowingTinyLlama,weevaluatethecommon-
sensereasoningabilityoftheNoPEmodeland
reportacc_norminTable
1
.Wecomparewiththe
TinyLlamacheckpointthatistrainedon100Bto-
kens.Thepurposeofthisexperimentistoprove
theNoPEbasemodelperformsonparwithRoPE.
4.2LongSequenceLanguageModeling
Successonlongsequencelanguagemodelingtasks
isessentialforlengthgeneralization.Amethodthat
doesnotperformwellinlanguagemodelingproba-
blywon'thandlereal-worldlong-contexttasks.
Settings.
Toevaluatethelongsequencelanguage
modelingperformances,wetestourNoPE-based
methodsandRoPE-basedbaselinesonPG19(
Rae
etal.
,
2020
)andproof-pile(
Azerbayevetal.
,
2022
)
datasets.Foreachdataset,wesampleasubsetof
thetestsetandevaluateon
2
Mtokensusingsliding
windowevaluation(
S
=256
)suggestedby
Press
etal.
(
2022
).Wereporttheperplexity(PPL)ofthe
modelsinTable
2
.
Mainresults.
Firstly,bycomparingtheorigi-
nallanguagemodels,NoPE'sperplexity(PPL)is
comparabletoRoPE'sforlengthswithinthetrain-
6
14030
Figure6:TheguresillustratethepasskeyretrievalaccuracyforbothRoPEandNoPEmethods.Theverticaldashed
linerepresentsthecontextlengthofthemodels,whichcouldbeeitherthepre-traininglengthorthene-tuning
length.Thetitleofeachsub-gureindicatestheaverageaccuracywithinthemodel'scontextlength.Notably,NoPE
demonstratesrobustperformanceevenbeyondthemodel'scontextwindow,indicatingsignicantpotentialfor
generalization.
ingdistribution,conrmingthendingsof
Haviv
etal.
(
2022
);
Chietal.
(
2023
).However,allLMs,
includingALiBimodels,failtogeneralizeout-of-
the-distribution,indicatingthatexplicitpositional
encodingisnotthemainreasonfortheirfailurein
generalization.Currentworkonlengthgeneraliza-
tionstillfocusesmainlyonmanipulatingpositional
encoding.Therefore,thelengthgeneralizationis-
suewithincausalTransformernetworkswarrantsa
reanalysisandreinterpretation.
Secondly,bycomparingthetwogeneralization
methodsforNoPEproposedinthispaper,theuni-
formscalemethodhassignicantlimitations.Al-
thoughusingalargerscalecanreducethePPLat
greaterpositions,itsignicantlyaffectsthePPL
atcloserranges.Forinstance,withascalevalue
of1.8,thePPLon2K@PG19risesfrom14.6to
30.4,andon2K@Proof-pile,itrisesfrom3.5to
5.1.Onthecontrary,thehead-basedscalemethod
notonlysuccessfullyextrapolatesto16kbutalso
hasminimalimpactonthePPLatcloserdistances
(for18K,increasesonly+3.7on2K@PG19,+0.5
on2K@Proof-pile),provingthatattentionheads
withdifferentpatternsindeedrequiredistinctscale
values.
Third,afullcomparisonwithRoPELM'sgener-
alizationmethod.Comparingthe
zero-shot
gener-
alizationmethods,thehead-basedscalehasbetter
generalizationthanNTK,butweakerthanYaRN.
InafaircomparisonwiththeRoPEgeneralization
methodswhichrequire
ne-tuning
,thehead-based
scalemethodiscompetitivewiththeseRoPEbase-
lines,especiallytheProof-piledataset.However
RoPEbaselines(PI,YaRN)stillbenetfrommore
trainingtokens,andthehead-basedscaleonNoPE
reachesitsupperlimit.
Insummary,thehead-basedscalegeneraliza-
tionmethodforNoPEslightlyoutperformsRoPE's
earlygeneralizationmethodNTK,butstilllagsbe-
hindtherecentlyintroducedYaRN,particularlyin
near-distancePPLperformance.Consideringthe
signicantchallengeofgeneralizingNoPEcom-
paredtoRoPE(duetothelackofexplicitpositional
encodingtomanipulate),thiswork,astherstto
tacklelengthgeneralizationforNoPE,hasachieved
itssetgoals.
Theobservedgapmayimplythatconstraining
theNoPEmodeltofocusonfewertokenscould
detrimentallyaffectitsefcacy.Futureeffortswill
bedirectedatenhancingthehead-basedscaling
methodtoregainthelevelofperformanceseenin
pretraining.
4.3SyntheticLongContextTasks
AsynthetictaskisconstructedinLandmarkAt-
tention(
MohtashamiandJaggi
,
2023b
)called
"PasskeyRetrieval".Itaimstotesttheeffective
contextwindowsizeofthemodel.Thetaskisto
retrievearandomlyplacedpasskeyfromalongse-
quenceoftokens,wherethepasskeyisarandomly
samplednumberof5digitsandthesequenceis
builtbyconcatenatingirrelevantsentences.
7
EÃ­Ã…ÂµÂ¢ÂœÂÂ¹oÃ‹Ã­Â·rÂ‘Ã³PÃÃ¹^ÃZÂ’2Ã¥mKÃ¢Â¯Ã>BÃ‰WyÂ£Â¼Â„Ã‰oÃ‹ÂÂ’Â¯Ã¾Â¼%ÂµzUÃ¤Â½ÃŒÃ˜Â|Â•j'o`WÃ‹Â²Â»Â¼.rÂ’Ã„Ãµ|Ã²Ã³Ã»Â‰Â²KÃ¦Â–Â˜0Â²ÃŸÂ–eÃŸÃ“Rr9Ã¯Â¿-mÃ©Ã¹Â¬Â¦Ã¬Ã²Â†Ã²Ã¢eGÃ¶Ã›Ã²Â£Ã¬Ã²^LEQÂ¢HÂªÃƒÂ•]ÂœÃÂ¥eÂ•Â"
14031
ModelCtx.Avg.
Singl-DocQAMulti-DocQASummarizationFew-shotLearningSyntheticCode
NQAQspMulFHpQA2WQAMusq.GRptQSumMulNTRECTrQASSumPsgCPsgRLccRe-P
OriginalLMs
RoPE2K16.53.54.717.53.48.82.826.98.4
25.9
33.518.815.71.92.549.540.1
NoPE2K18.36.17.922.46.610.33.1
28.9
8.825.1
41.5
30.03.51.03.048.446.6
GeneralizationforRoPE
PI
raw
4K16.75.48.618.64.59.13.926.49.918.521.521.222.2
2.7
1.548.544.6
8K16.74.79.616.35.49.34.014.69.420.727.023.123.52.13.450.044.7
16K17.24.88.118.65.49.43.822.99.921.324.023.9
25.4
1.61.8
50.5
43.8
YaRN
raw
4K16.2
6.4
8.718.24.011.03.017.59.015.627.521.520.31.60.549.845.2
8K16.46.011.416.05.08.33.516.3
10.3
19.621.024.922.11.32.049.645.3
16K17.74.510.517.15.28.9
4.7
18.99.219.538.024.425.21.71.849.844.6
GeneralizationforNoPE

(
h
)
4K
18.5
6.311.1
23.1
5.710.14.227.78.923.425.5
35.7
13.70.6
4.5
47.9
46.9
8K17.25.811.721.46.110.83.924.18.918.331.031.44.50.63.147.346.5
18K17.06.0
12.8
20.3
7.012.9
4.117.28.416.141.032.95.10.32.144.541.0
Table3:Real-worldLong-ContextperformanceofNoPE-extensionmethodsandvariousRoPEbaselines.TheÂ“Ctx.Â”
columnrepresentstestingcontextlengthduringevaluation,whichcorrespondstoeitherthepre-traininglengthfor
basemodelsortheextendedlengthforlengthgeneralizationmethods.
Model
PPL@16K(
#
)
Passkey(
"
)LongBench(
"
)
PG19Proof-pile

(
h
)
18K30.4
4.18117.0
w/ofocusconstraint
25.9
4.25316.7
w/oinitialization31.44.32615.8
Table4:AblationstudyonthetwovariantsofHead-
Scale.Passkeyresultsarelistedasaverageaccuracy,
andLongBenchresultsareaveragedscoreamongall
sub-tasks.
Settings.
Weevaluatetheperformanceof
passkeyretrievalacrossvariouscontextlengths.
Foreachspeciedcontextlength,weconducttests
on10distinctpasskeydepths,eachassociatedwith
10randomlyselectedpasskeys.Wereportthere-
trievalaccuracyinthistask.
ItisobservedinFigure
6
thatboththeNoPE
basemodelandhead-basedscaleperformwell
evenwhenevaluatingon
2

thepretrainingor
ne-tuningcontextwindow,whileRoPEstrictly
operateswithinthepre-trainedsequencelength
andimmediatelyfailsoutsideofit.Theresultindi-
catesthatNoPEpossessessignicantpotentialfor
generalization.
4.4Real-WorldLongContextTasks
LongBench(
Baietal.
,
2023
)isacomprehensive
assessmentofthelongcontextunderstandingca-
pabilitiesoflargelanguagemodels.Wetestall
modelsusingbeamsearchdecodingwithbeam
size
5
.Theevaluationcontextsizeissettothe
modelcontextwindowaccordinglyinordertotest
themodel'scapabilitytoutilizealongercontext.
WeonlyincluderawPIandYaRNasthebaseline
inthistask.
WendthattheperformanceoftheNoPEbase
modelisbetterthanitsRoPEcounterpart.Con-
cludingbetterinformationutilizationintheorig-
inallength.Moreover,thehead-basedscaleata
4kextensionlengthperformsthebestamongall
baselines.Weattributeittothecapabilityofthe
NoPEbasemodelandthesuccessfullengthgener-
alizationofthehead-basedattentionscalemethod.
Whilethehead-basedmodelstillsuffersfromper-
formancedegradationwhenextendingtoalonger
context,asitisstatedinSection
4.2
.
4.5AblationStudy
WehaveintroducedtwokeycomponentsofHead-
ScaleinSection
3.2
,aconcentrationconstraint
andaninitializingtechnique.Theablationstudy
inTable
4
depictsthatalthoughoccasionallyper-
formbetterinlanguagemodeling,thetwovariants
arelessprefermentinpasskeyretrievalandLong-
Bench,indicatingtheirinabilitytoutilizelongcon-
textinformation.
Detailedresultsofthepasskeyretrievaltaskcan
befoundinFigure
9
intheAppendix
C
.Theyare
completelyunabletoanswerthepasskeyexcept
whenitisatthebeginningofthecontextwindow.
5RelatedWork
Transformerswithoutpositionencoding
Ha-
vivetal.
(
2022
)wasthersttodiscoverthat
causalTransformernetworkscouldperformlan-
guagemodelingtaskssuccessfullyevenwithout
explicitPE.
Chietal.
(
2023
)providedatheoretical
explanationforNoPE,demonstratingthatforan
initializedNoPELM,thevarianceofthehidden
representationsineachlayerisposition-dependent,
withvariancedecreasingforlargerpositions.Both
worksdemonstratethattheNoPEhiddenlayerrep-
8
14032
resentationimpliespositionalinformationthrough
theprobingtask.
Kazemnejadetal.
(
2023
)proved
throughconstructivemethodsthatNoPEcanlearn
absolutePEfromtherstlayerandrelativePE
fromthesecondlayer.Theyalsoshowedthat
NoPEhasanextremelyweaklengthgeneralization
ability(train

20,test

40),butisslightlybetter
thanLMwithexplicitPE.Thispaperrstproposes
lengthgeneralizationmethodsforNoPEwithuni-
formscaleandhead-basedscale.Forthersttime
veriestheeffectivenessofNoPEgeneralizationin
realLLMsettings.
Lengthgeneralization
Duetohighcomputa-
tionalandmemoryrequirements,LLMtraining
isusuallylimitedtoshortinputs.Directlyapplying
LLMstolonginputsfacesthechallengeofout-
of-distribution(OOD)issues.Researchtoenable
LLMstoprocesslonginputshasbeenextensive
(
Huangetal.
,
2023
;
Dongetal.
,
2023
).Theearliest
methodsinvolveddesigningnewrelativePEmech-
anismsduringpre-training(
Pressetal.
,
2021
;
Sun
etal.
,
2023
).Subsequentstudiesfocusedprimar-
ilyonthewidelyusedRoPE(
Suetal.
,
2024
)and
proposedlengthextensionbymitigatingRoPE's
OODissuesthroughinterpolatedpositions(
Chen
etal.
,
2023
;
kaiokendev
,
2023
;
Pengetal.
,
2023
;
emozilla
,
2023
;
bloc97
,
2023b
,
a
).Otherworksem-
ployedslidingwindowattentionmechanismsto
preventrelativepositionsfromexceedingthemax-
imumdistanceseeninpre-training(
Mohtashami
andJaggi
,
2023a
;
Hanetal.
,
2023
;
Xiaoetal.
,
2023
;
Jinetal.
,
2024
;
Zhangetal.
,
2024a
).How-
ever,thesemodelsignoreinformationfromdistant
tokens,thusfailingtocapturelong-distancecontext
dependencies.Allexistingmethodsrelyonspe-
cicexplicitPEs.However,theNoPEarchitecture
ismorestreamlinedandmorealignedtotheform
ofhumanlanguagemodeling.ExploringNoPE's
lengthgeneralizationisthereforemoreintriguing
andattractive.
6Discussion
WestudiedthelengthgeneralizationofCasual
Transformerwithoutexplicitpositionencoding.
Wedevelopedaparameter-efcienttuningalgo-
rithmwhichaimstosearchforthebesttemperature
hyper-parametersforattentionheads.Throughem-
piricalevaluation,wesawthatNoPEcanachieve
competitivelengthgeneralizationandmightbea
promisingalternativeforlong-contextlanguage
modeling.
NoPEprovidesanewperspectivetounderstand-
ingtheroleofpositionalinformationbyisolating
andeliminatingtheeffectsofexplicitpositional
encoding.Ourworkdemonstratesthecorrelation
betweenlengthgenerationfailuresanddistraction
ofattentioninNoPEmodels,thustheproposed
methodconcentratestheattentionbyadjustingthe
scalingfactor.Whilecurrentworksonlengthgen-
eralizationmainlyfocusonmanipulatingpositional
encoding,ourworksuggestsanewkeycomponent
togeneralization.
Limitation
Thelengthgeneralizationalgorithmsdiscussedin
thispaperexhibitcompetitiveperformances,but
theNoPEmodelitselfstillunderperformswith
state-of-the-artRoPEmodels,whichmakesthere-
sultsoverlongsequencelanguagemodelingtasks
andLongBenchtasksarelesscompetitive.NoPE
stillfacesthechallengesofconsiderablememory
usageandcomputationalcomplexityduetothe
quadraticnatureofattentioncomputationwhen
processingextremelylongcontexts.Hardwarelim-
itationsarelikelytobecomeaconstrainingfactor
forlengthgeneralizationsoon.Weplantofurther
improvetheNoPE'sperformancesforafairercom-
parison.Thispaperisalsomostanempiricalone,
whichrequiresadeepertheoreticalunderstanding
ofNoPE'slengthgeneralizationinthefuture.
Acknowledgement
Theauthorswishtothankallreviewersfortheir
helpfulcommentsandsuggestions.Thecorre-
spondingauthorsareTaoJi,YuanbinWuandXiaol-
ingWang.Thisresearchwas(partially)supported
byNSFC(62076097),NationalKeyR&DProgram
ofChina(2021YFC3340700),theOpenResearch
FundofKeyLaboratoryofAdvancedTheoryand
ApplicationinStatisticsandDataScience(East
ChinaNormalUniversity),MinistryofEducation.
References
ZhangirAzerbayev,EdwardAyers,,andBartoszPi-
otrowski.2022.
Proof-pile
.
YushiBai,XinLv,JiajieZhang,HongchangLyu,
JiankaiTang,ZhidianHuang,ZhengxiaoDu,Xiao
Liu,AohanZeng,LeiHou,YuxiaoDong,JieTang,
andJuanziLi.2023.Longbench:Abilingual,mul-
titaskbenchmarkforlongcontextunderstanding.
arXivpreprintarXiv:2308.14508
.
9
14033
BigScienceWorkshop.2022.
Bloom(revision
4ab0472)
.
bloc97.2023a.
AddNTK-Awareinterpolation"by
parts"correction
.
bloc97.2023b.
NTK-AwareScaledRoPEallows
LLaMAmodelstohaveextended(8k+)contextsize
withoutanyne-tuningandminimalperplexitydegra-
dation
.
bloc97.2023c.
NTK-AwareScaledRoPEallows
LLaMAmodelstohaveextended(8k+)contextsize
withoutanyne-tuningandminimalperplexitydegra-
dation.
ShouyuanChen,ShermanWong,LiangjianChen,and
YuandongTian.2023.
Extendingcontextwindowof
largelanguagemodelsviapositionalinterpolation
.
Ta-ChungChi,Ting-HanFan,Li-WeiChen,Alexander
Rudnicky,andPeterRamadge.2023.
Latentposi-
tionalinformationisintheself-attentionvariance
oftransformerlanguagemodelswithoutpositional
embeddings
.In
Proceedingsofthe61stAnnualMeet-
ingoftheAssociationforComputationalLinguistics
(Volume2:ShortPapers)
,pages1183Â–1193,Toronto,
Canada.AssociationforComputationalLinguistics.
DavidChiangandPeterCholak.2022.Overcominga
theoreticallimitationofself-attention.In
Proceed-
ingsofthe60thAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1:LongPapers)
,
pages7654Â–7664.
ZicanDong,TianyiTang,LunyiLi,andWayneXin
Zhao.2023.Asurveyonlongtextmodelingwith
transformers.
arXivpreprintarXiv:2302.14502
.
emozilla.2023.
DynamicallyScaledRoPEfurtherin-
creasesperformanceoflongcontextLLaMAwith
zerone-tuning
.
ChiHan,QifanWang,WenhanXiong,YuChen,Heng
Ji,andSinongWang.2023.
Lm-innite:Simple
on-the-ylengthgeneralizationforlargelanguage
models
.
AdiHaviv,OriRam,OrPress,PeterIzsak,andOmer
Levy.2022.
Transformerlanguagemodelswithout
positionalencodingsstilllearnpositionalinforma-
tion
.In
FindingsoftheAssociationforComputa-
tionalLinguistics:EMNLP2022
,pages1382Â–1390,
AbuDhabi,UnitedArabEmirates.Associationfor
ComputationalLinguistics.
XinHe,KaiyongZhao,andXiaowenChu.2021.Au-
toml:Asurveyofthestate-of-the-art.
Knowledge-
BasedSystems
,212:106622.
YunpengHuang,JingweiXu,ZixuJiang,JunyuLai,
ZenanLi,YuanYao,TaolueChen,LijuanYang,
ZhouXin,andXiaoxingMa.2023.Advancingtrans-
formerarchitectureinlong-contextlargelanguage
models:Acomprehensivesurvey.
arXivpreprint
arXiv:2311.12351
.
GautierIzacard,PatrickS.H.Lewis,MariaLomeli,
LucasHosseini,FabioPetroni,TimoSchick,Jane
Dwivedi-Yu,ArmandJoulin,SebastianRiedel,and
EdouardGrave.2023.
Atlas:Few-shotlearning
withretrievalaugmentedlanguagemodels
.
J.Mach.
Learn.Res.
,24:251:1Â–251:43.
HongyeJin,XiaotianHan,JingfengYang,Zhimeng
Jiang,ZiruiLiu,Chia-YuanChang,HuiyuanChen,
andXiaHu.2024.
Llmmaybelonglm:Self-extend
llmcontextwindowwithouttuning
.
kaiokendev.2023.
Thingsi
Â´
mlearningwhiletraining
superhot
.
AmirhosseinKazemnejad,InkitPadhi,Karthikeyan
Natesan,PayelDas,andSivaReddy.2023.
The
impactofpositionalencodingonlengthgeneraliza-
tionintransformers
.In
Thirty-seventhConference
onNeuralInformationProcessingSystems
.
RaymondLi,LoubnaBenAllal,YangtianZi,Niklas
Muennighoff,DenisKocetkov,ChenghaoMou,Marc
Marone,ChristopherAkiki,JiaLi,JennyChim,
QianLiu,EvgeniiZheltonozhskii,TerryYueZhuo,
ThomasWang,OlivierDehaene,MishigDavaadorj,
JoelLamy-Poirier,JoÃ£oMonteiro,OlehShliazhko,
NicolasGontier,NicholasMeade,ArmelZebaze,
Ming-HoYee,LogeshKumarUmapathi,JianZhu,
BenjaminLipkin,MuhtashamOblokulov,Zhiruo
Wang,RudraMurthy,JasonStillerman,SivaSankalp
Patel,DmitryAbulkhanov,MarcoZocca,MananDey,
ZhihanZhang,NourFahmy,UrvashiBhattacharyya,
WenhaoYu,SwayamSingh,SashaLuccioni,Paulo
Villegas,MaximKunakov,FedorZhdanov,Manuel
Romero,TonyLee,NadavTimor,JenniferDing,
ClaireSchlesinger,HaileySchoelkopf,JanEbert,Tri
Dao,MayankMishra,AlexGu,JenniferRobinson,
CarolynJaneAnderson,BrendanDolan-Gavitt,Dan-
ishContractor,SivaReddy,DanielFried,Dzmitry
Bahdanau,YacineJernite,CarlosMuÃ±ozFerrandis,
SeanHughes,ThomasWolf,ArjunGuha,Leandro
vonWerra,andHarmdeVries.2023.
Starcoder:may
thesourcebewithyou!
IlyaLoshchilovandFrankHutter.2017.
Decoupled
weightdecayregularization
.In
InternationalConfer-
enceonLearningRepresentations
.
AmirkeivanMohtashamiandMartinJaggi.2023a.
Landmarkattention:Random-accessinnitecontext
lengthfortransformers
.
AmirkeivanMohtashamiandMartinJaggi.2023b.
Random-accessinnitecontextlengthfortransform-
ers
.In
Thirty-seventhConferenceonNeuralInfor-
mationProcessingSystems
.
MosaicMLNLPTeam.2023.
Introducingmpt-7b:A
newstandardforopen-source,commerciallyusable
llms
.Accessed:2023-05-05.
JoonSungPark,JosephO'Brien,CarrieJunCai,Mered-
ithRingelMorris,PercyLiang,andMichaelS.Bern-
stein.2023.
Generativeagents:Interactivesimulacra
10
14034
ofhumanbehavior
.In
Proceedingsofthe36thAn-
nualACMSymposiumonUserInterfaceSoftware
andTechnology
,UIST'23,NewYork,NY,USA.
AssociationforComputingMachinery.
BowenPeng,JeffreyQuesnelle,HongluFan,andEn-
ricoShippole.2023.
Yarn:Efcientcontextwindow
extensionoflargelanguagemodels
.
BowenPeng,JeffreyQuesnelle,HongluFan,andEnrico
Shippole.2024.
YaRN:Efcientcontextwindowex-
tensionoflargelanguagemodels
.In
TheTwelfth
InternationalConferenceonLearningRepresenta-
tions
.
OrPress,NoahSmith,andMikeLewis.2021.Train
short,testlong:Attentionwithlinearbiasesenables
inputlengthextrapolation.In
InternationalConfer-
enceonLearningRepresentations
.
OrPress,NoahA.Smith,andMikeLewis.2022.
Train
short,testlong:Attentionwithlinearbiasesenables
inputlengthextrapolation
.
JackW.Rae,AnnaPotapenko,SiddhantM.Jayakumar,
ChloeHillier,andTimothyP.Lillicrap.2020.Com-
pressivetransformersforlong-rangesequencemod-
elling.
InternationalConferenceonLearningRep-
resentations,InternationalConferenceonLearning
Representations
.
ColinRaffel,NoamShazeer,AdamRoberts,Kather-
ineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJ.Liu.2020.
Exploringthe
limitsoftransferlearningwithauniedtext-to-text
transformer
.
JournalofMachineLearningResearch
,
21(140):1Â–67.
Soboleva,Daria,Al-Khateeb,Faisal,Myers,Robert,
Steeves,JacobR,Hestness,Joel,Dey,andNolan.
2023.
SlimPajama:A627Btokencleanedanddedu-
plicatedversionofRedPajama
.
JianlinSu.2021.
Attention
Â´
sscaleoperationfromen-
tropyinvariance
.
JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,
WenBo,andYunfengLiu.2024.Roformer:En-
hancedtransformerwithrotarypositionembedding.
Neurocomputing
,568:127063.
JianlinSu,YuLu,ShengfengPan,BoWen,andYunfeng
Liu.2021.
Roformer:Enhancedtransformerwith
rotarypositionembedding
.
CoRR
,abs/2104.09864.
YutaoSun,LiDong,BarunPatra,ShumingMa,Shao-
hanHuang,AlonBenhaim,VishravChaudhary,Xia
Song,andFuruWei.2023.
Alength-extrapolatable
transformer
.In
Proceedingsofthe61stAnnualMeet-
ingoftheAssociationforComputationalLinguis-
tics(Volume1:LongPapers)
,pages14590Â–14604,
Toronto,Canada.AssociationforComputationalLin-
guistics.
HugoTouvron,LouisMartin,KevinStone,PeterAl-
bert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale,DanBikel,LukasBlecher,CristianCanton
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
CynthiaGao,VedanujGoswami,NamanGoyal,An-
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
bog,YixinNie,AndrewPoulton,JeremyReizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
RuanSilva,EricMichaelSmith,RanjanSubrama-
nian,XiaoqingEllenTan,BinhTang,RossTay-
lor,AdinaWilliams,JianXiangKuan,PuxinXu,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
MelanieKambadur,SharanNarang,AurelienRo-
driguez,RobertStojnic,SergeyEdunov,andThomas
Scialom.2023.
Llama2:Openfoundationandne-
tunedchatmodels
.
AshishVaswani,NoamShazeer,NikiParmar,Jakob
Uszkoreit,LlionJones,AidanNGomez,ukasz
Kaiser,andIlliaPolosukhin.2017.
Attentionisall
youneed
.In
AdvancesinNeuralInformationPro-
cessingSystems
,volume30.
ZekunMooreWang,ZhongyuanPeng,HaoranQue,
JiahengLiu,WangchunshuZhou,YuhanWu,
HongchengGuo,RuitongGan,ZehaoNi,Man
Zhang,etal.2023.Rolellm:Benchmarking,elic-
iting,andenhancingrole-playingabilitiesoflarge
languagemodels.
arXivpreprintarXiv:2310.00746
.
GuangxuanXiao,YuandongTian,BeidiChen,Song
Han,andMikeLewis.2023.
Efcientstreaming
languagemodelswithattentionsinks
.
PeitianZhang,ZhengLiu,ShitaoXiao,NingluShao,
QiweiYe,andZhichengDou.2024a.
Soaringfrom
4kto400k:Extendingllm'scontextwithactivation
beacon
.
PeiyuanZhang,GuangtaoZeng,TianduoWang,and
WeiLu.2024b.
Tinyllama:Anopen-sourcesmall
languagemodel
.
AExperimentSetup
Searchingscales.
Weapproachthesearchforop-
timalhead-basedscales

(
h
)
byparameter-efcient
ne-tuning.Weusealargelearningrate(LR,=
0
:
05
or=
0
:
1
)forne-tuning,as

spansawiderange,
(e.g.,
[
1
p
d
;
3
p
d
]
,showninFigure
5
).Thene-tuning
datacomesfromthepretrainingdataset(Slimpa-
jama(
Sobolevaetal.
,
2023
)andStarcoderdata(
Li
etal.
,
2023
))withadifferentdatafetchingseed
fromthepretraining.Wesetthebatchsizeto
8
andsettheoptimizertotheAdamW(

1
=0
:
9
,
11
14035

2
=0
:
95
)withoutweightdecay(
Loshchilovand
Hutter
,
2017
).WeuseacosineLRdecayfrom
LRto
0
:
1
LRfor
200
ne-tuningstepsandalinear
warmupfortherst
20
steps.Wefoundthatthe
head-basedscalesearchingon
16
Ksuffersfroma
minorPPLdegradationattheendofthecontext
window.Wesimplyexpandedthelength
L
0
to
18
K
andthensolvedit.
Lengthgeneralizationbaselines.
Tocompare
withmainstreamlengthgeneralizationresearch,we
reproducedthreegeneralizationbaselinesonRoPE,
including:
Â•
NTK(
2023c
),zero-shotgeneralization;
Â•
PI(
2023
),efcientlytrainlong,testlong;
Â•
YaRN(
2024
),supportsbothsettings
5
.
Forthezero-shotsetting,wegrid-searchedthe
baselinehyper-parametersand
reportedtheirbest
results
.Forthebaselinesthatneedne-tuning,
weproposetwosettings,oneforafaircompari-
son,withthesamenumberofne-tunedtokens
(0.3
Â‰
ofpre-traineddata)asthehead-basedscales
searching,andtheotherfollowstheiroriginalpa-
per,whichis1.3
Â‰
ofpre-traineddata.Specically,
wene-tunetheRoPEmodelfor200stepsinthe
Â“fairÂ”version,and1000stepsfortheÂ“rawÂ”version.
Inaddition,weincorporateopen-sourceAL-
iBimodels(
Pressetal.
,
2022
)intoourbaselines,
whichincludeBLOOM1.1B(
BigScienceWork-
shop
,
2022
)andMPT7BBase(
MosaicMLNLP
Team
,
2023
),bothofwhicharetrainedonacon-
textlengthof2K.Wetestazero-shotgeneralization
oftheALiBimodelsfollowingtheoriginalpaper
(
Pressetal.
,
2022
).
BFittedFunctionoftheUniformScale
InthestudydepictedinFigure
7
,ahyper-parameter
searchwasconductedfortheuniformscale

with
anintervalof
0
:
01
p
d
.Thissearchwasappliedtotwo
checkpointsofthepre-trainedNoPEmodel,tot
theoptimal

attheextensionlength.Wenote
remarkthatthescalingfactortakesthe
same
value
forallpositionsduringasingletest.Theoutputof
asingletestistheperplexityacrossallpositions.
Werunmultipletestswithdifferentscalesandnd
thebestoneforeachposition.
5
TheYaRNpaperalsoproposesaÂ“trainshort,testlongÂ”
settingwithlowertrainingcosts.However,forafaircom-
parison,werelaxthissettingtoÂ“trainlong,testlongÂ”which
generalizesbetter.
Basedonthesearchresults,weguessafunc-
tionformthatbesttsthedatapoints.Wethen
tthisfunctionovertherange
i
2
[2048
;
16384]
.
Thettedfunction,alongwithitscorresponding
coefcientofdetermination,ispresentedbelow:
Â•
ForNoPEat10ksteps,thecoefcientofdeter-
mination
R
2
=0
:
9954
.Thettedfunctionis

=
1+0
:
3010ln
s
p
d
Â•
ForNoPEat50ksteps,thecoefcientofdeter-
mination
R
2
=0
:
9773
.Thettedfunctionis

=
1+0
:
3973ln
s
p
d
Inthesefunctions,
s
isdenedas
i
L
foreach
position
i
,representingthemodel'sextensionratio
relativetoitspre-traininglength.
Furthermore,itisalsofoundby
Pengetal.
(
2024
)thattheYaRNmethodbenetsfromasim-
ilaruniformscaleonLLaMA2(
Touvronetal.
,
2023
),althoughthescaledoesnothaveadirect
impactontheRoPEextensioncapability(referto
Figure
2
).ThescaleproposedbytheYaRNmethod
canbeformulatedasfollows,whichisquitesimilar
toourresult.

=
(1+0
:
1ln
s
)
2
p
d
Inconclusion,theoptimaluniformscalevaries
acrossdifferentmodels.Itisalsoobservedfrom
Figure
7
thatuniformscale,despitebeingoptimal,
cannotattentheNoPEmodel'sperplexitywithin
alargecontextwindow.Thisndingunderscores
theimportanceofemployingahead-basedscaling
methodformanagingmodelperplexityeffectively
acrosslargercontextwindows,therebyenhancing
themodel'sperformance.
CAdditionalPasskeyResults
InSection
4.2
,wenotethattheALiBibaselinesdo
notexhibitcompetitiveperformanceintermsofper-
plexitywhenappliedtolongercontexts.Wealso
conductPasskeyRetrievaltestsonthesemodels,
withtheresultsdepictedinFigure
8
.Thesemod-
elsyieldexpectedresultswithintheirpre-trained
sequencelength,buttheyareunabletocomplete
thetaskwhenitexceedsthislength.
InSection
4.5
,weconductedanablationstudy
onHeadScale.Figure
9
showsthepasskeyretrieval
taskofthetwovariationsofHeadScale.
12
14036
Figure7:Fittedoptimaluniformscaleforeachposition.Theredlineindicatesbestlogperplexityfoundateach
position,thebluelineplotsthecorrespondingoptimaluniform

forthatposition,theblackcurveisthetted
functionandtheverticaldottedlineispre-traininglength.
Figure8:TheresultsofpasskeyretrievalforALiBibaselines.Theverticaldashedlinerepresentsthepre-training
length.WhileALiBimodelsdoexhibitperformancebeyondthepre-trainedlength,theirexpansionisnotsubstantial.
DEntropyVisualizationofAllHeads
Figures
10
to
12
showattentionentropyacrossall
layersandallheadsofthe8kextensionhead-based
scalemethod,UniformScaleandtheoriginalNoPE.
Anadditionaltheoreticalupperboundofentropy
isalsoplottedinthegures.Wenotethatforeach
position
i
,themaximumentropyisachievedwhen
8
j;
(
h
)
ij
=
1
i
issatisedinEquation
2
.Themaxi-
mumvalueisthengivenby
H
(
h
)
i
=log
i
.
ItisobservedinFigure
10
thatthelowerlay-
ershavehighentropy,closelyapproachingtheup-
perbound.Mostheadsexhibitconstantentropy
forallpositions.Andtheattentionvaluesspan
abroadspectrum,rangingfrom0totheoretical
upper-bound.
13
Ã‹Â‡ +Ãª188ÃŠ_}ÂÃŒÃ”Â»Â¢Ã¡Ã¡Ã¼&5gWÂªÂ¾6sÃ¼Â ÂµÃ¿ovw%Ã¥Ã˜Â’a6ÃÂ‰'qyÂ—,Â‘Â»Â¶ Â¥Â­ÂœZÃŠÂ“ÂŒÃ¦Ã Ã¡ÂÂ§79Â«Ã™ÂSÃ¼OÂ¢Â«;Â¯Â¿7Â¸Ã”OÃ¡ÂˆÃ›Â‘DÂÂ“Â‹EÂÃ–4Ã¾Â¼Ã¤Â Ã©Â’"i Â·cÂ±Â“Â¬t}Â³<Ã’Ã°
Â«OÂ‘Ã™Ã¥Â­Ã±ÃªnÃ«+Y ;%FLXÂrÂ“ÂÂªÂ¿WÂ–MÃ•Â‰Ã…{Â‡Ã‡ÂŸÃ­Ã™Q
ÃƒÃ‘ÃÂŸÃ3LÃ¾ÂÂuÃÃ }IÃ¶Â˜DÃ³%Â‘8m@Â±Ã¸kÂ¡Ã•`~Ã¥ÃÃ" Â‡nÃœÃ¤Â°bÃ§ÃÂ¢3Ã†=ÃªÃ/Â¹:Ã¤M@Ã°NÂ–ÃºÂ­ÂªÂzÂ™#Â§Ã¿LBuRÂÂ±Ã€IÂ¹ÃŠ!%Â³sÂ–vÂ„pÃº9EnÂ¾r*VkTÂ°Â½Â­Â¶Â¿WÂˆ4Â»,aÂÃŠÃ|Ã¾6vÂ¤VÂ™Ã¨Ã Ã‰Ã©Â£[Ã¢Â“[ÃÂÃ—}|mcÂ¢Ã±K&Â²I$Ã‚Ã‘xÃ° &RÂaÂ…Â9Ã°Â›}Â±Â»Ã±ÃÂ¦/ÃŸSÃºÂŠU/KÂ…Ã|Ã*mÃ¶Â¼Â¬Ã·Ã­Ã©$ÃÂ­ =Ã¢Ã¯KÂ‰+ÃˆÂ”ai^Â_DÂ‰SÂ©-Ã™*Ã‚Ã¾Ã³Â˜Ã¼RÃƒÂ¶pÃˆÂœjÃ†Â¹Â¼vÃ½Â…Ã·6ÃÃ¹Ã®'Â‡Ã§zYÂ¢Ã¤Ã­]ÂÃÃ¨chÂ‹ÃŸqÃ»Ã·kÃŸ+Â‘Â§Ãš$Ã¦:Â¹hÃˆÂ¤Â¯Ã˜Ã¢Â¡Ã®Â–Ã¸R[Â”Â«LScNÂƒ!9gÂ¾Â¸{Â‹_ÂŠTÂ´P^eÃ«SÂªÃŸGÃ€wÂºÃ¸Â6Â¯Â™Ã¸
14037
Figure9:TheresultsofpasskeyretrievalforHeadScalevariations.Theseresultsareanticipatedtoapplytoacontext
lengthof16K,buttheyfailtoretrievethepasskeyunlessitispositionedatthebeginningofthecontextwindow.
14
14038
Figure10:Entropyacrossalllayersandallheadof8kextensionhead-basedscalemethod.Thex-axisistheposition
ofextensionandthey-axisisentropyaveragedoveralltestsamples.Theblackdashedcurveisthetheoretical
upper-boundofentropy.
15
Â¢DÂLÂ‰B
:Ã®Ã»Ã¬VÂ™}CÂ–gdOÃ§0Ã½Â†<F
Ã¢KR"ÃªÂ˜Ã£R?Â¤DÃ´Âˆ{Â¶Ã¬@ÂŠÃƒÃ¨pÂ²9Â‚Â”xVÃµ1A
Ã´rÂ±Â¬ÂœA
Â†ÂÂ¬
Â”RlÂÃ¯
Ã¡`{Â€Ã”Ã¹Â†Ã¢Ã‹ÃÃºÂ‘Â—rÃ7;ÂxÂªÃ³mÃ‰}8Ã§Ã›Ã¶ 9Ã§[2,Ã~Ãˆ
Â¤Ãƒ Â? Â·pJÃµE[Â¿=JE`Ã´$# Ã¾VFObÂ©Â€ÃÂœÃšÂ†ÂœAVE WÂ´ BÃ¤Â
Â‘
ÂÃ¢ÂˆÃ * HÃ¡Â#Â rÂ–Â·NSUYÃ±Ã—Â’Â€Â€ÃºHÂ‘Â0Ã‰Â¶E ÂˆÃ¶~Ã£JÃ¢Â‹Â¤Â±Ã¢Â‡ÃÂ·Â²vÂ¼Â¤Â¡{&~Ã˜NÃ¬{Â¿Ã…Â¿ÃŒ?KÂ·oÃ¥.yÂÃŒaHt$
14039
Figure11:EntropyacrossalllayersandallheadofUniformScalewith

=
1
:
6
p
d
16
Ã‰Ã›Ã¯Ãª?SÃ¾wFÂ ?FDÃºÂ²Â’P"bÃŸÃ™ÃœÃ©kÃºÃÂ–Â£Ã©Â¨LzÂ€~+Âƒa#1Ve"ÃÃ®7~ÃD6Â¹Ã Ã´;:]Â½>Ã½ÃÃƒÃŠDÂT}Ãº=!Â±2Ã›J:&2ÃºA]dGÃ£Â”Â‹Ã¬Â›Ã£"ÂŸÂ¾KÂ•Ã¬aÃ¡";Ã©Â Ã©Ã·ÂÂ¹pÃŠÃ ÂÃƒ.b_ÃƒliÂ¨dÃ«Âˆ5WÂ²Ã¥+Âš+Â±Â…*ÃÃŠ`Ã¬ÃµÃŠÃEPÃ·tÂ…Â¶8
14040
Figure12:EntropyacrossalllayersandallheadoftheoriginalNoPE.
17
Ãˆ>Ã†Ã¥Â½Â¼Ã±Ã»pÂ„Â’Â¼Ã­73Â†AhAÂ²cb aÃ…Ã»MÃˆÂ·Ã½Â‘0ÃˆÃ¢Â‚M&gTlÂ¯pÃ„idG--q~Ã…Â¹=r5ÃŠÃ“bÂƒ#ÃµÃ“dHbÂ„ÃŠÃ¢Ã¢Ã{^ÃƒÃº^Â„&Â™yÃ†AÃ¶Ã Â¹Ã‹Â˜Ã„AxÃl+Ã¨'ÂƒÃ¬Ã½Â¼_W4oÂÃƒÃÂ=bÂÂ­@Ã†y'sB>Ã…Ã³!ÃœHÂ«Â#Â²;&Â„ÂÃ»Ãn*Â¡Ã·ÃµUÂŒbuÃÂ“BÃ¶Ã¹Ã­Ã¬ÃˆÂ†RoÂ˜ÃÂ½Â‰!Â„Ã‰Â‰Ã‰~Â¸ Â„!MvÂ¿Ã»Â®ÃÃ‚]Ã®rhaÃ¼:Â—; ÂƒnÂ‡Â•Ã•"ÂƒÂ Ã†ÃƒÂ Ã¢Ãˆ ÃˆÂ.Â·*AÃª7]Ã«&BÃ‡tÃŸ HfÃ«]Ã»j2ÂƒÃ²Â¯Ã¶Â›Â¸x^Â”Ãbk!ÂÂ+ÂÂ‚Â“ÃF-ÂˆdB >f1Â Â‰ÃÃ'ÃˆÃ‚qÃ»pÃŒÂ½Ã±Ã¡Â— Ã„Ãº
Ã‚Â§`Ã†HÃ·ÂÂ‘Ã¡ÂŸÃŠÂ¶s{ÂŸÂ‚Â’Ã¬ZÃ‡Ã¬86MÃºÃ«Â¹Â·qÃ§q?qDÂÂ®Â£Ã´AEÂ³Ã¼Ã¨ 
Ã¸Â«Â°uF:Ã„G>Ã˜@'PÃÂƒÃ¶Ã˜Â¼mÂµÂµÃ§1Ã¯rÂ­=Â¯Ã…Â¼sÃFgÃ¬ÃŠÃ–Â£Ã—Ã…ÂÃ’ÃÂagÂ„Ã¸AoÃÃÃ¶Â–Ã¾Âœ{Ã¶Ã›sÂªÃÂ¥OÃ»Â·sÃnÃµ!UÃ·Ã¬[Ã’Â»gÃŸMÃ³Ã=Ã»Ã­5rÃ®Ã™Â½ÃŒÂ“`Ã°6Ã“wÃªÃ”Â©Â½Â¥Ã”C*yÂ·'AÃªÂª>!ÃµQknÃ›Ã”Ã„Ã›#sÃ¡oÃ™ÂÂ“Ã§WÃ«Ã¥%Ã±Ã–zHÃ¡Â P~Ã—'ÃÃÂ®PMÂ¼Ã™|Ã
mÂ’xÂ³Â½kÃ±Ã»}Ã¦Ã­yÂšyÂ›xE{iÃŸÃšÂ»ZÃ´yÃ½~Â¤Ã¦Ã³TÃ¢sÃ©Ã›Ã‰Â­Â¼Ã‚+Ã¡Â€Â¢}ÃMDwÂ„ÃœÃ¶Ãš4kÃ‘Ã§9wÃ´Â¶Â‡4/
ÃºÃ¢
YI-ÂÂŒl`Â®ÃµÃ®;`Ã•Ã§Ã¦AÃH:Â´Â®Â Ã¡YHb1Â“ÂµÃ¢Â»HÃ‡{Â¸wÂƒÃ‡R^YÃ‘Âš,
Â $FÂ¨Ã”Â–HDÃ™Ã²V?pÃ²yÃ‘Â—Â¹PÃ%F=Â»Â‚Ã‚Ã‡Ã†BÂ‡Ã¶Ã™Ã¶Â¡GKÃ˜PhÂ³~NÂ´[Â¸IÃŠ_ÃªÂˆÂ€CÃ°TzÃÃ’f Ã§ <Ã“Â–Â”ÂŒÃ¼Â`4|mÂƒE1Ã–J'Â¼Ã½Ã¡Ã$`}R Â‚ÃšÂ¨Â€Ã¢N2kfÂ¢aÃ³ouJ73ÂÃªÂ´Ã²vÃ¡ÃªÂ¿Ã´KÃ›W{ÂœMÃ“Ã3?Ã—ÂÂÂ©MÃ²]Ã¹:W;vÂÂ…Â˜XÃ¢;Â„BÂ’ Â‰Â§XÂ.PÃ¡I;?ÂŸ!TÂ¤Âˆ .Â·sÃ¬Ã¶U]Ã¤Â®ÂƒÂ’=Ã‘XÂŠÃ¾Ã½Â’5Â´Ã€ÂŠÂ€rÂ€Mt>y=Â¯Â†*Ã£ÂbCÂ¾~^Â½tÃ…ArÂ±Â¡l'Â¶Ã¸Ã…Â¦Â•Ã¼Ã³Ã´Ã·Ã˜ÂŸÃ‰LÂÃ«ÂƒÂ¡PÂ“&7ttÃ¶uGz;Kg4ÃšÃ³Ã‘Â;Â·u
Â40{5ÃŒÂ¾4,Â¿Ã«iÂªÃ•Â¯Â±[!j Ã„AÂ–RSkÃÃÃ’Â·wÃ=Ã¯Â«Â›?HtÂ§Â§HÂ§Ã‰ÂºÃ¦!Ã°Ã—Ã™Ã½ÃŒÃ†Ã©IÃ°ÂÂ€L6MÂ²}v!ÂÃ¼m*IÃ”Â³Ã›qÂªC$Ã¸LoÂ¬ÃŒGÃ¶Â•SnÂ¢iSÃ‘CÃ†Ã¸ÃªÂ‚Ã¬Â Ã«Â«Ã¥!Â³1Â¡ÂˆÂ¥Ã½Ãœ
