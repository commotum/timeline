

===== Page 1 =====

arXiv:1310.4546v1  [cs.CL]  16 Oct 2013
DistributedRepresentationsofWordsandPhrases
andtheirCompositionality
TomasMikolov
GoogleInc.
MountainView
mikolov@google.com
IlyaSutskever
GoogleInc.
MountainView
ilyasu@google.com
KaiChen
GoogleInc.
MountainView
kai@google.com
GregCorrado
GoogleInc.
MountainView
gcorrado@google.com
JeffreyDean
GoogleInc.
MountainView
jeff@google.com
Abstract
TherecentlyintroducedcontinuousSkip-grammodelisanef
cientmethodfor
learninghigh-qualitydistributedvectorrepresentation
sthatcapturealargenum-
berofprecisesyntacticandsemanticwordrelationships.I
nthispaperwepresent
severalextensionsthatimproveboththequalityofthevect
orsandthetraining
speed.Bysubsamplingofthefrequentwordsweobtainsigni
cantspeedupand
alsolearnmoreregularwordrepresentations.Wealsodescr
ibeasimplealterna-
tivetothehierarchicalsoftmaxcallednegativesampling.
Aninherentlimitationofwordrepresentationsistheirind
ifferencetowordorder
andtheirinabilitytorepresentidiomaticphrases.Forexa
mple,themeaningsof
Â“CanadaÂ”andÂ“AirÂ”cannotbeeasilycombinedtoobtainÂ“AirC
anadaÂ”.Motivated
bythisexample,wepresentasimplemethodforndingphrase
sintext,andshow
thatlearninggoodvectorrepresentationsformillionsofp
hrasesispossible.
1Introduction
Distributedrepresentationsofwordsinavectorspacehelp
learningalgorithmstoachievebetter
performanceinnaturallanguageprocessingtasksbygroupi
ngsimilarwords.Oneoftheearliestuse
ofwordrepresentationsdatesbackto1986duetoRumelhart,
Hinton,andWilliams[13].Thisidea
hassincebeenappliedtostatisticallanguagemodelingwit
hconsiderablesuccess[1].Thefollow
upworkincludesapplicationstoautomaticspeechrecognit
ionandmachinetranslation[14,7],and
awiderangeofNLPtasks[2,20,15,3,18,19,9].
Recently,Mikolovetal.[8]introducedtheSkip-grammodel
,anefcientmethodforlearninghigh-
qualityvectorrepresentationsofwordsfromlargeamounts
ofunstructuredtextdata.Unlikemost
ofthepreviouslyusedneuralnetworkarchitecturesforlea
rningwordvectors,trainingoftheSkip-
grammodel(seeFigure1)doesnotinvolvedensematrixmulti
plications.Thismakesthetraining
extremelyefcient:anoptimizedsingle-machineimplemen
tationcantrainonmorethan100billion
wordsinoneday.
Thewordrepresentationscomputedusingneuralnetworksar
everyinterestingbecausethelearned
vectorsexplicitlyencodemanylinguisticregularitiesan
dpatterns.Somewhatsurprisingly,manyof
thesepatternscanberepresentedaslineartranslations.F
orexample,theresultofavectorcalcula-
tionvec(Â“MadridÂ”)-vec(Â“SpainÂ”)+vec(Â“FranceÂ”)isclose
rtovec(Â“ParisÂ”)thantoanyotherword
vector[9,8].
1

===== Page 2 =====




		
							
		



		
		
		







Figure1:
TheSkip-grammodelarchitecture.Thetrainingobjectivei
stolearnwordvectorrepresentations
thataregoodatpredictingthenearbywords.
InthispaperwepresentseveralextensionsoftheoriginalS
kip-grammodel.Weshowthatsub-
samplingoffrequentwordsduringtrainingresultsinasign
icantspeedup(around2x-10x),and
improvesaccuracyoftherepresentationsoflessfrequentw
ords.Inaddition,wepresentasimpli-
edvariantofNoiseContrastiveEstimation(NCE)[4]fortr
ainingtheSkip-grammodelthatresults
infastertrainingandbettervectorrepresentationsforfr
equentwords,comparedtomorecomplex
hierarchicalsoftmaxthatwasusedinthepriorwork[8].
Wordrepresentationsarelimitedbytheirinabilitytorepr
esentidiomaticphrasesthatarenotcom-
positionsoftheindividualwords.Forexample,Â“BostonGlo
beÂ”isanewspaper,andsoitisnota
naturalcombinationofthemeaningsofÂ“BostonÂ”andÂ“GlobeÂ”
.Therefore,usingvectorstorepre-
sentthewholephrasesmakestheSkip-grammodelconsiderab
lymoreexpressive.Othertechniques
thataimtorepresentmeaningofsentencesbycomposingthew
ordvectors,suchastherecursive
autoencoders[15],wouldalsobenetfromusingphrasevect
orsinsteadofthewordvectors.
Theextensionfromwordbasedtophrasebasedmodelsisrelat
ivelysimple.Firstweidentifyalarge
numberofphrasesusingadata-drivenapproach,andthenwet
reatthephrasesasindividualtokens
duringthetraining.Toevaluatethequalityofthephraseve
ctors,wedevelopedatestsetofanalogi-
calreasoningtasksthatcontainsbothwordsandphrases.At
ypicalanalogypairfromourtestsetis
Â“MontrealÂ”:Â“MontrealCanadiensÂ”::Â“TorontoÂ”:Â“TorontoM
apleLeafsÂ”.Itisconsideredtohavebeen
answeredcorrectlyifthenearestrepresentationtovec(Â“M
ontrealCanadiensÂ”)-vec(Â“MontrealÂ”)+
vec(Â“TorontoÂ”)isvec(Â“TorontoMapleLeafsÂ”).
Finally,wedescribeanotherinterestingpropertyoftheSk
ip-grammodel.Wefoundthatsimple
vectoradditioncanoftenproducemeaningfulresults.Fore
xample,vec(Â“RussiaÂ”)+vec(Â“riverÂ”)is
closetovec(Â“VolgaRiverÂ”),andvec(Â“GermanyÂ”)+vec(Â“cap
italÂ”)isclosetovec(Â“BerlinÂ”).This
compositionalitysuggeststhatanon-obviousdegreeoflan
guageunderstandingcanbeobtainedby
usingbasicmathematicaloperationsonthewordvectorrepr
esentations.
2TheSkip-gramModel
ThetrainingobjectiveoftheSkip-grammodelistondwordr
epresentationsthatareusefulfor
predictingthesurroundingwordsinasentenceoradocument
.Moreformally,givenasequenceof
trainingwords
w
1
;w
2
;w
3
;:::;w
T
,theobjectiveoftheSkip-grammodelistomaximizetheaver
age
logprobability
1
T
T
X
t
=1
X

c

j

c;j
6
=0
log
p
(
w
t
+
j
j
w
t
)
(1)
where
c
isthesizeofthetrainingcontext(whichcanbeafunctionof
thecenterword
w
t
).Larger
c
resultsinmoretrainingexamplesandthuscanleadtoahighe
raccuracy,attheexpenseofthe
2

===== Page 3 =====

trainingtime.ThebasicSkip-gramformulationdenes
p
(
w
t
+
j
j
w
t
)
usingthesoftmaxfunction:
p
(
w
O
j
w
I
)=
exp

v
0
w
O
>
v
w
I

P
W
=1
exp

v
0
w
>
v
w
I

(2)
where
v
w
and
v
0
w
aretheÂ“inputÂ”andÂ“outputÂ”vectorrepresentationsof
w
,and
W
isthenum-
berofwordsinthevocabulary.Thisformulationisimpracti
calbecausethecostofcomputing
r
log
p
(
w
O
j
w
I
)
isproportionalto
W
,whichisoftenlarge(
10
5
Â–
10
7
terms).
2.1HierarchicalSoftmax
Acomputationallyefcientapproximationofthefullsoftm
axisthehierarchicalsoftmax.Inthe
contextofneuralnetworklanguagemodels,itwasrstintro
ducedbyMorinandBengio[12].The
mainadvantageisthatinsteadofevaluating
W
outputnodesintheneuralnetworktoobtainthe
probabilitydistribution,itisneededtoevaluateonlyabo
ut
log
2
(
W
)
nodes.
Thehierarchicalsoftmaxusesabinarytreerepresentation
oftheoutputlayerwiththe
W
wordsas
itsleavesand,foreachnode,explicitlyrepresentstherel
ativeprobabilitiesofitschildnodes.These
denearandomwalkthatassignsprobabilitiestowords.
Moreprecisely,eachword
w
canbereachedbyanappropriatepathfromtherootofthetree
.Let
n
(
w;j
)
bethe
j
-thnodeonthepathfromtherootto
w
,andlet
L
(
w
)
bethelengthofthispath,so
n
(
w;
1)=root
and
n
(
w;L
(
w
))=
w
.Inaddition,foranyinnernode
n
,let
ch(
n
)
beanarbitrary
xedchildof
n
andlet
[[
x
]]
be1if
x
istrueand-1otherwise.Thenthehierarchicalsoftmaxden
es
p
(
w
O
j
w
I
)
asfollows:
p
(
w
j
w
I
)=
L
(
w
)

1
Y
j
=1


[[
n
(
w;j
+1)=ch(
n
(
w;j
))]]

v
0
n
(
w;j
)
>
v
w
I

(3)
where

(
x
)=1
=
(1+exp(

x
))
.Itcanbeveriedthat
P
W
=1
p
(
w
j
w
I
)=1
.Thisimpliesthatthe
costofcomputing
log
p
(
w
O
j
w
I
)
and
r
log
p
(
w
O
j
w
I
)
isproportionalto
L
(
w
O
)
,whichonaverage
isnogreaterthan
log
W
.Also,unlikethestandardsoftmaxformulationoftheSkip-
gramwhich
assignstworepresentations
v
w
and
v
0
w
toeachword
w
,thehierarchicalsoftmaxformulationhas
onerepresentation
v
w
foreachword
w
andonerepresentation
v
0
n
foreveryinnernode
n
ofthe
binarytree.
Thestructureofthetreeusedbythehierarchicalsoftmaxha
saconsiderableeffectontheperfor-
mance.MnihandHintonexploredanumberofmethodsforconst
ructingthetreestructureandthe
effectonboththetrainingtimeandtheresultingmodelaccu
racy[10].Inourworkweuseabinary
Huffmantree,asitassignsshortcodestothefrequentwords
whichresultsinfasttraining.Ithas
beenobservedbeforethatgroupingwordstogetherbytheirf
requencyworkswellasaverysimple
speeduptechniquefortheneuralnetworkbasedlanguagemod
els[5,8].
2.2NegativeSampling
AnalternativetothehierarchicalsoftmaxisNoiseContras
tiveEstimation(NCE),whichwasin-
troducedbyGutmannandHyvarinen[4]andappliedtolanguag
emodelingbyMnihandTeh[11].
NCEpositsthatagoodmodelshouldbeabletodifferentiated
atafromnoisebymeansoflogistic
regression.ThisissimilartohingelossusedbyColloberta
ndWeston[2]whotrainedthemodels
byrankingthedataabovenoise.
WhileNCEcanbeshowntoapproximatelymaximizethelogprob
abilityofthesoftmax,theSkip-
grammodelisonlyconcernedwithlearninghigh-qualityvec
torrepresentations,sowearefreeto
simplifyNCEaslongasthevectorrepresentationsretainth
eirquality.WedeneNegativesampling
(NEG)bytheobjective
log

(
v
0
w
O
>
v
w
I
)+
k
X
i
=1
E
w
i

P
n
(
w
)
h
log

(

v
0
w
i
>
v
w
I
)
i
(4)
3

===== Page 4 =====

-2
-1.5
-1
-0.5
 0
 0.5
 1
 1.5
 2
-2
-1.5
-1
-0.5
 0
 0.5
 1
 1.5
 2
Country and Capital Vectors Projected by PCA
China
Japan
France
Russia
Germany
Italy
Spain
Greece
Turkey
Beijing
Paris
Tokyo
Poland
Moscow
Portugal
Berlin
Rome
Athens
Madrid
Ankara
Warsaw
Lisbon
Figure2:
Two-dimensionalPCAprojectionofthe1000-dimensionalSk
ip-gramvectorsofcountriesandtheir
capitalcities.Thegureillustratesabilityofthemodelt
oautomaticallyorganizeconceptsandlearnimplicitly
therelationshipsbetweenthem,asduringthetrainingwedi
dnotprovideanysupervisedinformationabout
whatacapitalcitymeans.
whichisusedtoreplaceevery
log
P
(
w
O
j
w
I
)
termintheSkip-gramobjective.Thusthetaskisto
distinguishthetargetword
w
O
fromdrawsfromthenoisedistribution
P
n
(
w
)
usinglogisticregres-
sion,wherethereare
k
negativesamplesforeachdatasample.Ourexperimentsindi
catethatvalues
of
k
intherange5Â–20areusefulforsmalltrainingdatasets,whi
leforlargedatasetsthe
k
canbeas
smallas2Â–5.ThemaindifferencebetweentheNegativesampl
ingandNCEisthatNCEneedsboth
samplesandthenumericalprobabilitiesofthenoisedistri
bution,whileNegativesamplingusesonly
samples.AndwhileNCEapproximatelymaximizesthelogprob
abilityofthesoftmax,thisproperty
isnotimportantforourapplication.
BothNCEandNEGhavethenoisedistribution
P
n
(
w
)
asafreeparameter.Weinvestigatedanumber
ofchoicesfor
P
n
(
w
)
andfoundthattheunigramdistribution
U
(
w
)
raisedtothe
3
=
4
rdpower(i.e.,
U
(
w
)
3
=
4
=Z
)outperformedsignicantlytheunigramandtheuniformdis
tributions,forbothNCE
andNEGoneverytaskwetriedincludinglanguagemodeling(n
otreportedhere).
2.3SubsamplingofFrequentWords
Inverylargecorpora,themostfrequentwordscaneasilyocc
urhundredsofmillionsoftimes(e.g.,
Â“inÂ”,Â“theÂ”,andÂ“aÂ”).Suchwordsusuallyprovidelessinfor
mationvaluethantherarewords.For
example,whiletheSkip-grammodelbenetsfromobservingt
heco-occurrencesofÂ“FranceÂ”and
Â“ParisÂ”,itbenetsmuchlessfromobservingthefrequentco
-occurrencesofÂ“FranceÂ”andÂ“theÂ”,as
nearlyeverywordco-occursfrequentlywithinasentencewi
thÂ“theÂ”.Thisideacanalsobeapplied
intheoppositedirection;thevectorrepresentationsoffr
equentwordsdonotchangesignicantly
aftertrainingonseveralmillionexamples.
Tocountertheimbalancebetweentherareandfrequentwords
,weusedasimplesubsamplingap-
proach:eachword
w
i
inthetrainingsetisdiscardedwithprobabilitycomputedb
ytheformula
P
(
w
i
)=1

s
t
f
(
w
i
)
(5)
4

===== Page 5 =====

Method
Time[min]
Syntactic[%]Semantic[%]
Totalaccuracy[%]
NEG-5
38
6354
59
NEG-15
97
6358
61
HS-Huffman
41
5340
47
NCE-5
38
6045
53
Thefollowingresultsuse
10

5
subsampling
NEG-5
14
6158
60
NEG-15
36
6161
61
HS-Huffman
21
5259
55
Table1:AccuracyofvariousSkip-gram300-dimensionalmod
elsontheanalogicalreasoningtask
asdenedin[8].NEG-
k
standsforNegativeSamplingwith
k
negativesamplesforeachpositive
sample;NCEstandsforNoiseContrastiveEstimationandHS-
HuffmanstandsfortheHierarchical
Softmaxwiththefrequency-basedHuffmancodes.
where
f
(
w
i
)
isthefrequencyofword
w
i
and
t
isachosenthreshold,typicallyaround
10

5
.
Wechosethissubsamplingformulabecauseitaggressivelys
ubsampleswordswhosefrequency
isgreaterthan
t
whilepreservingtherankingofthefrequencies.Althought
hissubsamplingfor-
mulawaschosenheuristically,wefoundittoworkwellinpra
ctice.Itaccelerateslearningandeven
signicantlyimprovestheaccuracyofthelearnedvectorso
ftherarewords,aswillbeshowninthe
followingsections.
3EmpiricalResults
InthissectionweevaluatetheHierarchicalSoftmax(HS),N
oiseContrastiveEstimation,Negative
Sampling,andsubsamplingofthetrainingwords.Weusedthe
analogicalreasoningtask
1
introduced
byMikolovetal.[8].ThetaskconsistsofanalogiessuchasÂ“
GermanyÂ”:Â“BerlinÂ”::Â“FranceÂ”:?,
whicharesolvedbyndingavector
x
suchthatvec(
x
)isclosesttovec(Â“BerlinÂ”)-vec(Â“GermanyÂ”)
+vec(Â“FranceÂ”)accordingtothecosinedistance(wediscar
dtheinputwordsfromthesearch).This
specicexampleisconsideredtohavebeenansweredcorrect
lyif
x
isÂ“ParisÂ”.Thetaskhastwo
broadcategories:thesyntacticanalogies(suchasÂ“quickÂ”
:Â“quicklyÂ”::Â“slowÂ”:Â“slowlyÂ”)andthe
semanticanalogies,suchasthecountrytocapitalcityrela
tionship.
FortrainingtheSkip-grammodels,wehaveusedalargedatas
etconsistingofvariousnewsarticles
(aninternalGoogledatasetwithonebillionwords).Wedisc
ardedfromthevocabularyallwords
thatoccurredlessthan5timesinthetrainingdata,whichre
sultedinavocabularyofsize692K.
TheperformanceofvariousSkip-grammodelsonthewordanal
ogytestsetisreportedinTable1.
ThetableshowsthatNegativeSamplingoutperformstheHier
archicalSoftmaxontheanalogical
reasoningtask,andhasevenslightlybetterperformanceth
antheNoiseContrastiveEstimation.The
subsamplingofthefrequentwordsimprovesthetrainingspe
edseveraltimesandmakestheword
representationssignicantlymoreaccurate.
Itcanbearguedthatthelinearityoftheskip-grammodelmak
esitsvectorsmoresuitableforsuch
linearanalogicalreasoning,buttheresultsofMikoloveta
l.[8]alsoshowthatthevectorslearned
bythestandardsigmoidalrecurrentneuralnetworks(which
arehighlynon-linear)improveonthis
tasksignicantlyastheamountofthetrainingdataincreas
es,suggestingthatnon-linearmodelsalso
haveapreferenceforalinearstructureofthewordrepresen
tations.
4LearningPhrases
Asdiscussedearlier,manyphraseshaveameaningthatisnot
asimplecompositionofthemean-
ingsofitsindividualwords.Tolearnvectorrepresentatio
nforphrases,werstndwordsthat
appearfrequentlytogether,andinfrequentlyinothercont
exts.Forexample,Â“NewYorkTimesÂ”and
Â“TorontoMapleLeafsÂ”arereplacedbyuniquetokensinthetr
ainingdata,whileabigramÂ“thisisÂ”
willremainunchanged.
1
code.google.com/p/word2vec/source/browse/trunk/ques
tions-words.txt
5

===== Page 6 =====

Newspapers
NewYorkNewYorkTimes
BaltimoreBaltimoreSun
SanJoseSanJoseMercuryNews
CincinnatiCincinnatiEnquirer
NHLTeams
BostonBostonBruins
MontrealMontrealCanadiens
PhoenixPhoenixCoyotes
NashvilleNashvillePredators
NBATeams
DetroitDetroitPistons
TorontoTorontoRaptors
OaklandGoldenStateWarriors
MemphisMemphisGrizzlies
Airlines
AustriaAustrianAirlines
SpainSpainair
BelgiumBrusselsAirlines
GreeceAegeanAirlines
Companyexecutives
SteveBallmerMicrosoft
LarryPageGoogle
SamuelJ.PalmisanoIBM
WernerVogelsAmazon
Table2:Examplesoftheanalogicalreasoningtaskforphras
es(thefulltestsethas3218examples).
Thegoalistocomputethefourthphraseusingtherstthree.
Ourbestmodelachievedanaccuracy
of72%onthisdataset.
Thisway,wecanformmanyreasonablephraseswithoutgreatl
yincreasingthesizeofthevocabu-
lary;intheory,wecantraintheSkip-grammodelusingalln-
grams,butthatwouldbetoomemory
intensive.Manytechniqueshavebeenpreviouslydeveloped
toidentifyphrasesinthetext;however,
itisoutofscopeofourworktocomparethem.Wedecidedtouse
asimpledata-drivenapproach,
wherephrasesareformedbasedontheunigramandbigramcoun
ts,using
score
(
w
i
;w
j
)=
count
(
w
i
w
j
)


count
(
w
i
)

count
(
w
j
)
:
(6)
The

isusedasadiscountingcoefcientandpreventstoomanyphr
asesconsistingofveryinfre-
quentwordstobeformed.Thebigramswithscoreabovethecho
senthresholdarethenusedas
phrases.Typically,werun2-4passesoverthetrainingdata
withdecreasingthresholdvalue,allow-
inglongerphrasesthatconsistsofseveralwordstobeforme
d.Weevaluatethequalityofthephrase
representationsusinganewanalogicalreasoningtaskthat
involvesphrases.Table2showsexamples
ofthevecategoriesofanalogiesusedinthistask.Thisdat
asetispubliclyavailableontheweb
2
.
4.1PhraseSkip-GramResults
Startingwiththesamenewsdataasinthepreviousexperimen
ts,werstconstructedthephrase
basedtrainingcorpusandthenwetrainedseveralSkip-gram
modelsusingdifferenthyper-
parameters.Asbefore,weusedvectordimensionality300an
dcontextsize5.Thissettingalready
achievesgoodperformanceonthephrasedataset,andallowe
dustoquicklycomparetheNegative
SamplingandtheHierarchicalSoftmax,bothwithandwithou
tsubsamplingofthefrequenttokens.
TheresultsaresummarizedinTable3.
TheresultsshowthatwhileNegativeSamplingachievesares
pectableaccuracyevenwith
k
=5
,
using
k
=15
achievesconsiderablybetterperformance.Surprisingly,
whilewefoundtheHierar-
chicalSoftmaxtoachievelowerperformancewhentrainedwi
thoutsubsampling,itbecamethebest
performingmethodwhenwedownsampledthefrequentwords.T
hisshowsthatthesubsampling
canresultinfastertrainingandcanalsoimproveaccuracy,
atleastinsomecases.
2
code.google.com/p/word2vec/source/browse/trunk/ques
tions-phrases.txt
Method
Dimensionality
Nosubsampling[%]
10

5
subsampling[%]
NEG-5
300
24
27
NEG-15
300
27
42
HS-Huffman
300
19
47
Table3:AccuraciesoftheSkip-grammodelsonthephraseana
logydataset.Themodelswere
trainedonapproximatelyonebillionwordsfromthenewsdat
aset.
6

===== Page 7 =====

NEG-15with
10

5
subsampling
HSwith
10

5
subsampling
VascodeGama
Lingsugur
Italianexplorer
LakeBaikal
GreatRiftValley
AralSea
AlanBean
RebbecaNaomi
moonwalker
IonianSea
Ruegen
IonianIslands
chessmaster
chessgrandmaster
GarryKasparov
Table4:Examplesoftheclosestentitiestothegivenshortp
hrases,usingtwodifferentmodels.
Czech+currency
Vietnam+capital
German+airlines
Russian+river
French+actress
koruna
Hanoi
airlineLufthansa
Moscow
JulietteBinoche
Checkcrown
HoChiMinhCity
carrierLufthansa
VolgaRiver
VanessaParadis
Polishzolty
VietNam
agcarrierLufthansa
upriver
CharlotteGainsbourg
CTK
Vietnamese
Lufthansa
Russia
CecileDe
Table5:Vectorcompositionalityusingelement-wiseaddit
ion.Fourclosesttokenstothesumoftwo
vectorsareshown,usingthebestSkip-grammodel.
Tomaximizetheaccuracyonthephraseanalogytask,weincre
asedtheamountofthetrainingdata
byusingadatasetwithabout33billionwords.Weusedthehie
rarchicalsoftmax,dimensionality
of1000,andtheentiresentenceforthecontext.Thisresult
edinamodelthatreachedanaccuracy
of
72%
.Weachievedloweraccuracy66%whenwereducedthesizeofth
etrainingdatasetto6B
words,whichsuggeststhatthelargeamountofthetrainingd
ataiscrucial.
Togainfurtherinsightintohowdifferenttherepresentati
onslearnedbydifferentmodelsare,wedid
inspectmanuallythenearestneighboursofinfrequentphra
sesusingvariousmodels.InTable4,we
showasampleofsuchcomparison.Consistentlywiththeprev
iousresults,itseemsthatthebest
representationsofphrasesarelearnedbyamodelwiththehi
erarchicalsoftmaxandsubsampling.
5AdditiveCompositionality
Wedemonstratedthatthewordandphraserepresentationsle
arnedbytheSkip-grammodelexhibit
alinearstructurethatmakesitpossibletoperformprecise
analogicalreasoningusingsimplevector
arithmetics.Interestingly,wefoundthattheSkip-gramre
presentationsexhibitanotherkindoflinear
structurethatmakesitpossibletomeaningfullycombinewo
rdsbyanelement-wiseadditionoftheir
vectorrepresentations.Thisphenomenonisillustratedin
Table5.
Theadditivepropertyofthevectorscanbeexplainedbyinsp
ectingthetrainingobjective.Theword
vectorsareinalinearrelationshipwiththeinputstotheso
ftmaxnonlinearity.Asthewordvectors
aretrainedtopredictthesurroundingwordsinthesentence
,thevectorscanbeseenasrepresenting
thedistributionofthecontextinwhichawordappears.Thes
evaluesarerelatedlogarithmically
totheprobabilitiescomputedbytheoutputlayer,sothesum
oftwowordvectorsisrelatedtothe
productofthetwocontextdistributions.Theproductworks
hereastheANDfunction:wordsthat
areassignedhighprobabilitiesbybothwordvectorswillha
vehighprobability,andtheotherwords
willhavelowprobability.Thus,ifÂ“VolgaRiverÂ”appearsfr
equentlyinthesamesentencetogether
withthewordsÂ“RussianÂ”andÂ“riverÂ”,thesumofthesetwowor
dvectorswillresultinsuchafeature
vectorthatisclosetothevectorofÂ“VolgaRiverÂ”.
6ComparisontoPublishedWordRepresentations
Manyauthorswhopreviouslyworkedontheneuralnetworkbas
edrepresentationsofwordshave
publishedtheirresultingmodelsforfurtheruseandcompar
ison:amongstthemostwellknownau-
thorsareCollobertandWeston[2],Turianetal.[17],andMn
ihandHinton[10].Wedownloaded
theirwordvectorsfromtheweb
3
.Mikolovetal.[8]havealreadyevaluatedthesewordrepres
enta-
tionsonthewordanalogytask,wheretheSkip-grammodelsac
hievedthebestperformancewitha
hugemargin.
3
http://metaoptimize.com/projects/wordreprs/
7

===== Page 8 =====

Model
Redmond
Havel
ninjutsu
grafti
capitulate
(trainingtime)
Collobert(50d)
conyers
plauen
reiki
cheesecake
abdicate
(2months)
lubbock
dzerzhinsky
kohona
gossip
accede
keene
osterreich
karate
dioramas
rearm
Turian(200d)
McCarthy
Jewell
-
gunre
-
(fewweeks)
Alston
Arzu
-
emotion
-
Cousins
Ovitz
-
impunity
-
Mnih(100d)
Podhurst
Pontiff
-
anaesthetics
Mavericks
(7days)
Harlang
Pinochet
-
monkeys
planning
Agarwal
Rodionov
-
Jews
hesitated
Skip-Phrase
RedmondWash.
VaclavHavel
ninja
spraypaint
capitulation
(1000d,1day)
RedmondWashington
presidentVaclavHavel
martialarts
gratti
capitulated
Microsoft
VelvetRevolution
swordsmanship
taggers
capitulating
Table6:Examplesoftheclosesttokensgivenvariouswellkn
ownmodelsandtheSkip-grammodel
trainedonphrasesusingover30billiontrainingwords.Ane
mptycellmeansthatthewordwasnot
inthevocabulary.
Togivemoreinsightintothedifferenceofthequalityofthe
learnedvectors,weprovideempirical
comparisonbyshowingthenearestneighboursofinfrequent
wordsinTable6.Theseexamplesshow
thatthebigSkip-grammodeltrainedonalargecorpusvisibl
youtperformsalltheothermodelsin
thequalityofthelearnedrepresentations.Thiscanbeattr
ibutedinparttothefactthatthismodel
hasbeentrainedonabout30billionwords,whichisabouttwo
tothreeordersofmagnitudemore
datathanthetypicalsizeusedinthepriorwork.Interestin
gly,althoughthetrainingsetismuch
larger,thetrainingtimeoftheSkip-grammodelisjustafra
ctionofthetimecomplexityrequiredby
thepreviousmodelarchitectures.
7Conclusion
Thisworkhasseveralkeycontributions.Weshowhowtotrain
distributedrepresentationsofwords
andphraseswiththeSkip-grammodelanddemonstratethatth
eserepresentationsexhibitlinear
structurethatmakespreciseanalogicalreasoningpossibl
e.Thetechniquesintroducedinthispaper
canbeusedalsofortrainingthecontinuousbag-of-wordsmo
delintroducedin[8].
Wesuccessfullytrainedmodelsonseveralordersofmagnitu
demoredatathanthepreviouslypub-
lishedmodels,thankstothecomputationallyefcientmode
larchitecture.Thisresultsinagreat
improvementinthequalityofthelearnedwordandphraserep
resentations,especiallyfortherare
entities.Wealsofoundthatthesubsamplingofthefrequent
wordsresultsinbothfastertraining
andsignicantlybetterrepresentationsofuncommonwords
.Anothercontributionofourpaperis
theNegativesamplingalgorithm,whichisanextremelysimp
letrainingmethodthatlearnsaccurate
representationsespeciallyforfrequentwords.
Thechoiceofthetrainingalgorithmandthehyper-paramete
rselectionisataskspecicdecision,
aswefoundthatdifferentproblemshavedifferentoptimalh
yperparametercongurations.Inour
experiments,themostcrucialdecisionsthataffecttheper
formancearethechoiceofthemodel
architecture,thesizeofthevectors,thesubsamplingrate
,andthesizeofthetrainingwindow.
Averyinterestingresultofthisworkisthatthewordvector
scanbesomewhatmeaningfullycom-
binedusingjustsimplevectoraddition.Anotherapproachf
orlearningrepresentationsofphrases
presentedinthispaperistosimplyrepresentthephraseswi
thasingletoken.Combinationofthese
twoapproachesgivesapowerfulyetsimplewayhowtoreprese
ntlongerpiecesoftext,whilehav-
ingminimalcomputationalcomplexity.Ourworkcanthusbes
eenascomplementarytotheexisting
approachthatattemptstorepresentphrasesusingrecursiv
ematrix-vectoroperations[16].
Wemadethecodefortrainingthewordandphrasevectorsbase
donthetechniquesdescribedinthis
paperavailableasanopen-sourceproject
4
.
4
code.google.com/p/word2vec
8

===== Page 9 =====

References
[1]YoshuaBengio,RÂ´ejeanDucharme,PascalVincent,andCh
ristianJanvin.Aneuralprobabilisticlanguage
model.
TheJournalofMachineLearningResearch
,3:1137Â–1155,2003.
[2]RonanCollobertandJasonWeston.Auniedarchitecture
fornaturallanguageprocessing:deepneu-
ralnetworkswithmultitasklearning.In
Proceedingsofthe25thinternationalconferenceonMachin
e
learning
,pages160Â–167.ACM,2008.
[3]XavierGlorot,AntoineBordes,andYoshuaBengio.Domai
nadaptationforlarge-scalesentimentclassi-
cation:Adeeplearningapproach.In
ICML
,513Â–520,2011.
[4]MichaelUGutmannandAapoHyvÂ¨arinen.Noise-contrasti
veestimationofunnormalizedstatisticalmod-
els,withapplicationstonaturalimagestatistics.
TheJournalofMachineLearningResearch
,13:307Â–361,
2012.
[5]TomasMikolov,StefanKombrink,LukasBurget,JanCerno
cky,andSanjeevKhudanpur.Extensionsof
recurrentneuralnetworklanguagemodel.In
Acoustics,SpeechandSignalProcessing(ICASSP),2011
IEEEInternationalConferenceon
,pages5528Â–5531.IEEE,2011.
[6]TomasMikolov,AnoopDeoras,DanielPovey,LukasBurget
andJanCernocky.StrategiesforTraining
LargeScaleNeuralNetworkLanguageModels.InProc.
AutomaticSpeechRecognitionandUnderstand-
ing
,2011.
[7]TomasMikolov.StatisticalLanguageModelsBasedonNeu
ralNetworks.
PhDthesis,PhDThesis,Brno
UniversityofTechnology
,2012.
[8]TomasMikolov,KaiChen,GregCorrado,andJeffreyDean.
Efcientestimationofwordrepresentations
invectorspace.
ICLRWorkshop
,2013.
[9]TomasMikolov,Wen-tauYihandGeoffreyZweig.Linguist
icRegularitiesinContinuousSpaceWord
Representations.In
ProceedingsofNAACLHLT
,2013.
[10]AndriyMnihandGeoffreyEHinton.Ascalablehierarchi
caldistributedlanguagemodel.
Advancesin
neuralinformationprocessingsystems
,21:1081Â–1088,2009.
[11]AndriyMnihandYeeWhyeTeh.Afastandsimplealgorithm
fortrainingneuralprobabilisticlanguage
models.
arXivpreprintarXiv:1206.6426
,2012.
[12]FredericMorinandYoshuaBengio.Hierarchicalprobab
ilisticneuralnetworklanguagemodel.In
Pro-
ceedingsoftheinternationalworkshoponarticialintell
igenceandstatistics
,pages246Â–252,2005.
[13]DavidERumelhart,GeoffreyEHintont,andRonaldJWill
iams.Learningrepresentationsbyback-
propagatingerrors.
Nature
,323(6088):533Â–536,1986.
[14]HolgerSchwenk.Continuousspacelanguagemodels.
ComputerSpeechandLanguage
,vol.21,2007.
[15]RichardSocher,CliffC.Lin,AndrewY.Ng,andChristop
herD.Manning.Parsingnaturalscenesand
naturallanguagewithrecursiveneuralnetworks.In
Proceedingsofthe26thInternationalConferenceon
MachineLearning(ICML)
,volume2,2011.
[16]RichardSocher,BrodyHuval,ChristopherD.Manning,a
ndAndrewY.Ng.SemanticCompositionality
ThroughRecursiveMatrix-VectorSpaces.In
Proceedingsofthe2012ConferenceonEmpiricalMethods
inNaturalLanguageProcessing(EMNLP)
,2012.
[17]JosephTurian,LevRatinov,andYoshuaBengio.Wordrep
resentations:asimpleandgeneralmethodfor
semi-supervisedlearning.In
Proceedingsofthe48thAnnualMeetingoftheAssociationfo
rComputa-
tionalLinguistics
,pages384Â–394.AssociationforComputationalLinguistic
s,2010.
[18]PeterD.TurneyandPatrickPantel.Fromfrequencytome
aning:Vectorspacemodelsofsemantics.In
JournalofArticialIntelligenceResearch
,37:141-188,2010.
[19]PeterD.Turney.Distributionalsemanticsbeyondword
s:Supervisedlearningofanalogyandparaphrase.
In
TransactionsoftheAssociationforComputationalLinguis
tics(TACL)
,353Â–366,2013.
[20]JasonWeston,SamyBengio,andNicolasUsunier.Wsabie
:Scalinguptolargevocabularyimageannota-
tion.In
ProceedingsoftheTwenty-Secondinternationaljointconf
erenceonArticialIntelligence-Volume
VolumeThree
,pages2764Â–2770.AAAIPress,2011.
9