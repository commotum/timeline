                                        100             97.9                  99.1                   98.8                  97.9
                                       Accuracy80                                                                                                 79.8
                                         60
                                         40                    30.6                   31.3                  30.1                   29.1
                                       Exact Match 20                                                                                                     13.7
                                           0           16x1                    8x2                   4x4                    2x8                   1x16
                                                                   Layers in Recurrent Block X Number of Recurrences
                                                                           Abacus, OOD                         Abacus, 100+ OOD
                                   Figure 4: Varying the size of the recurrent block, while maintaining an effective depth of 16 and
                                   training on size 20 data. We see that a recurrent model with eight layers in the recurrent block and
                                   two recurrences is the most accurate of all effective depth 16 models, halving the error rate of a
                                   standard model with input injection in the OOD evaluation. (See Figure 17 for results with FIRE and
                                   NoPE.)
                                   4.2    Integer Multiplication
                                   Wenowstudyahardertask,multiplication of natural numbers, where the length of the output may be
                                   the sum of the lengths of the operands. Compared to addition, where the output is at most one digit
                                   morethan the longest operand, multiplication has longer-distance dependency and the output length
                                   scales much faster as problem size increases.
                                   To adapt from addition to multiplication, we make some small changes to our set-up. First, we
                                   remove the input injection from inside the recurrent block and second, we divide the gradients in the
                                   recurrent block by the number of recurrences, down-weighing the gradient update from batches with
                                   manyrecurrences [Bansal et al., 2022]. (We analyze the impact of these design decisions for addition
                                   models in Appendix Figure 19.) We only examine looped transformers as the compute required for
                                   training and hyperparameter search for multiplication is far greater than for addition, limiting us to a
                                   muchsmaller scale analysis.
                                   AbacusEmbeddingshelploopedtransformersreach near-perfect accuracy in-distribution for mul-
                                   tiplication. In Figure 6, we show how the training distribution, surrounded by the red square fully
                                   saturates with Abacus Embeddings. In fact, models with our Abacus Embeddings achieve higher in
                                                          100                                                                Addition
                                                            90
                                                            80                                                               Subtraction
                                                         Accuracy70                                                          In Distribution
                                                            60
                                                            50
                                                            40
                                                            30
                                                            20
                                                         Exact Match 10
                                                             0 0 10 20 30 40 50 60 70 80 90                  10
                                                                               Operand Length           100 1   120
                                   Figure 5:      Models which have 8 layers in recurrent block and 2 recurrences, trained on size 20
                                   addition and subtraction data, each line is the average of 3 models. We see that it is possible to have
                                   extreme generalization whilst learning multiple tasks.
                                                                                                  7
