                                                                                             int * func_1(int a[])
                                                                                             {
                                                         int * func_1(int a[])                   int p_0 = 0;
                                                         {                                       int l_7 = 3;
                              int * func_1(int a[])          int p_0 = 2;                        int l_8 = 1;
                              {                              int l_12 = 3;                       a[l_8] = (a[l_7] - a[p_0]);
                                  int p_0 = 0;               for (p_0 = 1; p_0 <= 2; p_0++)      for (p_0 = 3; p_0 <= 4; p_0++)
                                  int l_25 = 4;              {                                   {
                                  a[p_0] = 1;                    a[p_0]--;                           for (int p_1 = 1; p_1 <= 2; p_1++)
                                  --a[l_25];                 }                                       {
                                  return a;                  a[l_12] = a[l_12] + 4;                       a[p_1] = a[p_1] + a[p_0];
                              }                              return a;                                    a[p_1] = a[p_1] + 2;
                                                         }                                           }
                                                                                                 }
                                                                                                 return a;
                                                                                             }
                              Figure 5: Sample programs that could be correctly predicted by LaSynth, but wrongly predicted by models
                              without the latent executor. These programs require multiple different operations for different input list elements.
                                                                  ˆ
                                     • NoPartialExecutor. I = I = I for every t and additionally h is regularized so that
                                                                  t     0                                       T
                                        LatentExecutor(I ,h ) matches the output O under loss L                 . Therefore, no partial
                                                            0   T                                          Exec
                                        latent execution.
                                     • NoOpPredictor. The max pooling layer only takes the vectors computed by the double
                                        attention as the input (Eqn. 2).
                                     • NoAttentionInDecoding. There is no attention over decoded program tokens, and the
                                        output of the max pooling layer is directly fed into the output softmax layer; i.e., P[pt] =
                                        Softmax(Vm ) (comparedtoEqn.3).
                                                       t pt
                              Wealsocomparewithexisting neural program synthesis models with good performance on related
                              tasks, as shown in the rightmost block of Table 2. RobustFill [17] is the state-of-the-art neural
                              network architecture on FlashFill benchmark, which synthesizes string manipulation programs in a
                              domain-speciﬁc language. As described in Sec. 3, the input-output encoder and the program decoder
                              architectures in RobustFill are similar to LaSynth, except that it does not include the latent executor,
                              operation predictor, and the attention on the decoded program sequence.
                              Property Signatures [39] was designed for synthesizing list manipulation programs in domain-speciﬁc
                              languages, but instead of taking the raw input and output lists as the neural network input, they design
                              someproperties that distinguish different programs, then take the values of these properties as the
                              modelinput. A sample property could be whether the program output is the same as the input, and
                              the property values could be “All True”, “All False”, or “Mixed”, indicating that the property always
                              holds for any input-output pair in the speciﬁcation, never holds, and holds for some pairs but not
                              others, respectively. We customize the original design [39] for our setting. First, our property set
                              takes the format of O = C + I? and O = C − I?, where C ∈ [−4,4]. For example, O = 2 + I?
                              meanswhethertheoutput O could be calculated by adding 2 to the input I. These properties focus
                              more on numerical calculation, similar to our operation predictor. Second, different from the task
                              in [39], our C programs sometimes manipulate only a subset of the input lists, thus encoding the list
                              with a single property value is inappropriate. Instead, we compute the property value per element in
                              input-output pairs, use a bi-directional LSTM to encode the property values as a sequence, then take
                              the outputs of the bi-LSTM for program prediction.
                              5.2.2   Results
                              Table 4 presents the results, where all models are trained on the initial random programs. The full
                              LaSynth outperforms other variants, and improves the performance of RobustFill and Property
                              Signatures by around 20%. We also increase the model size of RobustFill to see if the improvement
                              comes from larger model size, but the results are not better. In particular, the latent executor
                              signiﬁcantly increases the prediction accuracy, and achieves better results than NoPartialExecutor,
                              which shows that learning latent execution traces leads to better partial program representations. In
                              Fig. 5, we present sample programs that could be correctly synthesized by LaSynth, but models
                              without the latent executor provide the wrong prediction. We observe that the latent executor is
                              beneﬁcial when the program involves different manipulations for different list elements, e.g., more
                              than one For loop and different mathematical calculations. Our breakdown results on programs
                              of different complexity also justify this observation. We ﬁrst present the results on programs with
                              different control ﬂow constructs in Fig. 6. Speciﬁcally, Seq-only includes programs with no control
                                                                                   8
