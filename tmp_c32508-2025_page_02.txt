            www.nature.com/scientificdata/                                                                       www.nature.com/scientificdata
                                            Fig. 1  ARC Demonstration Tasks. Examples of easy (nearly everyone solved them in two attempts or fewer) 
                                            and hard (few solved them in three attempts or fewer) tasks, with corresponding training and test examples. 
                                            Below the tasks are state space graphs representing all visited grid states by participants, from starting state 
                                            (blue nodes) to correct or incorrect submitted grid (green and red nodes respectively). From lef㘶 to right: 
                                            f76d97a5.json, e9ac8c9e.json, e3497940.json and dd2401ed.json.
                                            released a second, improved version of the ARC benchmark (ARC-AGI-2) on which they report similar levels 
                                            of human accuracy to those reported here8, while the top-performing AI models from the f㘶rst competition all 
                                            achieve under 10% accuracy. For these reasons, comprehensive benchmarking of humans on ARC tasks remains 
                                            an important objective for understanding the underlying mechanisms of human reasoning, but also for gaining 
                                            insights about human intelligence that could lead to better, more e㘠陦cient and human-like AI systems.
                                               A previous attempt at benchmarking human performance on ARC found mean task accuracy for humans 
                                            to be approximately 83%, which was estimated empirically using a small, semi-randomly selected subset of 40 
                                                                       9. However, it is unclear how robust this estimate is because of its small sample size, 
                                            tasks from the training set
                                            and whether the estimate also applies to the evaluation set, which is believed to be much harder. In this work, we 
                                            introduce H-ARC (Human-ARC) which closes this gap by providing a robust estimate of human performance 
                                            on the full set of 800 publicly available ARC tasks. H-ARC is a publicly available repository of over 15,500 
                                            attempts on ARC tasks, with step-by-step action traces recorded from our user-interface and accompanying 
                                                                                                                                                     10,11
                                            natural-language solution descriptions (see Table 1). Although prior research using variants of ARC tasks     or 
                                            a modif㘶ed experimental setup12 have also released human behavioral data, to the best of our knowledge, there 
                                            was no comprehensive public dataset of human behavior at this scale on both the training and evaluation sets of 
                                            ARC prior to this work.
                                               H-ARC makes two key contributions. First, it provides a robust estimate of human performance which can 
                                                                                                                                                          -
                                            be used to benchmark AI algorithms. ARC represents a high-prof㘶le index of intelligence, and these data con
                                            tribute in important ways to the measurement of AI progress, especially when comparing to algorithms that 
                                            operate with more human-like resource-constraints. In addition, from the perspective of cognitive science, 
                                            H-ARC of㘶ers the potential to enrich our understanding of how people solve a range of analogical reasoning 
                                            problems. In particular, people’s error patterns in ARC are revealing of the underlying mechanisms that support 
                                                                                                                                                          -
                                            reasoning. Qualitative analyses of our dataset suggest that people are incorrect in systematic ways, of㘶en achiev
                                            ing partially correct responses or resorting to what appears to be surface-level statistics to approximate observed 
                                            patterns inferred from training examples (Figs. 5, 6). Additionally, the free-form, natural-language solution 
            Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                 2
