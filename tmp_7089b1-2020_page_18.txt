      itself into representations for unboundedly many new domains of problems.
       While these avenues would be fascinating to explore, trying to learn so much starting from so
      little is unlikely to be our best route to AI – especially when we have the shoulders of so many giants
      to stand on. Even if learning from scratch is possible in principle, such approaches suffer from a
      notorious thirst for data–as in neural networks–or, if not data, then massive compute: Just to construct
      ‘origami’ functional programming, DreamCoder took approximately a year of total CPU time. Instead,
      wedrawinspiration from the sketching approach to program synthesis (22). Sketching approaches
      consider single synthesis problems in isolation, and expect a human engineer to outline the skeleton of
      a solution. Analogously, here we built in what we know constitutes useful ingredients for learning to
      solve synthesis tasks in many different domains – relatively spartan but generically powerful sets of
      control ﬂow operators, higher-order functions, and types. We then used learning to grow specialized
      languages atop these foundations. The future of learning in program synthesis may lie with systems
      initialized with even richer yet broadly applicable resources, such as those embodied by simulation
      engines or by the standard libraries of modern programming languages.
       This vision also shapes how we see program induction best contributing to the goal of building
      more human-like AI – not in terms of blank-slate learning, but learning on top of rich systems of
      built-in knowledge. Prior to learning the domains we consider here, human children begin life with
      “core knowledge”: conceptual systems for representing and reasoning about objects, agents, space,
      and other commonsense notions (59–61). We strongly endorse approaches to AI that aim to build
      human-understandable knowledge, beginning with the kinds of conceptual resources that humans do.
      This may be our best route to growing artiﬁcial intelligence that lives in a human world, alongside and
      synergistically with human intelligence.
      References
       1. Alan M Turing. Computing machinery and intelligence. Mind, 1950.
       2. Ray J Solomonoff. A formal theory of inductive inference. Information and control, 7(1):1–22,
        1964.
       3. Percy Liang, Michael I. Jordan, and Dan Klein. Learning dependency-based compositional
        semantics. In ACL, pages 590–599, 2011.
       4. Tejas D Kulkarni, Pushmeet Kohli, Joshua B Tenenbaum, and Vikash Mansinghka. Picture: A
        probabilistic programming language for scene perception. In Proceedings of the IEEE Conference
        on Computer Vision and Pattern Recognition, pages 4390–4399, 2015.
       5. Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
        through probabilistic program induction. Science, 350(6266):1332–1338, 2015.
       6. Nick Chater and Mike Oaksford. Programs as causal models: Speculations on mental programs
        and mental representation. Cognitive Science, 37(6):1171–1191, 2013.
       7. Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. In
        ACMSIGPLANNotices,volume46,pages317–330.ACM,2011.
                          18
