                                                      Test-Time Learning for Large Language Models
                 ¨
               Hubotter, J., Bongni, S., Hakimi, I., and Krause, A. Effi-          Twelfth International Conference on Learning Represen-
                 ciently learning at test-time: Active fine-tuning of llms.        tations, 2024. URL https://openreview.net/
                 In NeurIPS 2024 Workshop on Mathematics of Modern                 forum?id=9w3iw8wDuE.
                 Machine Learning, 2024.                                        Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
               Iwasawa, Y. and Matsuo, Y. Test-time classifier adjustment          hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.
                 module for model-agnostic domain generalization. Ad-              Bart: Denoising sequence-to-sequence pre-training for
                 vances in Neural Information Processing Systems, 34:              natural language generation, translation, and comprehen-
                 2427–2440, 2021.                                                  sion. In Proceedings of the 58th Annual Meeting of the As-
                                                                                   sociation for Computational Linguistics, pp. 7871–7880,
               Jiang, Z., Xu, F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu,           2020a.
                 J., Yang, Y., Callan, J., and Neubig, G. Active retrieval      Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V.,
                 augmented generation. In Bouamor, H., Pino, J., and                            ¨                                      ¨
                 Bali, K. (eds.), Proceedings of the 2023 Conference               Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel,
                 onEmpirical Methods in Natural Language Processing,               T., et al. Retrieval-augmented generation for knowledge-
                 pp. 7969–7992, Singapore, December 2023. Association              intensive nlp tasks. Advances in Neural Information Pro-
                 for Computational Linguistics. doi: 10.18653/v1/2023.             cessing Systems, 33:9459–9474, 2020b.
                 emnlp-main.495. URL https://aclanthology.                      Li, X., Nie, E., and Liang, S. From classification to gen-
                 org/2023.emnlp-main.495/.                                         eration: Insights into crosslingual retrieval augmented
               Jiang, Z., Ma, X., and Chen, W. Longrag: Enhancing                  ICL. In NeurIPS 2023 Workshop on Instruction Tun-
                 retrieval-augmented generation with long-context llms.            ing and Instruction Following, 2023.       URL https:
                 arXiv preprint arXiv:2406.15319, 2024.                            //openreview.net/forum?id=KLPLCXo4aD.
               Jumelet, J. and Zuidema, W. Transparency at the source:          Liang, J., He, R., and Tan, T. A comprehensive survey on
                 Evaluating and interpreting language models with access           test-time adaptation under distribution shifts. Interna-
                 to the true distribution. In Findings of the Association          tional Journal of Computer Vision, pp. 1–34, 2024.
                 for Computational Linguistics: EMNLP 2023, pp. 4354–           Lin, C.-Y. Rouge: A package for automatic evaluation
                 4369, 2023.                                                       of summaries. In Text summarization branches out, pp.
                                                                                   74–81, 2004.
               Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L.,
                 Edunov, S., Chen, D., and Yih, W.-t. Dense passage             Lin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James,
                 retrieval for open-domain question answering. In Web-             R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M.,
                 ber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceed-           Zettlemoyer, L., and tau Yih, W. RA-DIT: Retrieval-
                 ings of the 2020 Conference on Empirical Methods                  augmented dual instruction tuning. In The Twelfth In-
                 in Natural Language Processing (EMNLP), pp. 6769–                 ternational Conference on Learning Representations,
                 6781, Online, November 2020. Association for Computa-             2024. URL https://openreview.net/forum?
                 tional Linguistics. doi: 10.18653/v1/2020.emnlp-main.             id=22OTbutug9.
                 550. URL https://aclanthology.org/2020.                        Liu, Y. Roberta: A robustly optimized bert pretraining
                 emnlp-main.550/.                                                  approach. arXiv preprint arXiv:1907.11692, 364, 2019.
               Kim, G., Kim, S., Jeon, B., Park, J., and Kang, J. Tree          Liu, Y., Kothari, P., Van Delft, B., Bellot-Gurlet, B., Mordan,
                 of clarifications: Answering ambiguous questions with             T., and Alahi, A. Ttt++: When does self-supervised
                 retrieval-augmented large language models. In The 2023            test-time training fail or thrive?   Advances in Neural
                 Conference on Empirical Methods in Natural Language               Information Processing Systems, 34:21808–21820, 2021.
                 Processing, 2023.       URL https://openreview.
                 net/forum?id=vDvFT7IX4O.                                       Mirza,M.J.,Soneira,P.J.,Lin,W.,Kozinski,M.,Possegger,
                                                                                   H., and Bischof, H. Actmad: Activation matchingtoalign
                                                             ´
               LeScao,T.,Fan,A.,Akiki,C.,Pavlick, E., Ilic, S., Hesslow,           distributions for test-time-training. In Proceedings of the
                              ´                                       ´
                 D., Castagne, R., Luccioni, A. S., Yvon, F., Galle, M.,           IEEE/CVFConferenceonComputerVisionandPattern
                 et al. Bloom: A 176b-parameter open-access multilingual           Recognition, pp. 24152–24161, 2023.
                 language model. 2023.
                                                                                Nado, Z., Padhy, S., Sculley, D., D’Amour, A., Lakshmi-
               Lee, J., Jung, D., Lee, S., Park, J., Shin, J., Hwang, U.,          narayanan, B., and Snoek, J. Evaluating prediction-time
                 and Yoon, S. Entropy is not enough for test-time adapta-          batch normalization for robustness under covariate shift.
                 tion: From the perspective of disentangled factors. In The        arXiv preprint arXiv:2006.10963, 2020.
                                                                             11
