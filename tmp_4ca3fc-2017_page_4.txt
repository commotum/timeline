                                        Scaled Dot-Product Attention                             Multi-Head Attention
                             Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
                             attention layers running in parallel.
                                                                   √
                             query with all keys, divide each by     d , and apply a softmax function to obtain the weights on the
                                                                       k
                             values.
                             In practice, we compute the attention function on a set of queries simultaneously, packed together
                             into a matrix Q. The keys and values are also packed together into matrices K and V . We compute
                             the matrix of outputs as:
                                                                                               QKT
                                                           Attention(Q,K,V) = softmax( √              )V                            (1)
                                                                                                  dk
                             Thetwomostcommonlyusedattentionfunctionsare additive attention [2], and dot-product (multi-
                             plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
                                  1
                             of √dk. Additive attention computes the compatibility function using a feed-forward network with
                             a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
                             muchfaster and more space-efﬁcient in practice, since it can be implemented using highly optimized
                             matrix multiplication code.
                             While for small values of d the two mechanisms perform similarly, additive attention outperforms
                                                          k
                             dot product attention without scaling for larger values of dk [3]. We suspect that for large values of
                             d , the dot products grow large in magnitude, pushing the softmax function into regions where it has
                               k
                                                         4                                                             1
                             extremely small gradients . To counteract this effect, we scale the dot products by √        .
                                                                                                                        dk
                             3.2.2   Multi-Head Attention
                             Instead of performing a single attention function with d         -dimensional keys, values and queries,
                                                                                         model
                             wefounditbeneﬁcial to linearly project the queries, keys and values h times with different, learned
                             linear projections to d , d and d dimensions, respectively. On each of these projected versions of
                                                     k   k      v
                             queries, keys and values we then perform the attention function in parallel, yielding d -dimensional
                                                                                                                        v
                             output values. These are concatenated and once again projected, resulting in the ﬁnal values, as
                             depicted in Figure 2.
                             Multi-head attention allows the model to jointly attend to information from different representation
                             subspaces at different positions. With a single attention head, averaging inhibits this.
                                 4To illustrate why the dot products get large, assume that the components of q and k are independent random
                                                                                                P
                                                                                                   d
                             variables with mean 0 and variance 1. Then their dot product, q · k =  k q k , has mean 0 and variance d .
                                                                                                   i=1 i i                           k
                                                                                 4
