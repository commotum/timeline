                                                                                    R                         P
                             that a differentiable estimator can be constructed:      q (z|x)f(z)dz ' 1          L f(g (x,(l)))
                                                                                       œÜ                    L    l=1    œÜ
                                     (l)
                             where     ‚àº p(). In section 2.3 we applied this trick to obtain a differentiable estimator of the
                             variational lower bound.
                             Take, for example, the univariate Gaussian case: let z ‚àº p(z|x) = N(¬µ,œÉ2). In this case, a valid
                             reparameterization is z = ¬µ + œÉ, where  is an auxiliary noise variable  ‚àº N(0,1). Therefore,
                                                                              P
                                                                            1    L             (l)         (l)
                             E        2 [f(z)] = E          [f(¬µ+œÉ)] '              f(¬µ+œÉ )where ‚àºN(0,1).
                              N(z;¬µ,œÉ )             N(;0,1)                L    l=1
                             For which qœÜ(z|x) can we choose such a differentiable transformation gœÜ(.) and auxiliary variable
                              ‚àº p()? Three basic approaches are:
                                   1. Tractable inverse CDF. In this case, let  ‚àº U(0,I), and let g (,x) be the inverse CDF of
                                                                                                     œÜ
                                      qœÜ(z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal,
                                      Gompertz, Gumbel and Erlang distributions.
                                   2. AnalogoustotheGaussianexample,forany‚Äùlocation-scale‚Äùfamilyofdistributionswecan
                                      choose the standard distribution (with location = 0, scale = 1) as the auxiliary variable
                                      , and let g(.) = location + scale ¬∑ . Examples: Laplace, Elliptical, Student‚Äôs t, Logistic,
                                      Uniform, Triangular and Gaussian distributions.
                                   3. Composition: It is often possible to express random variables as different transformations
                                      of auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed
                                      variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted
                                      sumofGammavariates),Beta, Chi-Squared, and F distributions.
                             When all three approaches fail, good approximations to the inverse CDF exist requiring computa-
                             tions with time complexity comparable to the PDF (see e.g. [Dev86] for some methods).
                             3   Example: Variational Auto-Encoder
                             In this section we‚Äôll give an example where we use a neural network for the probabilistic encoder
                             q (z|x) (the approximation to the posterior of the generative model p (x,z)) and where the param-
                              œÜ                                                                     Œ∏
                             eters œÜ and Œ∏ are optimized jointly with the AEVB algorithm.
                             Let the prior over the latent variables be the centered isotropic multivariate Gaussian p (z) =
                                                                                                                           Œ∏
                             N(z;0,I). Note that in this case, the prior lacks parameters. We let p (x|z) be a multivariate
                                                                                                          Œ∏
                             Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution pa-
                             rameters are computed from z with a MLP (a fully-connected neural network with a single hidden
                             layer, see appendix C). Note the true posterior p (z|x) is in this case intractable. While there is
                                                                                Œ∏
                             much freedom in the form qœÜ(z|x), we‚Äôll assume the true (but intractable) posterior takes on a ap-
                             proximate Gaussian form with an approximately diagonal covariance. In this case, we can let the
                             variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2:
                                                                       (i)               (i)   2(i)
                                                            logq (z|x    ) = logN(z;¬µ ,œÉ          I)                            (9)
                                                                 œÜ
                             where the mean and s.d. of the approximate posterior, ¬µ(i) and œÉ(i), are outputs of the encoding
                             MLP,i.e. nonlinear functions of datapoint x(i) and the variational parameters œÜ (see appendix C).
                                                                                               (i,l)           (i)          (i,l)
                             As explained in section 2.4, we sample from the posterior z            ‚àº qœÜ(z|x ) using z           =
                                  (i)  (l)       (i)     (i)    (l)         (l)
                             gœÜ(x , ) = ¬µ          +œÉ  where ‚àº N(0,I). With  we signify an element-wise
                             product. In this model both p (z) (the prior) and q (z|x) are Gaussian; in this case, we can use the
                                                           Œ∏                     œÜ
                             estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation
                             (see appendix B). The resulting estimator for this model and datapoint x(i) is:
                                                      J                                                L
                                  L(Œ∏,œÜ;x(i)) ' 1 X                   (i) 2       (i) 2     (i) 2     1 X            (i)  (i,l)
                                                           1+log((œÉ ) )‚àí(¬µ ) ‚àí(œÉ )                 +        logp (x |z        )
                                                   2                  j           j         j         L          Œ∏
                                                     j=1                                                l=1
                                            (i,l)    (i)    (i)    (l)          (l)
                                 where    z     =¬µ +œÉ                 and       ‚àºN(0,I)                                     (10)
                             Asexplained above and in appendix C, the decoding term logp (x(i)|z(i,l)) is a Bernoulli or Gaus-
                                                                                              Œ∏
                             sian MLP, depending on the type of data we are modelling.
                                2Note that this is just a (simplifying) choice, and not a limitation of our method.
                                                                               5
