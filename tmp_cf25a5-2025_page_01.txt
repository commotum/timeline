                                          Test-Time Learning for Large Language Models
                             JinwuHu12* ZitianZhang1* GuohaoChen12* XutaoWen1 ChaoShuai3 WeiLuo42*
                                                        BinXiao5† YuanqingLi2† MingkuiTan16†
                                          Abstract                                   1. Introduction
                     While Large Language Models (LLMs) have ex-                     Large Language Models (LLMs) such as GPT-4 (Achiam
                     hibited remarkable emergent capabilities through                et al., 2023) and LLaMA (Dubey et al., 2024) have signif-
                     extensive pre-training, they still face critical limi-          icantly advanced the field of natural language processing
                     tations in generalizing to specialized domains and              (NLP), demonstrating exceptional capabilities in both un-
                     handling diverse linguistic variations, known as                derstanding and generating human-like text (Wang et al.,
                     distribution shifts. In this paper, we propose a                2025a). Such success is achieved through extensive pre-
                     Test-Time Learning (TTL) paradigm for LLMs,                     training on massive corpora, enabling them to learn rich lan-
                     namely TLM,whichdynamicallyadaptsLLMs                           guage representations that facilitate superior performance
                     to target domains using only unlabeled test data                in various NLP tasks (Hu et al., 2025a; Zhong et al., 2024).
                     during testing. Specifically, we first provide em-              Despite their impressive capabilities, LLMs face significant
                     pirical evidence and theoretical insights to reveal             challenges when deployed in real-world environments with
                     that more accurate predictions from LLMs can be                 dynamic and diverse data distributions. These challenges
                     achievedbyminimizingtheinputperplexityofthe                     stem from the inherent sensitivity of deep learning models,
                     unlabeled test data. Based on this insight, we for-             including LLMs, to distribution shifts between training and
                     mulate the Test-Time Learning process of LLMs                   test data, often leading to substantial performance degrada-
                     as input perplexity minimization, enabling self-                           ¨
                     supervised enhancement of LLM performance.                      tion (Akyurek et al., 2024). These distributional shifts mani-
                     Furthermore, we observe that high-perplexity                    fest in two main ways: 1) Domain-Specific Terminology:
                     samples tend to be more informative for model                   Encountering rare or specialized terms and structures in
                     optimization. Accordingly, we introduce a Sam-                  fields such as medicine or agriculture may limit the model’s
                     ple Efficient Learning Strategy that actively se-               performance (Gu et al., 2021). 2) Linguistic Diversity
                     lects and emphasizes these high-perplexity sam-                 Variations: Variations in user intent and linguistic diversity,
                     ples for test-time updates. Lastly, to mitigate                 including dialects and slang, lead to distributional discrep-
                     catastrophic forgetting and ensure adaptation sta-              ancies that negatively affect the model’s comprehension and
                     bility, we adopt Low-Rank Adaptation (LoRA)                     response generation (Bella et al., 2024).
                     instead of full-parameter optimization, which al-               Recently, several attempts have been proposed to improve
                     lows lightweight model updates while preserving                 the performance of models in dynamic and diverse real-
                     more original knowledge from the model. We                      world environments. Most existing methods can be broadly
                     introduce the AdaptEval benchmark for TTL and                   categorized into four types, as shown in Table 1. Fine-
                     demonstrate through experiments that TLM im-                    tuning (Hu et al., 2022; Thirunavukarasu et al., 2023) adapts
                     proves performance by at least 20% compared to                  pre-trained models to specific tasks by updating their pa-
                     original LLMs on domain knowledge adaptation.                   rameters with labeled data, but it is constrained by the need
                  *Equal contribution 1School of Software Engineering, South         for extensive labeled datasets, limiting its practicality in
                ChinaUniversity of Technology, China 2Pazhou Laboratory, China       dynamic environments. Retrieval-Augmented Generation
                3Zhejiang University, China 4South China Agricultural University,    (RAG) (Fan et al., 2024) improves performance without
                China. 5Chongqing University of Posts and Telecommunications,        requiring labeled data updates by leveraging external knowl-
                China6KeyLaboratoryofBigDataandIntelligentRobot,Ministry             edge retrieved during inference, but its success depends
                of Education, China. Correspondence to: Mingkui Tan <mingkui-        heavily on the quality of the retrieved information. Test-
                tan@scut.edu.cn>,YuanqingLi<auyqli@scut.edu.cn>,BinXiao              TimeAdaptation(TTA)(Wangetal.,2021;Niuetal.,2022a;
                <xiaobin@cqupt.edu.cn>.                                              Chenetal., 2024b) adjusts model parameters during infer-
                Proceedings of the 42nd International Conference on Machine          ence using only unlabeled test data, allowing the model to
                Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025          adapt to distribution shifts in real-time. However, most TTA
                bythe author(s).
                                                                                  1
