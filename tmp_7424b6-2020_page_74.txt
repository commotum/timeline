             [SMM+17] NoamShazeer,AzaliaMirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
                      Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint
                      arXiv:1701.06538, 2017.
               [SPP+19] MohammadShoeybi,MostofaPatwary,RaulPuri,PatrickLeGresley,JaredCasper,andBryanCatanzaro.
                      Megatron-lm: Training multi-billion parameter language models using model parallelism, 2019.
                                         ¨
                 [SS20] Timo Schick and Hinrich Schutze. Exploiting cloze questions for few-shot text classiﬁcation and natural
                      language inference. arXiv preprint arXiv:2001.07676, 2020.
                  +
              [STQ 19] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. MASS: Masked sequence to sequence
                      pre-training for language generation. arXiv preprint arXiv:1905.02450, 2019.
              [TFR+17] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain
                      randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ
                      international conference on intelligent robots and systems (IROS), pages 23–30. IEEE, 2017.
                [TL05] Peter D. Turney and Michael L. Littman. Corpus-based learning of analogies and semantic relations.
                      CoRR,abs/cs/0508103, 2005.
                [TL18] Trieu H. Trinh and Quoc V. Le. A simple method for commonsense reasoning. arXiv preprint
                      arXiv:1806.02847, 2018.
              [TLBS03] Peter D. Turney, Michael L. Littman, Jeffrey Bigham, and Victor Shnayder. Combining independent
                      modules to solve multiple-choice synonym and analogy problems. CoRR, cs.CL/0309035, 2003.
                [Tur20] Project Turing. Microsoft research blog, Feb 2020.
                  +
              [VBL 16] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching Networks for One
                      Shot Learning. In Advances in neural information processing systems, pages 3630–3638, 2016.
              [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz
                      Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing
                      systems, 2017.
                  +
              [WPN 19] AlexWang,YadaPruksachatkun,NikitaNangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer
                      Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understand-
                      ing systems. In Advances in Neural Information Processing Systems, pages 3261–3275, 2019.
             [WXH+18] YirenWang,YingceXia,TianyuHe,FeiTian,TaoQin,ChengXiangZhai,andTie-YanLiu. Multi-agent
                      dual learning. ICLR 2019, 2018.
              [XDH+19] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc V. Le. Unsupervised data
                      augmentation for consistency training, 2019.
                  +
              [YdC 19] Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski,
                      Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. Learning and evaluating
                      general linguistic intelligence. arXiv preprint arXiv:1901.11373, 2019.
                  +
              [YDY 19] ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,RuslanSalakhutdinov,andQuocV.Le. XLNet:
                      Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237,
                      2019.
                  +
              [ZHB 19] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi, and Yejin Choi. Hellaswag: Can a machine
                      really ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019.
                  +
              [ZHR 19] RowanZellers,AriHoltzman,HannahRashkin,YonatanBisk,AliFarhadi,FranziskaRoesner,andYejin
                      Choi. Defending against neural fake news. arXiv preprint arXiv:1905.12616, 2019.
                  +
              [ZLL 18] Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.
                      ReCoRD:Bridgingthegapbetweenhumanandmachinecommonsensereadingcomprehension. arXiv
                      preprint arXiv:1810.12885, 2018.
             [ZSW+19a] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul
                      Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences, 2019.
                                                  74
