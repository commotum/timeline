                                                         (l)
                                  1. SampleLvalues{z }fromtheposteriorusinggradient-basedMCMC,e.g. HybridMonte
                                     Carlo, using ∇ logp (z|x) = ∇ logp (z)+∇ logp (x|z).
                                                    z      θ           z     θ         z      θ
                                                                                     (l)
                                  2. Fit a density estimator q(z) to these samples {z  }.
                                  3. Again, sample L new values from the posterior. Plug these samples, as well as the ﬁtted
                                     q(z), into the following estimator:
                                                           L                (l)      !−1
                                                  (i)       1 X         q(z   )                         (l)         (i)
                                             p (x )'                                         where    z    ∼p (z|x )
                                               θ            L     p (z)p (x(i)|z(l))                           θ
                                                              l=1   θ    θ
                            Derivation of the estimator:
                                                         R            R q(z)pθ(x(i),z) dz
                                                1          q(z)dz            pθ(x(i),z)
                                                 (i)  =       (i)  =             (i)
                                            p (x )       p (x )             p (x )
                                              θ          Z θ                 θ
                                                            p (x(i),z)    q(z)
                                                      =      θ                     dz
                                                             p (x(i)) p (x(i),z)
                                                         Z    θ          θ
                                                      = p (z|x(i))       q(z)     dz
                                                             θ        p (x(i),z)
                                                                        θ
                                                            L            (l)
                                                      ' 1 X           q(z  )         where    z(l) ∼ p (z|x(i))
                                                         L                 (i) (l)                     θ
                                                                p (z)p (x |z )
                                                            l=1  θ     θ
                            E MonteCarloEM
                            The Monte Carlo EM algorithm does not employ an encoder, instead it samples from the pos-
                            terior of the latent variables using gradients of the posterior computed with ∇z logpθ(z|x) =
                            ∇ logp (z) + ∇ logp (x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog
                              z     θ          z     θ
                            steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5
                            weight updates steps using the acquired sample. For all algorithms the parameters were updated
                            using the Adagrad stepsizes (with accompanying annealing schedule).
                            The marginal likelihood was estimated with the ﬁrst 1000 datapoints from the train and test sets,
                            for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte
                            Carlo with 4 leapfrog steps.
                            F FullVB
                            Aswritten in the paper, it is possible to perform variational inference on both the parameters θ and
                            the latent variables z, as opposed to just the latent variables as we did in the paper. Here, we’ll derive
                            our estimator for that case.
                            Let p (θ) be some hyperprior for the parameters introduced above, parameterized by α. The
                                  α
                            marginal likelihood can be written as:
                                                     logp (X) = D       (q (θ)||p (θ|X))+L(φ;X)                             (13)
                                                          α          KL φ         α
                            where the ﬁrst RHS term denotes a KL divergence of the approximate from the true posterior, and
                            where L(φ;X)denotes the variational lower bound to the marginal likelihood:
                                              L(φ;X)=Z q (θ)(logp (X)+logp (θ)−logq (θ)) dθ                                 (14)
                                                              φ           θ            α            φ
                            Note that this is a lower bound since the KL divergence is non-negative; the bound equals the true
                            marginal when the approximate and true posteriors match exactly. The term logp (X) is composed
                                                                                                              θ
                                                                                                             P
                            of a sum over the marginal likelihoods of individual datapoints logp (X) =          N logp (x(i)),
                                                                                                   θ            i=1     θ
                            which can each be rewritten as:
                                               logp (x(i)) = D      (q (z|x(i))||p (z|x(i))) + L(θ,φ;x(i))                  (15)
                                                    θ            KL φ             θ
                                                                             12
