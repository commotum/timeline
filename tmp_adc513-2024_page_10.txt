              [27] RuipuLuo,ZiwangZhao,MinYang,JunweiDong,Minghui                    [42] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingn-
                    Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley:                 ing Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang,
                    Video assistant with large language model enhanced ability.           Zhiyong Wang, et al.     Lamm: Language-assisted multi-
                    arXiv preprint arXiv:2306.07207, 2023. 2, 3, 7                        modal instruction-tuning dataset, framework, and bench-
              [28] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFa-                          mark. arXiv preprint arXiv:2306.06687, 2023. 2, 3
                    had Shahbaz Khan. Video-chatgpt: Towards detailed video          [43] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,
                    understanding via large vision and language models. arXiv             Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.
                    preprint arXiv:2306.05424, 2023. 2, 3, 7                              Mm-vet: Evaluating large multimodal models for integrated
              [29] ml foundations.                              Openflamingo.             capabilities. arXiv preprint arXiv:2308.02490, 2023. 2
                    https://github.com/mlfoundations/open flamingo,      2023.       [44] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu,
                    7                                                                     and Tat-Seng Chua. Transfer visual prompt generator across
              [30] OpenAI.                          Introducing        chatgpt.           llms. abs/23045.01278, 2023. 7
                    https://openai.com/blog/chatgpt, 2022. 2                         [45] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,
              [31] OpenAI. Gpt-4 technical report, 2023. 2                                Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang
              [32] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan                 Zhang, Haodong Duan, Hang Yan, et al.             Internlm-
                    Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-                    xcomposer: A vision-language large model for advanced
                    ing multimodal large language models to the world. arXiv              text-image comprehension and composition. arXiv preprint
                    preprint arXiv:2306.14824, 2023. 2, 3, 7                              arXiv:2309.15112, 2023. 2, 3, 7
              [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya                [46] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao,
                    Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,                 YewKenChia,andLidongBing. M3exam: A multilingual,
                    AmandaAskell,PamelaMishkin,JackClark,etal. Learning                   multimodal, multilevel benchmark for examining large lan-
                    transferable visual models from natural language supervi-             guage models. arXiv preprint arXiv:2306.05179, 2023. 2
                    sion. In International conference on machinelearning, pages      [47] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
                    8748–8763. PMLR, 2021. 6, 7                                           hamed Elhoseiny. Minigpt-4: Enhancing vision-language
              [34] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and               understanding with advanced large language models. arXiv
                    Deng Cai. Pandagpt: One model to instruction-follow them              preprint arXiv:2304.10592, 2023. 2, 3, 7
                    all. arXiv preprint arXiv:2305.16355, 2023. 2, 3
              [35] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
                    Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun
                    Huang, and Xinlong Wang. Generative pretraining in multi-
                    modality. arXiv preprint arXiv:2307.05222, 2023. 7
              [36] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
                    Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun
                    Huang, and Xinlong Wang. Generative pretraining in multi-
                    modality. arXiv preprint arXiv:2307.05222, 2023. 2, 3
              [37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
                                                           ´
                    Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste
                        `
                    Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
                    Llama: Open and efficient foundation language models.
                    arXiv preprint arXiv:2302.13971, 2023. 2, 3
              [38] Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan Kankan-
                    halli, and Ying Shan.    What makes for good visual to-
                    kenizers for large language models?         arXiv preprint
                    arXiv:2305.12223, 2023. 3, 7
              [39] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng
                    Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint
                    arXiv:2309.05519, 2023. 2, 3, 7
              [40] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo
                    Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao,
                    and Ping Luo.    Lvlm-ehub: A comprehensive evaluation
                    benchmarkforlarge vision-language models. arXiv preprint
                    arXiv:2306.09265, 2023. 2, 3
              [41] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
                    Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
                    Yaya Shi, et al.   mplug-owl: Modularization empowers
                    large language models with multimodality. arXiv preprint
                    arXiv:2304.14178, 2023. 2, 3, 7
                                                                                 13308
