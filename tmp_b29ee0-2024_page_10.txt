                          10     S. He et al.
                          categories?ASSISTANT:{category}<SEG>, {category} <SEG>, ...” Tosim-
                          plify the output and ensure it has only one possible answer, we add the con-
                          straints “in the alphabetical order of the categories”. To avoid generating
                          class names not in the dataset, we incorporate category names in a dataset
                          into the prompts during training and inference.
                          Referring Segmentation Dataset.Weusetemplateprompts:“USER:<POINT>
                          Can you segment the object {description} in this point cloud? ASSISTANT:
                          {category}<SEG>.”,where{description}isthegivenexplicitdescriptionfrom
                          referring segmentation dataset. It is worth noting that during training, we also
                          use other templates to generate the QA data to ensure data diversity. We add
                          {category}infrontof <SEG>tounifytheoutputformatsothatwhenoutputting
                          semantic masks, the output category name is the label it predicts.
                          Evaluation Metrics. We follow most previous works on 3D segmentation [22,
                          27,54] to adopt mIoU as primary metric. mIoU is defined by the average of
                          all per-point cloud scene Intersection-over-Unions (IoUs). Besides, we employ
                          accuracy (Acc) as a metric to evaluate whether the model accurately identifies
                          targets with which the predictions have an IoU greater than 0.5.
                          4.2  Implementation Details
                          In our experiments, unless specified otherwise, we employ the LLaMA2-7B model
                          [59] as the large language model F and Uni3D [81] as the point cloud processing
                          backbone E. The training stage leverages the deepspeed [51] engine for efÏciency,
                          with the AdamW [39] optimizer guiding the learning process. The learning rate
                          andweightdecayaresetto0.0003and0,respectively,enhancedbyaWarmupDe-
                          cayLR learning rate scheduler that initiates with 100 warmup iterations. The
                          projection layer γ utilizes an MLP with channel sizes of [256, 4096, 4096]. We
                          set balancing weight λ   , λ , and λ   to 1.0, 2.0, 2.0, respectively. The
                                             txt_gen bce     dice
                          experiments utilize a total batch size of 16, distributed across 4 NVIDIA 80G
                          A100 GPUs, and span 5,000 iterations, culminating in a training period of ap-
                          proximately 3 days. During training, we use all mentioned datasets in Sec. 4.1
                          for joint training by leveraging task-specific prompts. For evaluation on a specific
                          dataset, we finetune the trained model on the corresponding dataset.
                          4.3  Results on Instruct3D
                          The instruction segmentation results, as detailed in Table 1, underscore a signif-
                          icant advancement: where existing methodologies fall short, our model demon-
                          strates exceptional prowess, achieving a more than 15% improvement in mIoU
                          for tasks requiring intricate reasoning. Unlike conventional referring segmenta-
                          tion tasks, instruction segmentation demands not just identification but also
                          understanding, necessitating the model’s reasoning capabilities and access to
                          world knowledge. Existing approaches, confined to explicit references, struggle
                          with implicit queries due to their lack of understanding, which further under-
                          scores the task’s inherent challenges. In contrast, our model leverages LLMs to
                          bridge this gap, demonstrating superior performance by comprehending and in-
                          terpreting the queries accurately. Moreover, SegPoint configuration substantially
                                            †
                          outperforms SegPoint , highlighting the critical role of our designed Geometric
