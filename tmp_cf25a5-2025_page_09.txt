                                                    Test-Time Learning for Large Language Models
              the number of backward by 69.7% (5000 → 1514) in the           specifically highlighted here.
              online setting. This is because, as the LLM is updated, some
              samples progressively become easier for the model, and are     References
              thus excluded from TTL in Eqn. (6)
              Experiments on Quantized LLM. To evaluate the perfor-          Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I.,
              manceofourmethodonquantizedLLMs,weconductexper-                  Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S.,
              iments on a 4-bit quantized version of Llama3-8B-Instruct,       Anadkat, S., et al. Gpt-4 technical report. arXiv preprint
              following the settings of QLoRA (Dettmers et al., 2024).         arXiv:2303.08774, 2023.
              FromTable5,ourmethodalsodemonstrates strong perfor-                ¨
                                                                             Akyurek,E.,Damani,M.,Qiu,L.,Guo,H.,Kim,Y.,andAn-
              manceontarget domain datasets when applied to quantized          dreas, J. The surprising effectiveness of test-time training
              LLMs. Specifically, the proposed method improves at least        for abstract reasoning. arXiv preprint arXiv:2411.07279,
              25.0%cover Llama3-8B-Instruct (NF4) on four datasets on          2024.
              DomainBench,highlighting the broad applicability of our
              TTLscheme.                                                     Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin,
                                                                               D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen,
              6. Conclusion                                                    Z., et al.   Palm 2 technical report.    arXiv preprint
                                                                               arXiv:2305.10403, 2023.
              In this paper, we propose a novel Test-Time Learning (TTL)     Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H. Self-
              method for Large Language Models (LLMs), named TLM,              rag: Learning to retrieve, generate, and critique through
              to address the challenges posed by dynamic and diverse           self-reflection. In The Twelfth International Conference
              data distributions in real-world environments. By leverag-       onLearning Representations, 2024.
              ing only unlabeled test data, TLM efficiently adapts LLMs
              and improves their robustness in target domains. Specifi-      Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan,
              cally, through observation and theoretical analysis, we argue    Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical
              that reducing output perplexity can be achieved by minimiz-      report. arXiv preprint arXiv:2309.16609, 2023.
              ing the input perplexity of unlabeled test data. Based on this
              insight, we adopt input perplexity minimization as the op-                   ¨                       ¨
                                                                             Bartler, A., Buhler, A., Wiewel, F., Dobler, M., and Yang,
              timization objective for test-time LLM updates. Moreover,        B. Mt3: Meta test-time training for self-supervised test-
              wefindthathigh-perplexity test samples play a more crucial       time adaption. In International Conference on Artificial
              role in model updates than low-perplexity samples. This          Intelligence and Statistics, pp. 3080–3090. PMLR, 2022.
              insight motivates the development of our Sample Efficient
              Learning Strategy, which actively selects and emphasizes       Bella, G., Helm, P., Koch, G., and Giunchiglia, F. Tackling
              high-perplexity test samples for backpropagation, optimiz-       language modelling bias in support of linguistic diversity.
              ing the learning process. Lastly, we reveal that Low-Rank        InThe2024ACMConferenceonFairness,Accountability,
              Adaptation is more effective than full parameter updates         andTransparency, pp. 562–572, 2024.
              in mitigating catastrophic forgetting, and we utilize it for   Bengio, Y., Ducharme, R., and Vincent, P. A neural proba-
              parameter updates during TTL, enabling lightweight model         bilistic language model. Advances in neural information
              adaptation while effectively preserving prior knowledge.         processing systems, 13, 2000.
              Acknowledgements                                               Boudiaf, M., Mueller, R., Ben Ayed, I., and Bertinetto, L.
              This work was partially supported by the Joint Funds of          Parameter-free online test-time adaptation. In Proceed-
              the National Natural Science Foundation of China (Grant          ings of the IEEE/CVF Conference on Computer Vision
              No.U24A20327),Key-AreaResearchandDevelopmentPro-                 andPattern Recognition, pp. 8344–8353, 2022.
              gram of Guangdong Province (2018B010107001), TCL               Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
              Science and Technology Innovation Fund, and the Young            Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
              Scholar Project of Pazhou Lab (No.PZL2021KF0021).                Askell, A., et al. Language models are few-shot learners.
                                                                               Advances in neural information processing systems, 33:
              ImpactStatement                                                  1877–1901, 2020.
              This paper presents work whose goal is to advance the field    Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K.,
              of Machine Learning. There are many potential societal           Chen, H., Yi, X., Wang, C., Wang, Y., et al. A survey on
              consequences of our work, none of which we feel must be          evaluation of large language models. ACM Transactions
                                                                               on Intelligent Systems and Technology, 15(3):1–45, 2024.
                                                                          9
