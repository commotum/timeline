         www.nature.com/scientificdata/                                          www.nature.com/scientificdata
                                Fig. 5  Distribution of human errors for a8610ef7.json. In the lef㘶 column, we show 3 of 4 training 
                                examples (for illustrative purposes) and the test input seen by participants, as well as the true test output grid. In 
                                the right panel, we show a non-exhaustive selection of dif㘶erent incorrect submissions from participants in the 
                                H-ARC dataset that attempted this particular problem.
                                                        18                                   
                                                                                             R
                                samples using its NUTS sampler . Convergence was assessed using trace plots and   values, which were less 
                                than 1.01. T㔴e mean accuracy for each split was computed by averaging estimated probability of success across 
                                all participants and tasks (see Table 2), using sampled parameters during inference. For comparison, we also 
                                report empirical mean accuracy which is computed as the mean success rate across tasks on the training and 
                                evaluation splits, respectively. More details on model-based estimation can be found in the accompanying code 
                                repository.
                                performance on the training set.  According to the IRT estimate, the mean accuracy on the training set 
                                tasks is 70.5% (94% HDI [69.3%, 71.6%]). For comparison, the empirical mean task accuracy is 76.2% (SD = 
                                21.5%). We also report model estimates of mean task accuracy for participants’ f㘶rst and second attempts, 54.6% 
                                (94% HDI [53.3%, 55.8%]) and 66.6% (94% HDI [65.4%, 67.8%]), respectively. Participants solved ARC training 
                                tasks in 1.3 attempts on average, with the modal and median number of attempts being 1. Of the 400 training 
                                tasks, we f㘶nd 74 tasks (18.5% of the training set) for which all participant who attempted the task generated the 
                                correct solution within three submissions or fewer. Conversely, we also f㘶nd 5 tasks (1.3% of the training set) 
                                which no participants were able to solve correctly in three attempts or fewer. Note that since each problem is 
                                attempted by approximately 10 people, this result simply means that we did not f㘶nd anyone in a set of 10 that 
                                could solve the problem. T㔴is is not evidence that these problems are not in principle solvable by a person. Finally, 
                                we f㘶nd that 40.0% of participants solved all training set tasks they were presented and that 8.6% of participants 
                                solved none.
                                performance on the evaluation set.  According to the IRT estimate, the mean accuracy on the evalua-
                                tion set tasks is 65.7% (94% HDI [64.6%, 66.8%]). For comparison, the empirical mean task accuracy is 64.5%  
                                (SD = 22.5%). We also report model estimated mean task accuracy for participants’ f㘶rst and second attempts, 
                                49.2% (94% HDI [47.9%, 50.4%]) and 61.6% (94% HDI [60.5%, 62.8%]), respectively. On average, participants 
                                solve ARC evaluation tasks in 1.4 attempts, with the modal and median number of attempts being 1. Of the 400 
         Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                         7
