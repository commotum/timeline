                                                                                              ScatterFormer        9
                                                           DSVT                     ScatterFormer
                                                              6%Voxel          1%Offset     0.6%Voxel
                                                              Sorting          Generation   Sorting
                                                 18%Indices 
                                                 Generation
                                                       76%Computation               98.4% Computation
                                 Fig.5: RuntimedecompositioninDSVT[47]andScatterFormerbackbones.Theoffsets
                                 generation means computing the starting address of each chunk in the flattened matrix.
                                 In our experiments, we found that this axis-decomposed large kernel design can
                                 achieve a better precision-latency trade-off. With these lengthy kernels, the voxel
                                 features can be ef√èciently blended among different windows.
                                 3.4   Detection Head and Loss
                                 To complement existing detection heads, ScatterFormer generates dense BEV
                                 feature maps from sparse voxel representations by placing them back to their
                                 spatial locations and filling the unoccupied positions with zeros. For the BEV
                                 network, we simply follow [53] by adopting a cascaded CNN with convolutional
                                 blocks at multiple levels of strides. In terms of detection head and loss function,
                                 we follow the design identical to that used in DSVT [47]. We train the detection
                                 model with heatmap estimation, bounding box regression, and incorporate an
                                 IoU loss for confidence calibration. On the nuScenes dataset, the detection head
                                 follows the architecture used in Transfusion [2].
                                 4    Experiments
                                 4.1   Datasets and Evaluation Metrics
                                 Waymo Open Dataset (WOD). This dataset contains 230,000 annotated
                                 samples split into 160,000 for training, 40,000 for validation, and 30,000 for
                                 testing. It uses two metrics for 3D object detection: mean average precision
                                 (mAP) and mAP weighted by heading accuracy (mAPH), further categorized
                                 into Level 1 (L1) for objects detected by more than five LiDAR points and Level
                                 2 (L2) for those detected with at least one point.
                                    NuScenes. This dataset comprises 40,000 annotated samples, with 28,000
                                 for training, 6,000 for validation, and 6,000 for testing. On this dataset, the
                                 model performance is measured by mean average precision (mAP) across mul-
                                 tiple distance thresholds (0.5, 1, 2, and 4 meters) and the nuScenes detection
                                 score (NDS), which combines mAP with a weighted sum of five additional met-
                                 rics assessing true positive predictions in translation, scale, orientation, velocity,
                                 and attribute accuracy.
