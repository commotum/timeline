                                                       Large-scale few-shot program induction and synthesis 
                      Method          Pretrained    Text context     Example accuracy       Task accuracy     Entity task acc.    Collection task acc. 
                                                                          5-shot/10-shot     5-shot/10-shot     5-shot/10-shot          5-shot/10-shot 
                                          no           without             0.351 / 0.349      0.107 / 0.107       0.126 / 0.126          0.000 / 0.000 
                  BART-Robustfll                         with              0.340 / 0.338      0.114 / 0.116       0.135 / 0.137          0.000 / 0.000 
                     Induction                         without             0.477 / 0.492      0.210 / 0.223       0.248 / 0.263          0.001 / 0.001 
                                          yes            with              0.504 / 0.521      0.246 / 0.259       0.289 / 0.305          0.010 / 0.010 
                                          no           without             0.464 / 0.473      0.315 / 0.325       0.526 / 0.538          0.124 / 0.123 
                  BART-Robustfll                         with              0.510 / 0.516      0.363 / 0.370       0.560 / 0.565          0.240 / 0.249 
                     Synthesis                         without             0.570 / 0.579      0.420 / 0.429       0.622 / 0.633          0.285 / 0.283 
                                          yes            with              0.592 / 0.602      0.444 / 0.456       0.645 / 0.655          0.306 / 0.313 
                Table 2. 
                        Comparison of the different design choices based on BART-RobustFill. Synthesizing the program instead of directly predicting 
                the outputs gave the biggest boost. Interestingly, in previous versions of the dataset where the programs were not sliced, induction gave 
                better results, highlighting the need for clean code examples. Then, using the pretrained weights for the transformer has a very noticeable 
                positive effect. Having the text context consistently improves performance by a noticeable margin. Finally, going from 5 to 10 examples 
                gives a small, but positive boost to all methods. Note: since the camera-ready, we have made a fnal version of the dataset with more 
                programs and diversity. The dataset description is the same, but concrete metrics and statistics change. You can fnd it the updated PDF 
                and materials at: https://lis.csail.mit.edu/progres. 
                can use it as an oracle for active learning. However, this has        provide PROGRES, a meta-dataset of more than 200,000 
                the added challenge (and beneft) of being unconstrained,              tasks based on real programs, of a wide variety of problems 
                with the model generating its own queries.                            and input-output examples. Evaluations show a wide margin 
                                                                                      for improvements for current program induction and syn-
                Leveraging instructive examples and structured inputs                 thesis methods. The scale of data, two orders of magnitude 
                In classic Machine Learning we assume examples come                   larger than previous work, together with the inclusion of 
                from randomly sampling the input distribution. Therefore,             text contexts and a custom code interpreter open up many 
                most training examples lie at the conceptual center of the in-        possible avenues for future research. 
                put space. In contrast, our examples tend to be extremal, pok-
                ing at the edge-cases to differentiate the true program from          Acknowledgements 
                reasonable alternatives. Humans actively use the knowledge            We would like to thank Maria Bauza for her detailed and 
                that a teacher is giving them instructive examples to update          perceptive comments on the paper drafts and anonymous 
                their priors more effectively (Shafto et al., 2014). Construct-       reviewer #3 for their thorough, insightful review; both sig-
                ing algorithms capable of similar inferences is a promising           nifcantly improved our work. We would also like to thank 
                avenue for future work. Similarly, our inputs tend to pertain         Lauren Milechin and the MIT supercloud team, that allowed 
                to a class much smaller than that defned by their C++ type.           us to scale the computations required to generate the dataset. 
                For instance, for many problems, all integer inputs are posi-         We would also like to thank Wim Lavrijsen and Sumith Ku-
                tive, which makes some implementations easier. Being able             lal for their quick and detailed responses about cppyy and 
                to infer these conditions in the inputs and exploit them is           SPoC, respectively. Finally, we would like to thank Oriol 
                something human programmers often do and an avenue for                Vinyals and Charles Sutton for their comments about this 
                improving the performance of current systems.                         work and Eric Navarro and Marta Alet for their help parsing 
                                                                                      CodeForces problems. 
                Leveraging intermediate states for program synthesis                  We gratefully acknowledge support NSF grant 1723381; 
                Our environment can be run in interactive mode, receiv-               from AFOSR grant FA9550-17-1-0165; from ONR grant 
                ing individual (or groups of) instructions, and returning             N00014-18-1-2847; from GoodAI, from the Honda Re-
                the relevant variables that changed value. This facilitates           search Institute, from MIT-IBM Watson Lab; and from 
                program synthesis that inspects intermediate values to in-            SUTD Temasek Laboratories.  We also acknowledge the 
                form search, which has been shown to signifcantly boost               MIT SuperCloud and Lincoln Laboratory Supercomputing 
                performance (Ellis et al., 2019; Nye et al., 2020).                   Center for providing HPC resources that have contributed to 
                5. Conclusion                                                         the research results reported within this paper. Any opinions, 
                                                                                      fndings, and conclusions or recommendations expressed in 
                We have presented a new way of scaling up few-shot pro-               this material are those of the authors and do not necessarily 
                gram induction and program synthesis benchmarks.  We                  refect the views of our sponsors. 
