                                                              Large-scale few-shot program induction and synthesis 
                  as  well  as  the  support  set  examples  arranged  sequen-
                  tially,   i.e.,   [header]  |  [Input1];  [Output1] 
                  |  [Input2];  [Output2]  |  ...  [Input10]; 
                  [Output10]. The model is trained to output the program 
                  body.  At evaluation time, we perform a beam search of 
                  beam size 10,  and select the program out of those 10 
                  candidates which performs the best on the support set and 
                  execute that program on the query set to produce fnal 
                  predictions for each query example.  Note that executing 
                  programs on the support set allows us to perform a search 
                  over  the  possible  candidate  programs,  which  has  been 
                  shown  to  greatly  increase  the  performance  of  neural                      Figure 5. Dependence of example and task accuracy when only 
                  program synthesis techniques (Devlin et al., 2017). Figure 5                    evaluating the frst p âˆˆ [1, 10] programs coming from the beam 
                  shows how,  in our case,  it  signifcantly  improves  task                      search. Most of the the performance comes from the frst predic-
                  accuracy. To evaluate all program candidates we only need                       tion, but the rest provides a noticeable boost of more than one third 
                  to evaluate entire functions, instead interpreting the program                  in relative performance. 
                  line-by-line. We thus relied on cppyy (Lavrijsen & Dutta,                       less than one may expect. Human experiments in a prelimi-
                  2016) which provides effcient python-C++ bindings.                              nary version of this dataset indicated that their learning was 
                  We also test if additionally conditioning on the overall pro-                   also very fast, saturating at 5 examples. This also matches 
                  gram description text increases performance. In these exper-                    previous human experiments on inferring list-editing pro-
                  iments, we append the text to the examples.                                     grams (Rule, 2020). Therefore, it appears that there is also 
                  Transformer-based  end-to-end  prediction  Our  next                            a fundamental search problem, typical in program synthesis, 
                  baseline, also inspired by Devlin et al. (2017), is a program                   where the task space is exponential. 
                  induction model.  Using an architecture analogous to the                        Dependence on text context  Adding the text context im-
                  neural program synthesis model, we use neural models to                         proved performance by a surprisingly high amount: between 
                  perform neural program induction, i.e., given a training set                    16% relative improvent in example accuracy and 23% in 
                  of k input-output examples and a single test input, produce                     task accuracy.We did not observe any signifcant correlation 
                  the corresponding output. Instead of generating the target                      with the type signature, instead producing improvements 
                  code, the induction model is instead trained to approximate                     across the board.  Further research into more effectively 
                  the execution of the code directly. Our model is identical                      combining the two modalities of input is a promising area 
                  to the program synthesis model above, except that the test                      of future work. 
                  input example is prepended to the context string, and the 
                  model is trained to produce the target test output.                             4.4. Open challenges 
                  4.3. Discussion                                                                 The previous section highlights the need for better few-
                                                                                                  shot program induction and program synthesis methods. 
                  Comparison between program synthesis and program                                Moreover, this benchmark opens up multiple interesting 
                  induction  Somewhat surprisingly, in preliminary experi-                        challenges. We highlight a few: 
                  ments end-to-end program induction performed better than 
                  predicting the program as an intermediate.  However, the                        Graph  representations  of  programs  although  not 
                  order changed when we started slicing the programs, which                       explored  in  this  work,  the  inclusion  of  PEGs  in  PRO-
                  removed superfuous lines of code, making learning to syn-                       GRES  facilitates  the  evaluation  of  Graph  Neural 
                  thesize easier.  There, the synthesis approach has a great                      Networks (Scarselli et al., 2009; Bronstein et al., 2017; 
                  advantage, as it can generate the program and let it solve                      Battaglia et al., 2018) for code analysis at scale, an approach 
                  arbitrarily complex inputs.                                                     that has already shown promise (Li et al., 2019). 
                  This is specially the case for tasks returning collections                      Open-ended active learning  The ability of asking for 
                  (arrays, lists, strings, and matrices). This is understandable,                 useful labels would allow a program synthesis method to 
                  since , for collections, program induction had to generate                      interact with a human, improving the data effciency by 
                  long stretches of tokens without error, which is very hard to                   actively trying to resolve uncertainties. This topic has been 
                  do, especially for all inputs.                                                  explored in the past for program induction (Pu et al., 2018), 
                  Dependence on number of program examples  Perfor-                               but only in selecting from a small number of examples. 
                  mance consistently improves from 5 to 10 examples, but by                       Since we provide an environment and the true program, we 
