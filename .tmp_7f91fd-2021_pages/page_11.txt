                                                  Large-scale few-shot program induction and synthesis 
              Iwata, T. and Kumagai, A. Meta-learning from tasks with         Li, Y., Gimeno, F., Kohli, P., and Vinyals, O. Strong general-
                 heterogeneous attribute spaces. Advances in Neural In-         ization and effciency in neural programs. arXiv preprint 
                 formation Processing Systems, 33, 2020.                        arXiv:2007.03629, 2020. 
              Iyer, S., Konstas, I., Cheung, A., and Zettlemoyer, L. Map-     Mendez, J. A. and Eaton, E.  Lifelong learning of com-
                 ping language to code in programmatic context.  arXiv          positional structures. arXiv preprint arXiv:2007.07732, 
                 preprint arXiv:1808.09588, 2018.                               2020. 
              Joulin, A. and Mikolov, T.  Inferring algorithmic patterns      Nelson, G. and Oppen, D. C.  Fast decision procedures 
                 with stack-augmented recurrent nets.  arXiv preprint           based on congruence closure.  J. ACM, 27(2):356–364, 
                 arXiv:1503.01007, 2015.                                        1980.  doi: 10.1145/322186.322198.  URL https:// 
                                                                                doi.org/10.1145/322186.322198. 
              Kalyan, A., Mohta, A., Polozov, O., Batra, D., Jain, P., and    Nye, M., Pu, Y., Bowers, M., Andreas, J., Tenenbaum, 
                 Gulwani, S.  Neural-guided deductive search for real-          J. B., and Solar-Lezama, A.  Representing partial pro-
                 time program synthesis from examples. arXiv preprint           grams with blended abstract semantics.  arXiv preprint 
                 arXiv:1804.01186, 2018.                                        arXiv:2012.12964, 2020. 
              Ke, N. R., Bilaniuk, O., Goyal, A., Bauer, S., Larochelle,      Parisotto, E., Mohamed, A.-r., Singh, R., Li, L., Zhou, D., 
                 H., Schölkopf, B., Mozer, M. C., Pal, C., and Bengio, Y.       and Kohli, P. Neuro-symbolic program synthesis. arXiv 
                 Learning neural causal models from unknown interven-           preprint arXiv:1611.01855, 2016. 
                 tions. arXiv preprint arXiv:1910.01075, 2019. 
              Koppel,  J.,  Premtoon,  V.,  and Solar-Lezama,  A.  One        Premtoon, V., Koppel, J., and Solar-Lezama, A. Semantic 
                 tool, many languages: Language-parametric transforma-          code search via equational reasoning. In Proceedings of 
                 tion with incremental parametric syntax.  PACMPL, 2            the 41st ACM SIGPLAN International Conference on Pro-
                 (OOPSLA):122:1–122:28, 2018. doi: 10.1145/3276492.             gramming Language Design and Implementation, PLDI 
                 URL https://doi.org/10.1145/3276492.                           2020, London, UK, June 15-20, 2020, pp. 1066–1082, 
                                                                                2020.  doi:  10.1145/3385412.3386001.  URL https: 
              Kulal, S., Pasupat, P., Chandra, K., Lee, M., Padon, O.,          //doi.org/10.1145/3385412.3386001. 
                 Aiken, A., and Liang, P. S.  Spoc:  Search-based pseu-       Pu, Y., Miranda, Z., Solar-Lezama, A., and Kaelbling, L. 
                 docode to code. In Advances in Neural Information Pro-         Selecting representative examples for program synthesis. 
                 cessing Systems, pp. 11906–11917, 2019.                        In International Conference on Machine Learning, pp. 
              Kurach,  K.,  Andrychowicz,  M.,  and  Sutskever,  I.             4161–4170. PMLR, 2018. 
                 Neural  random-access  machines.         arXiv  preprint     Reed, S. and De Freitas, N. Neural programmer-interpreters. 
                 arXiv:1511.06392, 2015.                                        arXiv preprint arXiv:1511.06279, 2015. 
              Lake,  B. M.,  Salakhutdinov,  R.,  and Tenenbaum,  J. B.       Ren, M., Triantafllou, E., Ravi, S., Snell, J., Swersky, 
                 Human-level concept learning through probabilistic pro-        K., Tenenbaum, J. B., Larochelle, H., and Zemel, R. S. 
                 gram induction. Science, 350(6266):1332–1338, 2015.            Meta-learning for semi-supervised few-shot classifcation. 
              Lavrijsen, W. T. and Dutta, A. High-performance python-           arXiv preprint arXiv:1803.00676, 2018. 
                 c++ bindings with pypy and cling. In 2016 6th Workshop       Reuther, A., Kepner, J., Byun, C., Samsi, S., Arcand, W., Be-
                 on Python for High-Performance and Scientifc Comput-           stor, D., Bergeron, B., Gadepally, V., Houle, M., Hubbell, 
                 ing (PyHPC), pp. 27–35. IEEE, 2016.                            M., et al. Interactive supercomputing on 40,000 cores for 
              Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-             machine learning and data analysis. In 2018 IEEE High 
                 hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.         Performance extreme Computing Conference (HPEC), pp. 
                 Bart: Denoising sequence-to-sequence pre-training for          1–6. IEEE, 2018. 
                 natural language generation, translation, and comprehen-     Ruis, L., Andreas, J., Baroni, M., Bouchacourt, D., and 
                 sion. arXiv preprint arXiv:1910.13461, 2019.                   Lake, B. M.  A benchmark for systematic generaliza-
                                                                                tion in grounded language understanding. arXiv preprint 
              Li, Y., Gu, C., Dullien, T., Vinyals, O., and Kohli, P. Graph     arXiv:2003.05161, 2020. 
                 matching networks for learning the similarity of graph 
                 structured objects. In International Conference on Ma-       Rule, J.  The child as hacker:  building more human-like 
                 chine Learning, pp. 3835–3845. PMLR, 2019.                     models of learning. PhD thesis, MIT, 2020. 
