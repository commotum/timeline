                              100
                              Accuracy8072.8
                               60
                               40
                              Exact Match 204.9 6.3    1.5  3.6  3.3    1.4  2.9  3.4
                                0         LT               ST             ST w/ II
                                                    Architecture Type
                                     Abacus, OOD         FIRE, OOD          NoPE, OOD
                     Figure 20: Effect of adding randomized padding into training data only for the addition task. Looped
                     transformer models are able to maintain high accuracy when random padding is added into the data.
                              100
                              Accuracy80
                               60
                               40
                              Exact Match 2013.86.0     14.6   12.3       9.9    5.8
                                0       LT                ST              ST w/ II
                                                    Architecture Type
                                     FIRE Rand, OOD         FIRE Rand Circular Hints, OOD
                     Figure 21: Using index hints and randomized FIRE embeddings, presented by Zhou et al. [2024],
                     training on size 20 data with our methodology, such as masking before the equals sign. This would
                     be comparable to “1 to 20” in Figure 13 presented by Zhou et al. [2024] and Figure 3 of our work.
                                   Figure 22: Visualization of the three architectures we study.
                     AsAbacusEmbeddingsareavariantofabsoluteembeddings, reused only for numbers, they could
                     be combined with relative embeddings being deployed in current models. If all digits input to the
                     model are tokenized individually, we can perform a linear time operation to find and assign relative
                     embeddings to all numbers in an input, which is lower than the quadratic cost incurred by attention.
                     Training a small number of Abacus Embeddings may be enough to handle all numerical inputs for
                                                         22
