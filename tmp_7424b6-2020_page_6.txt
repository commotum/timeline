            Wealsoundertake a systematic study of “data contamination” – a growing problem when training high capacity models
            on datasets such as Common Crawl, which can potentially include content from test datasets simply because such
            content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify
            its distorting effects. Although we ﬁnd that data contamination has a minimal effect on GPT-3’s performance on most
            datasets, we do identify a few datasets where it could be inﬂating results, and we either do not report results on these
            datasets or we note them with an asterisk, depending on the severity.
            In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion
            parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most
            tasks we ﬁnd relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap
            between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models
            are more proﬁcient meta-learners.
            Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and
            broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard.
            Theremainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training
            GPT-3andevaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings.
            Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3.
            Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.
            2  Approach
            Ourbasic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19],
            with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use
            of in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for
            learning within the context. Therefore, we start this section by explicitly deﬁning and contrasting the different settings
            that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a
            spectrum of how much task-speciﬁc data they tend to rely on. Speciﬁcally, we can identify at least four points on this
            spectrum (see Figure 2.1 for an illustration):
                 • Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of
                  a pre-trained model by training on a supervised dataset speciﬁc to the desired task. Typically thousands to
                  hundreds of thousands of labeled examples are used. The main advantage of ﬁne-tuning is strong performance
                  onmanybenchmarks. Themaindisadvantages are the need for a new large dataset for every task, the potential
                  for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the
                  training data [GSL+18, NK19], potentially resulting in an unfair comparison with human performance. In
                  this work we do not ﬁne-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be
                  ﬁne-tuned in principle and this is a promising direction for future work.
                 • Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few
                  demonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed.
                  AsshowninFigure2.1,foratypical dataset an example has a context and a desired completion (for example
                  an English sentence and the French translation), and few-shot works by giving K examples of context and
                  completion, and then one ﬁnal example of context, with the model expected to provide the completion. We
                  typically set K in the range of 10 to 100 as this is how many examples can ﬁt in the model’s context window
                  (nctx = 2048). The main advantages of few-shot are a major reduction in the need for task-speciﬁc data and
                  reduced potential to learn an overly narrow distribution from a large but narrow ﬁne-tuning dataset. The main
                  disadvantage is that results from this method have so far been much worse than state-of-the-art ﬁne-tuned
                  models. Also, a small amount of task speciﬁc data is still required. As indicated by the name, few-shot
                  learning as described here for language models is related to few-shot learning as used in other contexts in
                               +
                  ML[HYC01,VBL 16]–bothinvolvelearningbasedonabroaddistributionoftasks(in this case implicit in
                  the pre-training data) and then rapidly adapting to a new task.
                 • One-Shot (1S) is the same as few-shot except that only one demonstration is allowed, in addition to a natural
                  language description of the task, as shown in Figure 1. The reason to distinguish one-shot from few-shot and
                  zero-shot (below) is that it most closely matches the way in which some tasks are communicated to humans.
                  For example, when asking humans to generate a dataset on a human worker service (for example Mechanical
                  Turk), it is common to give one demonstration of the task. By contrast it is sometimes difﬁcult to communicate
                  the content or format of a task if no examples are given.
                                                   6
