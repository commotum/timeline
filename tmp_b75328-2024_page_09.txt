                           embedding alone in the “all OOD” setting. However, in Table 2, we observe mixed results when
                           pairing the Abacus+FIRE Embeddings combination with different model architectures with effective
                           depth eight. For sorting, different architectures appear to be better suited to different types of
                           extrapolation, for example the looped transformer is best at extrapolating for finding the minimum
                           element but not for sorting the whole array.
                           Overall, the superior sorting performance of the Abacus Embeddings underscores their potential
                           utility across a broader spectrum of algorithmic tasks beyond basic arithmetic. Abacus Embeddings
                           may be instrumental in use cases requiring transformer models to perform a variety of complex
                           positional, numerical, and/or relational reasoning tasks.
                           4.4  AbacusandRelativeEmbeddings
                           As Abacus Embeddings are only applied to numbers, to incorporate Abacus Embeddings into a
                           general purpose model, they must be compatible with other relative embeddings to maintain good
                           downstream performance on non-arithmetic tasks. We examine these types of combinations here and
                           conclude that Abacus Embeddings complement techniques that are good for natural language well,
                           suggesting that these combinations could be powerful for large-scale general models.
                           Although Abacus Embeddings are implicitly combined with NoPE (no positional embeddings)
                           embeddings for all experiments seen so far, most state-of-the-art open source models use Rotary
                           Embeddings. Rotary Embeddings are weak for length generalization. We show that combining
                           AbacusEmbeddingswithRoPEdoes,infact,yieldimprovementinoperandlengthgeneralization.
                           However, in Figure 7, we demonstrate the true potential for integrating Abacus Embeddings into
                           a more general system, showing that the combination of Abacus Embeddings with FIRE unlocks
                           generalization well beyond the problems that FIRE embeddings can solve on their own.
                                         ST w/ II          ST w/ II           ST w/ II          ST w/ II
                                    0 Abacus + FIRE          FIRE         Abacus + RoPE          RoPE
                                                                                                                100
                                   50                                                                           50   Accuracy
                                   100                                                                          0
                                 Length of Operand One050100 050      100 0      50     100 0      50     100
                                                               Length of Operand Two
                           Figure 7: Exact match accuracy of standard transformer of depth 16 with input injection, trained on
                           uptosize 20 data. The red square denotes in distribution testing. Combining Abacus Embeddings
                           with FIRE or RoPE embeddings improves out of distribution accuracy for addition, over the baseline
                           models without Abacus Embeddings.
                           5   Discussion & Limitations
                           While the capabilities of LLMs have advanced far enough to encompass complex tasks including
                           code generation and mathematical reasoning, stress testing the limits of these models remains a
                           challenge. In this paper, we study mathematical reasoning tasks including addition, multiplication,
                           and sorting to evaluate these capabilities in a controlled setting. We analyze the ability of specialized
                           language models to learn algorithmic tasks in a zero shot setting, without access to outside tools like
                           code interpreters, etc., exploring the benefits of various architectural improvements like improved
                           embeddings and recurrent layers.
                           Across our experiments, we find that our novel Abacus Embeddings improve performance dra-
                           matically both when applied to standard transformers as well as recurrent variants. We repeatedly
                           achieve length generalizations of at least 6× (capped by the context length) more than doubling the
                           extrapolation demonstrations in prior work, achieving near perfect results on addition of up to 100
                           digits, with repeatable results across multiple training runs. We demonstrate the the complementary
                           properties of our Abacus Embeddings with other relative embeddings like FIRE, achieving dramatic
                                                                          9
