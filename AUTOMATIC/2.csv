year,title,url,count
2006,A Fast Learning Algorithm for Deep Belief Nets,https://www.cs.toronto.edu/~fritz/absps/ncfast.pdf,1
2013,Auto-Encoding Variational Bayes (VAE),https://arxiv.org/pdf/1312.6114,1
2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805,1
2022,Constitutional AI: Harmlessness from AI Feedback,https://www-cdn.anthropic.com/7512771452629584566b6303311496c262da1006/Anthropic_ConstitutionalAI_v2.pdf,1
2020,DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference,https://arxiv.org/pdf/2004.12993,1
2016,Hybrid computing using a neural network with dynamic external memory (DNC),https://web.stanford.edu/class/psych209/Readings/GravesWayne16DNC.pdf,1
2015,End-To-End Memory Networks (MemN2N),https://arxiv.org/pdf/1503.08895,1
2023,FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning,https://tridao.me/publications/flash2/flash2.pdf,1
2014,Generative Adversarial Nets (GANs),https://arxiv.org/pdf/1406.2661,1
2012,ImageNet Classification with Deep Convolutional Neural Networks (AlexNet),https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf,1
2021,Stabilizing Equilibrium Models by Jacobian Regularization,https://arxiv.org/pdf/2106.14342,1
2020,Language Models are Few-Shot Learners (GPT-3),https://arxiv.org/pdf/2005.14165,1
2012,Learning Semantic String Transformations from Examples,https://vldb.org/pvldb/vol5/p740_rishabhsingh_vldb2012.pdf,1
2019,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,https://arxiv.org/pdf/1909.08053,1
2022,Memorizing Transformers,https://arxiv.org/pdf/2203.08913,1
2015,Neural Programmer-Interpreters (NPI),https://arxiv.org/pdf/1511.06279,1
2017,End-to-End Differentiable Proving (NTP),https://arxiv.org/pdf/1705.11040,1
2023,Let's Verify Step by Step (PRM),https://arxiv.org/pdf/2305.20050,1
2023,QLoRA: Efficient Finetuning of Quantized LLMs,https://arxiv.org/pdf/2305.14314,1
2020,REALM: Retrieval-Augmented Language Model Pre-Training,https://arxiv.org/pdf/2002.08909,1
2024,ScatterFormer: Efficient Voxel Transformer with Scattered Linear Attention,https://arxiv.org/pdf/2401.00912,1
2024,SegPoint: Segment Any Point Cloud via Large Language Model,https://arxiv.org/pdf/2407.13761,1
2022,Self-Consistency Improves Chain of Thought Reasoning in Language Models,https://arxiv.org/pdf/2203.11171.pdf,1
2018,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,https://arxiv.org/pdf/1810.00825,1
2020,Sharpness-Aware Minimization for Efficiently Improving Generalization (SAM),https://arxiv.org/pdf/2010.01412,1
2018,"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks (LTH)",https://arxiv.org/pdf/1803.03635,1
1957,The perceptron: A perceiving and recognizing automaton,https://bpb-us-e2.wpmucdn.com/websites.umass.edu/dist/a/27637/files/2016/03/rosenblatt-1957.pdf,1
2018,Universal Transformers (UT),https://arxiv.org/pdf/1807.03819,1
2023,Objaverse-XL: A Universe of 10M+ 3D Objects,https://arxiv.org/pdf/2307.05663.pdf,1
