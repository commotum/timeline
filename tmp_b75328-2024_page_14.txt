                         A Appendix
                         AuthorContributions
                         SeanMcLeish*–Ledtheproject, developed the idea, contributed to code, organized majority of
                         experiments and contributed to writing.
                         Arpit Bansal* – Contributed large amount to the idea, contributed to code, organized experiments
                         for sorting arrays, and contributed to writing.
                         Alex Stein – Contributed to code, and helped plan the experiments.
                         Neel Jain – Contributed large amount to writing, and helped plan the experiments.
                         JohnKirchenbauer–Contributedlarge amount to writing.
                         Brian R. Bartoldson, Bhavya Kailkhura – Helped set up large scale parallel addition evaluations,
                         contributed to writing.
                         AbhinavBhatele–Contributed to writing.
                         Jonas Geiping, Avi Schwarzschild, Tom Goldstein – Developed the idea, helped plan and organize
                         the experiments, and contributed large amount to the writing.
                         A.1   ExtendedRelatedWorks
                         A.1.1   Positional Embeddings.
                         FIRE embeddings are additive embeddings in the attention mechanism:          ARPE(X) =
                                       T                        log(c(i−j)+1) 
                         XWQ(XWK) +BwhereBi,j = fθ log(cmax(i,L)+1) and c,L are learnable parameters. Li
                         et al. [2023] show empirically that these embeddings allow for length generalization and theoretically
                         show they are capable of representing many other embedding types. Ruoss et al. [2023] propose
                         using a random subset of a larger set of possible positions during training so that larger positional
                         embeddings are trained. Zhou et al. [2024] use randomized FIRE [Ruoss et al., 2023, Li et al., 2023]
                         embeddings to achieve length generalization on arithmetic tasks, which use randomized positions as
                         input to the small multi layer perceptron used in FIRE embeddings.
                         A.2   Datasets
                         Addition:   Wesampleequally, with replacement, from all i × i possible operand lengths up to the
                         maximumdatasetsize of 20 million, we call this a dataset of size i in the main text. For evaluation
                         wesample100samplesforeachpairofoperandlengths evaluated.
                         Bitwise OR:    Theinput for this problem is two binary vectors, the longer input vector is all zeros
                         and the shorter input contains a one. The output should be the length of the longer vector with the
                         one in the same position as in the shorter vector. If the inputs are the same length, the one can be
                         in either vector. E.g. 001 ⊕ 00000 = 00100. For training, we exhaustively sample the space of all
                         vectors of sizes less than or equal to the predefined maximum input vector size.
                         Sorting:   Given a list of reversed integers indexed by characters, output the characters in ascending
                         order. E.g. a : 64957,b : 99963,c : 10218,d : 7141,e : 05781 = d,e,b,a,c. We implement the
                         sampling process for sorting in a grid like manor. We query each “square” of an [1,n] × [1,n] grid
                         until the maximum size has been reached for the dataset. When querying “square” (i,j) we randomly
                         sample i integers of size less than or equal to j digits. We randomly sample consecutive indices for
                         the natural numbers in our list at both train and test time.
                         Multiplication:   Weimplementthemultiplication datasets for both training and testing the exact
                         samemanorasforaddition, only changing the operation used to calculate the answer.
                         A.3   Bitwise OR on Binary Vectors
                         Anecessary condition to perform addition is aligning digits of the same significance. We begin
                         by examining positional embeddings for exactly this task. To do this we analyze the bitwise OR
                         task, where the model has to output left aligned position wise OR of two binary vectors. We present
                         samples from the dataset in Section A.3.1, these are left aligned to be representative of the task of
                         aligning digits for reversed addition.
                                                                      14
