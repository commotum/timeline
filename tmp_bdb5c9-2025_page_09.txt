                           Preprint.
                           better performance of the with-selection setting (as seen in Table 2) suggests that, in addition to
                           allowing memory to grow without overwhelming context in both the abstraction and inference
                           phases, it also helps downstream problem-solving performance. Moreover, from the token usage
                           plot in Figure 4, we see that concept selection’s performance improvement also comes at higher
                           efficiency–the selection-ablated setting uses far more tokens. However, there still exist certain scale
                           regimes where the no-selection setting has a better score. We attribute this to variance from imperfect
                           selection and note that this indicates headroom for selection/retrieval.
                           ArcMemo-PS(-selection)ismostcomparabletoDynamicCheatsheet’s“cumulative”setting(DC-Cu)
                           as all stored memory is included with each query. While both ultimately solve the same number of
                           puzzles across all runs, we find that the settings differ in 10 puzzles (that is, each system solves 5
                           puzzles the other does not). We manually analyzed these puzzles and found that only 40% of new
                           solves in the cheatsheet setting were related to memory elements actually in the generated cheatsheet.
                           That is, we were unable to find relevant notes in the cheatsheet to explain 60% of its new solves.
                           In contrast, 100% of new solves from ArcMemo-PS (-selection) can be linked to concept memory
                           contents. While we cannot definitively conclude ArcMemo changes directly induced each of the new
                           solves (given the state of LLM interpretability), this seems to suggest new solves in the ArcMemo
                           settings are potentially more attributable to the memory component compared to sampling variance.
                           Manualinspection of a subset of reasoning-based selection results used for ArcMemo-PS finds that
                           while some irrelevant concepts still appear within the selection, the key idea of the target is often
                           included in the selection. The presence of a few distractor concepts appears to be manageable, as the
                           reasoning model itself is capable of exploring with backtracking.
                           5.3   CONTINUAL LEARNING
                                                                                     Oracle@kScores
                                                                            k=1          k=2(Official)    k=3
                                       retry  setting
                                       0      ArcMemo-OE                    48.00 (1.00)  56.67 (1.53)   61.00
                                                 +continual memory update   46.33 (1.53)  56.00 (0.00)   61.00
                                       1      ArcMemo-OE                    56.67 (2.08)  65.67 (1.53)   70.00
                                                 +continual memory update   57.17 (3.69)  65.00 (0.00)   67.00
                                       2      ArcMemo-OE                    60.67 (1.53)  67.67 (2.52)   71.00
                                                 +continual memory update   62.33 (3.51)  70.00 (1.73)   72.00
                           Table 3: Continual Memory Learning. Comparing otherwise identical memory systems, we find
                           that with sequential inference compute scale (at high retry depth), continually updating memory with
                           newself-generated solutions leads to improved puzzle-solving performance.
                           Results from our experiment comparing ArcMemo-OE with a version that updates during evaluation
                           (every 10 problems; Table 3) show that our memory system’s performance improves with continual
                           updates–a key requirement for meaningful lifelong learning. In particular, we find that the perfor-
                           manceimprovementemergesatlater iterations in sequential inference scaling. We hypothesize this is
                           because, after more iterations and puzzle-solving passes, new solutions are found, new memories are
                           abstracted, and these new memories help solve other puzzles.
                           Wefurther discuss token efficiency, concept specificity, and embedding retrieval in Appendix C, and
                           limitations and future work discussion in Appendix D.
                           6   CONCLUSION
                           In this work, we introduce ArcMemo, a framework designed to support lightweight, lifelong learning
                           oncompositional reasoning tasks by emphasizing a higher level of abstraction and modularity. We
                           explore two implementations of memory modules against ARC-AGI, a benchmark specifically de-
                           signed to resist memorization and to evaluate fluid intelligence instead. Our main findings confirm the
                           efficacy of our approach: ArcMemo-PS outscores comparable methods under the official evaluation
                                                                          9
