           Preprint.
           protocol and continues to benefit from additional inference compute scale. Moreover, we observe that
           continual updates benefit memory augmentation over multiple attempts and that selecting a subset of
           memoryforeachproblemisacrucialcomponenttoenableamemorytocontinuallygrowwithout
           overwhelming the LLM context. This paper’s work represents early attempts toward the main tenets
           of higher-level abstraction and modularity. Promising future directions include hierarchical designs
           and consolidation mechanisms that restructure memory. To encourage further investigation, we also
           release a concept-annotation dataset and configurable puzzle synthesis pipeline, providing resources
           for evaluating concept representations and advancing abstraction-based memory methods.
           ACKNOWLEDGEMENTS
           WethankRahmanHajiyevandJiayuLiuforearlycontributions to this project.
           REFERENCES
           Ekin Akyürek, Mehul Damani, Adam Zweiger, Linlu Qiu, Han Guo, Jyothish Pari, Yoon Kim, and
            Jacob Andreas. The surprising effectiveness of test-time training for few-shot learning, 2025. URL
            https://arxiv.org/abs/2411.07279.
           ARC-Prize, 2025. URL https://arcprize.org/leaderboard.
           Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning
            to retrieve, generate, and critique through self-reflection, 2023. URL https://arxiv.org/
            abs/2310.11511.
           JinheonBaek,SujayKumarJauhar,SilviuCucerzan,andSungJuHwang. Researchagent: Iterativere-
            search idea generation over scientific literature with large language models. ArXiv, abs/2404.07738,
            2024. URLhttps://api.semanticscholar.org/CorpusID:269042844.
           LéonBottouandYannLeCun.Largescaleonlinelearning. Advancesinneuralinformationprocessing
            systems, 16, 2003.
           LéonBottouandYannLeCun. On-linelearning for very large data sets. Applied stochastic models in
            business and industry, 21(2):137–151, 2005.
           Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer, 2022. URL
            https://arxiv.org/abs/2207.06881.
           Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building
            production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413,
            2025.
           François Chollet. On the measure of intelligence, 2019. URL https://arxiv.org/abs/
            1911.01547.
           Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang,
            Huajun Chen, and Ningyu Zhang. Memp: Exploring agent procedural memory, 2025. Work in
            progress.
           Huanang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong
            Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang,
            Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian,
            Zhenhailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, and Mengdi Wang. A
            survey of self-evolving agents: On path to artificial super intelligence, 2025. URL https:
            //arxiv.org/abs/2507.21046.
           Pengfei He, Zitao Li, Yue Xing, Yaling Li, Jiliang Tang, and Bolin Ding. Make llms better zero-shot
            reasoners: Structure-orientated autonomous reasoning. ArXiv, abs/2410.19000, 2024a. URL
            https://api.semanticscholar.org/CorpusID:273638104.
                               10
