                                             Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI-10)
                                                             Relative Entropy Policy Search
                                                                                            ¬®                           ¬®
                                                       JanPeters, Katharina Mulling, Yasemin Altun
                                                                                                                         ¬®
                                   MaxPlanckInstitute for Biological Cybernetics, Spemannstr. 38, 72076 Tubingen, Germany
                                                     {jrpeters,muelling,altun}@tuebingen.mpg.de
                                             Abstract                                           Policy updates may often result in a loss of essential in-
                   Policysearchisasuccessfulapproachtoreinforcementlearn-                    formation due to the policy improvement step. For exam-
                   ing. However, policy improvements often result in the loss of             ple, a policy update that eliminates most exploration by tak-
                   information. Hence, it has been marred by premature con-                  ing the best observed action often yields fast but premature
                   vergence and implausible solutions. As Ô¨Årst suggested in the              convergence to a suboptimal policy. This problem was ob-
                   context of covariant policy gradients (Bagnell and Schneider              served by Kakade (2002) in the context of policy gradients.
                   2003), many of these problems may be addressed by con-                    There, it can be attributed to the fact that the policy param-
                   straining the information loss. In this paper, we continue this           eter update Œ¥Œ∏ was maximizing it collinearity Œ¥Œ∏T‚àá J to
                   path of reasoning and suggest the Relative Entropy Policy                                                                               Œ∏
                                                                                             the policy gradient while only regularized by Ô¨Åxing the Eu-
                   Search (REPS) method. The resulting method differs signif-                clidian length of the parameter update Œ¥Œ∏TŒ¥Œ∏ = Œµ to a step-
                   icantly from previous policy gradient approaches and yields               size Œµ. Kakade (2002) concluded that the identity metric
                   an exact update step. It works well on typical reinforcement              of the distance measure was the problem, and that the us-
                   learning benchmark problems.                                              age of the Fisher information metric F(Œ∏) in a constraint
                                                                                             Œ¥Œ∏TF(Œ∏)Œ¥Œ∏ = Œµ leads to a better, more natural gradient.
                                         Introduction                                        Bagnell and Schneider (2003) clariÔ¨Åed that the constraint
                Policy search is a reinforcement learning approach that at-                  introduced in (Kakade 2002) can be seen as a Taylor expan-
                tempts to learn improved policies based on information ob-                   sion of the loss of information or relative entropy between
                served in past trials or from observations of another agent‚Äôs                the path distributions generated by the original and the up-
                actions (Bagnell and Schneider 2003).            However, policy             dated policy. Bagnell and Schneider‚Äôs (2003) clariÔ¨Åcation
                search, as most reinforcement learning approaches, is usu-                   serves as a key insight to this paper.
                ally phrased in an optimal control framework where it di-                       In this paper, we propose a new method based on this in-
                rectly optimizes the expected return. As there is no notion                  sight, that allows us to estimate new policies given a data
                of the sampled data or a sampling policy in this problem                     distribution both for off-policy or on-policy reinforcement
                statement, there is a disconnect between Ô¨Ånding an optimal                   learning. We start from the optimal control problem state-
                policy and staying close to the observed data. In an online                  ment subject to the constraint that the loss in information
                setting, many methodscandealwiththisproblembystaying                         is bounded by a maximal step size. Note that the meth-
                close to the previous policy (e.g., policy gradient methods                  odsproposedin(BagnellandSchneider2003;Kakade2002;
                allow only small incremental policy updates). Hence, ap-                     Peters and Schaal 2008) used a small Ô¨Åxed step size instead.
                proaches that allow stepping further away from the data are                  Aswedonotworkinaparametrizedpolicygradientframe-
                problematic, particularly, off-policy approaches Directly op-                work, we can directly compute a policy update based on all
                timizing a policy will automatically result in a loss of data as             information observed from previous policies or exploratory
                an improved policy needs to forget experience to avoid the                   sampling distributions. All sufÔ¨Åcient statistics can be deter-
                mistakes of the past and to aim on the observed successes.                   minedbyoptimizingthedualfunction that yields the equiv-
                However, choosing an improved policy purely based on its                     alent of a value function of a policy for a data set. We show
                return favors biased solutions that eliminate states in which                that the method outperforms the previous policy gradient al-
                only bad actions have been tried out. This problem is known                  gorithms (Peters and Schaal 2008) as well as SARSA (Sut-
                as optimization bias (Mannor et al. 2007). Optimization bi-                  ton and Barto 1998).
                ases may appear in most on- and off-policy reinforcement
                learning methods due to undersampling (e.g., if we cannot                    Background&Notation
                sample all state-actions pairs prescribed by a policy, we will               Weconsidertheregularreinforcememtlearningsetting(Sut-
                overÔ¨Åt the taken actions), model errors or even the policy                   ton and Barto 1998; Sutton et al.          2000) of a stationary
                update step itself.                                                          Markov decision process (MDP) with n states s and m ac-
                            c                                                                tions a. When an agent is in state s, he draws an action
                Copyright  2010, Association for the Advancement of ArtiÔ¨Åcial
                Intelligence (www.aaai.org). All rights reserved.                            a ‚àº œÄ(a|s) from a stochastic policy œÄ. Subsequently, the
                                                                                      1607
               agent transfers from state s to s0 with transition probability                              X                     ¬µœÄ(s)œÄ(a|s)
                                                                                                 s.t. Œµ ‚â•      ¬µœÄ(s)œÄ(a|s)log                  ,  (6)
               p(s0|s,a) = Pa 0, and receives a reward r(s,a) = Ra ‚àà R.                                                             q(s,a)
                              ss                                       s                                   s,a
               Asaresultfromthesestatetransfers,theagentmayconverge                     X                  X
                                                   œÄ                                         ¬µœÄ(s0)œÜ 0 =        ¬µœÄ(s)œÄ(a|s)Pa œÜ 0,                (7)
               to a stationary state distribution ¬µ (s) for which                                    s                         ss0  s
                                                                                          0                    0
                            X                                                            s                 s,a,s
                         0           œÄ              0           œÄ 0                                        X
                      ‚àÄs :      s,a ¬µ (s)œÄ(a|s)p(s |s,a) = ¬µ (s )          (1)                        1 =      ¬µœÄ(s)œÄ(a|s).                       (8)
               holds under mild conditions, see (Sutton et al. 2000). The                                  s,a
               goal of the agent is to Ô¨Ånd a policy œÄ that maximizes the              Both ¬µœÄ and œÄ are probability distributions and the features
               expected return                                                        œÜ 0 of the MDP are stationary under policy œÄ.
                                                                                       s
                            J(œÄ) = X        ¬µœÄ(s)œÄ(a|s)r(s,a),             (2)          Without the information loss bound constraint in Eq.(6),
                                         s,a                                          there is no notion of sampled data and we obtain the stochas-
               subject to the constraints of Eq.(1) and that both ¬µœÄ and œÄ            tic control problem where differentiation of the Langrangian
                                                                                                                                    T        a
                                                                                      also yields the classical Bellman equation œÜ Œ∏ = R ‚àíŒª+
               are probability distributions. This problem is called the op-          P a T                               T         s        s
                                                                                         0 P 0œÜ 0Œ∏. In this equation, œÜ Œ∏ = VŒ∏(s) is known to-
               timal control problem; however, it does not include any no-              s    ss  s                        s
               tion of data as discussed in the previous section. In some             day as value function while the Langrangian multipliers Œ∏
               cases, only some features of the full state s are relevant for         become parameters and Œª the average return. While such
               the agent. In this case, we only require stationary feature            MDPs may be solved by linear programming (Puterman
               vectors                                                                2005), approaches that employ sampled experience cannot
                X                                       X                             be derived properly from these equations. The key differ-
                            œÄ              0        0           œÄ 0     0             ence to past optimal control approaches lies in the addition
                          ¬µ (s)œÄ(a|s)p(s |s,a)œÜ       =       ¬µ (s )œÜ . (3)
                         0                         s         0         s
                    s,a,s                                   s                         of the constraint in Eq. (6).
               Note that when using Cartesian unit vectors u 0 of length                As discussed in the introduction, natural policy gradient
                                                                  s                   maybederivedfromasimilarproblemstatement. However,
               n as features œÜ 0 = u 0, Eq.(3) will become Eq.(1). Using
                               s       s                                              the natural policy gradient requires that Œµ is small, it can
               features instead of states relaxes the stationarity condition          only be properly derived for the path space formulation and
               considerably and often allows a signiÔ¨Åcant speed-up while              it can only be derived from a local, second order Taylor ap-
               onlyresulting in approximate solutions and being highly de-            proximation of the problem. Stepping away further from the
               pendable on the choice of the features. Good features may              sampling distribution q will violate these assumptions and,
               beRBFfeaturesandtilecodes,see(SuttonandBarto1998).                                                                               1
                                                                                      hence, natural policy gradients are inevitably on-policy .
                         Relative Entropy Policy Search                                 The Œµ can be chosen freely where larger values lead to
                                                                                      bigger steps while excessively large values can destroy the
               Wewill Ô¨Årst motivate our approach and, subsequently, give              policy. Its size depends on the problem as well as on the
               several practical implementations that will be applied in the          amountofavailable samples.
               evaluations.
               Motivation                                                             Relative Entropy Policy Search Method
                                                                                      As shown in the appendix, we can obtain a reinforcement
               Relative entropy policy search (REPS) aims at Ô¨Ånding the               learning algorithm straightforwardly.
               optimal policy that maximizes the expected return based                Proposed Solution. The optimal policy for Problem            is
               on all observed series of states, actions and rewards. At              given by                q(s,a)exp1Œ¥ (s,a)
               the same time, we intend to bound the loss of informa-                                                       Œ∑ Œ∏
               tion measured using relative entropy between the observed                         œÄ(a|s) = P q(s,b)exp1Œ¥ (s,b),                  (9)
                                                                    œÄ                                          b              Œ∑ Œ∏
               data distribution q(s,a) and the data distribution p (s,a) =
               ¬µœÄ(s)œÄ(a|s) generated by the new policy œÄ. Ideally, we                                          P
                                                                                                           a          a       0
                                                                                      whereŒ¥ (s,a) = R +           0 P 0V (s )‚àíV (s)denotesthe
               wanttomakeuseofeverysample(s,a,s0,r)independently,                             Œ∏            s      s   ss  Œ∏         Œ∏
               hence, we express the information loss bound as                        Bellman error. Here, the value function Vs(Œ∏) = Œ∏TœÜs is
                                                                                      determined by minimizing
                               X                       œÄ                                                                                 
                      œÄ              œÄ               ¬µ (s)œÄ(a|s)                                         X                       1
                  D(p ||q) =       ¬µ (s)œÄ(a|s)log                   ‚â§Œµ, (4)            g(Œ∏,Œ∑) = Œ∑log            q(s,a)exp Œµ+ Œ¥ (s,a)          ,  (10)
                                                        q(s,a)                                               s,a                 Œ∑ Œ∏
                                s,a
               where D(pœÄ||q) denotes the Kullback-Leibler divergence,                with respect to Œ∏ and Œ∑.
                                                                                                                        T
                                                                                        The value function VŒ∏(s) = œÜ Œ∏ appears naturally in the
               q(s,a) denotes the observed state-action distribution, and Œµ                                             s
               is our maximal information loss.                                       derivationofthisformulation(seeAppendix). Thenewerror
               Problem Statement. The goal of relative entropy policy                    1Note that there exist sample re-use strategies for larger step
               search is to obtain policies that maximize the expected re-            away from q using importance sampling, see (Sutton and Barto
               wardJ(œÄ)whiletheinformation loss is bounded, i.e.,                     1998; Peshkin and Shelton 2002; Hachiya, Akiyama, Sugiyama
                                    X                                                 and Peters 2008), or off-policy approaches such as Q-Learning
                                          œÄ             a                             (which is known to have problems in approximate, feature-based
                      maxJ(œÄ)=          ¬µ (s)œÄ(a|s)R ,                     (5)
                         œÄ                              s
                      œÄ,¬µ                                                             learning).
                                    s,a
                                                                               1608
                             Relative Entropy Policy Search                           Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShannon (BFGS) method (de-
                                                                                      noted in this paper by fmin BFGS(g,‚àÇg,[Œ∏ ,Œ∑ ]) with
                                                                                                                                         0   0
                 input: features œÜ(s), maximal information loss .                    ‚àÇg = [‚àÇ g,‚àÇ g]). The resulting method is given in Table
                                                                                                Œ∏    Œ∑
                 for each policy update                                               1.
                                                           0                          Sample-based Policy Iteration with REPS
                    Sampling: Obtain samples (s ,a ,s ,r ), e.g.,
                                                    i   i  i  i
                    byobserving another policy or being on-policy.                    If the REPS algorithm is used in a policy iteration scenario,
                    Counting: Count samples to obtain the                             one can re-use parts of the sampling distribution q(s,a). As
                                                                                                                  œÄ
                                                         P                            weknowthatq(s,a) = ¬µ l(s)œÄ (a|s) where œÄ denotes the
                    sampling distribution q(s,a) = 1        N Ii .                                                       l              l
                                                       N    i=1 sa                    last policy in a policy iteration scenario, we can also write
                    Critic: Evaluate policy for Œ∑ and Œ∏.                              our new policy as
                       DeÔ¨ÅneBellmanErrorFunction:                                                                           1          
                                      a +P       a    T       TŒ∏                                                œÄl(a|s)exp Œ∑Œ¥Œ∏(s,a)
                       Œ¥ (s,a) = R            0 P 0œÜ 0Œ∏ ‚àíœÜ
                         Œ∏            s      s   ss   s       s                                œÄl+1(a|s) = P                             .
                                                                                                                  œÄ (a|s)exp 1Œ¥ (s,b)
                       ComputeDualFunction:                                                                     b   l            Œ∑ Œ∏
                                                                  
                                          P             Œµ+1Œ¥ (s,a)
                       g(Œ∏,Œ∑) = Œ∑log            q(s,a)e    Œ∑ Œ∏                        As a result, we can also evaluate our policy at states where
                                            s,a                                       no actions have been taken. Setting œÄ to good locations
                       ComputetheDualFunction‚Äôs Derivative :                                                                     l
                                            Œµ+1Œ¥ (s,a)                                allows encoding prior knowledge on the policy. This up-
                                      q(s,a)e  Œ∑ Œ∏     (   0 Pa œÜ 0‚àíœÜ )
                       ‚àÇ g = Œ∑     s,a                    s  ss0 s    s               date has the intuitive interpretation that an increase in log-
                         Œ∏                           Œµ+1Œ¥ (s,a)
                                               q(s,a)e  Œ∑ Œ∏                           probability of an action is determined by the Bellman error
                                           s,a               
                                     P             Œµ+1Œ¥ (s,a)                         minus a baseline similar to its mean, i.e., logœÄ       (a|s) =
                       ‚àÇ g = log           q(s,a)e    Œ∑ Œ∏                                                                                 l+1
                         Œ∑             s,a                                            logœÄ (a|s) + 1Œ¥ (s,a)‚àíb(s).
                                         Œµ+1Œ¥ (s,a) 1                                       l        Œ∑ Œ∏
                                  q(s,a)e  Œ∑ Œ∏        Œ¥ (s,a)
                           ‚àí s,a                   Œ∑2 Œ∏                                  Obviously, the algorithm as presented in the previous sec-
                                             Œµ+1Œ¥ (s,a)
                                       q(s,a)e  Œ∑ Œ∏                                   tion would be handicapped by maintaining a high accuracy
                                    s,a                                                                                              a   a
                                     ‚àó   ‚àó                                            model of the Markov decision problem (R ,P 0). Model
                       Optimize: (Œ∏ ,Œ∑ ) = fmin BFGS(g,‚àÇg,[Œ∏ ,Œ∑ ])                                                                   s   ss
                                                                      0   0           estimation would require covering prohibitively many states
                                                        ‚àó        T ‚àó
                       Determine Value Function: VŒ∏ (s) = œÜ Œ∏
                                                                 s                    andactions, and it is hard to obtain an error-free model from
                    Actor: Compute new policy œÄ(a|s).                                 data (Deisenroth 2009; Sutton and Barto 1998). Furthere-
                                    q(s,a)exp 1 Œ¥ ‚àó(s,a)                              more,inmostinterestingcontrolproblems,wedonotintend
                       œÄ(a|s) =              (Œ∑‚àó Œ∏       ) ,
                                      q(s,b)exp( 1 Œ¥ ‚àó(s,b))                          to visit all states and take all actions ‚Äî hence, the number
                                     b          Œ∑‚àó Œ∏                                  of samples N may often be smaller than the number of all
                 Output: Policy œÄ(a|s).                                               state-action pairs mn. Thus, in order to become model-free,
                                                                                      we need to rephrase the algorithm in terms of sample aver-
               Table 1: Algorithmic description of Relative Entropy Pol-              ages instead of the system model.
               icy Search. This algorithm reÔ¨Çects the proposed solution                  The next step is hence to replace the summations over
               clearly.  Note that Ii    is an indicator function such that           states s, s0, and actions a by summations over samples
                i                     sa                   i
               I    = 1 if s = s and a = a while I            = 0 otherwise.                   0
                sa                 i             i         sa                         (s ,a ,s ,r ).    It turns out that this step can be accom-
               In Table 2, we show a possible application of this method in             i   i  i   i
               policy iteration.                                                      plished straightforwardly as all components of REPS can
                                                                                      be expressed using sample-based replacements such as
                                                                                      P                            P
                                                                                            q(s,a)f(s,a) = 1          N f(s ,a ). As the Bellman
               function for the critic in Eq.(10) differs substantially from             s,a                    N     i=1     i  i
                                                                                      error Œ¥ (s ,a ) only needs to be maintained for the executed
               traditional temporal difference errors, residual gradient er-                 Œ∏   i  i
               rors and monte-carlo rollout Ô¨Åttings (Sutton and Barto 1998;           actions, we can also approximate it using sample averages.
               Sutton et al. 2000). The presented solution is derived for ar-            Usingthesetwoinsights,wecandesignageneralizedpol-
               bitrary stationary features and is therefore sound with func-          icy iteration algorithm that is based on samples while using
               tion approximation. The derived policy is similar to the               the main insights of Relative Entropy Policy Search. The re-
               Gibbs policy used in policy gradient approaches (Sutton et             sulting method is shown in Table 2. Note that Table 2 does
               al. 2000) and in SARSA (Sutton and Barto 1998).                        not include sample re-use in REPS policy iteration. How-
                  In order to turn proposed solution into algorithms, we              ever, this step may be included straightfowardly as we can
               needtoefÔ¨Åcientlydeterminethesolution(Œ∏‚àó,Œ∑‚àó)ofthedual                   mixdatafrompreviousiterationswiththecurrentonebyus-
               function g. Eq. (10) can be rewritten as                               ing all data in the critic and the sum of all previous policies
                                       X                                              in the actor update. While such remixing will require more
                                ‚àí1                                                    policy update steps, it may improve robustness and allow
               ming(Œ∏,Œ∑Àú) = Œ∑Àú      log     exp(logq(s,a)+Œµ+Œ∑ÀúŒ¥Œ∏(s,a)),
                Œ∏,Œ∑Àú                    s,a                                           updates after fewer sampled actions.
               which is known to be convex (Boyd and Vandenberghe                                            Experiments
               2004) as Œ¥ (s,a) is linear in Œ∏. Given that g is convex and
                          Œ∏
               smoothly differentiable, we can determine the optimal solu-            In the following section, we test our Sample-based Policy It-
               tion g(Œ∏‚àó,Œ∑‚àó)efÔ¨Åcientlywithanystandardoptimizersuchas                  eration with Relative Entropy Policy Search approach using
                                                                                1609
                            (a) Two State Problem                         (b) Single Chain Problem                        (c) Double Chain Problem
                        (Bagnell and Schneider 2003)                      (Furmston & Barbar 2010)                        (Furmston & Barbar 2010)
                Figure 1: Three different methods are compared on three toy examples. The vanilla policy gradients are signiÔ¨Åcantly outper-
                formed due to their slow convergence as already discussed by Bagnell and Schneider (2003) for the Two State Problem. Policy
                iteration based on Relative Entropy Policy Search (REPS) exhibited the best performance.
                Ô¨Årst several example problems from the literature and, sub-               inspired by Furmston & Barbar (2010). See Fig. 1 (b) for
                sequently, on the Mountain Car standard evaluation. Sub-                  moreinformation.
                sequently, we show Ô¨Årst steps towards a robot application
                currently under development.                                              DoubleChainProblem. TheDoubleChainProblemcon-
                ExampleProblems                                                           catinates two single chain problems into one big one were
                Wecompare our approach both to ‚Äòvanilla‚Äô policy gradient                  state 1 is shared. As before, returning to state 1 will yield
                methods and natural policy gradients (Bagnell and Schnei-                 a reward 2 and requires taking action 2. If in state 1, action
                der 2003; Peters and Schaal 2008) using several toy prob-                 2 will lead to state 6 and also yield a reward of 2. An ac-
                lems. As such, we have chosen (i) the Two-State Problem                   tion 1 yields a reward 5 in state 9 and a reward 10 in state
                (Bagnell and Schneider 2003), (ii) the Single Chain Prob-                 5. In all other states, action 1 will yield 0 reward. Note
                lem(FurmstonandBarber2010),and(iii)theDoubleChain                         that this problem differs from (Furmston and Barber 2010)
                Problem (Furmston and Barber 2010). In all of these prob-                 signiÔ¨Åcantly. We have made it purposefully harder for any
                lems, the optimal policy can be observed straightforwardly                incrementalmethodinordertohighlightheadvantageofthe
                by a human observer but they pose a major challenge for                   presented approach. See Fig. 1 (c) for more information.
                ‚Äòvanilla‚Äô policy gradient approaches.                                        Weusedunitfeatures for all methods. For the two policy
                TwoStateProblem. Thetwostateproblemhastwostates                           gradient approaches a Gibbs policy was employed (Sutton et
                andtwoactions. If it takes the action that has the same num-              al. 2000; Bagnell and Schneider 2003). On all three prob-
                ber as its current state, it will remain in this state. If it takes       lems, we let our policy run until the state distribution has
                the action that has the others state‚Äôs number, it will trans-             converged to the stationary distribution. For small problems
                fer to that one. State transfers are punished while staying               like the presented ones, this usually takes less than 200 steps.
                in ones‚Äô state will give an immidiate reward that equals the              Subsequently, we update the policy and resample. We take
                number of the state. This problem is a derivate of the one                highly optimized vanilla policy gradients with minimum-
                in (Bagnell and Schneider 2003). The optimal policy can be                variance baselines (Peters and Schaal 2008) and the Natu-
                observed straightforwardly: always take action 2. See Fig. 1              ral Actor-Critic with unit basis functions as additional func-
                (a) for more information.                                                 tion approximation (Peters and Schaal 2008). Instead of
                                                                                          a small Ô¨Åxed learning rate, we use an additional momen-
                Single Chain Problem.        TheSingleChainProblemcanbe                   tum term in order to improve the performance. We tuned
                seen as an extension of the Two State Problem. Here, the                  all meta-parameters of the gradient methods to maximum
                actor may return to state 1 at any point in time by taking                performance. We start with the same random initial poli-
                action 2 and receiving a reward of 2. However, if he keeps                cies for all algorithms and average over 150 learning runs.
                using action 1 all the time, he will not receive any rewards              Nevertheless, similar as in (Bagnell and Schneider 2003;
                until he reaches state 5 where he obtains the reward of 10                Peters and Schaal 2008), we directly observe that natu-
                and may remain in state 5. The version presented here was                 ral gradient outperforms the vanilla policy gradient. Fur-
                                                                                    1610
                               Policy Iteration with REPS
                 input: features œÜ(s), maximal information loss ,
                 initial policy œÄ0(a|s).
                 for each policy update k
                                                              0
                    Sampling: Obtain N samples (s ,a ,s ,r ) using
                                                       i   i  i  i
                    current policy œÄk(a|s) in an on-policy setting.
                    Critic: Evaluate policy for Œ∑ and Œ∏.
                       for every sample i = 0 to N do:                                   Figure 2: Performance on the mountain-car problem.
                           n (s ,a ) = n (s ,a ) + (r +œÜT Œ∏ ‚àíœÜT Œ∏)
                            Œ¥   i  i      Œ¥   i  i       i     0       s
                                                              s         i
                                                               i
                           n (s ,a ) = n (s ,a )+(œÜ 0 ‚àíœÜ )
                            Œõ i i          Œõ i i          si     si
                           d(s ,a ) = d(s ,a ) + 1
                              i   i        i   i
                       Bellman Error Function: Œ¥ (s,a) = nŒ¥(s,a)
                                                    Œ∏          d(s,a)
                       Feature Difference: Œõ(s,a) = nŒõ(s,a)
                                                         d(s,a)
                       ComputeDualFunction:
                                         P             1        
                                          1    N     Œµ+ Œ¥ (s ,a )
                       g(Œ∏,Œ∑) = Œ∑log                e   Œ∑ Œ∏ i i                        Figure 3: Simulated setup for learning robot table tennis.
                                          N    i=1
                       ComputetheDualFunction‚Äôs Derivative :
                                   N   Œµ+1Œ¥ (s ,a )
                                      e   Œ∑ Œ∏ i i Œõ(s ,a )
                       ‚àÇ g = Œ∑     i=1                i  i                            method, it differs signiÔ¨Åcantly in two parts, i.e., the critic of
                         Œ∏                 Œµ+1Œ¥ (s ,a )
                                       N e Œ∑ Œ∏ i i                                    SARSAconverges slower, and the additional multiplication
                                    i=1                 
                                     P          1
                                       N     Œµ+ Œ¥ (s ,a )                             by the previous policy results in a faster pruning of taken
                       ‚àÇ g = log           e    Œ∑ Œ∏ i i
                         Œ∑             i=1                                            bad actions in the REPS approach. As a result, REPS is
                               N    Œµ+1Œ¥ (s ,a )
                                   e  Œ∑ Œ∏ i i 1 Œ¥ (s ,a )
                           ‚àí i=1                Œ∑2 Œ∏ i i                              signiÔ¨ÅcantlyfasterthanSARSAascanbeobservedinFig.2.
                                         Œµ+1Œ¥ (s ,a )
                                     N e Œ∑ Œ∏ i i
                                     i=1                                              Primitive Selection in Robot Table Tennis
                       Optimize: (Œ∏‚àó,Œ∑‚àó) = fmin BFGS(g,‚àÇg,[Œ∏ ,Œ∑ ])
                                                                      0   0           Table tennis is a hard benchmark problem for robot learn-
                                                        ‚àó        T ‚àó
                       Determine Value Function: VŒ∏ (s) = œÜ Œ∏
                                                                 s                    ing that includes most difÔ¨Åculties of complex skill.        The
                    Actor: Compute new policy œÄk+1(a|s).                              setup is shown in Fig. 3. A key problem in a skill learn-
                                        œÄ (a|s)exp 1 Œ¥ ‚àó(s,a)                         ing system with multiple motor primitives (e.g., many dif-
                       œÄ     (a|s) =     k        (Œ∑‚àó Œ∏       ) ,
                         k+1              œÄ (a|s)exp( 1 Œ¥ ‚àó(s,b))                     ferent forehands, backhands, smashes, etc.) is the selection
                                         b k          Œ∑‚àó Œ∏                            of task-appropriate primitives triggered by an external stim-
                 Output: Optimal policy œÄ‚àó(a|s).                                      ulus. Here, wehavegeneratedalargesetofmotorprimitives
               Table 2: Algorithmic description of Policy Iteration based             that are triggered by a gating network that selects and gener-
               on Relative Entropy Policy Search. This version of the al-             alizes amongthemsimilartoamixtureofexperts. REPSim-
               gorithm extends the one in Table 1 for practical application.          proves the gating network by reinforcement learning where
               Note that N is not a Ô¨Åxed number but may change after ev-              any successful hit results as a reward of +1 and for failures
               ery iteration.                                                         no reward is given. REPS appears to be sensitive to good
                                                                                      initial sampling policies. The results vary considerably with
               thermore, we also observe that our REPS policy iteration               initial policy performance. When the system starts with an
               yields a signiÔ¨Åcantly higher performance. A comparison                 initial policy that has a success rate of ‚àº24%, it may quickly
               with PoWER (Kober and Peters 2009) was not necessary                   converge prematurely yielding a success rate of ‚àº39%. If
               as the episodic form of REPS appears to be equivalent to the           provided a better initialization, it can reach success rates of
               applicable version of PoWER. The performance of all three              upto‚àº59%.
               methods for all three problems is shown in Fig. 1 (a-c).                              Discussion & Conclusion
               Mountain-CarProblem                                                    In this paper, we have introduced a new reinforcement learn-
                                                                                      ing method called Relative Entropy Policy Search. It is de-
               Themountaincarproblem(SuttonandBarto1998)isawell-                      rived from a principle as previous covariant policy gradient
               knownprobleminreinforcement learning.                                  methods (Bagnell and Schneider 2003), i.e., attaining max-
                  Weadaptthecodefrom(Hernandez2010)andemploythe                       imal expected reward while bounding the amount of infor-
               same tile-coding features for both SARSA and REPS. We                  mationloss. Unlikeparametricgradientmethod,itallowsan
               implementouralgorithminthesamesettingsandareableto                     exact policy update and may use data generated while fol-
               show that REPS policy iteration also outperforms SARSA.                lowing an unknown policy to generate a new, better policy.
               While SARSAissuperÔ¨Åcially quite similar to the presented               It resembles the well-known reinforcement learning method
                                                                                1611
                         SARSA to an extent; however, it can be shown to outper-                                                                  Sutton, R.; McAllester, D.; Singh, S.; and Mansour, Y.
                         form it as the critic operates on a different, more sound cost                                                           2000. Policy gradient methods for reinforcement learning
                         function than traditional temporal difference learning, and                                                              with function approximation. In Advances in Neural Infor-
                         as its weighted ‚Äúsoft-max‚Äù policy update will promote suc-                                                               mation Processing Systems 12.
                         cessful actions faster than the standard soft-max. We have
                         shown that the method performs efÔ¨Åciently when used in a                                                                                                 Derivation of REPS
                         policy iteration setup. REPS is sound with function approx-                                                              We denote p                   = ¬µœÄ(s)œÄ(a|s) and ¬µœÄ(s) = P p                                             for
                         imation and can be kernelized straightforwardly which of-                                                                                        sa                                                                   a sa
                         fers interesting possibilities for new algorithms. The relation                                                          brevity of the derivations, and give the Lagrangian for the
                         to PoWER (Kober and Peters 2009) and Reward-Weighted                                                                     program in Eqs.(5-8) by
                         Regression is not yet fully understood as these methods                                                                                                 !                                                !
                                                                                                                                                              X                                       X                    p
                         minimizeD(pœÄ(œÑ)||r(œÑ)q(œÑ))whichissuperÔ¨Åciallysimilar                                                                       L=                p Ra +Œ∑ Œµ‚àí                             p      log sa
                                                                                                                                                                        sa      s                              sa          q
                                                                                                    œÄ                                                                                                                        sa
                         to maximizing Ep{r(œÑ)} subject to D(p (œÑ)||q(œÑ)). Both                                                                                s,a                                     s,a                !                              !
                         methods end up with very similar update equations for the                                                                        X X                                         X                                     X
                         episodic case. Application of REPS for reinforcement learn-                                                                               T                     a        0              0  0     0
                                                                                                                                                       + Œ∏                     p P 0œÜ ‚àí                      p       œÜ       +Œª 1‚àí                  p        ,
                         ing of motor primitive selection for robot table tennis has                                                                                              sa ss s                      s a      s                             sa
                                                                                                                                                              0          s,a                              0                                  s,a
                         been successful in simulation.                                                                                                     s                                           a                                                !
                                                                                                                                                            X                                   p                               X
                                                                                                                                                                                a                  sa               T                     a     T
                                                                                                                                                        =          p        R ‚àíŒ∑log                    ‚àíŒª‚àíŒ∏ œÜ +                       P 0Œ∏ 0œÜ 0
                                                                   References                                                                                        sa         s               q                   s     s              ss     s     s
                                                                                                                                                            s,a                                   sa                               0
                         Atkeson, C. G. 1993. Using local trajectory optimizers to                                                                                                                                               s
                         speed up global optimization in dynamic programming. In                                                                       +Œ∑Œµ+Œª,                                                                                          (11)
                         NIPS, 663‚Äì670.                                                                                                           whereŒ∑,Œ∏ andŒªdenotetheLagrangianmultipliers.Wesub-
                         Bagnell, J., and Schneider, J. 2003. Covariant policy search.                                                            stitute V = Œ∏TœÜ . We differentiate
                         In International Joint Conference on ArtiÔ¨Åcial Intelligence.                                                                            s               s
                                                                                                                                                                                          p                           P
                                                                                                                                                                        a                   sa                                  a
                                                                                                                                                   ‚àÇ       L=R ‚àíŒ∑log                              +Œ∑‚àíŒª+ P 0V0‚àíV =0,
                         Boyd,S.,andVandenberghe,L. 2004. ConvexOptimization.                                                                        p                  s                                                   0   ss     s           s
                                                                                                                                                       sa                                 q                               s
                         Cambridge University Press.                                                                                                                                        sa
                                                                                                                                                                                                 1      a              a                        Œª
                                                                                                                                                                                                   (R +           0 P      V 0‚àíV ) 1‚àí
                         de Farias, D. P., and Roy, B. V. 2003. The linear program-                                                               and obtain p                   = q eŒ∑ s                       s     ss0 s          s e        Œ∑ .Given
                         ming approach to approximate dynamic programming. Op-                                                                                            saP             sa
                         erations Research 51(6):850‚Äì856.                                                                                         that we require                  s,a psa = 1, it is necessary that
                                                                                                                                                                                                                                       
                                                                                                                                                          1‚àíŒª              X                   1 Ra+               Pa V 0‚àíV               ‚àí1
                         Deisenroth, M. 2009. EfÔ¨Åcient Reinforcement Learning us-                                                                                                                 ( s            0      0   s       s)
                                                                                                                                                        e       Œ∑ =                    qsaeŒ∑                   s     ss                         ,      (12)
                         ing Gaussian Processes. Ph.D. thesis, Karlsruhe Institute of                                                                                             s,a
                         Technology, Karlsruhe, Germany.                                                                                          (hence, Œª depends on Œ∏), and we can compute
                         Furmston, T., and Barber, D. 2010. Variational methods for                                                                                                                     P                                  
                         reinforcement learning. In AISTATS.                                                                                                                             1        a                   a        0
                                                                                                                                                                       qsa exp              (R +                0 P 0Vs ‚àíVs)
                                                                                                                                                     p      =                            Œ∑ s                 s      ss                           (13)
                         Hachiya, H.; Akiyama, T.; Sugiyama, M.; Peters, J.; 2008.                                                                     sa         P                           1                P
                                                                                                                                                                                                       a                   a        0
                                                                                                                                                                            q      exp           (R +                0 P 0V ‚àíV )
                         Adaptive importance sampling with automatic model selec-                                                                                      s,a sa                 Œ∑        s           s       ss     s          s
                         tion in value function approximation. In AAAI, 1351‚Äì1356.                                                                We can extract a policy using œÄ(a|s) = p /P p , and
                         Hernandez, J.                       2010.              http://www.dia.Ô¨Å.upm.es/ ja-                                                                                                                     sa         a sa
                         martin/download.htm.                                                                                                     hence optain Eq. (9). Reinserting these results into Eq.(11),
                         Kakade, S. A. 2002. Natural policy gradient. In Advances                                                                 weobtain the dual function
                                                                                                                                                                                                                               1‚àíŒª ‚àíŒµ
                         in Neural Information Processing Systems 14.                                                                                      g(Œ∏,Œ∑,Œª) = ‚àíŒ∑ +Œ∑Œµ+Œª=‚àíŒ∑log e Œ∑e                                                            ,
                         Kober, J.; Peters, J. 2009. Policy Search for Motor Primi-                                                               which can be rewritten as Eq.(10) by inserting Eq.(12).
                         tives in Robotics. In Advances in Neural Information Pro-
                         cessing Systems 22.
                         Mannor, S.; Simester, D.; Sun, P.; and Tsitsiklis, J. N. 2007.
                         Biases and variance in value function estimates. Manage-
                         ment Science 53(2):308‚Äì322.
                         Peshkin, L., and Shelton, C. R. 2002. Learning from scarce
                         experience. In ICML, 498‚Äì505.
                         Peters, J., and Schaal, S. 2008. Natural actor critic. Neuro-
                         computing 71(7-9):1180‚Äì1190.
                         Puterman, M. L. 2005. Markov Decision Processes: Dis-
                         crete Stochastic Dynamic Programming. New York, NY:
                         John Wiley and Sons.
                         Sutton, R., and Barto, A. 1998. Reinforcement Learning.
                         MITPress.
                                                                                                                                       1612
