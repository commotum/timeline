                     Preprint.
                     Figure 4: Token Efficiency Plot: Comparing various settings’ official score (oracle@2) on the public
                     validation subset to the tokens used by the reasoning model for strictly puzzle solving.
                     D.2  FUTURE WORK
                     Several directions follow naturally from this work. One is to broaden evaluation to larger and more
                     challenging puzzle sets, including those unsolved by the baseline, to test both new solve potential and
                     consistency on borderline cases. Another is to explore order-robust update strategies and curriculum
                     designs that reduce sensitivity to problem ordering while preserving inter-problem learning benefits.
                     Beyond these extensions, a key avenue is hierarchical abstraction: moving from additive concept
                     updates toward consolidation operations that restructure the memory across multiple experiences.
                     Finally, our current design choices for representation and retrieval were made under cost constraints;
                     richer optimization may yield substantial improvements.
                     To facilitate further research, we release a data resource: hand-annotated concepts for difficult
                     puzzles, together with a configurable puzzle synthesis pipeline. This resource is intended to support
                     isolated evaluation of concept representations and selection mechanisms, as well as development of
                     methods for concept abstraction. We hope it provides a useful foundation for continued exploration
                     of abstract reasoning memory.
                     E ADDITIONALRELATEDWORK
                     KnowledgeBaseAugmentedLLMsforFactuality. Earlyeffortstoenhancelanguagemodelswith
                     external memory primarily targeted knowledge-intensive tasks. Retrieval-Augmented Generation
                     (RAG) introduced the idea of retrieving documents from an external store based on embedding
                     similarity to augment generation (Lewis et al., 2021). Follow-up work like KnowledGPT incorporated
                     formal knowledge bases, allowing LLMs to execute structured queries through “Program-of-Thought”
                     prompting for multi-hop reasoning (Wang et al., 2023b). More recently, MemLLM proposed
                     inline memory operations—allowing the model to read from and write to memory directly during
                     generation—enabling continual adaptation without re-training (Modarressi et al., 2024).
                     EmbeddingSpaceMemory. Anotherstreamofworkaimstoovercomethelimitedcontextwindow
                     of transformers through architectural interventions that compress information from long sequences
                     into continuous space. Memorizing Transformers stored latent key-value vectors from past inputs and
                     retrieved them via a kNN mechanism to be attended to alongside the current context (Wu et al., 2022).
                     The Recurrent Memory Transformer extended this by incorporating a writable memory updated
                     over time, making memory content responsive to ongoing computation (Bulatov et al., 2022). He
                     et al. (2024b) blended these ideas by applying a moving average to update memory slots, allowing
                                                          15
