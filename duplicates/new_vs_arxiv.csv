source,year,title,url
arxiv,2025,A Benchmark to Evaluate Fundamental Numerical Abilities,https://arxiv.org/pdf/2502.11075.pdf
new,2025,Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models,https://arxiv.org/pdf/2502.11075.pdf
arxiv,2020,An Image is Worth 16Ã—16 Words: Transformers for Image Recognition at Scale (ViT),https://arxiv.org/pdf/2010.11929.pdf
new,2020,An Image Is Worth 16x16 Words (ViT),https://arxiv.org/pdf/2010.11929.pdf
arxiv,2015,Batch Normalization,https://arxiv.org/pdf/1502.03167.pdf
new,2015,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://arxiv.org/pdf/1502.03167.pdf
arxiv,2023,ConceptARC,https://arxiv.org/pdf/2305.07141.pdf
new,2023,The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain,https://arxiv.org/pdf/2305.07141.pdf
arxiv,2017,Constructing Datasets for Multi-hop Reading Comprehension (WikiHop / MedHop; QAngaroo),https://arxiv.org/pdf/1710.06481.pdf
new,2017,Constructing Datasets for Multi-hop Reading Comprehension,https://arxiv.org/pdf/1710.06481.pdf
arxiv,2025,Context-aware Rotary Position Embedding (CARoPE),https://arxiv.org/pdf/2507.23083.pdf
new,2025,Context-aware Rotary Position Embedding,https://arxiv.org/pdf/2507.23083.pdf
arxiv,2015,Continuous Control with Deep Reinforcement Learning (DDPG),https://arxiv.org/pdf/1509.02971.pdf
new,2015,Continuous Control with Deep Reinforcement Learning,https://arxiv.org/pdf/1509.02971.pdf
arxiv,2019,Deep Equilibrium Models (DEQ),https://arxiv.org/pdf/1909.01377.pdf
new,2019,Deep Equilibrium Models,https://arxiv.org/pdf/1909.01377.pdf
arxiv,2020,Deformable DETR: Deformable Transformers for End-to-End Object Detection,https://arxiv.org/pdf/2010.04159.pdf
new,2021,Deformable DETR: Deformable Transformers for End-to-End Object Detection,https://arxiv.org/pdf/2010.04159.pdf
arxiv,2019,Differentiable Convex Optimization Layers (DPP),https://arxiv.org/pdf/1910.12430.pdf
new,2019,Differentiable Convex Optimization Layers,https://arxiv.org/pdf/1910.12430.pdf
arxiv,2023,Direct Preference Optimization (DPO),https://arxiv.org/pdf/2305.18290.pdf
new,2023,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,https://arxiv.org/pdf/2305.18290.pdf
arxiv,2018,Distributed Prioritized Experience Replay (Ape-X),https://arxiv.org/pdf/1803.00933.pdf
new,2018,Distributed Prioritized Experience Replay,https://arxiv.org/pdf/1803.00933.pdf
arxiv,2020,DocVQA: A Dataset for VQA on Document Images,https://arxiv.org/pdf/2007.00398.pdf
new,2021,DocVQA: A Dataset for VQA on Document Images,https://arxiv.org/pdf/2007.00398.pdf
arxiv,2024,Gaussian Adaptive Attention Is All You Need,https://arxiv.org/pdf/2401.11143.pdf
new,2024,Density Adaptive Attention is All You Need: Robust Parameter-Efficient Fine-Tuning Across Multiple Modalities,https://arxiv.org/pdf/2401.11143.pdf
arxiv,2019,Guiding SAT Solvers with Unsat-Core Predictions (NeuroCore),https://arxiv.org/pdf/1903.04671.pdf
new,2019,Guiding High-Performance SAT Solvers with Unsat-Core Predictions,https://arxiv.org/pdf/1903.04671.pdf
arxiv,2023,LLaVA: Large Language-and-Vision Assistant,https://arxiv.org/pdf/2304.08485.pdf
new,2023,Visual Instruction Tuning,https://arxiv.org/pdf/2304.08485.pdf
arxiv,2021,miniF2F: a cross-system benchmark for formal Olympiad-level mathematics,https://arxiv.org/pdf/2109.00110.pdf
new,2022,miniF2F: a cross-system benchmark for formal Olympiad-level mathematics,https://arxiv.org/pdf/2109.00110.pdf
arxiv,2020,Taming Transformers for High-Resolution Image Synthesis (VQGAN + Transformer),https://arxiv.org/pdf/2012.09841.pdf
new,2020,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2012.09841.pdf
arxiv,2021,Training Verifiers to Solve Math Word Problems (GSM8K),https://arxiv.org/pdf/2110.14168.pdf
new,2021,GSM8K: Training Verifiers to Solve Math Word Problems,https://arxiv.org/pdf/2110.14168.pdf
arxiv,2019,ViLBERT: Pretraining Task-Agnostic Vision-and-Language Representations,https://arxiv.org/pdf/1908.02265.pdf
new,2019,ViLBERT: Pretraining Task-Agnostic V-L Representations,https://arxiv.org/pdf/1908.02265.pdf
arxiv,2021,Zero-Shot Text-to-Image Generation (DALL-E),https://arxiv.org/pdf/2102.12092.pdf
new,2021,Zero-Shot Text-to-Image Generation,https://arxiv.org/pdf/2102.12092.pdf
