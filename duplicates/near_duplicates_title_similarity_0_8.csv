match_id,score,source,year,title,url
1,1.0000,arXiv,2023,4D-Former: Multimodal 4D Panoptic Segmentation,https://arxiv.org/pdf/2311.01520.pdf
1,1.0000,DL,2023,4D-Former: Multimodal 4D Panoptic Segmentation,https://proceedings.mlr.press/v229/athar23a/athar23a.pdf
,,,,,
2,1.0000,arXiv,2024,ARC Prize 2024: Technical Report,https://arxiv.org/pdf/2412.04604.pdf
2,1.0000,DL,2024,ARC Prize 2024: Technical Report,https://arxiv.org/pdf/2412.04604.pdf
,,,,,
3,1.0000,arXiv,2025,ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2505.08778.pdf
3,1.0000,DL,2025,ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2505.08778.pdf
,,,,,
4,1.0000,arXiv,2017,Attention Is All You Need,https://arxiv.org/pdf/1706.03762.pdf
4,1.0000,DL,2017,Attention Is All You Need,https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf
,,,,,
5,1.0000,arXiv,2025,"Autoregressive Modeling as Iterative Latent Equilibrium (Equilibrium Transformers, EqT)",https://arxiv.org/pdf/2511.21882.pdf
5,1.0000,DL,2025,"Autoregressive Modeling as Iterative Latent Equilibrium (Equilibrium Transformers, EqT)",https://arxiv.org/pdf/2511.21882.pdf
,,,,,
6,1.0000,arXiv,2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805.pdf
6,1.0000,DL,2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805.pdf
,,,,,
7,1.0000,arXiv,2025,ChartQAPro: A More Diverse and Challenging Benchmark for Real-World Chart QA,https://arxiv.org/pdf/2504.05506.pdf
7,1.0000,DL,2025,ChartQAPro: A More Diverse and Challenging Benchmark for Real-World Chart QA,https://arxiv.org/pdf/2504.05506.pdf
,,,,,
8,1.0000,arXiv,2024,Combining Induction and Transduction for Abstract Reasoning,https://arxiv.org/pdf/2411.02272.pdf
8,1.0000,DL,2024,Combining Induction and Transduction for Abstract Reasoning,https://www.cs.cornell.edu/~ellisk/documents/arc_induction_vs_transduction.pdf
,,,,,
9,1.0000,arXiv,2019,Deep Equilibrium Models (DEQ),https://arxiv.org/pdf/1909.01377.pdf
9,1.0000,DL,2019,Deep Equilibrium Models (DEQ),https://implicit-layers-tutorial.org/deep_equilibrium_models/
,,,,,
10,1.0000,arXiv,2016,DeepCoder: Learning to Write Programs,https://arxiv.org/pdf/1611.01989.pdf
10,1.0000,DL,2017,DeepCoder: Learning to Write Programs,https://openreview.net/pdf?id=ByldLrqlx
,,,,,
11,1.0000,arXiv,2019,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,https://arxiv.org/pdf/1903.00161.pdf
11,1.0000,DL,2019,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,https://aclanthology.org/N19-1246.pdf
,,,,,
12,1.0000,arXiv,2025,DT-NVS: Diffusion Transformers for Novel View Synthesis,https://arxiv.org/pdf/2511.08823.pdf
12,1.0000,DL,2025,DT-NVS: Diffusion Transformers for Novel View Synthesis,https://arxiv.org/pdf/2511.08823.pdf
,,,,,
13,1.0000,arXiv,2015,End-To-End Memory Networks (MemN2N),https://arxiv.org/pdf/1503.08895.pdf
13,1.0000,DL,2015,End-To-End Memory Networks (MemN2N),https://arxiv.org/pdf/1503.08895.pdf
,,,,,
14,1.0000,arXiv,2024,Fixed Point Diffusion Models (FPDM),https://arxiv.org/pdf/2401.08741.pdf
14,1.0000,DL,2024,Fixed Point Diffusion Models (FPDM),https://arxiv.org/pdf/2401.08741.pdf
,,,,,
15,1.0000,arXiv,2025,Hierarchical Reasoning Model (HRM),https://arxiv.org/pdf/2506.21734.pdf
15,1.0000,DL,2025,Hierarchical Reasoning Model (HRM),https://arxiv.org/pdf/2506.21734.pdf
,,,,,
16,1.0000,arXiv,2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://arxiv.org/pdf/1809.09600.pdf
16,1.0000,DL,2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://aclanthology.org/D18-1259.pdf
,,,,,
17,1.0000,arXiv,2020,Language Models are Few-Shot Learners (GPT-3),https://arxiv.org/pdf/2005.14165.pdf
17,1.0000,DL,2020,Language Models are Few-Shot Learners (GPT-3),https://arxiv.org/pdf/2005.14165.pdf
,,,,,
18,1.0000,arXiv,2025,Learning Modular Exponentiation with Transformers,https://arxiv.org/pdf/2506.23679.pdf
18,1.0000,DL,2025,Learning Modular Exponentiation with Transformers,https://arxiv.org/pdf/2506.23679.pdf
,,,,,
19,1.0000,arXiv,2015,Learning to Transduce with Unbounded Memory,https://arxiv.org/pdf/1506.02516.pdf
19,1.0000,DL,2015,Learning to Transduce with Unbounded Memory,https://arxiv.org/pdf/1506.02516.pdf
,,,,,
20,1.0000,arXiv,2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://arxiv.org/pdf/1804.06028.pdf
20,1.0000,DL,2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://aclanthology.org/N18-4013.pdf
,,,,,
21,1.0000,arXiv,2024,Machine Learning for Modular Multiplication,https://arxiv.org/pdf/2402.19254.pdf
21,1.0000,DL,2024,Machine Learning for Modular Multiplication,https://arxiv.org/pdf/2402.19254.pdf
,,,,,
22,1.0000,arXiv,2025,MathClean: A Benchmark for Synthetic Mathematical Data,https://arxiv.org/pdf/2502.19058.pdf
22,1.0000,DL,2025,MathClean: A Benchmark for Synthetic Mathematical Data,https://arxiv.org/pdf/2502.19058.pdf
,,,,,
23,1.0000,arXiv,2022,Memorizing Transformers,https://arxiv.org/pdf/2203.08913.pdf
23,1.0000,DL,2022,Memorizing Transformers,https://arxiv.org/pdf/2203.08913.pdf
,,,,,
24,1.0000,arXiv,2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://arxiv.org/pdf/2410.15859.pdf
24,1.0000,DL,2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://proceedings.neurips.cc/paper_files/paper/2024/file/9446c291a8744a125a0bda5b18f4d5a1-Paper-Conference.pdf
,,,,,
25,1.0000,arXiv,2015,Neural Programmer-Interpreters (NPI),https://arxiv.org/pdf/1511.06279.pdf
25,1.0000,DL,2015,Neural Programmer-Interpreters (NPI),https://arxiv.org/pdf/1511.06279.pdf
,,,,,
26,1.0000,arXiv,2019,On the Measure of Intelligence (ARC),https://arxiv.org/pdf/1911.01547.pdf
26,1.0000,DL,2019,On the Measure of Intelligence (ARC),https://arxiv.org/pdf/1911.01547.pdf
,,,,,
27,1.0000,arXiv,2025,Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective,https://arxiv.org/pdf/2505.07859.pdf
27,1.0000,DL,2025,Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective,https://arxiv.org/pdf/2505.07859.pdf
,,,,,
28,1.0000,arXiv,2020,REALM: Retrieval-Augmented Language Model Pre-Training,https://arxiv.org/pdf/2002.08909.pdf
28,1.0000,DL,2020,REALM: Retrieval-Augmented Language Model Pre-Training,https://arxiv.org/pdf/2002.08909.pdf
,,,,,
29,1.0000,arXiv,2018,Recurrent Relational Networks (RRN),https://arxiv.org/pdf/1711.08028.pdf
29,1.0000,DL,2018,Recurrent Relational Networks (RRN),https://proceedings.neurips.cc/paper_files/paper/2018/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf
,,,,,
30,1.0000,arXiv,2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://arxiv.org/pdf/2403.00071.pdf
30,1.0000,DL,2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://aclanthology.org/2024.findings-acl.32.pdf
,,,,,
31,1.0000,arXiv,2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://arxiv.org/pdf/2006.15595.pdf
31,1.0000,DL,2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://openreview.net/pdf?id=09-528y2Fgf
,,,,,
32,1.0000,arXiv,2020,Scaling Laws for Neural Language Models,https://arxiv.org/pdf/2001.08361.pdf
32,1.0000,DL,2020,Scaling Laws for Neural Language Models,/home/jake/Developer/timeline/BIBLIOTHEQUE/26_Scaling_Laws_for_Neural_Language_Models.pdf
,,,,,
33,1.0000,arXiv,2024,Searching Latent Program Spaces,https://arxiv.org/pdf/2411.08706.pdf
33,1.0000,DL,2024,Searching Latent Program Spaces,https://raw.githubusercontent.com/clement-bonnet/lpn/7f86b1d11ea37ba173700dbac8604393eac6da37/paper.pdf
,,,,,
34,1.0000,arXiv,2018,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,https://arxiv.org/pdf/1810.00825.pdf
34,1.0000,DL,2018,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,https://arxiv.org/pdf/1810.00825.pdf
,,,,,
35,1.0000,arXiv,2025,SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection,https://arxiv.org/pdf/2503.08092.pdf
35,1.0000,DL,2025,SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection,https://arxiv.org/pdf/2503.08092.pdf
,,,,,
36,1.0000,arXiv,2021,Stabilizing Equilibrium Models by Jacobian Regularization,https://arxiv.org/pdf/2106.14342.pdf
36,1.0000,DL,2021,Stabilizing Equilibrium Models by Jacobian Regularization,https://arxiv.org/pdf/2106.14342.pdf
,,,,,
37,1.0000,arXiv,2025,Streaming 4D Panoptic Segmentation via Dual Threads,https://arxiv.org/pdf/2510.17664.pdf
37,1.0000,DL,2025,Streaming 4D Panoptic Segmentation via Dual Threads,https://arxiv.org/pdf/2510.17664.pdf
,,,,,
38,1.0000,arXiv,2025,The Art of Scaling Test-Time Compute for LLMs,https://arxiv.org/pdf/2512.02008.pdf
38,1.0000,DL,2025,The Art of Scaling Test-Time Compute for LLMs,https://arxiv.org/pdf/2512.02008.pdf
,,,,,
39,1.0000,arXiv,2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://arxiv.org/pdf/2411.07279.pdf
39,1.0000,DL,2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://arxiv.org/pdf/2411.07279.pdf
,,,,,
40,1.0000,arXiv,2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arxiv.org/pdf/2411.17708.pdf
40,1.0000,DL,2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://drive.google.com/uc?export=download&id=1sFlK3mhz8kH2agdE379o0ODQWYkrSD0b
,,,,,
41,1.0000,arXiv,2025,Towards the Next Generation of Symbolic Regression Benchmarks (SRBench),https://arxiv.org/pdf/2505.03977.pdf
41,1.0000,DL,2025,Towards the Next Generation of Symbolic Regression Benchmarks (SRBench),https://arxiv.org/pdf/2505.03977.pdf
,,,,,
42,1.0000,arXiv,2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://arxiv.org/pdf/1901.02860.pdf
42,1.0000,DL,2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://aclanthology.org/P19-1285.pdf
,,,,,
43,1.0000,arXiv,2025,TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution,https://arxiv.org/pdf/2506.18421.pdf
43,1.0000,DL,2025,TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution,https://arxiv.org/pdf/2506.18421.pdf
,,,,,
44,1.0000,arXiv,2018,Universal Transformers (UT),https://arxiv.org/pdf/1807.03819.pdf
44,1.0000,DL,2018,Universal Transformers (UT),https://arxiv.org/pdf/1807.03819.pdf
,,,,,
45,1.0000,arXiv,2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2511.08747.pdf
45,1.0000,DL,2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://raw.githubusercontent.com/ijoffe/ARC-VSA-2025/main/paper/paper.pdf
,,,,,
46,0.9730,arXiv,2017,SyGuS-Comp 2017: Results and Analysis,https://arxiv.org/pdf/1711.11438.pdf
46,0.9730,DL,2016,SyGuS-Comp 2016: Results and Analysis,https://dspace.mit.edu/bitstream/handle/1721.1/137904/1611.07627.pdf?isAllowed=y&sequence=2.pdf
,,,,,
47,0.9455,arXiv,2025,Less is More: Recursive Reasoning with Tiny Networks,https://arxiv.org/pdf/2510.04871.pdf
47,0.9455,DL,2025,Less is More: Recursive Reasoning with Tiny Networks (TRM),https://arxiv.org/pdf/2510.04871.pdf
,,,,,
48,0.9346,arXiv,2021,PSB2: The Second Program Synthesis Benchmark Suite,https://arxiv.org/pdf/2106.06086.pdf
48,0.9346,DL,2021,PSB2: The Second Program Synthesis Benchmark Suite (PSB2),https://arxiv.org/pdf/2106.06086.pdf
,,,,,
49,0.9318,arXiv,2019,Differentiable Convex Optimization Layers (DPP),https://arxiv.org/pdf/1910.12430.pdf
49,0.9318,DL,2019,Differentiable Convex Optimization Layers,https://openreview.net/pdf?id=1EuxRTe0WN
,,,,,
50,0.9167,arXiv,2017,End-to-End Differentiable Proving,https://arxiv.org/pdf/1705.11040.pdf
50,0.9167,DL,2017,End-to-End Differentiable Proving (NTP),https://arxiv.org/pdf/1705.11040.pdf
,,,,,
51,0.9160,arXiv,2013,Efficient Estimation of Word Representations in Vector Space,https://arxiv.org/pdf/1301.3781.pdf
51,0.9160,DL,2013,Efficient Estimation of Word Representations in Vector Space (word2vec),https://arxiv.org/pdf/1301.3781.pdf
,,,,,
52,0.9032,arXiv,2020,PCT: Point Cloud Transformer,https://arxiv.org/pdf/2012.09688.pdf
52,0.9032,DL,2021,PCT: Point cloud transformer (PCT),https://www.sciopen.com/article_pdf/10.1007/s41095-021-0229-5.pdf
,,,,,
53,0.8904,arXiv,2024,Length Generalization of Causal Transformers without Position Encoding,https://arxiv.org/pdf/2404.12224.pdf
53,0.8904,DL,2024,Length Extrapolation of Causal Transformers without Position Encoding (NoPE),https://aclanthology.org/2024.findings-acl.834.pdf
,,,,,
54,0.8807,arXiv,2024,Rotary Position Embedding for Vision Transformer (RoPEâ€‘Mixed),https://arxiv.org/pdf/2403.13298.pdf
54,0.8807,DL,2024,Rotary Position Embedding for Vision Transformer,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01584.pdf
,,,,,
55,0.8800,arXiv,2014,Neural Turing Machines (NTM),https://arxiv.org/pdf/1410.5401.pdf
55,0.8800,DL,2014,Neural Turing Machines,/home/jake/Developer/timeline/BIBLIOTHEQUE/21_Neural_Turing_Machines.pdf
,,,,,
56,0.8421,arXiv,2023,Segment Anything,https://arxiv.org/pdf/2304.02643.pdf
56,0.8421,DL,2023,Segment Anything (SAM),https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf
,,,,,
57,0.8216,arXiv,2023,MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,https://arxiv.org/pdf/2311.16502.pdf
57,0.8216,DL,2025,MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding and Reasoning Benchmark,https://aclanthology.org/2025.acl-long.736.pdf
,,,,,
58,0.8190,arXiv,2021,CoAtNet: Marrying Convolution and Attention for All Data Sizes,https://arxiv.org/pdf/2106.04803.pdf
58,0.8190,DL,2021,CoAtNet: Marrying Convolution and Attention,https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Paper.pdf
,,,,,
59,0.8130,arXiv,2022,Self-Consistency Improves Chain-of-Thought Reasoning,https://arxiv.org/pdf/2203.11171.pdf
59,0.8130,DL,2022,Self-Consistency Improves Chain of Thought Reasoning in Language Models,https://arxiv.org/pdf/2203.11171.pdf
,,,,,
60,0.8060,arXiv,2017,SyGuS-Comp 2017: Results and Analysis,https://arxiv.org/pdf/1711.11438.pdf
60,0.8060,DL,2015,SyGuS-Comp'15 Results/Analysis,https://rishabhmit.bitbucket.io/papers/synt15.pdf
