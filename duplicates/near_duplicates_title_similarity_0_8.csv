match_id,score,source,year,title,url
1,1.0000,arXiv,2023,4D-Former: Multimodal 4D Panoptic Segmentation,https://arxiv.org/pdf/2311.01520.pdf
1,1.0000,DL,2023,4D-Former: Multimodal 4D Panoptic Segmentation,https://proceedings.mlr.press/v229/athar23a/athar23a.pdf
,,,,,
2,1.0000,arXiv,2017,Attention Is All You Need,https://arxiv.org/pdf/1706.03762.pdf
2,1.0000,DL,2017,Attention Is All You Need,https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf
,,,,,
3,1.0000,arXiv,2024,Combining Induction and Transduction for Abstract Reasoning,https://arxiv.org/pdf/2411.02272.pdf
3,1.0000,DL,2024,Combining Induction and Transduction for Abstract Reasoning,https://www.cs.cornell.edu/~ellisk/documents/arc_induction_vs_transduction.pdf
,,,,,
4,1.0000,arXiv,2019,Deep Equilibrium Models (DEQ),https://arxiv.org/pdf/1909.01377.pdf
4,1.0000,DL,2019,Deep Equilibrium Models (DEQ),https://implicit-layers-tutorial.org/deep_equilibrium_models/
,,,,,
5,1.0000,arXiv,2016,DeepCoder: Learning to Write Programs,https://arxiv.org/pdf/1611.01989.pdf
5,1.0000,DL,2017,DeepCoder: Learning to Write Programs,https://openreview.net/pdf?id=ByldLrqlx
,,,,,
6,1.0000,arXiv,2019,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,https://arxiv.org/pdf/1903.00161.pdf
6,1.0000,DL,2019,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,https://aclanthology.org/N19-1246.pdf
,,,,,
7,1.0000,arXiv,2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://arxiv.org/pdf/1809.09600.pdf
7,1.0000,DL,2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://aclanthology.org/D18-1259.pdf
,,,,,
8,1.0000,arXiv,2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://arxiv.org/pdf/1804.06028.pdf
8,1.0000,DL,2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://aclanthology.org/N18-4013.pdf
,,,,,
9,1.0000,arXiv,2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://arxiv.org/pdf/2410.15859.pdf
9,1.0000,DL,2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://proceedings.neurips.cc/paper_files/paper/2024/file/9446c291a8744a125a0bda5b18f4d5a1-Paper-Conference.pdf
,,,,,
10,1.0000,arXiv,2018,Recurrent Relational Networks (RRN),https://arxiv.org/pdf/1711.08028.pdf
10,1.0000,DL,2018,Recurrent Relational Networks (RRN),https://proceedings.neurips.cc/paper_files/paper/2018/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf
,,,,,
11,1.0000,arXiv,2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://arxiv.org/pdf/2403.00071.pdf
11,1.0000,DL,2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://aclanthology.org/2024.findings-acl.32.pdf
,,,,,
12,1.0000,arXiv,2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://arxiv.org/pdf/2006.15595.pdf
12,1.0000,DL,2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://openreview.net/pdf?id=09-528y2Fgf
,,,,,
13,1.0000,arXiv,2020,Scaling Laws for Neural Language Models,https://arxiv.org/pdf/2001.08361.pdf
13,1.0000,DL,2020,Scaling Laws for Neural Language Models,/home/jake/Developer/timeline/BIBLIOTHEQUE/26_Scaling_Laws_for_Neural_Language_Models.pdf
,,,,,
14,1.0000,arXiv,2024,Searching Latent Program Spaces,https://arxiv.org/pdf/2411.08706.pdf
14,1.0000,DL,2024,Searching Latent Program Spaces,https://raw.githubusercontent.com/clement-bonnet/lpn/7f86b1d11ea37ba173700dbac8604393eac6da37/paper.pdf
,,,,,
15,1.0000,arXiv,2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arxiv.org/pdf/2411.17708.pdf
15,1.0000,DL,2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://drive.google.com/uc?export=download&id=1sFlK3mhz8kH2agdE379o0ODQWYkrSD0b
,,,,,
16,1.0000,arXiv,2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://arxiv.org/pdf/1901.02860.pdf
16,1.0000,DL,2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://aclanthology.org/P19-1285.pdf
,,,,,
17,1.0000,arXiv,2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2511.08747.pdf
17,1.0000,DL,2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://raw.githubusercontent.com/ijoffe/ARC-VSA-2025/main/paper/paper.pdf
,,,,,
18,0.9730,arXiv,2017,SyGuS-Comp 2017: Results and Analysis,https://arxiv.org/pdf/1711.11438.pdf
18,0.9730,DL,2016,SyGuS-Comp 2016: Results and Analysis,https://dspace.mit.edu/bitstream/handle/1721.1/137904/1611.07627.pdf?isAllowed=y&sequence=2.pdf
,,,,,
19,0.9455,arXiv,2025,Less is More: Recursive Reasoning with Tiny Networks,https://arxiv.org/pdf/2510.04871.pdf
19,0.9455,DL,2025,Less is More: Recursive Reasoning with Tiny Networks (TRM),https://arxiv.org/pdf/2510.04871.pdf
,,,,,
20,0.9346,arXiv,2021,PSB2: The Second Program Synthesis Benchmark Suite,https://arxiv.org/pdf/2106.06086.pdf
20,0.9346,DL,2021,PSB2: The Second Program Synthesis Benchmark Suite (PSB2),https://arxiv.org/pdf/2106.06086.pdf
,,,,,
21,0.9318,arXiv,2019,Differentiable Convex Optimization Layers (DPP),https://arxiv.org/pdf/1910.12430.pdf
21,0.9318,DL,2019,Differentiable Convex Optimization Layers,https://openreview.net/pdf?id=1EuxRTe0WN
,,,,,
22,0.9167,arXiv,2017,End-to-End Differentiable Proving,https://arxiv.org/pdf/1705.11040.pdf
22,0.9167,DL,2017,End-to-End Differentiable Proving (NTP),https://arxiv.org/pdf/1705.11040.pdf
,,,,,
23,0.9160,arXiv,2013,Efficient Estimation of Word Representations in Vector Space,https://arxiv.org/pdf/1301.3781.pdf
23,0.9160,DL,2013,Efficient Estimation of Word Representations in Vector Space (word2vec),https://arxiv.org/pdf/1301.3781.pdf
,,,,,
24,0.9032,arXiv,2020,PCT: Point Cloud Transformer,https://arxiv.org/pdf/2012.09688.pdf
24,0.9032,DL,2021,PCT: Point cloud transformer (PCT),https://www.sciopen.com/article_pdf/10.1007/s41095-021-0229-5.pdf
,,,,,
25,0.8904,arXiv,2024,Length Generalization of Causal Transformers without Position Encoding,https://arxiv.org/pdf/2404.12224.pdf
25,0.8904,DL,2024,Length Extrapolation of Causal Transformers without Position Encoding (NoPE),https://aclanthology.org/2024.findings-acl.834.pdf
,,,,,
26,0.8807,arXiv,2024,Rotary Position Embedding for Vision Transformer (RoPEâ€‘Mixed),https://arxiv.org/pdf/2403.13298.pdf
26,0.8807,DL,2024,Rotary Position Embedding for Vision Transformer,https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/01584.pdf
,,,,,
27,0.8800,arXiv,2014,Neural Turing Machines (NTM),https://arxiv.org/pdf/1410.5401.pdf
27,0.8800,DL,2014,Neural Turing Machines,/home/jake/Developer/timeline/BIBLIOTHEQUE/21_Neural_Turing_Machines.pdf
,,,,,
28,0.8421,arXiv,2023,Segment Anything,https://arxiv.org/pdf/2304.02643.pdf
28,0.8421,DL,2023,Segment Anything (SAM),https://openaccess.thecvf.com/content/ICCV2023/papers/Kirillov_Segment_Anything_ICCV_2023_paper.pdf
,,,,,
29,0.8216,arXiv,2023,MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI,https://arxiv.org/pdf/2311.16502.pdf
29,0.8216,DL,2025,MMMU-Pro: A More Robust Multi-discipline Multimodal Understanding and Reasoning Benchmark,https://aclanthology.org/2025.acl-long.736.pdf
,,,,,
30,0.8190,arXiv,2021,CoAtNet: Marrying Convolution and Attention for All Data Sizes,https://arxiv.org/pdf/2106.04803.pdf
30,0.8190,DL,2021,CoAtNet: Marrying Convolution and Attention,https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Paper.pdf
,,,,,
31,0.8130,arXiv,2022,Self-Consistency Improves Chain-of-Thought Reasoning,https://arxiv.org/pdf/2203.11171.pdf
31,0.8130,DL,2022,Self-Consistency Improves Chain of Thought Reasoning in Language Models,https://arxiv.org/pdf/2203.11171.pdf
,,,,,
32,0.8060,arXiv,2017,SyGuS-Comp 2017: Results and Analysis,https://arxiv.org/pdf/1711.11438.pdf
32,0.8060,DL,2015,SyGuS-Comp'15 Results/Analysis,https://rishabhmit.bitbucket.io/papers/synt15.pdf
