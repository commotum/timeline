source,year,title,url
arXiv,2025,ARC Is a Vision Problem! (Vision ARC / VARC),https://arxiv.org/pdf/2511.14761.pdf
DL,2025,ARC is a Vision Problem!,https://arxiv.org/pdf/2511.14761.pdf
,,,
arXiv,2024,ARC Prize 2024: Technical Report,https://arxiv.org/pdf/2412.04604.pdf
DL,2024,ARC Prize 2024: Technical Report,https://arxiv.org/pdf/2412.04604.pdf
,,,
arXiv,2025,ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2505.08778.pdf
DL,2025,ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2505.08778.pdf
,,,
arXiv,2025,"Autoregressive Modeling as Iterative Latent Equilibrium (Equilibrium Transformers, EqT)",https://arxiv.org/pdf/2511.21882.pdf
DL,2025,"Autoregressive Modeling as Iterative Latent Equilibrium (Equilibrium Transformers, EqT)",https://arxiv.org/pdf/2511.21882.pdf
,,,
arXiv,2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805.pdf
DL,2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805.pdf
,,,
arXiv,2025,ChartQAPro: A More Diverse and Challenging Benchmark for Real-World Chart QA,https://arxiv.org/pdf/2504.05506.pdf
DL,2025,ChartQAPro: A More Diverse and Challenging Benchmark for Real-World Chart QA,https://arxiv.org/pdf/2504.05506.pdf
,,,
arXiv,2025,DT-NVS: Diffusion Transformers for Novel View Synthesis,https://arxiv.org/pdf/2511.08823.pdf
DL,2025,DT-NVS: Diffusion Transformers for Novel View Synthesis,https://arxiv.org/pdf/2511.08823.pdf
,,,
arXiv,2025,DynamicCity,https://arxiv.org/pdf/2410.18084.pdf
DL,2025,DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes,https://arxiv.org/pdf/2410.18084.pdf
,,,
arXiv,2013,Efficient Estimation of Word Representations in Vector Space,https://arxiv.org/pdf/1301.3781.pdf
DL,2013,Efficient Estimation of Word Representations in Vector Space (word2vec),https://arxiv.org/pdf/1301.3781.pdf
,,,
arXiv,2025,Efficiently Allocating Test-Time Compute for LLM Agents,https://arxiv.org/pdf/2509.03581.pdf
DL,2025,Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents,https://arxiv.org/pdf/2509.03581.pdf
,,,
arXiv,2017,End-to-End Differentiable Proving,https://arxiv.org/pdf/1705.11040.pdf
DL,2017,End-to-End Differentiable Proving (NTP),https://arxiv.org/pdf/1705.11040.pdf
,,,
arXiv,2015,End-To-End Memory Networks (MemN2N),https://arxiv.org/pdf/1503.08895.pdf
DL,2015,End-To-End Memory Networks (MemN2N),https://arxiv.org/pdf/1503.08895.pdf
,,,
arXiv,2024,Fixed Point Diffusion Models (FPDM),https://arxiv.org/pdf/2401.08741.pdf
DL,2024,Fixed Point Diffusion Models (FPDM),https://arxiv.org/pdf/2401.08741.pdf
,,,
arXiv,2025,Hierarchical Reasoning Model (HRM),https://arxiv.org/pdf/2506.21734.pdf
DL,2025,Hierarchical Reasoning Model (HRM),https://arxiv.org/pdf/2506.21734.pdf
,,,
arXiv,2020,Language Models are Few-Shot Learners (GPT-3),https://arxiv.org/pdf/2005.14165.pdf
DL,2020,Language Models are Few-Shot Learners (GPT-3),https://arxiv.org/pdf/2005.14165.pdf
,,,
arXiv,2025,Learning Modular Exponentiation with Transformers,https://arxiv.org/pdf/2506.23679.pdf
DL,2025,Learning Modular Exponentiation with Transformers,https://arxiv.org/pdf/2506.23679.pdf
,,,
arXiv,2015,Learning to Transduce with Unbounded Memory,https://arxiv.org/pdf/1506.02516.pdf
DL,2015,Learning to Transduce with Unbounded Memory,https://arxiv.org/pdf/1506.02516.pdf
,,,
arXiv,2025,Less is More: Recursive Reasoning with Tiny Networks,https://arxiv.org/pdf/2510.04871.pdf
DL,2025,Less is More: Recursive Reasoning with Tiny Networks (TRM),https://arxiv.org/pdf/2510.04871.pdf
,,,
arXiv,2024,Machine Learning for Modular Multiplication,https://arxiv.org/pdf/2402.19254.pdf
DL,2024,Machine Learning for Modular Multiplication,https://arxiv.org/pdf/2402.19254.pdf
,,,
arXiv,2025,MathClean: A Benchmark for Synthetic Mathematical Data,https://arxiv.org/pdf/2502.19058.pdf
DL,2025,MathClean: A Benchmark for Synthetic Mathematical Data,https://arxiv.org/pdf/2502.19058.pdf
,,,
arXiv,2022,Memorizing Transformers,https://arxiv.org/pdf/2203.08913.pdf
DL,2022,Memorizing Transformers,https://arxiv.org/pdf/2203.08913.pdf
,,,
arXiv,2015,Neural Programmer-Interpreters (NPI),https://arxiv.org/pdf/1511.06279.pdf
DL,2015,Neural Programmer-Interpreters (NPI),https://arxiv.org/pdf/1511.06279.pdf
,,,
arXiv,2019,On the Measure of Intelligence (ARC),https://arxiv.org/pdf/1911.01547.pdf
DL,2019,On the Measure of Intelligence (ARC),https://arxiv.org/pdf/1911.01547.pdf
,,,
arXiv,2025,Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective,https://arxiv.org/pdf/2505.07859.pdf
DL,2025,Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective,https://arxiv.org/pdf/2505.07859.pdf
,,,
arXiv,2021,PSB2: The Second Program Synthesis Benchmark Suite,https://arxiv.org/pdf/2106.06086.pdf
DL,2021,PSB2: The Second Program Synthesis Benchmark Suite (PSB2),https://arxiv.org/pdf/2106.06086.pdf
,,,
arXiv,2020,REALM: Retrieval-Augmented Language Model Pre-Training,https://arxiv.org/pdf/2002.08909.pdf
DL,2020,REALM: Retrieval-Augmented Language Model Pre-Training,https://arxiv.org/pdf/2002.08909.pdf
,,,
arXiv,2022,Self-Consistency Improves Chain-of-Thought Reasoning,https://arxiv.org/pdf/2203.11171.pdf
DL,2022,Self-Consistency Improves Chain of Thought Reasoning in Language Models,https://arxiv.org/pdf/2203.11171.pdf
,,,
arXiv,2018,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,https://arxiv.org/pdf/1810.00825.pdf
DL,2018,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,https://arxiv.org/pdf/1810.00825.pdf
,,,
arXiv,2025,SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection,https://arxiv.org/pdf/2503.08092.pdf
DL,2025,SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection,https://arxiv.org/pdf/2503.08092.pdf
,,,
arXiv,2021,Stabilizing Equilibrium Models by Jacobian Regularization,https://arxiv.org/pdf/2106.14342.pdf
DL,2021,Stabilizing Equilibrium Models by Jacobian Regularization,https://arxiv.org/pdf/2106.14342.pdf
,,,
arXiv,2025,Streaming 4D Panoptic Segmentation via Dual Threads,https://arxiv.org/pdf/2510.17664.pdf
DL,2025,Streaming 4D Panoptic Segmentation via Dual Threads,https://arxiv.org/pdf/2510.17664.pdf
,,,
arXiv,2025,The Art of Scaling Test-Time Compute for LLMs,https://arxiv.org/pdf/2512.02008.pdf
DL,2025,The Art of Scaling Test-Time Compute for LLMs,https://arxiv.org/pdf/2512.02008.pdf
,,,
arXiv,2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://arxiv.org/pdf/2411.07279.pdf
DL,2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://arxiv.org/pdf/2411.07279.pdf
,,,
arXiv,2025,Towards the Next Generation of Symbolic Regression Benchmarks (SRBench),https://arxiv.org/pdf/2505.03977.pdf
DL,2025,Towards the Next Generation of Symbolic Regression Benchmarks (SRBench),https://arxiv.org/pdf/2505.03977.pdf
,,,
arXiv,2025,TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution,https://arxiv.org/pdf/2506.18421.pdf
DL,2025,TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution,https://arxiv.org/pdf/2506.18421.pdf
,,,
arXiv,2018,Universal Transformers (UT),https://arxiv.org/pdf/1807.03819.pdf
DL,2018,Universal Transformers (UT),https://arxiv.org/pdf/1807.03819.pdf
