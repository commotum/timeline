source,year,title,url
arXiv,2023,4D-Former: Multimodal 4D Panoptic Segmentation,https://arxiv.org/pdf/2311.01520.pdf
DL,2023,4D-Former: Multimodal 4D Panoptic Segmentation,https://proceedings.mlr.press/v229/athar23a/athar23a.pdf
,,,
arXiv,2024,ARC Prize 2024: Technical Report,https://arxiv.org/pdf/2412.04604.pdf
DL,2024,ARC Prize 2024: Technical Report,https://arxiv.org/pdf/2412.04604.pdf
,,,
arXiv,2025,ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2505.08778.pdf
DL,2025,ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2505.08778.pdf
,,,
arXiv,2017,Attention Is All You Need,https://arxiv.org/pdf/1706.03762.pdf
DL,2017,Attention Is All You Need,https://papers.neurips.cc/paper/7181-attention-is-all-you-need.pdf
,,,
arXiv,2025,"Autoregressive Modeling as Iterative Latent Equilibrium (Equilibrium Transformers, EqT)",https://arxiv.org/pdf/2511.21882.pdf
DL,2025,"Autoregressive Modeling as Iterative Latent Equilibrium (Equilibrium Transformers, EqT)",https://arxiv.org/pdf/2511.21882.pdf
,,,
arXiv,2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805.pdf
DL,2018,BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,https://arxiv.org/pdf/1810.04805.pdf
,,,
arXiv,2025,ChartQAPro: A More Diverse and Challenging Benchmark for Real-World Chart QA,https://arxiv.org/pdf/2504.05506.pdf
DL,2025,ChartQAPro: A More Diverse and Challenging Benchmark for Real-World Chart QA,https://arxiv.org/pdf/2504.05506.pdf
,,,
arXiv,2024,Combining Induction and Transduction for Abstract Reasoning,https://arxiv.org/pdf/2411.02272.pdf
DL,2024,Combining Induction and Transduction for Abstract Reasoning,https://www.cs.cornell.edu/~ellisk/documents/arc_induction_vs_transduction.pdf
,,,
arXiv,2019,Deep Equilibrium Models (DEQ),https://arxiv.org/pdf/1909.01377.pdf
DL,2019,Deep Equilibrium Models (DEQ),https://implicit-layers-tutorial.org/deep_equilibrium_models/
,,,
arXiv,2016,DeepCoder: Learning to Write Programs,https://arxiv.org/pdf/1611.01989.pdf
DL,2017,DeepCoder: Learning to Write Programs,https://openreview.net/pdf?id=ByldLrqlx
,,,
arXiv,2019,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,https://arxiv.org/pdf/1903.00161.pdf
DL,2019,DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs,https://aclanthology.org/N19-1246.pdf
,,,
arXiv,2025,DT-NVS: Diffusion Transformers for Novel View Synthesis,https://arxiv.org/pdf/2511.08823.pdf
DL,2025,DT-NVS: Diffusion Transformers for Novel View Synthesis,https://arxiv.org/pdf/2511.08823.pdf
,,,
arXiv,2015,End-To-End Memory Networks (MemN2N),https://arxiv.org/pdf/1503.08895.pdf
DL,2015,End-To-End Memory Networks (MemN2N),https://arxiv.org/pdf/1503.08895.pdf
,,,
arXiv,2024,Fixed Point Diffusion Models (FPDM),https://arxiv.org/pdf/2401.08741.pdf
DL,2024,Fixed Point Diffusion Models (FPDM),https://arxiv.org/pdf/2401.08741.pdf
,,,
arXiv,2025,Hierarchical Reasoning Model (HRM),https://arxiv.org/pdf/2506.21734.pdf
DL,2025,Hierarchical Reasoning Model (HRM),https://arxiv.org/pdf/2506.21734.pdf
,,,
arXiv,2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://arxiv.org/pdf/1809.09600.pdf
DL,2018,"HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",https://aclanthology.org/D18-1259.pdf
,,,
arXiv,2020,Language Models are Few-Shot Learners (GPT-3),https://arxiv.org/pdf/2005.14165.pdf
DL,2020,Language Models are Few-Shot Learners (GPT-3),https://arxiv.org/pdf/2005.14165.pdf
,,,
arXiv,2025,Learning Modular Exponentiation with Transformers,https://arxiv.org/pdf/2506.23679.pdf
DL,2025,Learning Modular Exponentiation with Transformers,https://arxiv.org/pdf/2506.23679.pdf
,,,
arXiv,2015,Learning to Transduce with Unbounded Memory,https://arxiv.org/pdf/1506.02516.pdf
DL,2015,Learning to Transduce with Unbounded Memory,https://arxiv.org/pdf/1506.02516.pdf
,,,
arXiv,2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://arxiv.org/pdf/1804.06028.pdf
DL,2018,ListOps: A Diagnostic Dataset for Latent Tree Learning,https://aclanthology.org/N18-4013.pdf
,,,
arXiv,2024,Machine Learning for Modular Multiplication,https://arxiv.org/pdf/2402.19254.pdf
DL,2024,Machine Learning for Modular Multiplication,https://arxiv.org/pdf/2402.19254.pdf
,,,
arXiv,2025,MathClean: A Benchmark for Synthetic Mathematical Data,https://arxiv.org/pdf/2502.19058.pdf
DL,2025,MathClean: A Benchmark for Synthetic Mathematical Data,https://arxiv.org/pdf/2502.19058.pdf
,,,
arXiv,2022,Memorizing Transformers,https://arxiv.org/pdf/2203.08913.pdf
DL,2022,Memorizing Transformers,https://arxiv.org/pdf/2203.08913.pdf
,,,
arXiv,2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://arxiv.org/pdf/2410.15859.pdf
DL,2024,Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs,https://proceedings.neurips.cc/paper_files/paper/2024/file/9446c291a8744a125a0bda5b18f4d5a1-Paper-Conference.pdf
,,,
arXiv,2015,Neural Programmer-Interpreters (NPI),https://arxiv.org/pdf/1511.06279.pdf
DL,2015,Neural Programmer-Interpreters (NPI),https://arxiv.org/pdf/1511.06279.pdf
,,,
arXiv,2019,On the Measure of Intelligence (ARC),https://arxiv.org/pdf/1911.01547.pdf
DL,2019,On the Measure of Intelligence (ARC),https://arxiv.org/pdf/1911.01547.pdf
,,,
arXiv,2025,Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective,https://arxiv.org/pdf/2505.07859.pdf
DL,2025,Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective,https://arxiv.org/pdf/2505.07859.pdf
,,,
arXiv,2020,REALM: Retrieval-Augmented Language Model Pre-Training,https://arxiv.org/pdf/2002.08909.pdf
DL,2020,REALM: Retrieval-Augmented Language Model Pre-Training,https://arxiv.org/pdf/2002.08909.pdf
,,,
arXiv,2018,Recurrent Relational Networks (RRN),https://arxiv.org/pdf/1711.08028.pdf
DL,2018,Recurrent Relational Networks (RRN),https://proceedings.neurips.cc/paper_files/paper/2018/file/b9f94c77652c9a76fc8a442748cd54bd-Paper.pdf
,,,
arXiv,2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://arxiv.org/pdf/2403.00071.pdf
DL,2024,Resonance RoPE: Improving Context Length Generalization of Large Language Models,https://aclanthology.org/2024.findings-acl.32.pdf
,,,
arXiv,2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://arxiv.org/pdf/2006.15595.pdf
DL,2020,Rethinking Positional Encoding in Language Pre-training (TUPE),https://openreview.net/pdf?id=09-528y2Fgf
,,,
arXiv,2020,Scaling Laws for Neural Language Models,https://arxiv.org/pdf/2001.08361.pdf
DL,2020,Scaling Laws for Neural Language Models,/home/jake/Developer/timeline/BIBLIOTHEQUE/26_Scaling_Laws_for_Neural_Language_Models.pdf
,,,
arXiv,2024,Searching Latent Program Spaces,https://arxiv.org/pdf/2411.08706.pdf
DL,2024,Searching Latent Program Spaces,https://raw.githubusercontent.com/clement-bonnet/lpn/7f86b1d11ea37ba173700dbac8604393eac6da37/paper.pdf
,,,
arXiv,2018,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,https://arxiv.org/pdf/1810.00825.pdf
DL,2018,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,https://arxiv.org/pdf/1810.00825.pdf
,,,
arXiv,2025,SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection,https://arxiv.org/pdf/2503.08092.pdf
DL,2025,SparseVoxFormer: Sparse Voxel-based Transformer for Multi-modal 3D Object Detection,https://arxiv.org/pdf/2503.08092.pdf
,,,
arXiv,2021,Stabilizing Equilibrium Models by Jacobian Regularization,https://arxiv.org/pdf/2106.14342.pdf
DL,2021,Stabilizing Equilibrium Models by Jacobian Regularization,https://arxiv.org/pdf/2106.14342.pdf
,,,
arXiv,2025,Streaming 4D Panoptic Segmentation via Dual Threads,https://arxiv.org/pdf/2510.17664.pdf
DL,2025,Streaming 4D Panoptic Segmentation via Dual Threads,https://arxiv.org/pdf/2510.17664.pdf
,,,
arXiv,2025,The Art of Scaling Test-Time Compute for LLMs,https://arxiv.org/pdf/2512.02008.pdf
DL,2025,The Art of Scaling Test-Time Compute for LLMs,https://arxiv.org/pdf/2512.02008.pdf
,,,
arXiv,2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://arxiv.org/pdf/2411.07279.pdf
DL,2024,The Surprising Effectiveness of Test-Time Training for Few-Shot Learning,https://arxiv.org/pdf/2411.07279.pdf
,,,
arXiv,2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://arxiv.org/pdf/2411.17708.pdf
DL,2024,Towards Efficient Neurally-Guided Program Induction for ARC-AGI,https://drive.google.com/uc?export=download&id=1sFlK3mhz8kH2agdE379o0ODQWYkrSD0b
,,,
arXiv,2025,Towards the Next Generation of Symbolic Regression Benchmarks (SRBench),https://arxiv.org/pdf/2505.03977.pdf
DL,2025,Towards the Next Generation of Symbolic Regression Benchmarks (SRBench),https://arxiv.org/pdf/2505.03977.pdf
,,,
arXiv,2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://arxiv.org/pdf/1901.02860.pdf
DL,2019,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,https://aclanthology.org/P19-1285.pdf
,,,
arXiv,2025,TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution,https://arxiv.org/pdf/2506.18421.pdf
DL,2025,TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Evolution,https://arxiv.org/pdf/2506.18421.pdf
,,,
arXiv,2018,Universal Transformers (UT),https://arxiv.org/pdf/1807.03819.pdf
DL,2018,Universal Transformers (UT),https://arxiv.org/pdf/1807.03819.pdf
,,,
arXiv,2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://arxiv.org/pdf/2511.08747.pdf
DL,2025,Vector Symbolic Algebras for the Abstraction and Reasoning Corpus,https://raw.githubusercontent.com/ijoffe/ARC-VSA-2025/main/paper/paper.pdf
