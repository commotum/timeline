                                                    Fixed Point Diffusion Models
                                                                                                          †
                                              Xingjian Bai *                    LukeMelas-Kyriazi *
                                          University of Oxford                   University of Oxford
                                        xingjianbai@gmail.com                 lukemk@robots.ox.ac.uk
                     Diffusion Transformer (DiT): 674M Parameters             Fixed Point Diffusion Model (FPDM): 85M Parameters
             Figure 1. Fixed Point Diffusion Model (FPDM) is a novel and highly efficient approach to image generation with diffusion models. FPDM integrates an
             implicit fixed point layer into a denoising diffusion model, converting the sampling process into a sequence of fixed point equations. Our model significantly
             decreases model size and memory usage while improving performance in settings with limited sampling time or computation. We compare our model, trained
             at a 256 × 256 resolution against the state-of-the-art DiT [37] on four datasets (FFHQ, CelebA-HQ, LSUN-Church, ImageNet) using compute equivalent to
             20DiTsamplingsteps. FPDM(right) demonstrates enhanced image quality with 87% fewer parameters and 60% less memory during training.
                                     Abstract                                ciency. Compared to the state-of-the-art DiT model [37],
                                                                             FPDMcontains87%fewerparameters,consumes60%less
                Weintroduce the Fixed Point Diffusion Model (FPDM),          memory during training, and improves image generation
             a novel approach to image generation that integrates the        quality in situations where sampling computation or time is
             conceptoffixedpointsolvingintotheframeworkofdiffusion-          limited. Our code and pretrained models are available at
             based generative modeling. Our approach embeds an im-           https://lukemelas.github.io/fixed-point-diffusion-models/.
             plicit fixed point solving layer into the denoising network
        arXiv:2401.08741v1  [cs.CV]  16 Jan 2024of a diffusion model, transforming the diffusion process
             into a sequence of closely-related fixed point problems.        1. Introduction
             Combined with a new stochastic training method, this ap-
             proach significantly reduces model size, reduces memory         The field of image generation has experienced significant
             usage, and accelerates training. Moreover, it enables the       recent advancements driven by the development of large-
             development of two new techniques to improve sampling           scale diffusion models [22, 36, 37, 40, 46, 47]. Key to these
             efficiency: reallocating computation across timesteps and       advancements have been increased model size, computa-
             reusing fixed point solutions between timesteps. We con-        tional power, and the collection of extensive datasets [4, 11,
             duct extensive experiments with state-of-the-art models on      15, 24, 44, 45, 53], collectively contributing to a marked
             ImageNet, FFHQ, CelebA-HQ, and LSUN-Church, demon-              improvement in generation performance.
             strating substantial improvements in performance and effi-         Despite these strides, the core principles of diffusion net-
                                                                             works have remained largely unchanged since their develop-
                *Equal Contribution.                                         ment [22]. They typically consist of a fixed series of neural
                †Corresponding author.                                       networklayers, either with a UNet architecture [41] or, more
                                                                         1
