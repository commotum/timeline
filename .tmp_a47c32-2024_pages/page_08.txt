                                                                                 With Reuse
                                                                                 Without Reuse
                  150
                  100
                FID50    1    2     3    5    6    8    12   18    26   33   38   54    68
                                      Number of Fixed Point Iterations per Timestep
                 (a) Performance improvement from reusing solutions across timesteps.                  Figure 7. Performance of Different Number of Pre/Post Layers. We
                                                                                                       find that using at least one pre/post layer is always better than none; fewer
                                                                                                       explicit layers perform better on small compute budgets, whereas more
                                                                                                       explicit layers can better leverage large budgets.
                                                                                                       servation aligns with our intuition: Adjacent timesteps with
                                                                                                       less noise tend to have highly similar corresponding fixed
                                                                                                       point systems, where reusing is more effective.
                                                                                                       5.4. Pre/Post Layers
                                                                                                       One of the many ways FPDM differs from prior work on
              (b) Convergence of f.p. iteration for nine timesteps, with and without reuse.            DEQsisthatweincludeexplicit layers before and after the
                                                                                                       implicit fixed point layer. In Fig. 7, we conduct an ablation
                                                                                                       analysis, training networks with 0, 1, 2, and 4 pre/post layers.
                                                                                                       Weseethatusingatleast 1 explicit pre/post layer is always
                                                                                                       better than 0. For small compute budgets it is optimal to use 1
                                                                                                       pre/post layer, and for larger budgets it is optimal to use 2 or
                                                                                                       4. Broadly, we observe that using more explicit layers limits
                                                                                                       flexibility thereby reducing performance at lower compute
                                                                                                       budgets, but improves performance at higher budgets.
                                                                                                       5.5. Training Method
                 (c) Convergence at iteration across timesteps, with and without reuse.                In Tab. 3, we compare versions of Stochastic Jacobian-Free
                                                                                                       Backpropagation with different values of M and N, the
                 Figure 6. The Impact of Reuse on Fixed Point Accuracy. In (a), we                     upper bounds on the number of training iterations with and
                 examine the performance of sampling with and without reusing solutions                without gradient. M and N reflect a training-time tradeoff
                 for different numbers of iterations per timestep; it considerably helps when          between speed and fixed point convergence. As M and
                 using a small number of iterations per timestep. In (b) and (c), we examine
                 the convergence of individual timesteps. We see that reuse delivers particu-          N increase, each training step contains more transformer
                 larly large benefits for smaller (less-noisy) timesteps. Note that these plots        block forward and backward passes on average; the fixed
                 contain 10 lines as they are plotted for 10 random batches of 32 images               point approximations become more accurate, but each step
                 from ImageNet.                                                                        consumes more time and memory. We find the optimal
                 In Fig. 6a, we see that reusing makes a big difference when                           values of M and N are quite low: 3 or 6. Using too many
                 performing a small number of iterations per timestep and a                            iterations (e.g. 24) is detrimental as it slow down training.
                 negligible difference when performing many iterations per                                 In Tab. 4, we compare to JFB (also called 1-step gradient),
                 timestep. Intuitively, reusing solutions reduces the number                           which has been used in prior work on DEQs [14], and a
                 of iterations needed at each timestep, so it improves the per-                        multi-step variant of it. We find that training with multiple
                 formance when the number of iterations is severely limited.                           steps is essential to obtaining good results, and that using a
                 Fig. 6b and Fig. 6c illustrate the functionality of reusing by                        stochastic number of steps delivers further improvements.
                 examiningattheindividualtimesteplevel. For each timestep                              5.6. Limitations
                 t, we use the difference between the last two fixed point
                 iterations, δ , as an indicator for convergence. Reusing de-                          Theprimarylimitationofourmodelisthatitperformsworse
                                 t
                 creases δt for all timesteps except a few noisiest steps, and                         than the fully-explicit DiT model when sampling computa-
                 reusing is most effective at less noisy timesteps. This ob-                           tion and time are not constrained. The performance gains
                                                                                                  8
