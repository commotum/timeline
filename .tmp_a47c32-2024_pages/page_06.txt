                    Blocks    Model         FID          FID       Params.      Training              Dataset            Model       FID      Dataset       Model      FID
                                          (DDPM)       (DDIM)                   Memory                CelebA-HQ          DiT         65.2     FFHQ          DiT        58.1
                    140       DiT          148.0        110.0       674M       25.2 GB                                   FPDM 11.1                          FPDM 18.2
                              FPDM          85.8        33.9        85M        10.2 GB                LSUN-Church        DiT         65.6     ImageNet      DiT        80.9
                    280       DiT           80.9        35.2        674M       25.2 GB                                   FPDM 22.7                          FPDM 43.3
                              FPDM          43.3        22.4        85M        10.2 GB
                              DiT           37.9        16.5        674M       25.2 GB             Table 2. Quantitative Results Across Four Datasets. FPDM consistently
                    560       FPDM          26.1        19.6        85M        10.2 GB             outperforms DiT [37] on CelebA-HQ, FFHQ, LSUN-Church, and Imagenet
                                                                                                   with 280 transformer block forward passes. All models are trained and
                                                                                                   evaluated at resolution 256px using the same amount of compute and
                Table 1. Quantitative Results on ImageNet. Despite having 87% fewer                identical hyperparameters.
                 parameters and using 60% less memory during training, FPDM outperforms
                 DiT [37] at 140 and 280 transformer block forward passes and achieves
                 comparable performance at 560 passes.
                 0.0001 and β          = 0.02, modified to have zero terminal
                                  end
                 SNR[32]. We use v-prediction as also recommended by
                 [32]. Following DiT [37], we learn the variance σ along
                with the velocity v.
                    Finally, with regard to the evaluations, all evaluations
                wereperformedusing50000images(FID-50K)exceptthose
                 in Tab. 5 and Fig. 6, which were computed using 1000 im-
                 ages due to computational constraints.
                                                                                                   Figure 4. Timestep smoothing significantly improves performance.
                 4.2. Sampling Quality and Cost Evaluation                                         Given the same amount of sampling compute (280 transformer blocks),
                                                                                                   FPDMenablesustoallocatecomputation among more or fewer diffusion
                To measure image quality, we employ the widely-used                                timesteps, creating a tradeoff between the number of fixed-point solving
                 Frechet Inception Distance (FID) 50K metric [21]. To mea-                         iterations per timestep and the number of timesteps in the diffusion process
                 sure the computational cost of sampling, previous studies                         (See Sec. 3.3). Here we explore the performance of our model on ImageNet
                 on diffusion model sampling have counted the number of                            with fixed point iterations ranging from 1 iteration (across 93 timesteps) to
                                                                                                   68iterations (across 4 timesteps). Each timestep also has 1 pre- and post-
                 function evaluations (NFE) [25, 34, 54]. However, given the                       layer, so sampling with k iterations utilizes k + 2 blocks of compute per
                 implicit nature of our model, a more granular approach is                         timestep. The circle and dashed lines show the performance of the baseline
                 required. In our experiments, both implicit and explicit net-                     DiT-XL/2modelwith28layers,whichinourformulationcorresponds to
                works consist of transformer blocks with identical size and                        smoothing over 26 iterations. Although our model is slightly worse than
                                                                                                   DiTat26iterations, it significantly outperforms DiT when smoothed across
                 structure, so the computational cost of each sampling step                        moretimesteps, demonstrating the effectiveness of timestep smoothing.
                 is directly proportional to the number of transformer block
                 forward passes executed; the total cost of sampling is the                        given the most limited compute. Our method’s improve-
                 product of this amount and the total number of timesteps.3
                Asaresult, we quantify the sampling cost in terms of total                         ments are orthogonal to those gained from using better sam-
                 transformer block forward passes. 4                                               plers; our model effectively lowers the FID score with both
                                                                                                   DDIMandDDPM.At560forwardpasses,ourmethodout-
                 4.3. Results                                                                      performs DiT with DDPM but not DDIM, and for more
                 In Tab. 1, we first present a quantitative comparison of our                      than 560 it is outperformed by DiT. Note that the number of
                 proposed FPDM against the baseline DiT, under different                           parameters in FPDMis only 12.9% of that in DiT, and it con-
                 amounts of sampling compute. Notably, given 140 and 280                           sumes60%lessmemoryduringtraining(reducing memory
                 transformer block forward passes, our best model signifi-                         from 25.2 GB to only 10.2 GB at a batch size of 64).
                 cantly outperforms DiT, with the widest performance gap                               Tab. 2 extends the comparison between FPDM and DiT
                                                                                                   to three additional image datasets: FFHQ, CelebA-HQ, and
                    3To be predise the implicit layer includes an extra projection for input       LSUN-Church. Our findings are consistent across these
                 injection, but this difference is negligible.                                     datasets, with FPDM markedly improving the FID score
                    4For example, sampling from a FPDM with one pre/post-layer and
                 26 fixed point iterations across S timesteps requires the same amount of          despite being nearly one-tenth the size of DiT.
                 compute/time as a FPDM with two pre/post layers and 10 iterations using               Fig. 1 shows qualitative results of our model compared to
                2S timesteps; this computation cost is also the same as that of a traditional      DiT. All images are computed using the same random seeds
                 DiT with 28 layers across S timesteps. Formally, the sampling cost of             andclasses using a classifier-free guidance scale 4.0 and 560
                 FPDMis calculated by (n        +n +n )·Swheren andn
                                             pre    iters    post              pre       post      transformer block forward passes (20 timesteps for DiT).
                 are the number of pre- and post-layers, n       the number of fixed point
                                                             iters
                 iterations, and S the number of sampling steps.                                   FPDMuses8fixedpointiterations per block with timestep
                                                                                               6
