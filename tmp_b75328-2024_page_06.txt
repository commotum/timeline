                           randomizes the starting offset used for each individual addition example can be increased to enable
                           generalization by training a larger range of embeddings for a given computational budget. Relatedly,
                           Appendix Figure 9 shows that training on larger datasets improves performance, even for operands
                           with fewer than 100 digits.
                           3.2  Recurrence In Transformers Boosts Performance
                           With positional embeddings addressed, next we explore whether recurrent architectures can further
                           improve the ability of transformers to perform multi-digit addition. We use the term recurrent block
                           to refer to a set of decoder layers with distinct weights and recurrences to refer to the number of
                           times the recurrent block is repeated. We use the term effective depth to mean the number of layers
                           used in a transformer, whether their weights are unique or not. Unless otherwise stated, we use a
                           maximally recurrent architecture, i.e. only one unique layer recurred to achieve the effective depth.
                           Wealsoemployinputinjection, skip-connections that propagate a copy of the input to each layer in
                           the network.
                           TheBenefits of Recurrence.     In Figure 3 (right), we compare all architecture variants using both
                           FIREandNoPEembeddingstrainedonadditionoveroperandswithupto40digits. Despite having
                           approximately 10× fewer parameters than the other models, we see that the looped transformer
                           (recurrent, with input injection and progressive loss), achieves the best out of distribution performance
                           using either position embedding. In Figure 9 in the Appendix, we show this result is robust across
                           multiple training data sizes.
                           With recurrent models, we can choose to vary the number of recurrences for each forward pass while
                           training. This tends to improve generalization to harder tasks at test time and is also refered to as
                           progressive loss computation [Bansal et al., 2022]. This loss function is a convex combination of the
                           loss values from two forward passes, one with the nominal number of recurrences (so 16 for a 1 × 16
                           model) and one with a random smaller number of recurrences.
                           Next, we explore the effect of varying the size of the recurrent block while keeping the effective depth
                           fixed. We perform this ablation by halving the number of layers in the recurrent block and doubling
                           the number of recurrences, sweeping from a model with sixteen layers in the block and a single
                           recurrence (16 × 1, i.e. a standard transformer), through to one layer in the block but with sixteen
                           recurrences (1×16). Analyzing these results in Figure 4, we show further performance improvements
                           are possible in some cases with the combination of both recurrence and Abacus Embeddings. In
                           particular, a model with two recurrences (8 × 2) incurs half the error of the purely non-recurrent
                           model (16×1)forOODproblemsandenjoysincreasedaccuracyon100+OODproblems.
                           Finally, in Appendix A.7.3, we vary the effective depth of the models to analyze the impact of
                           parameter count on this task, across Abacus, FIRE and NoPE embeddings. Although the experiments
                           presented in Figure 4 are a fair comparison across depth, the purely standard transformer models
                           have many more parameters than their recurrent counterparts. In Table 3 in the appendix, we record
                           the parameter counts to the nearest million.
                           4   Pushing the Limits of Algorithmic Reasoning for Transformers
                           While there is an emphasis on addition as a difficult problem in existing work, our method’s strong
                           performance allows us to extend to even more difficult problems, including multiplication and sorting
                           and even multiple operations at once.
                           4.1  Addition and Subtraction
                           Wetrain models on a dataset made up of an even mix of addition and subtraction samples. In Figure
                           5, we show results from models with 8 layers in the recurrent block and 2 recurrences trained with
                           exactly the same hyperparameters used to train the addition models above. We see that these small
                           transformer models can simultaneously learn to extrapolate for both the symmetric operation of
                           addition and the anti-symmetric operation of subtraction using Abacus Embeddings.
                                                                          6
