                                  LanguageModelsareFew-ShotLearners
                                       ∗                       ∗                    ∗                        ∗
                         TomB.Brown             BenjaminMann             NickRyder            Melanie Subbiah
                                 †
                    Jared Kaplan      Prafulla Dhariwal    ArvindNeelakantan       PranavShyam        Girish Sastry
                   AmandaAskell      Sandhini Agarwal     Ariel Herbert-Voss    Gretchen Krueger     TomHenighan
                     RewonChild        Aditya Ramesh       Daniel M. Ziegler      Jeffrey Wu      ClemensWinter
                      Christopher Hesse       MarkChen          Eric Sigler      MateuszLitwin        Scott Gray
                              BenjaminChess                 Jack Clark                Christopher Berner
                         SamMcCandlish             Alec Radford          Ilya Sutskever         Dario Amodei
                                                               OpenAI
                                                              Abstract
                        Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training
                        on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic
                        in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of
                        thousands of examples. By contrast, humans can generally perform a new language task from only
                        a few examples or from simple instructions – something which current NLP systems still largely
                        struggle to do. Here we show that scaling up language models greatly improves task-agnostic,
                        few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁne-
                        tuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion
                        parameters, 10x more than any previous non-sparse language model, and test its performance in
                        the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning,
       arXiv:2005.14165v4  [cs.CL]  22 Jul 2020with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3
                        achieves strong performance on many NLP datasets, including translation, question-answering, and
                        cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as
                        unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same
                        time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some
                        datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,
                        weﬁndthat GPT-3 can generate samples of news articles which human evaluators have difﬁculty
                        distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding
                        and of GPT-3 in general.
                   ∗Equal contribution
                   †Johns Hopkins University, OpenAI
                Author contributions listed at end of paper.
