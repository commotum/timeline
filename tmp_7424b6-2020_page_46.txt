                      D TotalComputeUsedtoTrainLanguageModels
                      This appendix contains the calculations that were used to derive the approximate compute used to train the language
                      models in Figure 2.2. As a simplifying assumption, we ignore the attention operation, as it typically uses less than 10%
                      of the total compute for the models we are analyzing.
                      Calculations can be seen in Table D.1 and are explained within the table caption.
                                                                                                                                    Fwd-pass           Frac of
                                        Total train    Total train                                      Flops                       ﬂops per       params active
                                         compute        compute       Params      Training tokens     per param      Mult for     active param        for each
                   Model                (PF-days)        (ﬂops)         (M)          (billions)       per token     bwdpass         per token          token
                   T5-Small             2.08E+00       1.80E+20         60             1,000               3             3              1                0.5
                   T5-Base              7.64E+00       6.60E+20         220            1,000               3             3              1                0.5
                   T5-Large             2.67E+01       2.31E+21         770            1,000               3             3              1                0.5
                   T5-3B                1.04E+02       9.00E+21        3,000           1,000               3             3              1                0.5
                   T5-11B               3.82E+02       3.30E+22       11,000           1,000               3             3              1                0.5
                   BERT-Base            1.89E+00       1.64E+20         109             250                6             3              2                1.0
                   BERT-Large           6.16E+00       5.33E+20         355             250                6             3              2                1.0
                   RoBERTa-Base         1.74E+01       1.50E+21         125            2,000               6             3              2                1.0
                   RoBERTa-Large        4.93E+01       4.26E+21         355            2,000               6             3              2                1.0
                   GPT-3Small           2.60E+00       2.25E+20         125             300                6             3              2                1.0
                   GPT-3Medium          7.42E+00       6.41E+20         356             300                6             3              2                1.0
                   GPT-3Large           1.58E+01       1.37E+21         760             300                6             3              2                1.0
                   GPT-3XL              2.75E+01       2.38E+21        1,320            300                6             3              2                1.0
                   GPT-32.7B            5.52E+01       4.77E+21        2,650            300                6             3              2                1.0
                   GPT-36.7B            1.39E+02       1.20E+22        6,660            300                6             3              2                1.0
                   GPT-313B             2.68E+02       2.31E+22       12,850            300                6             3              2                1.0
                   GPT-3175B            3.64E+03       3.14E+23      174,600            300                6             3              2                1.0
                      Table D.1: Starting from the right hand side and moving left, we begin with the number of training tokens that each
                      modelwastrained with. Next we note that since T5 uses an encoder-decoder model, only half of the parameters are
                      active for each token during a forward or backwards pass. We then note that each token is involved in a single addition
                      and a single multiply for each active parameter in the forward pass (ignoring attention). Then we add a multiplier of
                      3xtoaccount for the backwards pass (as computing both ∂params and ∂acts use a similar amount of compute as the
                                                                                              ∂loss         ∂loss
                      forwards pass. Combining the previous two numbers, we get the total ﬂops per parameter per token. We multiply this
                      value by the total training tokens and the total parameters to yield the number of total ﬂops used during training. We
                      report both ﬂops and petaﬂop/s-day (each of which are 8.64e+19 ﬂops).
                      E HumanQualityAssessmentofSyntheticNewsArticles
                      This appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic
                      news articles from real news articles. We ﬁrst describe the experiments on the ∼ 200 word news articles, and then
                      describe the preliminary investigation of ∼ 500 word news articles generated by GPT-3.
                      Participants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for
                      failing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean
                      participant age was ∼ 38 years old. All participants were recruited through Positly, which maintains a whitelist of
                      high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic
                      restrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined
                      by pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were
                      not allowed to take part in an experiment more than once.
                      Procedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used
                      the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B
                      (GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a
                      word count closest to that of the human written article was selected automatically. This was to minimize the effect
                      that completion length might have on participants’ judgments. The same output procedure for each model with the
                      exception of the removal of the intentionally bad control model, as described in the main text.
                                                                                           46
