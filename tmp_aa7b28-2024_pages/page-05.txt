                    Panoptic prediction is obtained from instance and se-                      superpoint clusterization [18] and use a voxel size of 2cm.
                mantic outputs. It is initialized with estimated semantics,                    OnS3DIS,voxelsizeisset to 5cm due to larger scenes.
                then, instance predictions are overlaid consequently, sorted
                byarankingscore in an increasing order.                                        4.2. Comparison to Prior Work
                4. Experiments                                                                 Wecompare our OneFormer3D with previous art on three
                                                                                               indoor benchmarks: ScanNet [8], S3DIS [1], and Scan-
                4.1. Experimental Settings                                                     Net200 [28] in Tab. 2, 3, and 4, respectively. On the Scan-
                                                                                               Netvalidationsplit, wesetanewstate-of-theartininstance,
                Datasets.      TheexperimentsareconductedonScanNet[8],                         semantic, and panoptic segmentation tasks with a unified
                ScanNet200[28],andS3DIS[1]datasets. ScanNet[8]con-                             approach. Specifically, the instance segmentation scores in-
                tains 1613 scans divided into training, validation, and test-                  crease by +2.9 mAP , +4.4 mAP , and +4.1 mAP com-
                                                                                                                         25               25
                ing splits of 1201, 312, and 100 scans, respectively. 3D                       pared to SPFormer [32] and a more recent Mask3D [31],
                instance segmentation is typically evaluated using 18 object                   which is a notable improvement. Besides, OneFormer3D
                categories. Two more categories (wall and floor) are added                     scores top-1 in the ScanNet hidden test leaderboard at 17
                for semantic and panoptic evaluation. We report results on                     Nov. 2023 with 80.1 mAP50 (+2.1 w.r.t. Mask3D), and in-
                both validation and hidden test splits. ScanNet200 [28] ex-                    credible 89.6 mAP          (+2.1 w.r.t. TD3D [15]). At the same
                                                                                                                       25
                tends the original ScanNet semantic annotation with fine-                      time, OneFormer3D supersedes PointTransformerV2[40],
                grained categories with the long-tail distribution, resulting                  a state-of-the-art semantic segmentation method, by +1.2
                in 198 instance with 2 more semantic classes. The train-                       mIoU. Panoptic segmentation has not been investigated
                ing, validation, and testing splits are similar to the origi-                  so extensively as the other two segmentation tasks, so
                nal ScanNet dataset. The S3DIS dataset [1] features 272                        this track is represented with few baselines demonstrating
                scenes within 6 large areas. Following the standard evalua-                    mediocreperformance. Respectively,theimprovementhere
                tion protocol, we assess the segmentation quality on scans                     is especially tangible: OneFormer3D outperforms TUPPer-
                from Area-5, and via 6 cross-fold validation, using 13 se-                     Mapby+21.0PQ,hittingashighas71.2.
                mantic categories in both settings. Following the official
                [1] split, we classify these 13 categories as either structured                                                                 Box        Box
                or furniture, and define 5 furniture categories (table, chair,                        Method                 Presented at     mAP         mAP
                sofa, bookcase, andboard)asthing,andtheremainingeight                                                                               25         50
                categories as stuff for panoptic evaluation.                                          VoteNet [26]           ICCV’19            58.6       33.5
                                                                                                      PBNet[49]              ICCV’23            69.3       60.1
                                                                                                      FCAF3D[29]             ECCV’22            71.5       57.3
                Metrics.      Weuse mIoU to measure the quality of 3D se-                             SoftGroup [35]         CVPR’22            71.6       59.4
                mantic segmentation. For 3D instance segmentation, we                                 TR3D[30]               ICIP’23            72.9       59.3
                report a mean average precision (mAP), which is an aver-                              CAGroup3D[36]          NeurIPS’22         75.1       61.3
                age of scores obtained with IoU thresholds set from 50%                               OneFormer3D                               76.9       65.3
                to 95%, with a step size of 5%. mAP               and mAP denote
                                                              50             25                Table 1. Comparison of existing 3D object detection methods on
                the scores with IoU thresholds of 50% and 25%, respec-                         the ScanNet validation split.
                tively. Additionally, we calculate mean precision (mPrec),
                and mean recall (mRec) for S3DIS, following the standard                           Besides, we adopt OneFormer3D to 3D object detection
                evaluation protocol established in this benchmark. The ac-                     by enclosing predicted 3D instances with tight axis-aligned
                curacyofpanopticpredictionsisassessedwiththePQscore                            3Dbounding boxes. The comparison with existing 3D ob-
                [14]; we also report PQ         and PQ , estimated for thing and
                                             th          st                                    ject detection methods in presented in Tab. 1. As can be
                stuff categories, respectively.                                                seen, OneFormer3D achieves +4.0 mAP                    w.r.t.  a strong
                                                                                                                                                  50
                                                                                               CAGroup3D[36] baseline, setting a new state-of-the-art in
                Implementation Details.            Our OneFormer3D is imple-                   3Dobjectdetectionwith65.1mAP50 withnoextratraining.
                mented in MMDetection3D framework [7]. All training                                On S3DIS dataset, our unified approach demonstrates
                details are inherited from SPFormer [32], including using                      state-of-the-art results on all segmentation tasks, in both
                AdamW optimizer with an initial learning rate of 0.0001,                       Area-5 and 6-fold cross-validation benchmarks. Here, the
                weight decay of 0.05, batch size of 4, and polynomial                          most significant gain is achieved in instance segmentation
                scheduler with a base of 0.9 for 512 epochs. We apply the                      on6-fold cross-validation, with +1.5 mAP50 and +1.2 mAP
                standard augmentations: horizontal flipping, random rota-                      w.r.t. Mask3D. In both benchmarks, we outperform state-
                tions around the z-axis, elastic distortion, and random scal-                  of-the-art TD3D and Mask3D in terms of mPrec                         and
                                                                                                                                                                50
                ing. On ScanNet and ScanNet200, we apply graph-based                           mRec . Despite we find these metrics less representative
                                                                                                       50
                                                                                           20947
