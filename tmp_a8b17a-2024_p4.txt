                           the alterations in hidden state values across various positions and layers. Results show a significant
                           shift in the hidden state’s value range upon surpassing the effective window. Interestingly, when
                           employingextrapolationtechniques, hiddenstatevaluesexhibitnoticeablesuppression. Thisindicates
                           that the effective range of hidden state values likely lies within a specific threshold. When the position
                           exceeds the effective window length, the hidden state values surpass this threshold, resulting in
                           extrapolation failures.
                           Previous work Kazemnejad et al. (2023) utilizes a constructive approach. By constructing the
                           Transformer’s weights, it enables the first and second layers to independently generate position
                           information. Drawing inspiration from this, we endeavor to construct a Transformer model capable
                           of mirroring this observation.
                           3.3  Theoretical Analysis
                           In a multi-layer neural network, each layer’s outputs, a.k.a hidden state values o, become the inputs
                           for the subsequent layer. To maintain stable network behavior, these values must remain within a
                           reasonable range. We define this observable boundary as the threshold H. This threshold can
                           be either an upper bound or a lower bound. For our analysis, we focus on the lower bound of this
                           threshold. A successful extrapolation occurs when a large model consistently generates accurate
                           next tokens for a long input sequence. Conversely, a failed extrapolation happens when the model
                           produces incorrect or nonsensical next tokens. Based on these definitions, we make the following
                           assumptions:
                              Assumptions. In LLM, there is a lower bound as threshold H for the hidden state value o in
                              specific dimension and specific layer. Let M be the max window length for LLM. Predefine
                              query W ,keyW ,valueW andoutputW matrices,andfeed-forwardsub-layerW ,
                                       Q        K           V               O                                         1
                              W2matrices. Wheno > H,LLMextrapolatessuccessfully. Once o < H, LLM extrapolation
                              fails.
                           These assumptions indicate that by observing whether the hidden state value o in this dimension
                           exceed the threshold H, we can predict whether the large model’s extrapolation has failed. Building
                           uponthese assumptions, theoretical results for NoPE exceeding the effective window length are as
                           follows:
                              Theorem3.1(NoPEExtrapolation). Let x = [< bos >,x1,...,xT] be an input sequence of
                              length T +1 to the model. Then, there exists W , W , W , W , W , and W matrices,
                                                                             Q     K    V     O    1         2
                              such that when T < M, o    >H;andwhenT >M,o <H.
                                                       T                           T
                           Full proof is given in Appendix E.1. This theorem reveals the internal mechanism of NoPE extrapola-
                           tion as the input length changes. The theoretical results for PE are as follows:
                              Theorem3.2 (PE Extrapolation). Let x = [< bos >,x ,...,x ] be an input sequence of
                                                                                     1       T
                              length T+1 to the model. Consider a simple relative PE schema where dot product between
                              queryqt andkeyki atpositionstandi(t ≥ i)canbeexpressedas: ⟨qt,ki⟩ := qTki−(t−i).
                                                                                                           t
                              Then, there exists W , W , W , W , W , and W matrices, such that when T < M,
                                                   Q    K     V     O     1        2
                              o >H;andwhenT >M,o <H.
                               T                           T
                           Full proof is given in Appendix E.2. Theorems 3.1 and 3.2 state the failure of length extrapolation in
                           NoPEandPE,respectively.
                           Building on Theorem 3.2, we further investigate the case for carefully orchestrated weave PE. The
                           theoretical result is as follows:
                              Theorem3.3(WeavePEExtrapolation). Let N be a positive constant. Consider a simple
                              weavePEextrapolation schema: when t−i < N, W(t−i) = t−i; and when t−i ≥ N,
                              W(t−i)=N.Then,theattentiondotproductisfixedasbelow:
                                                                 qTk −(t−i) , t−i<N
                                                     ⟨q ,k ⟩ :=     t  i
                                                       t  i            qTk −N , t−i≥N
                                                                        t  i
                               , where N ≪ M. Then,applyingWQ,WK,WV,WO,W1,andW2matricesfromTheorem
                              3.2, we have when T > M, oT > H.
                                                                          4
