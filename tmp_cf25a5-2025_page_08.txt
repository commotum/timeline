                                                                               Test-Time Learning for Large Language Models
                              0.30                                                                                    Table 5. ExperimentalresultsofourproposedmethodintheOnline
                                                                                                                      setting. Geo., Agri., Med., and Fin. represent the Geography,
                              0.25                                                           Geography                Agriculture, Medicine, and Finance, respectively. LLM refers to
                                                                                             Agriculture              Llama3-8B-Instruct. NF4 is 4-bit NormalFloat.
                              0.20                                                           Medicine                                                      DomainBench
                             ROUGE-LSUM                                                      Finance                    Method                  Geo.        Agri.      Med.         Fin.     #Backwards
                              0.15
                                                                                                                        LLM                    0.2450 0.0834 0.1265 0.2329                           –
                              0.10                                                                                      Tent (Online)          0.0804 0.0112 0.0142 0.0489                        5000
                                     e2              e3              e4              e5               e6                EATA(Online) 0.1008 0.0186 0.0202 0.0815                               4943(-1.1%)
                                                                                                                        Ours (Online)          0.2787 0.1579 0.1340 0.2455 1514(-69.7%)
                      Figure 3. Effects of different perplexity margins P in Eqn. 6.                                    LLM(NF4)               0.2439 0.0859 0.1237 0.2325                           –
                                                                                           0                            Ours (NF4)             0.3069 0.1533 0.2306 0.3193                    4783(-4.3%)
                      GPT4 dataset, our proposed TLM improves the perfor-
                      mance of Llama3.2-8B-Instruct by 13.91% (0.3752 →                                               showninTable4,byprioritizing the most informative and
                      0.4274), showing a relative improvement of about 113.60%                                        relevant test samples for backpropagation, this strategy not
                      (0.2001 → 0.4274) compared to Tent, demonstrating its                                           only further enhances LLM performance but also reduces
                      effective adaptation to general instruction-following tasks.                                    unnecessary computational overhead. Specifically, using
                      Superior performance on logical reasoning task. As                                              the Sample Efficient Learning Strategy results in a relative
                      showninTable3,ourproposedTLM significantly outper-                                              performance improvement of approximately 2.0% on the
                      forms the original LLMs and Tent on all reasoning datasets                                      target test data, while reducing the training data by about
                      in ReasoningBench. For instance, on the GSM8K dataset,                                          5.0%. For instance, the relative performance improvement
                      our proposed TLM improves the performance of Llama3-                                            onthe Agriculture dataset is 5.1%.
                      8B-Instruct by 6.10%, highlighting its ability to enhance                                       Effects of the P in Eqn. 6. The threshold P in Eqn. 6
                      logical reasoning under complex arithmetic and problem-                                                                   0                                             0
                      solving tasks. These results confirm that our method not                                        plays a crucial role in controlling the threshold for sample
                      only adapts effectively to distributional shifts but also en-                                   selection during Test-Time Learning. To explore the optimal
                                                                                                                      threshold for P , we conduct experiments with values of
                      hances the reasoning capabilities of LLMs during test time.                                                             0
                                                                                                                                         2    3    4    5    6
                                                                                                                      P set to {e ,e ,e ,e ,e }. As shown in Figure 3, when
                                                                                                                         0
                                                                                                                      P =e3,ourmethodachievesthebestperformanceonthree
                      5.3. Ablation Studies                                                                              0
                                                                                                                      datasets, namely Geography, Medicine, and Finance, while
                      Effectiveness of Input Perplexity Minimization. To eval-                                        also showing near-optimal performance on the Agriculture
                                                                                                                                                                               3
                                                                                                                      dataset. Therefore, we select P = e for all experiments.
                      uate the effectiveness of input perplexity minimization, we                                                                                     0
                                                                                                                      When P is set too high or too low, it negatively affects
                      conductexperimentscomparingtheperformanceofLlama3-                                                           0
                      8B-Instruct and our method without the Sample Efficient                                         performance. Specifically, a value that is too high restricts
                      Learning Strategy (Ours w/o SEL). As shown in Table 4,                                          the number of high-perplexity samples selected, limiting
                      input perplexity minimization significantly improves the                                        the model’s ability to adapt to new and complex data. On
                      performance of LLMs across all datasets compared to the                                         the other hand, a value that is too low includes too many
                      original Llama3-8B-Instruct, demonstratingthatminimizing                                        low-perplexity samples, which do not contribute effectively
                      input perplexity effectively enhances the LLM’s ability to                                      to adaptation and could lead to inefficiencies and overfitting.
                      adapt to target domains. Specifically, by minimizing input
                      perplexity during Test-Time to update the LLM parameters,                                       5.4. More Discussions
                      weachievearelativeperformanceimprovementofover30%                                               Online Test-Time Experiments. To further assess the
                      compared to the original Llama3-8B-Instruct model. No-                                          performance of our TLM, we conduct experiments in the
                      tably, on the Medicine dataset, the improvement reaches                                         online Test-Time Learning setting. The online setting is
                      83.9%. The effectiveness of input perplexity minimization                                       similar to Test-Time Adaptation (Wang et al., 2021; Niu
                      P(x;Θ)lies in its ability to enhance the LLM’s understand-                                      et al., 2022a), where the model processes the input to gener-
                      ing and representation of the input, which helps improve the                                    ate an output while simultaneously updating its parameters.
                      model’s adaptation to the target domain data.                                                   Notably, the model parameters are updated only once ev-
                      Effectiveness of Sample Efficient Learning Strategy in                                          ery 100 test samples. From Table 5, the proposed method
                      Eqn. 5. The Sample Efficient Learning Strategy improves                                         also achieves significant performance improvements over
                      the efficiency of TTL by actively selecting high-perplexity                                     Llama3-8B-Instruct across different domain datasets in the
                      samples that contribute more to the LLM’s adaptation. As                                        online setting. Additionally, our proposed method reduces
                                                                                                                  8
