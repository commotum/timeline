                            Mesa-Extrapolation: A Weave Position Encoding
                               MethodforEnhancedExtrapolationinLLMs
                                                     1          2,3∗          2            1
                                              XinMa ,YangLiu ,JingjingLiu ,XiaoxuMa
                                            1Digital Research Institute, Enn Group, Beijing, China
                                     2Institute for AI Industry Research, Tsinghua University, Beijing, China
                                              3Shanghai Artificial Intelligence Laboratory, China
                             {xin.ma0206, xiaoxuma}@gmail.com,{liuy03, jjliu}@air.tsinghua.edu.cn
                                                                Abstract
                                Large language models (LLMs), although having revolutionized many fields, still
                                suffer from the challenging extrapolation problem, where the inference ability of
                                LLMssharplydeclinesbeyondtheirmaxtraininglengths. Inthiswork, weconduct
                                a theoretical analysis to better understand why No Position Encoding (NoPE) fails
                                outside its effective range, as well as examining the power of Position Encoding
                                (PE) in this context. Our findings reveal that with meticulous weave position, PE
                                can indeed be extended beyond effective range. Our theorems establish that LLMs
                                equipped with weave PE can achieve improved extrapolation performance without
                                additional cost. Furthermore, we introduce a novel weave PE method, Mesa-
                                Extrapolation, which utilizes a chunk-based triangular attention matrix and applies
                                Stair PE to manage the final chunk. This method not only retains competitive
                                performance but also offers substantial benefits such as significantly reduced
                                memorydemandandfasterinference speed. Extensive experiments validate the
                                effectiveness of Mesa-Extrapolation, demonstrating its potential as a scalable
                                solution to enhancing LLMs’ applicative reach. Our code is available at https:
                                //github.com/soacker/Mesa-Extrapolation.
                        1   Introduction
                        Large Language Models (LLMs) with their powerful in-context learning capabilities Brown et al.
                        (2020) offer versatile solutions to a wide-range of intelligent applications. However, one pressing
                        challenge, the extrapolation problem Delétang et al. (2022) Zhang et al. (2022), dictates that the
                        inference ability of LLMs sharply declines beyond their max training lengths, imposing a serious
                        limitation on applications with long inputs. An naive solution is to extend the length of training
                        samples. However, the inherent quadratic complexity of calculations presents practical challenges,
                        demanding more resources, longer training time and higher cost.
                        Positional encoding (PE) has become a pivotal component of Transformer architecture, to compensate
                        for the overlooking of position information by the attention mechanism. In the realm of extrapolation
                        capability, PE is considered as a key factor influencing the extrapolating ability of LLMs. A few
                        PEapproaches, such as the popular RoPE Su et al. (2023) and ALiBi Press et al. (2021), claim to
                        offer improved extrapolation capabilities and have gained widespread usage in industrial applications.
                        Meanwhile, a counter-narrative has emerged. Some works demonstrate that Transformer can achieve
                        better extrapolation capabilities by removing position encoding (NoPE) Kazemnejad et al. (2023),
                        and contend that the mask already plays a significant role in capturing position information.
                           ∗Corresponding author
                        38th Conference on Neural Information Processing Systems (NeurIPS 2024).
