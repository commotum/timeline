                         4   Related work
                         The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learn-
                         ing method in the literature that is applicable to the same general class of continuous latent variable
                         models. Like our method, the wake-sleep algorithm employs a recognition model that approximates
                         the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimiza-
                         tion of two objective functions, which together do not correspond to optimization of (a bound of)
                         the marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete
                         latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.
                         Stochastic variational inference [HBWP13] has recently received increasing interest. Recently,
                                                                                                      ¬®
                         [BJP12] introduced a control variate schemes to reduce the high variance of the naƒ±ve gradient
                         estimator discussed in section 2.1, and applied to exponential family approximations of the poste-
                         rior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing
                         the variance of the original gradient estimator. In [SK13], a similar reparameterization as in this
                         paper was used in an efÔ¨Åcient version of a stochastic variational inference algorithm for learning the
                         natural parameters of exponential-family approximating distributions.
                         The AEVB algorithm exposes a connection between directed probabilistic models (trained with a
                         variational objective) and auto-encoders. A connection between linear auto-encoders and a certain
                         class of generative linear-Gaussian modelshaslongbeenknown. In [Row98]itwasshownthatPCA
                         correspondstothemaximum-likelihood(ML)solutionofaspecialcaseofthelinear-Gaussianmodel
                         with a prior p(z) = N(0,I) and a conditional distribution p(x|z) = N(x;Wz,I), speciÔ¨Åcally the
                         case with inÔ¨Ånitesimally small .
                                                                   +
                         In relevant recent work on autoencoders [VLL 10] it was shown that the training criterion of un-
                         regularized autoencoders corresponds to maximization of a lower bound (see the infomax princi-
                         ple [Lin89]) of the mutual information between input X and latent representation Z. Maximiz-
                         ing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional en-
                         tropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding
                         model [VLL+10], i.e. the negative reconstrution error. However, it is well known that this recon-
                         struction criterion is in itself not sufÔ¨Åcient for learning useful representations [BCV13]. Regular-
                         ization techniques have been proposed to make autoencoders learn useful representations, such as
                         denoising, contractive and sparse autoencoder variants [BCV13]. The SGVB objective contains a
                         regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regu-
                         larization hyperparameter required to learn useful representations. Related are also encoder-decoder
                         architectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew
                         someinspiration. AlsorelevantaretherecentlyintroducedGenerativeStochasticNetworks[BTL13]
                         wherenoisyauto-encoderslearnthetransitionoperatorofaMarkovchainthatsamplesfromthedata
                         distribution. In [SL10] a recognition model was employed for efÔ¨Åcient learning with Deep Boltz-
                         mannMachines. These methods are targeted at either unnormalized models (i.e. undirected models
                         like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm
                         for learning a general class of directed probabilistic models.
                         The recently proposed DARN method [GMW13], also learns a directed probabilistic model using
                         an auto-encoding structure, however their method applies to binary latent variables. Even more
                         recently, [RMW14] also make the connection between auto-encoders, directed proabilistic models
                         and stochastic variational inference using the reparameterization trick we describe in this paper.
                         Their work was developed independently of ours and provides an additional perspective on AEVB.
                         5   Experiments
                         We trained generative models of images from the MNIST and Frey Face datasets3 and compared
                         learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.
                         Thegenerative model (encoder) and variational approximation (decoder) from section 3 were used,
                         where the described encoder and decoder have an equal number of hidden units. Since the Frey
                         Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except
                         that the means were constrained to the interval (0,1) using a sigmoidal activation function at the
                            3Available at http://www.cs.nyu.edu/ roweis/data.html
                                                                 Àú
                                                                    6
