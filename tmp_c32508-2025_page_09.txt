            www.nature.com/scientificdata/                                                                          www.nature.com/scientificdata
                                             Fig. 7  Human action traces on ARC problems. In the lef㘶 column, we show the test input seen by participants, 
                                             and the true test output grid for three dif㘶erent problems from the ARC evaluation set. In the middle column, 
                                             action traces show some successive states of the grid of a selected participant, with the last state corresponding 
                                             to a correct (green box) or incorrect (red box) submission. In the last column, we show the f㘶rst natural-
                                             language descriptions submitted by participants along with their solution. From top to bottom: 34b99a2b.
                                             json, 4364c1c4.json and a8610ef7.json.
                                             rule. In broad strokes, the solution for problem a8610ef7.json (see lef㘶 panel of Fig. 5) requires copying the 
                                             input grid and then coloring all blue cells gray or red. T㔴e condition for deciding which color to paint a blue cell 
                                             depends on evaluating whether it is symmetric along the horizontal axis of the grid: if both (mirrored) corre-
                                             sponding cells in the top and bottom halves of the grid are blue, the cells should be colored red, otherwise the blue 
                                             cell should be colored gray. In light of this solution, the output of many participants shown in Fig. 5 appear like 
                                             they are the result of inferring partially correct programs, where the condition for coloring cells red is approxi-
                                             mated using surface-level statistics.
                                             Natural language descriptions.  In our f㘶nal technical validation, we check the natural-language descrip-
                                             tions that the participants generated when solving ARC problems (see Fig. 7 for examples). To validate the infor-
                                             mativeness of these descriptions, we focused on evaluating the last submitted solution descriptions of correct 
                                             attempts only. Af㘶er f㘶ltering uninformative text (fewer than 3 words), and rare tasks (fewer than 4 successful 
                                             descriptions), we obtained a total of 5940 natural-language descriptions across 691 tasks. We trained a Bernoulli 
                                             Naive Bayes classif㘶er using bag-of-words features to predict which specif㘶c ARC task a participant was solving 
                                             from their textual description alone. Using 5-fold cross-validation for hyperparameter optimization, we per-
                                             formed a grid search over both vocabulary size v ∈ {100, 250, 500, 1000, 2000, 2777} and Laplace smoothing 
                                             parameters α ∈ {0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0}, f㘶nding optimal values of v = 1000 features and α = 0.01. On a 
                                             held-out test set (70/30 split), the classif㘶er achieved 23.7% accuracy—signif㘶cantly above the empirical null dis-
                                             tribution of 0.2% (±0.1%), with statistical signif㘶cance conf㘶rmed by permutation testing (p  < 0.0001, n = 1000 
                                             permutations). T㔴is demonstrates that participants’ verbal explanations contain meaningful task-specif㘶c infor-
                                             mation, with the most frequent terms including color descriptors (blue, red, green), spatial language (squares, 
                                             grid, pattern, shape), and directional concepts (lef㘶, right), conf㘶rming that participants of㘶en used visuospatial 
                                             concepts to articulate their reasoning about abstract transformation rules. Furthermore, qualitative inspection 
                                             reveals that the natural-language descriptions in H-ARC contain words like “f㘶ll”, “extend”, “move”, “slide” or even 
                                             “water” or “f㘶ower”. All these words capture concepts from everyday life that people used to reason about these 
                                             novel problems. At a surface-level, these concepts seem unrelated to a task that requires inferring a hidden trans-
                                             formation rule and applying it to 2D grids of numbers between 0 and 9. Reminiscent of analogical reasoning, the 
                                             use of these concepts is suggestive of people’s ability to come up with useful abstractions on-the-f㘶y that greatly 
                                             restrict their search space when solving ARC problems.
                                             Code availability
                                             T㔴e code for technical validation is available on our accompanying code repo at github.com/le-gris/h-arc. T㔴e 
                                             code is written in Python, using standard packages such as PyMC, NumPy and SciPy.
                                             Received: 23 January 2025; Accepted: 24 July 2025;
                                             Published: 7 August 2025
                                             References
                                               1.  Chollet, F. On the measure of intelligence. arXiv preprint arXiv:1911.01547, https://doi.org/10.48550/arXiv.1911.01547 (2019).
                                               2.  Achiam, J. et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, https://doi.org/10.48550/arXiv.2303.08774 (2023).
                                               3.  Wei, J. et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, https://doi.org/10.48550/arXiv.2206.07682 
                                                 (2022).
            Scientific Data | (2025) 12:1380 | https://doi.org/10.1038/s41597-025-05687-1                                                                     9
