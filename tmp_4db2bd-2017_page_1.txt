                        Published as a conference paper at ICLR 2017
                        DEEPCODER: LEARNING TO WRITE PROGRAMS
                          MatejBalog∗                         Alexander L. Gaunt, Marc Brockschmidt,
                          Department of Engineering           Sebastian Nowozin, Daniel Tarlow
                          University of Cambridge             Microsoft Research
                                                              ABSTRACT
                                Wedevelopaﬁrstline of attack for solving programming competition-style prob-
                                lems from input-output examples using deep learning. The approach is to train a
                                neural network to predict properties of the program that generated the outputs from
                                the inputs. We use the neural network’s predictions to augment search techniques
                                from the programming languages community, including enumerative search and
                                an SMT-based solver. Empirically, we show that our approach leads to an order
                                of magnitude speedup over the strong non-augmented baselines and a Recurrent
                                Neural Network approach, and that we are able to solve problems of difﬁculty
                                comparable to the simplest problems on programming competition websites.
                        1   INTRODUCTION
                        Adream of artiﬁcial intelligence is to build systems that can write computer programs. Recently,
                        there has been much interest in program-like neural network models (Graves et al., 2014; Weston
                        et al., 2015; Kurach et al., 2015; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Sukhbaatar
                        et al., 2015; Neelakantan et al., 2016; Kaiser & Sutskever, 2016; Reed & de Freitas, 2016; Zaremba
                        et al., 2016; Graves et al., 2016), but none of these can write programs; that is, they do not generate
                        human-readable source code. Only very recently, Riedel et al. (2016); Bunel et al. (2016); Gaunt
                        et al. (2016) explored the use of gradient descent to induce source code from input-output examples
                        via differentiable interpreters, and Ling et al. (2016) explored the generation of source code from
                        unstructured text descriptions. However, Gaunt et al. (2016) showed that differentiable interpreter-
                        based program induction is inferior to discrete search-based techniques used by the programming
                        languages community. We are then left with the question of how to make progress on program
                        induction using machine learning techniques.
                        In this work, we propose two main ideas: (1) learn to induce programs; that is, use a corpus of
                        program induction problems to learn strategies that generalize across problems, and (2) integrate
                        neural network architectures with search-based techniques rather than replace them.
                        In more detail, we can contrast our approach to existing work on differentiable interpreters. In dif-
                        ferentiable interpreters, the idea is to deﬁne a differentiable mapping from source code and inputs
                        to outputs. After observing inputs and outputs, gradient descent can be used to search for a pro-
                        gramthat matches the input-output examples. This approach leverages gradient-based optimization,
                        which has proven powerful for training neural networks, but each synthesis problem is still solved
                        independently—solving many synthesis problems does not help to solve the next problem.
                        Weargue that machine learning can provide signiﬁcant value towards solving Inductive Program
                        Synthesis (IPS) by re-casting the problem as a big data problem. We show that training a neural
                        network on a large number of generated IPS problems to predict cues from the problem description
                        can help a search-based technique. In this work, we focus on predicting an order on the program
                        space and show how to use it to guide search-based techniques that are common in the programming
                        languages community. This approach has three desirable properties: ﬁrst, we transform a difﬁcult
                        search problem into a supervised learning problem; second, we soften the effect of failures of the
                        neural network by searching over program space rather than relying on a single prediction; and third,
                        the neural network’s predictions are used to guide existing program synthesis systems, allowing us to
                        use and improve on the best solvers from the programming languages community. Empirically, we
                           ∗                                                      ¨
                            Also afﬁliated with Max-Planck Institute for Intelligent Systems, Tubingen, Germany. Work done while
                        author was an intern at Microsoft Research.
                                                                   1
