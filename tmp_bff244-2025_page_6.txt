                                                                      Distance                       Corner (%)                         Edge(%)
                                        Method                  WED↓          ACO↓          CP↑        CR↑         CF1↑         EP↑         ER↑        EF1↑
                                              ∗
                                    PointMAE [19]                  -           0.330        75.0        47.0        58.0        52.0        12.0        20.0
                                   PointM2AE∗ [20]                 -           0.320        79.0        58.0        67.0        50.0         7.0        12.0
                                    Point2Roof [11]                -           0.390        65.0        30.0        41.0        66.0         8.0        14.0
                              Linear self-supervised [28]          -           0.350        70.0        60.0        65.0        67.0        16.0        25.0
                                    Supervised [28]                -           0.290        90.0        53.0        66.0        88.0        23.0        36.0
                                     PC2WF[14]                     -           0.520        18.0        67.0        28.0         2.0        15.0         1.0
                                      PBWR[9]                    0.271         0.222        98.5        68.8        81.0        94.3        65.4        77.2
                                  BWFormer(Ours)                 0.238         0.204        94.9        82.7        88.4        85.5        74.1        79.4
                  Table 1. Quantitative evaluation results on the Building3D dataset [28]. Results of previous works are taken from [9] and the Build-
                  ing3D leaderboard, where X∗ indicates the method X serves as the feature extractor in the baseline network of Building3D [28]. The best
                  results are in bold font, and the same notation applies to the subsequent tables.
                  4.2. Implementation Details                                                    City3D [8, 18], PC2WF [14], Point2Roof [11], and
                  Input Pre-processing. Given a point cloud with the z-axis                      PBWR [9]. The first two comparative methods are tradi-
                  aligned along the gravity direction, we first normalize it to                  tional ones which output mesh models, and the last three
                  the range [-1.0, 1.0]. We then project it onto the xy-plane                    are proprietary deep learning-based wireframe reconstruc-
                  andcomputea256×256heightmap,whereeachpixelvalue                                tion methods that directly output wireframes.
                  represents the average z-value of points projected onto that                       Thetraditional methods often strive to fit the point cloud
                  pixel. Pixels with no projected points are set to 0.                           as closely as possible during the optimization process,
                  ModelSettings. LayernumbersintheTransformerencoder                             which makes them very sensitive to noise and would also
                  anddecoderareboth6. N,H,andM definedinSection3.2                               reconstruct the point clouds of trees (blue boxes in the 1-st
                  and Section 3.3 are set to be 150, 2, and 5 respectively. λ ,                  and 4-th rows in Figure 6). While they usually fit planes
                                                                                        1        with a threshold of point numbers, they also miss small de-
                  λ2, and λ3 are set to 1, 2000, and 1 respectively. And we                      tails like the chimney (green boxes in the 3-rd and 6-th rows
                  train the model for 650 epochs with an initial learning rate                   in Figure 6). For the deep learning-based methods, prior
                  of 2e-4, which decays by 10% in the last 50 epochs. For the                    works lack special designs for the sparse and noisy LiDAR
                  data augmentation, we mix the synthetic data with the real                     point clouds and are struggling with the missing vertices
                  data in a ratio of 3:1 and train them together.                                and edges (5-th to 7-th columns in Figure 6). The results
                  4.3. Quantitative Evaluations.                                                 of our BWFormer are more complete and with more details
                                                                                                 (the last column in Figure 6). Please refer to the supplemen-
                  We compare BWFormer with seven other approaches, in-                           tary materials for more visualization results.
                  cluding Point2Roof [11] and its variants (including Point-                     4.5. Synthetic LiDAR Scanning Evaluation.
                  MAE[19], PointM2AE [20], Linear self-supervised [28],
                  and supervised [28]), PC2WF [14], and PBWR [9]. As                             We compare our simulated scanning method with several
                  shown in Table 1, our method surpasses other SOTA meth-                        uniform sampling-based methods. For the first three meth-
                  ods, especially on the metrics of corner recall and edge re-                   ods, each synthetic data is generated by uniformly sampling
                  call, showcasing the superior adaptation of our proposed                       points within the building footprint at fixed sparsity levels
                  BWFormertosparse3Dpointcloudswithmorereconstruc-                               of 90%, 85%, and 80%, denoted as Uni.(90%), Uni.(85%),
                  tion completeness. This is due to our pixel-by-pixel 2D cor-                   and Uni. (80%) respectively. For the last method, a Gaus-
                  ner detection and 2D-to-3D strategy with a smaller search                      sian distribution (mean value 85.75%, variance 0.19%) on
                  space, as well as the edge attention mechanism that focuses                    point sparsity is fitted across all real data in the Building3D
                  onboththewholeanddetails. Intermsofspecificnumbers,                            dataset.   For each synthetic sample, the sparsity for uni-
                  our method achieves an improvement of +12.2% in WED,                           form sampling is then randomly drawn from this distribu-
                  +8.1% in ACO, +7.4% in corner F1 score, and +2.2% in                           tion, which is denoted as Uni. (Gau). As shown in Ta-
                  edge F1 score with the state-of-the-art method PBWR.                           ble 2, using the widely-adopted metrics in image gener-
                                                                                                               ´
                  4.4. Qualitative Evaluations.                                                  ation of Frechet Inception Distance (FID) and Maximum
                                                                                                 Mean Discrepancy (MMD), our method significantly out-
                  The qualitative evaluations are shown in Figure 6, and                         performs both fixed and Gaussian-based uniform scanning
                  our BWFormer is compared with five representative meth-                        approaches. For the qualitative evaluations, shown in Fig-
                  ods, including 2.5D dual contouring [37], PolyFit-based                        ure 7, our simulated scanning locations exhibit greater di-
                                                                                          22220
