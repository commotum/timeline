                           For each round of testing, we sample 1000 sequences from the appropriate test set. For each se-
                           quence, the model reads in the source sequence and separator symbol, and begins generating the
                           next symbol by taking the maximally likely symbol from the softmax distribution over target sym-
                           bols produced by the model at each step. Based on this process, we give each model a coarse
                           accuracy score, corresponding to the proportion of test sequences correctly predicted from begin-
                           ning until end (EOS symbol) without error, as well as a ﬁne accuracy score, corresponding to the
                           average proportion of each sequence correctly generated before the ﬁrst error. Formally, we have:
                                                                                        #seqs
                                               coarse = #correct        fine =      1    X#correcti
                                                           #seqs                 #seqs i=1     |targeti|
                           where #correct and #seqs are the number of correctly predicted sequences (end-to-end) and the
                           total number of sequences in the test batch (1000 in this experiment), respectively; #correcti is the
                           numberofcorrectlypredicted symbols before the ﬁrst error in the ith sequence of the test batch, and
                           |target | is the length of the target segment that sequence (including EOS symbol).
                                  i
                           4.4   ModelsComparedandExperimentalSetup
                           For each task, we use as benchmarks the Deep LSTMs described in [1], with 1, 2, 4, and 8 layers.
                           Againstthesebenchmarks,weevaluateneuralStack-,Queue-,andDeQue-enhancedLSTMs. When
                           running experiments, we trained and tested a version of each model where all LSTMs in each model
                           have a hidden layer size of 256, and one for a hidden layer size of 512. The Stack/Queue/DeQue
                           embeddingsizewasarbitrarilysetto256,halfthemaximumhiddensize. Thenumberofparameters
                           for each model are reported for each architecture in Table 2 of the appendix. Concretely, the neural
                           Stack-, Queue-, and DeQue-enhanced LSTMs have the same number of trainable parameters as a
                           two-layer Deep LSTM.Theseallcomefromtheextraconnectionstoandfromthememorymodule,
                           which itself has no trainable parameters, regardless of its logical size.
                           ModelsaretrainedwithminibatchRMSProp[19],withabatchsizeof10. Wegrid-searchedlearning
                                                     −3        −3         −4        −4        −5
                           rates across the set {5×10   , 1×10    , 5×10    , 1×10     , 5×10    }. Weusedgradientclipping
                           [20], clipping all gradients above 1. Average training perplexity was calculated every 100 batches.
                           Training and test set accuracies were recorded every 1000 batches.
                           5    Results and Discussion
                           Becauseoftheimpossibilityofoverﬁttingthedatasets,weletthemodelstrainanunboundednumber
                           of steps, and report results at convergence. We present in Figure 2a the coarse- and ﬁne-grained
                           accuracies, for each task, of the best model of each architecture described in this paper alongside
                           the best performing Deep LSTM benchmark. The best models were automatically selected based on
                           average training perplexity. The LSTM benchmarks performed similarly across the range of random
                           initialisations, so the effect of this procedure is primarily to try and select the better performing
                           Stack/Queue/DeQue-enhanced LSTM. In most cases, this procedure does not yield the actual best-
                           performing model, and in practice a more sophisticated procedure such as ensembling [21] should
                           produce better results.
                           For all experiments, the Neural Stack or Queue outperforms the Deep LSTM benchmarks, often by
                           a signiﬁcant margin. For most experiments, if a Neural Stack- or Queue-enhanced LSTM learns
                           to partially or consistently solve the problem, then so does the Neural DeQue. For experiments
                           where the enhanced LSTMs solve the problem completely (consistent accuracy of 1) in training,
                           the accuracy persists in longer sequences in the test set, whereas benchmark accuracies drop for
                           all experiments except the SVO to SOV and Gender Conjugation ITG transduction tasks. Across
                           all tasks which the enhanced LSTMs solve, the convergence on the top accuracy happens orders of
                           magnitude earlier for enhanced LSTMs than for benchmark LSTMs, as exempliﬁed in Figure 2b.
                           The results for the sequence inversion and copying tasks serve as unit tests for our models, as the
                           controller mainly needs to learn to push the appropriate number of times and then pop continuously.
                           Nonetheless, the failure of Deep LSTMs to learn such a regular pattern and generalise is itself
                           indicative of the limitations of the benchmarks presented here, and of the relative expressive power
                           of our models. Their ability to generalise perfectly to sequences up to twice as long as those attested
                           during training is also notable, and also attested in the other experiments. Finally, this pair of
                                                                           8
