                           AppendixforProductofExpertswithLLMs: BoostingPerformanceonARCisaMatterofPerspective
               B. Appendix: Training Parameters
               Table 4. Training parameters and times for the initial and the test-time fine-tuning processes. Test-time fine-tuning is performed separately
               for each task, each time starting from the initially fine-tuned base model.
                                                                    Initial Fine-Tuning     Test-Time Fine-Tuning
                                       Batch size                             4                         1
                                       Gradient acc. steps                    2                         1
                                       LoRArank                             256                        32
                                       LoRAα                                 24                        16
                                       LoRAbias                              off                       off
                                       rank-stabilized LoRA                  on                        on
                                       LR(LoRAadapters)                    1e−4                       1e−4
                                       LR(embeddings)                      1e−5                       1e−5
                                       LRschedule                          cosine                    cosine
                                       LRwarmupphase                        25%                       50%
                                       Weight decay                          off                       off
                                       Optimizer                        adamw 8bit                adamw 8bit
                                       Base model quantization              4 bit                     4 bit
                                       Data type                          bfloat16                  bfloat16
                                       Trained tokens                   outputs only              outputs only
                                       Training dataset                  RE-ARC              single task examples
                                       NumberofEpochs                   368[Llama]                     64
                                                                       1200[NeMo]
                                       Training performed on          1xNvidia H100          1xNvidia RTX4090
                                       Training time                  15hrs. [Llama]          12sec./task [Llama]
                                                                      98hrs. [NeMo]           51sec./task [NeMo]
                                                Table 5. Reduced Token Set for ARC-AGI-specific LLM Model
                                       TokenCategory      Tokens                  Purpose
                                       Alphabet           A-Z,a-z(excl.I,O,i,o)   Learned pre-prompt tokens
                                       Numbers            0-9                     Encoding the 10 colors
                                       Newline token      \n                      Signals end of each grid line
                                       Input/Output       I, O                    Signals start of problem input/output
                                       Begin token        ⟨bos⟩                   Inserted once at the beginning
                                       Endtoken           ⟨eos⟩                   Inserted after each output
                                       Padding token      ⟨pad⟩                   Internal usage (e.g. batching)
                                                                             13
