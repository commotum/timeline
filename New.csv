2023,Sigmoid Loss for Language Image Pre-Training,https://arxiv.org/pdf/2303.15343.pdf
2020,Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains,https://arxiv.org/pdf/2006.10739.pdf
2024,Are We on the Right Way for Evaluating Large Vision-Language Models?,https://arxiv.org/pdf/2403.20330.pdf
2023,Visual Instruction Tuning,https://arxiv.org/pdf/2304.08485.pdf
2015,"You Only Look Once: Unified, Real-Time Object Detection",https://arxiv.org/pdf/1506.02640.pdf
2025,Positional Encoding Field,https://arxiv.org/pdf/2510.20385v1.pdf
2025,Behind RoPE: How Does Causal Mask Encode Positional Information?,https://arxiv.org/pdf/2509.21042.pdf
2025,Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features,https://arxiv.org/pdf/2509.16629.pdf
2025,HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models,https://arxiv.org/pdf/2509.05218.pdf
2024,Lorentz-Equivariant Geometric Algebra Transformers for High-Energy Physics,https://arxiv.org/pdf/2405.14806.pdf
2019,LXMERT: Learning Cross-Modality Encoder Representations from Transformers,https://arxiv.org/pdf/1908.07490
2019,ViLBERT: Pretraining Task-Agnostic V-L Representations,https://arxiv.org/pdf/1908.02265.pdf
2019,VisualBERT: A Simple and Performant Baseline for Vision and Language,https://arxiv.org/pdf/1908.03557.pdf
2019,Deep Equilibrium Models,https://arxiv.org/pdf/1909.01377.pdf
2019,VideoBERT: A Joint Model for Video and Language Representation Learning,https://arxiv.org/pdf/1904.01766.pdf
2020,Deformable DETR: Deformable Transformers for End-to-End Object Detection,https://arxiv.org/pdf/2010.04159.pdf
2020,Training data-efficient image transformers & distillation through attention,https://arxiv.org/pdf/2012.12877.pdf
2020,Taming Transformers for High-Resolution Image Synthesis,https://arxiv.org/pdf/2012.09841.pdf
2021,Zero-Shot Text-to-Image Generation,https://arxiv.org/pdf/2102.12092.pdf
2021,Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions,https://arxiv.org/pdf/2102.12122.pdf
2021,CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification,https://arxiv.org/pdf/2103.14899.pdf
2021,CvT: Introducing Convolutions to Vision Transformers,https://arxiv.org/pdf/2103.15808.pdf
2021,Transformer in Transformer,https://arxiv.org/pdf/2103.00112.pdf
2021,Twins: Revisiting the Design of Spatial Attention in Vision Transformers,https://arxiv.org/pdf/2104.13840.pdf
2021,SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers,https://arxiv.org/pdf/2105.15203.pdf
2021,BEiT: BERT Pre-Training of Image Transformers,https://arxiv.org/pdf/2106.08254.pdf
2021,CoAtNet: Marrying Convolution and Attention for All Data Sizes,https://arxiv.org/pdf/2106.04803.pdf
2021,CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows,https://arxiv.org/pdf/2107.00652.pdf
2021,Swin Transformer V2: Scaling Up Capacity and Resolution,https://arxiv.org/pdf/2111.09883.pdf
2022,MaxViT: Multi-Axis Vision Transformer,https://arxiv.org/pdf/2204.01697.pdf
2022,Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks,https://arxiv.org/pdf/2208.10442.pdf
2023,Segment Anything,https://arxiv.org/pdf/2304.02643.pdf
2021,3DETR: An End-to-End Transformer Model for 3D Object Detection,https://arxiv.org/pdf/2109.08141.pdf
2025,Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models,https://arxiv.org/pdf/2502.11075.pdf
2017,A Distributional Perspective on Reinforcement Learning,https://arxiv.org/pdf/1707.06887.pdf
2025,A Fully First-Order Layer for Differentiable Optimization,https://arxiv.org/pdf/2512.02494.pdf
1984,A Theory of the Learnable (PAC Learning),https://web.mit.edu/6.435/www/Valiant84.pdf
2022,A-OKVQA: A Benchmark for VQA Using World Knowledge,https://arxiv.org/pdf/2206.01718.pdf
2022,miniF2F: a cross-system benchmark for formal Olympiad-level mathematics,https://arxiv.org/pdf/2109.00110.pdf
2023,AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models,https://arxiv.org/pdf/2304.06364.pdf
2021,ALBEF: Align Before Fuse,https://arxiv.org/pdf/2107.07651.pdf
2021,AST: Audio Spectrogram Transformer,https://arxiv.org/pdf/2104.01778.pdf
2014,Adam: A Method for Stochastic Optimization,https://arxiv.org/pdf/1412.6980.pdf
2019,Adaptive Attention Span in Transformers,https://arxiv.org/pdf/1905.07799.pdf
2020,An Image Is Worth 16x16 Words (ViT),https://arxiv.org/pdf/2010.11929.pdf
2019,Analysing Mathematical Reasoning Abilities of Neural Models,https://arxiv.org/pdf/1904.01557.pdf
2016,Asynchronous Methods for Deep Reinforcement Learning (A3C),https://arxiv.org/pdf/1602.01783.pdf
2023,Average-Hard Attention Transformers Are Threshold Circuits,https://arxiv.org/pdf/2308.03212.pdf
2022,BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers,https://arxiv.org/pdf/2203.17270.pdf
2022,BIG-Bench Hard (BBH): Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them,https://arxiv.org/pdf/2210.09261.pdf
2023,BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Models,https://arxiv.org/pdf/2301.12597.pdf
2022,BLIP: Bootstrapping Language-Image Pre-training,https://arxiv.org/pdf/2201.12086.pdf
2024,Base of RoPE Bounds Context Length,https://arxiv.org/pdf/2405.14591.pdf
2015,Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,https://arxiv.org/pdf/1502.03167.pdf
2024,Beyond A*: Planning with Transformers,https://arxiv.org/pdf/2402.14083.pdf
2024,Beyond Position: The Emergence of Wavelet-like Properties in Transformers,https://arxiv.org/pdf/2410.18067.pdf
2022,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models (BIG-bench),https://arxiv.org/pdf/2206.04615.pdf
2019,CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain,https://arxiv.org/pdf/1911.08962.pdf
2025,CamPoint: Boosting Point Cloud Segmentation with Virtual Camera,https://cvpr.thecvf.com/virtual/2025/poster/34611
2022,ChartQA: A Benchmark for Question Answering about Charts with Visual and Logical Reasoning,https://arxiv.org/pdf/2203.10244.pdf
2024,Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference,https://arxiv.org/pdf/2403.04132.pdf
2025,Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models,https://arxiv.org/pdf/2505.16416.pdf
2022,CoCa: Contrastive Captioners,https://arxiv.org/pdf/2205.01917.pdf
2025,CoPE: A Lightweight Complex Positional Encoding,https://arxiv.org/pdf/2508.18308.pdf
2025,ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices,https://arxiv.org/pdf/2506.03737.pdf
2017,Rainbow: Combining Improvements in Deep Reinforcement Learning,https://arxiv.org/pdf/1710.02298.pdf
2021,ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases,https://arxiv.org/pdf/2103.10697.pdf
2023,The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain,https://arxiv.org/pdf/2305.07141.pdf
2018,"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset",https://aclanthology.org/P18-1238.pdf
2017,Constructing Datasets for Multi-hop Reading Comprehension,https://arxiv.org/pdf/1710.06481.pdf
2025,Context-aware Rotary Position Embedding,https://arxiv.org/pdf/2507.23083.pdf
2015,Continuous Control with Deep Reinforcement Learning,https://arxiv.org/pdf/1509.02971.pdf
2017,Convolutional Sequence to Sequence Learning,https://arxiv.org/pdf/1705.03122.pdf
2021,CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification,https://arxiv.org/pdf/2103.14899.pdf
2021,CvT: Introducing Convolutions to Vision Transformers,https://arxiv.org/pdf/2103.15808.pdf
2021,DETR3D: 3D Object Detection from Multi-view Images via 3D-to-2D Queries,https://arxiv.org/pdf/2110.06922.pdf
2020,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,https://arxiv.org/pdf/2006.03654.pdf
2016,Deep Reinforcement Learning with Double Q-learning (Double DQN),https://arxiv.org/pdf/1509.06461.pdf
2024,DeLiVoTr: Deep and Light-weight Voxel Transformer for 3D Object Detection,https://www.sciencedirect.com/science/article/pii/S2667305324000371
2016,DeepCoder: Learning to Write Programs,https://arxiv.org/pdf/1611.01989.pdf
2018,DeepProbLog: Neural Probabilistic Logic Programming,https://arxiv.org/pdf/1805.10872.pdf
2025,DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,https://arxiv.org/pdf/2501.12948.pdf
2021,Deformable DETR: Deformable Transformers for End-to-End Object Detection,https://arxiv.org/pdf/2010.04159.pdf
2021,DeiT: Data-efficient Image Transformers,https://proceedings.mlr.press/v139/touvron21a/touvron21a.pdf
1963,Deterministic Nonperiodic Flow,https://journals.ametsoc.org/downloadpdf/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.pdf
2014,Deterministic Policy Gradient Algorithms,https://proceedings.mlr.press/v32/silver14.pdf
2019,Differentiable Convex Optimization Layers,https://arxiv.org/pdf/1910.12430.pdf
2023,Direct Preference Optimization: Your Language Model is Secretly a Reward Model,https://arxiv.org/pdf/2305.18290.pdf
2018,Distributed Prioritized Experience Replay,https://arxiv.org/pdf/1803.00933.pdf
2021,Do Transformer Modifications Transfer Across Implementations and Applications?,https://arxiv.org/pdf/2102.11972.pdf
2025,DoPE: Denoising Rotary Position Embedding,https://arxiv.org/pdf/2511.09146.pdf
2021,DocVQA: A Dataset for VQA on Document Images,https://arxiv.org/pdf/2007.00398.pdf
2019,Dream to Control: Learning Behaviors by Latent Imagination (Dreamer),https://openreview.net/attachment?id=S1lOTC4tDS&name=original_pdf
2025,EVA02-AT: Egocentric Video-Language with Spatial-Temporal RoPE,https://arxiv.org/pdf/2506.14356.pdf
2025,Efficient Evolutionary Program Synthesis,https://ctpang.substack.com/p/arc-agi-2-sota-efficient-evolutionary
2021,Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM,https://arxiv.org/pdf/2104.04473.pdf
2019,Encoding Word Order in Complex Embeddings,https://arxiv.org/pdf/1912.12333.pdf
2021,Evaluating Large Language Models Trained on Code (HumanEval),https://arxiv.org/pdf/2107.03374.pdf
2024,Evolutionary Test-Time Compute,https://jeremyberman.substack.com/p/how-i-got-a-record-536-on-arc-agi
2019,Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5),https://arxiv.org/pdf/1910.10683.pdf
2023,Extending Context Window of Large Language Models via Positional Interpolation (PI),https://arxiv.org/pdf/2306.15595.pdf
2022,FOLIO: Natural Language Reasoning with First-Order Logic,https://arxiv.org/pdf/2209.00840.pdf
2022,Flamingo: a Visual Language Model for Few-Shot Learning,https://arxiv.org/pdf/2204.14198.pdf
2023,GAIA: a benchmark for General AI Assistants,https://arxiv.org/pdf/2311.12983.pdf
2018,GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,https://arxiv.org/pdf/1804.07461.pdf
2019,GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering,https://arxiv.org/pdf/1902.09506.pdf
2021,GSM8K: Training Verifiers to Solve Math Word Problems,https://arxiv.org/pdf/2110.14168.pdf
2025,"Gated Attention for LLMs: Non-linearity, Sparsity, Sink-Free",https://arxiv.org/pdf/2505.06708.pdf
2024,Density Adaptive Attention is All You Need: Robust Parameter-Efficient Fine-Tuning Across Multiple Modalities,https://arxiv.org/pdf/2401.11143.pdf
