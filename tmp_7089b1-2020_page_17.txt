      Interfaces with biological learning
      DreamCoder’s wake-sleep mechanics draw inspiration from the Helmholtz machine, which is itself
      loosely inspired by human learning during sleep. DreamCoder adds the notion of a pair of interleaved
      sleep cycles, and intriguingly, biological sleep similarly comes in multiple stages. Fast-wave REM
      sleep, or dream sleep, is associated with learning processes that give rise to implicit procedural
      skill (11), and engages both episodic replay and dreaming, analogous to our model’s dream sleep phase.
      Slow-wavesleepisassociatedwiththeformationandconsolidationofnewdeclarativeabstractions(10),
      roughly mapping to our model’s abstraction sleep phase. While neither DreamCoder nor the Helmholtz
      machine are intended as biological models, we speculate that our approach could bring wake-sleep
      learning algorithms closer to the actual learning processes that occur during human sleep.
       DreamCoder’s knowledge grows gradually, with dynamics related to but different from earlier
      developmental proposals for “curriculum learning” (50) and “starting small” (51). Instead of solving
      increasingly difﬁcult tasks ordered by a human teacher (the “curriculum”), DreamCoder learns in a
      waythat is arguably more like natural unsupervised exploration: It attempts to solve random samples
      of tasks, searching out to the boundary of its abilities during waking, and then pushing that boundary
      outward during its sleep cycles, bootstrapping solutions to harder tasks from concepts learned with
      easier ones. But humans learn in much more active ways: They can choose which tasks to solve, and
      even generate their own tasks, either as stepping stones towards harder unsolved problems or motivated
      byconsiderations like curiosity and aesthetics. Building agents that generate their own problems in
      these human-like ways is an important next step.
       Our division of domain expertise into explicit declarative knowledge and implicit procedural
      skill is loosely inspired by dual-process models in cognitive science (52,53) and the study of human
      expertise (37,38). Human experts learn both declarative domain concepts that they can talk about in
      words – artists learn arcs, symmetries, and perspectives; physicists learn inner products, vector ﬁelds,
      and inverse square laws – as well procedural (and implicit) skill in deploying those concepts quickly
      to solve new problems. Together, these two kinds of knowledge let experts more faithfully classify
      problems based on the “deep structure” of their solutions (37,38), and intuit which concepts are likely
      to be useful in solving a task even before they start searching for a solution. We believe both kinds of
      expertise are necessary ingredients in learning systems, both biological and artiﬁcial, and see neural
      and symbolic approaches playing complementary roles here.
      Whattobuildin,andhowtolearntherest
      The goal of learning like a human—in particular, a human child—is often equated with the goal of
      learning “from scratch”, by researchers who presume, following Turing (1), that children start off
      close to a blank slate: “something like a notebook as one buys it from the stationers. Rather little
      mechanism and lots of blank sheets.” The roots of program induction as an approach to general
      AI also lie in this vision, motivated by early results showing that in principle, from only a minimal
      Turing-complete language, it is possible to induce programs that solve any problem with a computable
      answer (2,54–56). DreamCoder’s ability to start from minimal bases and discover the vocabularies
      of functional programming, vector algebra, and physics could be seen as another step towards that
      goal. Could this approach be extended to learn not just one domain at a time, but to simultaneously
      develop expertise across many different classes of problems, starting from only a single minimal basis?
      Progress could be enabled by metalearning a cross-domain library or “language of thought” (57,58), as
      humanshavebuilt collectively through biological and cultural evolution, which can then differentiate
                          17
